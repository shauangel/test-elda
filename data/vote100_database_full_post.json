[{"link": "https://stackoverflow.com/questions/1074212/how-to-see-the-raw-sql-queries-django-is-running", "question": {"id": "1074212", "title": "How to see the raw SQL queries Django is running?", "content": "<p>Is there a way to show the SQL that Django is running while performing a query?</p>\n", "abstract": "Is there a way to show the SQL that Django is running while performing a query?"}, "answers": [{"id": 1074224, "score": 532, "vote": 0, "content": "<p>See the docs FAQ: \"<a href=\"http://docs.djangoproject.com/en/stable/faq/models/#how-can-i-see-the-raw-sql-queries-django-is-running\" rel=\"noreferrer\">How can I see the raw SQL queries Django is running?</a>\"</p>\n<p><code>django.db.connection.queries</code> contains a list of the SQL queries:</p>\n<pre><code class=\"python\">from django.db import connection\nprint(connection.queries)\n</code></pre>\n<p>Querysets also have a <a href=\"https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet\" rel=\"noreferrer\"><code>query</code> attribute</a> containing the query to be executed:</p>\n<pre><code class=\"python\">print(MyModel.objects.filter(name=\"my name\").query)\n</code></pre>\n<p>Note that the output of the query is not valid SQL, because:</p>\n<blockquote>\n<p>\"Django never actually interpolates the parameters: it sends the query and the parameters separately to the database adapter, which performs the appropriate operations.\"</p>\n</blockquote>\n<p>From Django bug report <a href=\"https://code.djangoproject.com/ticket/17741\" rel=\"noreferrer\">#17741</a>.</p>\n<p>Because of that, you should not send query output directly to a database.</p>\n<p>If you need to reset the queries to, for example, see how many queries are running in a given period, you can use <code>reset_queries</code> from <code>django.db</code>:</p>\n<pre><code class=\"python\">from django.db import reset_queries\n\nreset_queries()\nprint(connection.queries)\n&gt;&gt;&gt; []\n</code></pre>\n", "abstract": "See the docs FAQ: \"How can I see the raw SQL queries Django is running?\" django.db.connection.queries contains a list of the SQL queries: Querysets also have a query attribute containing the query to be executed: Note that the output of the query is not valid SQL, because: \"Django never actually interpolates the parameters: it sends the query and the parameters separately to the database adapter, which performs the appropriate operations.\" From Django bug report #17741. Because of that, you should not send query output directly to a database. If you need to reset the queries to, for example, see how many queries are running in a given period, you can use reset_queries from django.db:"}, {"id": 31450706, "score": 101, "vote": 0, "content": "<p><a href=\"https://pypi.python.org/pypi/django-extensions/\">Django-extensions</a> have a command <a href=\"http://django-extensions.readthedocs.org/en/latest/shell_plus.html\">shell_plus</a> with a parameter <code>print-sql</code></p>\n<pre><code class=\"python\">./manage.py shell_plus --print-sql\n</code></pre>\n<p>In django-shell all executed queries will be printed</p>\n<p>Ex.:</p>\n<pre><code class=\"python\">User.objects.get(pk=1)\nSELECT \"auth_user\".\"id\",\n       \"auth_user\".\"password\",\n       \"auth_user\".\"last_login\",\n       \"auth_user\".\"is_superuser\",\n       \"auth_user\".\"username\",\n       \"auth_user\".\"first_name\",\n       \"auth_user\".\"last_name\",\n       \"auth_user\".\"email\",\n       \"auth_user\".\"is_staff\",\n       \"auth_user\".\"is_active\",\n       \"auth_user\".\"date_joined\"\nFROM \"auth_user\"\nWHERE \"auth_user\".\"id\" = 1\n\nExecution time: 0.002466s [Database: default]\n\n&lt;User: username&gt;\n</code></pre>\n", "abstract": "Django-extensions have a command shell_plus with a parameter print-sql In django-shell all executed queries will be printed Ex.:"}, {"id": 1078011, "score": 68, "vote": 0, "content": "<p>Take a look at <strong>debug_toolbar</strong>, it's very useful for debugging.</p>\n<p>Documentation and source is available at <a href=\"http://django-debug-toolbar.readthedocs.io/\" rel=\"noreferrer\">http://django-debug-toolbar.readthedocs.io/</a>.</p>\n<p><a href=\"https://i.stack.imgur.com/VR75R.png\" rel=\"noreferrer\"><img alt=\"Screenshot of debug toolbar\" src=\"https://i.stack.imgur.com/VR75R.png\"/></a></p>\n", "abstract": "Take a look at debug_toolbar, it's very useful for debugging. Documentation and source is available at http://django-debug-toolbar.readthedocs.io/. "}, {"id": 11235636, "score": 38, "vote": 0, "content": "<p>The query is actually embedded in the models API:</p>\n<pre><code class=\"python\">q = Query.objects.values('val1','val2','val_etc')\n\nprint(q.query)\n</code></pre>\n", "abstract": "The query is actually embedded in the models API:"}, {"id": 23975393, "score": 33, "vote": 0, "content": "<p>No other answer covers this method, so:</p>\n<p>I find by far the most useful, simple, and reliable method is to ask your database.  For example on Linux for Postgres you might do:</p>\n<pre><code class=\"python\">sudo su postgres\ntail -f /var/log/postgresql/postgresql-8.4-main.log\n</code></pre>\n<p>Each database will have slightly different procedure.  In the database logs you'll see not only the raw SQL, but any connection setup or transaction overhead django is placing on the system.</p>\n", "abstract": "No other answer covers this method, so: I find by far the most useful, simple, and reliable method is to ask your database.  For example on Linux for Postgres you might do: Each database will have slightly different procedure.  In the database logs you'll see not only the raw SQL, but any connection setup or transaction overhead django is placing on the system."}, {"id": 1074436, "score": 18, "vote": 0, "content": "<p>Though you can do it with with the code supplied, I find that using the debug toolbar app is a great tool to show queries. You can download it from github <a href=\"http://github.com/dcramer/django-debug-toolbar/tree/master\" rel=\"noreferrer\">here</a>.</p>\n<p>This gives you the option to show all the queries ran on a given page along with the time to query took. It also sums up the number of queries on a page along with total time for a quick review. This is a great tool, when you want to look at what the Django ORM does behind the scenes. It also have a lot of other nice features, that you can use if you like.</p>\n", "abstract": "Though you can do it with with the code supplied, I find that using the debug toolbar app is a great tool to show queries. You can download it from github here. This gives you the option to show all the queries ran on a given page along with the time to query took. It also sums up the number of queries on a page along with total time for a quick review. This is a great tool, when you want to look at what the Django ORM does behind the scenes. It also have a lot of other nice features, that you can use if you like."}, {"id": 20246650, "score": 17, "vote": 0, "content": "<p>Another option, see logging options in settings.py described by this post</p>\n<p><a href=\"http://dabapps.com/blog/logging-sql-queries-django-13/\" rel=\"noreferrer\">http://dabapps.com/blog/logging-sql-queries-django-13/</a></p>\n<p>debug_toolbar slows down each page load on your dev server, logging does not so it's faster. Outputs can be dumped to console or file, so the UI is not as nice. But for views with lots of SQLs, it can take a long time to debug and optimize the SQLs through debug_toolbar since each page load is so slow.</p>\n", "abstract": "Another option, see logging options in settings.py described by this post http://dabapps.com/blog/logging-sql-queries-django-13/ debug_toolbar slows down each page load on your dev server, logging does not so it's faster. Outputs can be dumped to console or file, so the UI is not as nice. But for views with lots of SQLs, it can take a long time to debug and optimize the SQLs through debug_toolbar since each page load is so slow."}, {"id": 49318635, "score": 12, "vote": 0, "content": "<p>I developed an extension for this purpose, so you can easily put a decorator on your view function and see how many queries are executed.</p>\n<p>To install:</p>\n<pre><code class=\"python\">$ pip install django-print-sql\n</code></pre>\n<p>To use as context manager:</p>\n<pre><code class=\"python\">from django_print_sql import print_sql\n\n# set `count_only` to `True` will print the number of executed SQL statements only\nwith print_sql(count_only=False):\n\n  # write the code you want to analyze in here,\n  # e.g. some complex foreign key lookup,\n  # or analyzing a DRF serializer's performance\n\n  for user in User.objects.all()[:10]:\n      user.groups.first()\n</code></pre>\n<p>To use as decorator:</p>\n<pre><code class=\"python\">from django_print_sql import print_sql_decorator\n\n\n@print_sql_decorator(count_only=False)  # this works on class-based views as well\ndef get(request):\n    # your view code here\n</code></pre>\n<p>Github: <a href=\"https://github.com/rabbit-aaron/django-print-sql\" rel=\"noreferrer\">https://github.com/rabbit-aaron/django-print-sql</a></p>\n", "abstract": "I developed an extension for this purpose, so you can easily put a decorator on your view function and see how many queries are executed. To install: To use as context manager: To use as decorator: Github: https://github.com/rabbit-aaron/django-print-sql"}, {"id": 3930290, "score": 11, "vote": 0, "content": "<p>If you make sure your settings.py file has:</p>\n<ol>\n<li><code>django.core.context_processors.debug</code> listed in <code>CONTEXT_PROCESSORS</code></li>\n<li><code>DEBUG=True</code></li>\n<li>your <code>IP</code> in the <code>INTERNAL_IPS</code> tuple</li>\n</ol>\n<p>Then you should have access to the <code>sql_queries</code> variable.  I append a footer to each page that looks like this:</p>\n<pre><code class=\"python\">{%if sql_queries %}\n  &lt;div class=\"footNav\"&gt;\n    &lt;h2&gt;Queries&lt;/h2&gt;\n    &lt;p&gt;\n      {{ sql_queries|length }} Quer{{ sql_queries|pluralize:\"y,ies\" }}, {{sql_time_sum}} Time\n    {% ifnotequal sql_queries|length 0 %}\n      (&lt;span style=\"cursor: pointer;\" onclick=\"var s=document.getElementById('debugQueryTable').style;s.disp\\\nlay=s.display=='none'?'':'none';this.innerHTML=this.innerHTML=='Show'?'Hide':'Show';\"&gt;Show&lt;/span&gt;)\n    {% endifnotequal %}\n    &lt;/p&gt;\n    &lt;table id=\"debugQueryTable\" style=\"display: none;\"&gt;\n      &lt;col width=\"1\"&gt;&lt;/col&gt;\n      &lt;col&gt;&lt;/col&gt;\n      &lt;col width=\"1\"&gt;&lt;/col&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th scope=\"col\"&gt;#&lt;/th&gt;\n          &lt;th scope=\"col\"&gt;SQL&lt;/th&gt;\n          &lt;th scope=\"col\"&gt;Time&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        {% for query in sql_queries %}\n          &lt;tr class=\"{% cycle odd,even %}\"&gt;\n            &lt;td&gt;{{ forloop.counter }}&lt;/td&gt;\n            &lt;td&gt;{{ query.sql|escape }}&lt;/td&gt;\n            &lt;td&gt;{{ query.time }}&lt;/td&gt;\n          &lt;/tr&gt;\n        {% endfor %}\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n  &lt;/div&gt;\n{% endif %}\n</code></pre>\n<p>I got the variable <code>sql_time_sum</code> by adding the line </p>\n<pre><code class=\"python\">context_extras['sql_time_sum'] = sum([float(q['time']) for q in connection.queries])\n</code></pre>\n<p>to the debug function in django_src/django/core/context_processors.py.</p>\n", "abstract": "If you make sure your settings.py file has: Then you should have access to the sql_queries variable.  I append a footer to each page that looks like this: I got the variable sql_time_sum by adding the line  to the debug function in django_src/django/core/context_processors.py."}, {"id": 62439786, "score": 9, "vote": 0, "content": "<p>Just to add, in django, if you have a query like:</p>\n<pre><code class=\"python\">MyModel.objects.all()\n</code></pre>\n<p>do:</p>\n<pre><code class=\"python\">MyModel.objects.all().query.sql_with_params()\n</code></pre>\n<p>or:</p>\n<pre><code class=\"python\">str(MyModel.objects.all().query)\n</code></pre>\n<p>to get the sql string</p>\n", "abstract": "Just to add, in django, if you have a query like: do: or: to get the sql string"}, {"id": 70105819, "score": 9, "vote": 0, "content": "<p>This is a much late answer but for the others are came here by searching.</p>\n<p>I want to introduce a logging method, which is very simple; add <code>django.db.backends</code> logger in settins.py</p>\n<pre><code class=\"python\">LOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n        },\n    },\n    'loggers': {\n        'django.db.backends': {\n            'handlers': ['console'],\n            'level': 'DEBUG',\n        },\n    },\n}\n</code></pre>\n<p>I am also using an environment variable to set the level.\nSo when I want to see the SQL queries I just set the environment variable, and debug log shows the actual queries.</p>\n", "abstract": "This is a much late answer but for the others are came here by searching. I want to introduce a logging method, which is very simple; add django.db.backends logger in settins.py I am also using an environment variable to set the level.\nSo when I want to see the SQL queries I just set the environment variable, and debug log shows the actual queries."}, {"id": 66471586, "score": 7, "vote": 0, "content": "<p>Django SQL Sniffer is another alternative for viewing (and seeing the stats of) raw executed queries coming out of any process utilising Django ORM. I've built it to satisfy a particular use-case that I had, which I haven't seen covered anywhere, namely:</p>\n<ul>\n<li>no changes to the source code that the target process is executing (no need to register a new app in django settings, import decorators all over the place etc.)</li>\n<li>no changes to logging configuration (e.g. because I'm interested in one particular process, and not the entire process fleet that the configuration applies to)</li>\n<li>no restarting of target process needed (e.g. because it's a vital component, and restarts may incur some downtime)</li>\n</ul>\n<p>Therefore, Django SQL Sniffer can be used ad-hoc, and attached to an already running process. The tool then \"sniffs\" the executed queries and prints them to console as they are executed. When the tool is stopped a statistical summary is displayed with outlier queries based on some possible metric (count, max duration and total combined duration).</p>\n<p>Here's a screenshot of an example where I attached to a Python shell\n<a href=\"https://i.stack.imgur.com/6MU8m.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/6MU8m.png\"/></a></p>\n<p>You can check out the live demo and more details on the <a href=\"https://github.com/gruuya/django-sql-sniffer\" rel=\"noreferrer\">github page</a>.</p>\n", "abstract": "Django SQL Sniffer is another alternative for viewing (and seeing the stats of) raw executed queries coming out of any process utilising Django ORM. I've built it to satisfy a particular use-case that I had, which I haven't seen covered anywhere, namely: Therefore, Django SQL Sniffer can be used ad-hoc, and attached to an already running process. The tool then \"sniffs\" the executed queries and prints them to console as they are executed. When the tool is stopped a statistical summary is displayed with outlier queries based on some possible metric (count, max duration and total combined duration). Here's a screenshot of an example where I attached to a Python shell\n You can check out the live demo and more details on the github page."}, {"id": 58385672, "score": 4, "vote": 0, "content": "<p>I put this function in a util file in one of the apps in my project:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import logging\nimport re\n\nfrom django.db import connection\n\nlogger = logging.getLogger(__name__)\n\ndef sql_logger():\n    logger.debug('TOTAL QUERIES: ' + str(len(connection.queries)))\n    logger.debug('TOTAL TIME: ' + str(sum([float(q['time']) for q in connection.queries])))\n\n    logger.debug('INDIVIDUAL QUERIES:')\n    for i, query in enumerate(connection.queries):\n        sql = re.split(r'(SELECT|FROM|WHERE|GROUP BY|ORDER BY|INNER JOIN|LIMIT)', query['sql'])\n        if not sql[0]: sql = sql[1:]\n        sql = [(' ' if i % 2 else '') + x for i, x in enumerate(sql)]\n        logger.debug('\\n### {} ({} seconds)\\n\\n{};\\n'.format(i, query['time'], '\\n'.join(sql)))\n</code></pre>\n<p>Then, when needed, I just import it and call it from whatever context (usually a view) is necessary, e.g.:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># ... other imports\nfrom .utils import sql_logger\n\nclass IngredientListApiView(generics.ListAPIView):\n    # ... class variables and such\n\n    # Main function that gets called when view is accessed\n    def list(self, request, *args, **kwargs):\n        response = super(IngredientListApiView, self).list(request, *args, **kwargs)\n\n        # Call our function\n        sql_logger()\n\n        return response\n</code></pre>\n<p>It's nice to do this outside the template because then if you have API views (usually Django Rest Framework), it's applicable there too.</p>\n", "abstract": "I put this function in a util file in one of the apps in my project: Then, when needed, I just import it and call it from whatever context (usually a view) is necessary, e.g.: It's nice to do this outside the template because then if you have API views (usually Django Rest Framework), it's applicable there too."}, {"id": 52117390, "score": 3, "vote": 0, "content": "<p>I believe this ought to work if you are using PostgreSQL:</p>\n<pre><code class=\"python\">from django.db import connections\nfrom app_name import models\nfrom django.utils import timezone\n\n# Generate a queryset, use your favorite filter, QS objects, and whatnot.\nqs=models.ThisDataModel.objects.filter(user='bob',date__lte=timezone.now())\n\n# Get a cursor tied to the default database\ncursor=connections['default'].cursor()\n\n# Get the query SQL and parameters to be passed into psycopg2, then pass\n# those into mogrify to get the query that would have been sent to the backend\n# and print it out. Note F-strings require python 3.6 or later.\nprint(f'{cursor.mogrify(*qs.query.sql_with_params())}')\n</code></pre>\n", "abstract": "I believe this ought to work if you are using PostgreSQL:"}, {"id": 47542953, "score": 2, "vote": 0, "content": "<p>The following returns the query as valid SQL, based on <a href=\"https://code.djangoproject.com/ticket/17741\" rel=\"nofollow noreferrer\">https://code.djangoproject.com/ticket/17741</a>:</p>\n<pre><code class=\"python\">def str_query(qs):\n    \"\"\"\n    qs.query returns something that isn't valid SQL, this returns the actual\n    valid SQL that's executed: https://code.djangoproject.com/ticket/17741\n    \"\"\"\n    cursor = connections[qs.db].cursor()\n    query, params = qs.query.sql_with_params()\n    cursor.execute('EXPLAIN ' + query, params)\n    res = str(cursor.db.ops.last_executed_query(cursor, query, params))\n    assert res.startswith('EXPLAIN ')\n    return res[len('EXPLAIN '):]\n</code></pre>\n", "abstract": "The following returns the query as valid SQL, based on https://code.djangoproject.com/ticket/17741:"}, {"id": 48440571, "score": 2, "vote": 0, "content": "<p>I've made a small snippet you can use:</p>\n<pre><code class=\"python\">from django.conf import settings\nfrom django.db import connection\n\n\ndef sql_echo(method, *args, **kwargs):\n    settings.DEBUG = True\n    result = method(*args, **kwargs)\n    for query in connection.queries:\n        print(query)\n    return result\n\n\n# HOW TO USE EXAMPLE:\n# \n# result = sql_echo(my_method, 'whatever', show=True)\n</code></pre>\n<p>It takes as parameters function (contains sql queryies) to inspect and args, kwargs needed to call that function. As the result it returns what function returns and prints SQL queries in a console.</p>\n", "abstract": "I've made a small snippet you can use: It takes as parameters function (contains sql queryies) to inspect and args, kwargs needed to call that function. As the result it returns what function returns and prints SQL queries in a console."}, {"id": 66225225, "score": 2, "vote": 0, "content": "<p>To get <strong>result query from django to database(with correct parameter substitution)</strong>\nyou could use this function:</p>\n<pre><code class=\"python\">from django.db import connection\n\ndef print_database_query_formatted(query):\n    sql, params = query.sql_with_params()\n    cursor = connection.cursor()\n    cursor.execute('EXPLAIN ' + sql, params)\n    db_query = cursor.db.ops.last_executed_query(cursor, sql, params).replace('EXPLAIN ', '')\n\n    parts = '{}'.format(db_query).split('FROM')\n    print(parts[0])\n    if len(parts) &gt; 1:\n        parts = parts[1].split('WHERE')\n        print('FROM{}'.format(parts[0]))\n        if len(parts) &gt; 1:\n            parts = parts[1].split('ORDER BY')\n            print('WHERE{}'.format(parts[0]))\n            if len(parts) &gt; 1:\n                print('ORDER BY{}'.format(parts[1]))\n\n# USAGE\nusers = User.objects.filter(email='admin@admin.com').order_by('-id')\nprint_database_query_formatted(users.query)\n</code></pre>\n<p><strong>Output example</strong></p>\n<pre><code class=\"python\">SELECT \"users_user\".\"password\", \"users_user\".\"last_login\", \"users_user\".\"is_superuser\", \"users_user\".\"deleted\", \"users_user\".\"id\", \"users_user\".\"phone\", \"users_user\".\"username\", \"users_user\".\"userlastname\", \"users_user\".\"email\", \"users_user\".\"is_staff\", \"users_user\".\"is_active\", \"users_user\".\"date_joined\", \"users_user\".\"latitude\", \"users_user\".\"longitude\", \"users_user\".\"point\"::bytea, \"users_user\".\"default_search_radius\", \"users_user\".\"notifications\", \"users_user\".\"admin_theme\", \"users_user\".\"address\", \"users_user\".\"is_notify_when_buildings_in_radius\", \"users_user\".\"active_campaign_id\", \"users_user\".\"is_unsubscribed\", \"users_user\".\"sf_contact_id\", \"users_user\".\"is_agree_terms_of_service\", \"users_user\".\"is_facebook_signup\", \"users_user\".\"type_signup\" \nFROM \"users_user\" \nWHERE \"users_user\".\"email\" = 'admin@admin.com' \nORDER BY \"users_user\".\"id\" DESC\n</code></pre>\n<p><strong>It based on this ticket comment:</strong> <a href=\"https://code.djangoproject.com/ticket/17741#comment:4\" rel=\"nofollow noreferrer\">https://code.djangoproject.com/ticket/17741#comment:4</a></p>\n", "abstract": "To get result query from django to database(with correct parameter substitution)\nyou could use this function: Output example It based on this ticket comment: https://code.djangoproject.com/ticket/17741#comment:4"}, {"id": 70798909, "score": 2, "vote": 0, "content": "<p>There's another way that's very useful if you need to reuse the query for some custom SQL. I've used this in an analytics app that goes far beyond what Django's ORM can do comfortably, so I'm including ORM-generated SQL as subqueries.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from django.db import connection\nfrom myapp.models import SomeModel\n\nqueryset = SomeModel.objects.filter(foo='bar')\n\nsql_query, params = queryset.query.as_sql(None, connection)\n</code></pre>\n<p>This will give you the SQL with placeholders, as well as a tuple with query params to use. You can pass this along to the DB directly:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">with connection.connection.cursor(cursor_factory=DictCursor) as cursor:\n    cursor.execute(sql_query, params)\n    data = cursor.fetchall()\n</code></pre>\n", "abstract": "There's another way that's very useful if you need to reuse the query for some custom SQL. I've used this in an analytics app that goes far beyond what Django's ORM can do comfortably, so I'm including ORM-generated SQL as subqueries. This will give you the SQL with placeholders, as well as a tuple with query params to use. You can pass this along to the DB directly:"}, {"id": 67373007, "score": 0, "vote": 0, "content": "<p>To <strong>generate SQL</strong> for <strong>CREATE / UPDATE / DELETE</strong> /  commands, which are <strong>immediate</strong> in Django</p>\n<pre><code class=\"python\">from django.db.models import sql\n\ndef generate_update_sql(queryset, update_kwargs):\n    \"\"\"Converts queryset with update_kwargs\n    like : queryset.update(**update_kwargs) to UPDATE SQL\"\"\"\n\n    query = queryset.query.clone(sql.UpdateQuery)\n    query.add_update_values(update_kwargs)\n    compiler = query.get_compiler(queryset.db)\n    sql, params = compiler.as_sql()\n    return sql % params\n</code></pre>\n<pre><code class=\"python\">from django.db.models import sql\n\ndef generate_delete_sql(queryset):\n    \"\"\"Converts select queryset to DELETE SQL \"\"\"\n    query = queryset.query.chain(sql.DeleteQuery)\n    compiler = query.get_compiler(queryset.db)\n    sql, params = compiler.as_sql()\n    return sql % params\n</code></pre>\n<pre><code class=\"python\">from django.db.models import sql\n\ndef generate_create_sql(model, model_data):\n    \"\"\"Converts queryset with create_kwargs\n    like if was: queryset.create(**create_kwargs) to SQL CREATE\"\"\"\n    \n    not_saved_instance = model(**model_data)\n    not_saved_instance._for_write = True\n\n    query = sql.InsertQuery(model)\n\n    fields = [f for f in model._meta.local_concrete_fields if not isinstance(f, AutoField)]\n    query.insert_values(fields, [not_saved_instance], raw=False)\n\n    compiler = query.get_compiler(model.objects.db)\n    sql, params = compiler.as_sql()[0]\n    return sql % params\n</code></pre>\n<p>Tests &amp; usage</p>\n<pre><code class=\"python\">    def test_generate_update_sql_with_F(self):\n        qs = Event.objects.all()\n        update_kwargs = dict(description=F('slug'))\n        result = generate_update_sql(qs, update_kwargs)\n        sql = \"UPDATE `api_event` SET `description` = `api_event`.`slug`\"\n        self.assertEqual(sql, result)\n\n    def test_generate_create_sql(self):\n        result = generate_create_sql(Event, dict(slug='a', app='b', model='c', action='e'))\n        sql = \"INSERT INTO `api_event` (`slug`, `app`, `model`, `action`, `action_type`, `description`) VALUES (a, b, c, e, , )\"\n        self.assertEqual(sql, result)\n</code></pre>\n", "abstract": "To generate SQL for CREATE / UPDATE / DELETE /  commands, which are immediate in Django Tests & usage"}, {"id": 74047588, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">from django.db import reset_queries, connection\nclass ShowSQL(object):\n    def __enter__(self):\n        reset_queries()\n        return self\n\n    def __exit__(self, *args):\n        for sql in connection.queries:\n            print('Time: %s\\nSQL: %s' % (sql['time'], sql['sql']))\n</code></pre>\n<p>Then you can use:</p>\n<pre><code class=\"python\">with ShowSQL() as t:\n    some queries &lt;select&gt;|&lt;annotate&gt;|&lt;update&gt; or other \n</code></pre>\n<p>it prints</p>\n<ul>\n<li>Time: %s</li>\n<li>SQL: %s</li>\n</ul>\n", "abstract": "Then you can use: it prints"}, {"id": 61540725, "score": -1, "vote": 0, "content": "<p><strong>View Queries</strong> using django.db.connection.queries</p>\n<pre><code class=\"python\">from django.db import connection\nprint(connection.queries)\n</code></pre>\n<p><strong>Access raw SQL query on QuerySet object</strong></p>\n<pre><code class=\"python\"> qs = MyModel.objects.all()\n print(qs.query)\n</code></pre>\n", "abstract": "View Queries using django.db.connection.queries Access raw SQL query on QuerySet object"}, {"id": 62180995, "score": -1, "vote": 0, "content": "<p>For Django 2.2:</p>\n<p>As most of the answers did not helped me much when using <code>./manage.py shell</code>. Finally i found the answer. Hope this helps to someone.</p>\n<p>To view all the queries:</p>\n<pre><code class=\"python\">from django.db import connection\nconnection.queries\n</code></pre>\n<p>To view query for a single query:</p>\n<pre><code class=\"python\">q=Query.objects.all()\nq.query.__str__()\n</code></pre>\n<p><code>q.query</code> just displaying the object for me. \nUsing the <code>__str__()</code>(String representation) displayed the full query.</p>\n", "abstract": "For Django 2.2: As most of the answers did not helped me much when using ./manage.py shell. Finally i found the answer. Hope this helps to someone. To view all the queries: To view query for a single query: q.query just displaying the object for me. \nUsing the __str__()(String representation) displayed the full query."}, {"id": 73427167, "score": -1, "vote": 0, "content": "<p>For <strong>Django</strong>, the best way to check <strong>the exact SQL queries</strong> is checking <strong>the logs of specific databases themselves</strong>.</p>\n<p>For example, when developing <strong>Django website</strong>, I wanted to check <strong>the atomic transaction queries \"BEGIN\" and \"COMMIT\" with PostgreSQL(Version 14)</strong>.</p>\n<p>So, to generate <strong>the logs of exact PostgreSQL queries</strong>, I added the code below to <strong>the end of \"postgresql.conf\" which is \"C:\\Program Files\\PostgreSQL\\14\\data\\postgresql.conf\" in my windows 11 machine</strong>:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">log_min_duration_statement = 0\n</code></pre>\n<p>As shown below, I added the code above to <strong>the end of \"postgresql.conf\"</strong>:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">#------------------------------------------------------------------------------\n# CUSTOMIZED OPTIONS\n#------------------------------------------------------------------------------\n\n# Add settings for extensions here\nlog_min_duration_statement = 0\n</code></pre>\n<p>Then, after querying <strong>PostgreSQL queries</strong> with <strong>Django Admin</strong>, I could check <strong>the atomic transaction queries \"BEGIN\" and \"COMMIT\" with PostgreSQL</strong> as shown below by opening <strong>\"postgresql-2022-08-20_000000.log\" which is \"C:\\Program Files\\PostgreSQL\\14\\data\\log\\postgresql-2022-08-20_000000.log\" in my windows 11 machine</strong>:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">2022-08-20 22:09:12.549 JST [26756] LOG:  duration: 0.025 ms  statement: BEGIN\n2022-08-20 22:09:12.550 JST [26756] LOG:  duration: 1.156 ms  statement: SELECT \"store_person\".\"id\", \"store_person\".\"first_name\", \"store_person\".\"last_name\" FROM \"store_person\" WHERE \"store_person\".\"id\" = 33 LIMIT 21\n2022-08-20 22:09:12.552 JST [26756] LOG:  duration: 0.178 ms  statement: UPDATE \"store_person\" SET \"first_name\" = 'Bill', \"last_name\" = 'Gates' WHERE \"store_person\".\"id\" = 33\n2022-08-20 22:09:12.554 JST [26756] LOG:  duration: 0.784 ms  statement: INSERT INTO \"django_admin_log\" (\"action_time\", \"user_id\", \"content_type_id\", \"object_id\", \"object_repr\", \"action_flag\", \"change_message\") VALUES ('2022-08-20T13:09:12.553273+00:00'::timestamptz, 1, 20, '33', 'Bill Gates', 2, '[]') RETURNING \"django_admin_log\".\"id\"\n2022-08-20 22:09:12.557 JST [26756] LOG:  duration: 1.799 ms  statement: COMMIT\n</code></pre>\n<p>In addition, I could also check <strong>the logs of exact MSSQL(Microsoft SQL Server) queries</strong> with <strong>MSSQL</strong> but I couldn't check <strong>the logs of exact SQLite queries</strong> with <strong>SQLite</strong> because it seems like <strong>SQLite</strong> doesn't have the function to save <strong>the logs of exact SQLite queries</strong>.</p>\n<p>In addition again, I used <a href=\"https://stackoverflow.com/questions/4375784/log-all-sql-queries/20161527#20161527\"><strong>@GianMarco's answer(solution)</strong></a> by adding the code below to <strong>\"settings.py\"</strong> to show <strong>the logs of exact SQL queries</strong> on <strong>console</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">\"settings.py\"\n\nLOGGING = {\n    'version': 1,\n    'filters': {\n        'require_debug_true': {\n            '()': 'django.utils.log.RequireDebugTrue',\n        }\n    },\n    'handlers': {\n        'console': {\n            'level': 'DEBUG',\n            'filters': ['require_debug_true'],\n            'class': 'logging.StreamHandler',\n        }\n    },\n    'loggers': {\n        'django.db.backends': {\n            'level': 'DEBUG',\n            'handlers': ['console'],\n        }\n    }\n}\n</code></pre>\n<p>But, it only showed <strong>the SQL queries \"SELECT\", \"UPDATE\", \"INSERT\" and so on</strong> without <strong>the atomic transaction queries \"BEGIN\" and \"COMMIT\"</strong> as shown below:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">(0.000) SELECT \"store_person\".\"id\", \"store_person\".\"first_name\", \"store_person\".\"last_name\" FROM \"store_person\" WHERE \"store_person\".\"id\" = 33 LIMIT 21; args=(33,)\n(0.000) UPDATE \"store_person\" SET \"first_name\" = 'Bill', \"last_name\" = 'Gates' WHERE \"store_person\".\"id\" = 33; args=('Bill', 'Gates', 33)\n(0.000) INSERT INTO \"django_admin_log\" (\"action_time\", \"user_id\", \"content_type_id\", \"object_id\", \"object_repr\", \"action_flag\", \"change_message\") VALUES ('2022-08-20T13:55:19.226541+00:00'::timestamptz, 1, 20, '33', 'Bill Gates', 2, '[]') RETURNING \"django_admin_log\".\"id\"; args=(datetime.datetime(2022, 8, 20, 13, 55, 19, 226541, tzinfo=&lt;UTC&gt;), 1, 20, '33', 'Bill Gates', 2, '[]')\n</code></pre>\n<p>And, I used <a href=\"https://stackoverflow.com/questions/1074212/how-to-see-the-raw-sql-queries-django-is-running/1074224#1074224\"><strong>@geowa4's answer(solution)</strong></a> by adding the code below to <strong>\"settings.py\"</strong> to show <strong>the logs of exact SQL queries</strong> on <strong>console</strong>. *I used <strong>\"Django==3.1.7\"</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># \"settings.py\"\n\nfrom django.db import connection \n\nprint(connection.queries) # Causes error\n</code></pre>\n<p>But, I got the error below:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">django.core.exceptions.ImproperlyConfigured: settings.DATABASES is improperly configured. Please supply the ENGINE value. Check settings documentation for more details.\n</code></pre>\n<p>So, I removed <strong>\"print(connection.queries)\"</strong> as shown below, then the error was solved but I couldn't show <strong>any logs of exact SQL queries</strong> on <strong>console</strong> at all:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># \"settings.py\"\n\nfrom django.db import connection \n\n# print(connection.queries) # Causes error\n</code></pre>\n", "abstract": "For Django, the best way to check the exact SQL queries is checking the logs of specific databases themselves. For example, when developing Django website, I wanted to check the atomic transaction queries \"BEGIN\" and \"COMMIT\" with PostgreSQL(Version 14). So, to generate the logs of exact PostgreSQL queries, I added the code below to the end of \"postgresql.conf\" which is \"C:\\Program Files\\PostgreSQL\\14\\data\\postgresql.conf\" in my windows 11 machine: As shown below, I added the code above to the end of \"postgresql.conf\": Then, after querying PostgreSQL queries with Django Admin, I could check the atomic transaction queries \"BEGIN\" and \"COMMIT\" with PostgreSQL as shown below by opening \"postgresql-2022-08-20_000000.log\" which is \"C:\\Program Files\\PostgreSQL\\14\\data\\log\\postgresql-2022-08-20_000000.log\" in my windows 11 machine: In addition, I could also check the logs of exact MSSQL(Microsoft SQL Server) queries with MSSQL but I couldn't check the logs of exact SQLite queries with SQLite because it seems like SQLite doesn't have the function to save the logs of exact SQLite queries. In addition again, I used @GianMarco's answer(solution) by adding the code below to \"settings.py\" to show the logs of exact SQL queries on console: But, it only showed the SQL queries \"SELECT\", \"UPDATE\", \"INSERT\" and so on without the atomic transaction queries \"BEGIN\" and \"COMMIT\" as shown below: And, I used @geowa4's answer(solution) by adding the code below to \"settings.py\" to show the logs of exact SQL queries on console. *I used \"Django==3.1.7\": But, I got the error below: So, I removed \"print(connection.queries)\" as shown below, then the error was solved but I couldn't show any logs of exact SQL queries on console at all:"}]}, {"link": "https://stackoverflow.com/questions/26672077/django-model-vs-model-objects-create", "question": {"id": "26672077", "title": "Django Model() vs Model.objects.create()", "content": "<p>What it the difference between running two commands:</p>\n<pre><code class=\"python\">foo = FooModel()\n</code></pre>\n<p>and</p>\n<pre><code class=\"python\">bar = BarModel.objects.create()\n</code></pre>\n<p>Does the second one immediately create a <code>BarModel</code> in the database, while for <code>FooModel</code>, the <code>save()</code> method has to be called explicitly to add it to the database?</p>\n", "abstract": "What it the difference between running two commands: and Does the second one immediately create a BarModel in the database, while for FooModel, the save() method has to be called explicitly to add it to the database?"}, "answers": [{"id": 26672182, "score": 340, "vote": 0, "content": "<p><a href=\"https://docs.djangoproject.com/en/stable/topics/db/queries/#creating-objects\" rel=\"noreferrer\">https://docs.djangoproject.com/en/stable/topics/db/queries/#creating-objects</a></p>\n<blockquote>\n<p>To create and save an object in a single step, use the <code>create()</code> method.</p>\n</blockquote>\n", "abstract": "https://docs.djangoproject.com/en/stable/topics/db/queries/#creating-objects To create and save an object in a single step, use the create() method."}, {"id": 63147325, "score": 45, "vote": 0, "content": "<p>The differences between <code>Model()</code> and <code>Model.objects.create()</code> are the following:</p>\n<hr/>\n<ol>\n<li><p>INSERT vs UPDATE</p>\n<p><code>Model.save()</code> does either INSERT or UPDATE of an object in a DB, while <code>Model.objects.create()</code> does only INSERT.</p>\n<p><code>Model.save()</code> does</p>\n<ul>\n<li><p><strong>UPDATE</strong> If the object\u2019s primary key attribute is set to a value that evaluates to <code>True</code></p>\n</li>\n<li><p><strong>INSERT</strong> If the object\u2019s primary key attribute is not set or if the UPDATE didn\u2019t update anything (e.g. if primary key is set to a value that doesn\u2019t exist in the database).</p>\n</li>\n</ul>\n</li>\n</ol>\n<hr/>\n<ol start=\"2\">\n<li><p>Existing primary key</p>\n<p>If primary key attribute is set to a value and such primary key already exists, then <code>Model.save()</code> performs UPDATE, but <code>Model.objects.create()</code> raises <code>IntegrityError</code>.</p>\n<p>Consider the following <em>models.py:</em></p>\n<pre><code class=\"python\">class Subject(models.Model):\n   subject_id = models.PositiveIntegerField(primary_key=True, db_column='subject_id')\n   name = models.CharField(max_length=255)\n   max_marks = models.PositiveIntegerField()\n</code></pre>\n<ol>\n<li><p>Insert/Update to db with <code>Model.save()</code></p>\n<pre><code class=\"python\">physics = Subject(subject_id=1, name='Physics', max_marks=100)\nphysics.save()\nmath = Subject(subject_id=1, name='Math', max_marks=50)  # Case of update\nmath.save()\n</code></pre>\n<p>Result:</p>\n<pre><code class=\"python\">Subject.objects.all().values()\n&lt;QuerySet [{'subject_id': 1, 'name': 'Math', 'max_marks': 50}]&gt;\n</code></pre>\n</li>\n<li><p>Insert to db with <code>Model.objects.create()</code></p>\n<pre><code class=\"python\">Subject.objects.create(subject_id=1, name='Chemistry', max_marks=100)\nIntegrityError: UNIQUE constraint failed: m****t.subject_id\n</code></pre>\n</li>\n</ol>\n<p><strong>Explanation:</strong> In the example, <code>math.save()</code> does an UPDATE (changes <code>name</code> from <em>Physics</em> to <em>Math</em>, and <code>max_marks</code> from 100 to 50), because <code>subject_id</code> is a primary key and <code>subject_id=1</code> already exists in the DB. But <code>Subject.objects.create()</code> raises <code>IntegrityError</code>, because, again the primary key <code>subject_id</code> with the value <code>1</code> already exists.</p>\n</li>\n</ol>\n<hr/>\n<ol start=\"3\">\n<li><p>Forced insert</p>\n<p><code>Model.save()</code> can be made to behave as <code>Model.objects.create()</code> by using <code>force_insert=True</code> parameter: <code>Model.save(force_insert=True)</code>.</p>\n</li>\n</ol>\n<hr/>\n<ol start=\"4\">\n<li><p>Return value</p>\n<p><code>Model.save()</code> return <code>None</code> where <code>Model.objects.create()</code> return model instance i.e. <code>package_name.models.Model</code></p>\n</li>\n</ol>\n<hr/>\n<p><strong>Conclusion:</strong> <code>Model.objects.create()</code> does model initialization and performs <code>save()</code> with <code>force_insert=True</code>.</p>\n<p>Excerpt from the source code of <code>Model.objects.create()</code></p>\n<pre><code class=\"python\">def create(self, **kwargs):\n    \"\"\"\n    Create a new object with the given kwargs, saving it to the database\n    and returning the created object.\n    \"\"\"\n    obj = self.model(**kwargs)\n    self._for_write = True\n    obj.save(force_insert=True, using=self.db)\n    return obj\n</code></pre>\n<p>For more details follow the links:</p>\n<ol>\n<li><p><a href=\"https://docs.djangoproject.com/en/stable/ref/models/querysets/#create\" rel=\"noreferrer\">https://docs.djangoproject.com/en/stable/ref/models/querysets/#create</a></p>\n</li>\n<li><p><a href=\"https://github.com/django/django/blob/2d8dcba03aae200aaa103ec1e69f0a0038ec2f85/django/db/models/query.py#L440\" rel=\"noreferrer\">https://github.com/django/django/blob/2d8dcba03aae200aaa103ec1e69f0a0038ec2f85/django/db/models/query.py#L440</a></p>\n</li>\n</ol>\n", "abstract": "The differences between Model() and Model.objects.create() are the following: INSERT vs UPDATE Model.save() does either INSERT or UPDATE of an object in a DB, while Model.objects.create() does only INSERT. Model.save() does UPDATE If the object\u2019s primary key attribute is set to a value that evaluates to True INSERT If the object\u2019s primary key attribute is not set or if the UPDATE didn\u2019t update anything (e.g. if primary key is set to a value that doesn\u2019t exist in the database). Existing primary key If primary key attribute is set to a value and such primary key already exists, then Model.save() performs UPDATE, but Model.objects.create() raises IntegrityError. Consider the following models.py: Insert/Update to db with Model.save() Result: Insert to db with Model.objects.create() Explanation: In the example, math.save() does an UPDATE (changes name from Physics to Math, and max_marks from 100 to 50), because subject_id is a primary key and subject_id=1 already exists in the DB. But Subject.objects.create() raises IntegrityError, because, again the primary key subject_id with the value 1 already exists. Forced insert Model.save() can be made to behave as Model.objects.create() by using force_insert=True parameter: Model.save(force_insert=True). Return value Model.save() return None where Model.objects.create() return model instance i.e. package_name.models.Model Conclusion: Model.objects.create() does model initialization and performs save() with force_insert=True. Excerpt from the source code of Model.objects.create() For more details follow the links: https://docs.djangoproject.com/en/stable/ref/models/querysets/#create https://github.com/django/django/blob/2d8dcba03aae200aaa103ec1e69f0a0038ec2f85/django/db/models/query.py#L440"}, {"id": 46561707, "score": 23, "vote": 0, "content": "<p>The two syntaxes are not equivalent and it can lead to unexpected errors.\nHere is a simple example showing the differences.\nIf you have a model:</p>\n<pre><code class=\"python\">from django.db import models\n\nclass Test(models.Model):\n\n    added = models.DateTimeField(auto_now_add=True)\n</code></pre>\n<p>And you create a first object:</p>\n<pre><code class=\"python\">foo = Test.objects.create(pk=1)\n</code></pre>\n<p>Then you try to create an object with the same primary key:</p>\n<pre><code class=\"python\">foo_duplicate = Test.objects.create(pk=1)\n# returns the error:\n# django.db.utils.IntegrityError: (1062, \"Duplicate entry '1' for key 'PRIMARY'\")\n\nfoo_duplicate = Test(pk=1).save()\n# returns the error:\n# django.db.utils.IntegrityError: (1048, \"Column 'added' cannot be null\")\n</code></pre>\n", "abstract": "The two syntaxes are not equivalent and it can lead to unexpected errors.\nHere is a simple example showing the differences.\nIf you have a model: And you create a first object: Then you try to create an object with the same primary key:"}, {"id": 42141029, "score": 13, "vote": 0, "content": "<p>UPDATE 15.3.2017:</p>\n<p>I have opened a Django-issue on this and it seems to be preliminary accepted here:\n<a href=\"https://code.djangoproject.com/ticket/27825\" rel=\"noreferrer\">https://code.djangoproject.com/ticket/27825</a></p>\n<p>My experience is that when using the <code>Constructor</code> (<code>ORM</code>) class by references with Django <code>1.10.5</code> there might be some inconsistencies in the data (i.e. the attributes of the created object may get the type of the input data instead of the casted type of the ORM object property)\nexample: </p>\n<p><code>models</code></p>\n<pre><code class=\"python\">class Payment(models.Model):\n     amount_cash = models.DecimalField()\n</code></pre>\n<p><code>some_test.py</code> - <code>object.create</code></p>\n<pre><code class=\"python\">Class SomeTestCase:\n    def generate_orm_obj(self, _constructor, base_data=None, modifiers=None):\n        objs = []\n        if not base_data:\n            base_data = {'amount_case': 123.00}\n        for modifier in modifiers:\n            actual_data = deepcopy(base_data)\n            actual_data.update(modifier)\n            # Hacky fix,\n            _obj = _constructor.objects.create(**actual_data)\n            print(type(_obj.amount_cash)) # Decimal\n            assert created\n           objs.append(_obj)\n        return objs\n</code></pre>\n<p><code>some_test.py</code> - <code>Constructor()</code></p>\n<pre><code class=\"python\">Class SomeTestCase:\n    def generate_orm_obj(self, _constructor, base_data=None, modifiers=None):\n        objs = []\n        if not base_data:\n            base_data = {'amount_case': 123.00}\n        for modifier in modifiers:\n            actual_data = deepcopy(base_data)\n            actual_data.update(modifier)\n            # Hacky fix,\n            _obj = _constructor(**actual_data)\n            print(type(_obj.amount_cash)) # Float\n            assert created\n           objs.append(_obj)\n        return objs\n</code></pre>\n", "abstract": "UPDATE 15.3.2017: I have opened a Django-issue on this and it seems to be preliminary accepted here:\nhttps://code.djangoproject.com/ticket/27825 My experience is that when using the Constructor (ORM) class by references with Django 1.10.5 there might be some inconsistencies in the data (i.e. the attributes of the created object may get the type of the input data instead of the casted type of the ORM object property)\nexample:  models some_test.py - object.create some_test.py - Constructor()"}, {"id": 70058143, "score": 3, "vote": 0, "content": "<p><code>Model.objects.create()</code> creates a model instance and saves it. <code>Model()</code> only creates an in memory model instance. It's not saved to the database until you call the instance's <code>save()</code> method to save it. That's when validation happens also.</p>\n", "abstract": "Model.objects.create() creates a model instance and saves it. Model() only creates an in memory model instance. It's not saved to the database until you call the instance's save() method to save it. That's when validation happens also."}]}, {"link": "https://stackoverflow.com/questions/2548493/how-do-i-get-the-id-after-insert-into-mysql-database-with-python", "question": {"id": "2548493", "title": "How do I get the &quot;id&quot; after INSERT into MySQL database with Python?", "content": "<p>I execute an INSERT INTO statement</p>\n<pre><code class=\"python\">cursor.execute(\"INSERT INTO mytable(height) VALUES(%s)\",(height))\n</code></pre>\n<p>and I want to get the primary key.</p>\n<p>My table has 2 columns: </p>\n<pre><code class=\"python\">id      primary, auto increment\nheight  this is the other column.\n</code></pre>\n<p>How do I get the \"id\", after I just inserted this? </p>\n", "abstract": "I execute an INSERT INTO statement and I want to get the primary key. My table has 2 columns:  How do I get the \"id\", after I just inserted this? "}, "answers": [{"id": 2548531, "score": 313, "vote": 0, "content": "<p>Use <code>cursor.lastrowid</code> to get the last row ID inserted on the cursor object, or <code>connection.insert_id()</code> to get the ID from the last insert on that connection.</p>\n", "abstract": "Use cursor.lastrowid to get the last row ID inserted on the cursor object, or connection.insert_id() to get the ID from the last insert on that connection."}, {"id": 3790542, "score": 130, "vote": 0, "content": "<p>Also, <code>cursor.lastrowid</code> (a dbapi/PEP249 extension supported by MySQLdb):</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import MySQLdb\n&gt;&gt;&gt; connection = MySQLdb.connect(user='root')\n&gt;&gt;&gt; cursor = connection.cursor()\n&gt;&gt;&gt; cursor.execute('INSERT INTO sometable VALUES (...)')\n1L\n&gt;&gt;&gt; connection.insert_id()\n3L\n&gt;&gt;&gt; cursor.lastrowid\n3L\n&gt;&gt;&gt; cursor.execute('SELECT last_insert_id()')\n1L\n&gt;&gt;&gt; cursor.fetchone()\n(3L,)\n&gt;&gt;&gt; cursor.execute('select @@identity')\n1L\n&gt;&gt;&gt; cursor.fetchone()\n(3L,)\n</code></pre>\n<p><code>cursor.lastrowid</code> is somewhat cheaper than <code>connection.insert_id()</code> and much cheaper than another round trip to MySQL.</p>\n", "abstract": "Also, cursor.lastrowid (a dbapi/PEP249 extension supported by MySQLdb): cursor.lastrowid is somewhat cheaper than connection.insert_id() and much cheaper than another round trip to MySQL."}, {"id": 4581359, "score": 41, "vote": 0, "content": "<p>Python DBAPI spec also define 'lastrowid' attribute for cursor object, so...</p>\n<pre><code class=\"python\">id = cursor.lastrowid\n</code></pre>\n<p>...should work too, and it's per-connection based obviously.</p>\n", "abstract": "Python DBAPI spec also define 'lastrowid' attribute for cursor object, so... ...should work too, and it's per-connection based obviously."}, {"id": 2548546, "score": 8, "vote": 0, "content": "<pre><code class=\"python\">SELECT @@IDENTITY AS 'Identity';\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">SELECT last_insert_id();\n</code></pre>\n", "abstract": "or"}, {"id": 62804482, "score": 0, "vote": 0, "content": "<p>This might be just a requirement of PyMySql in Python, but I found that I had to name the exact table that I wanted the ID for:</p>\n<p>In:</p>\n<pre><code class=\"python\">cnx = pymysql.connect(host='host',\n                            database='db',\n                            user='user',\n                            password='pass')\ncursor = cnx.cursor()\nupdate_batch = \"\"\"insert into batch set type = \"%s\" , records = %i, started = NOW(); \"\"\"\nsecond_query = (update_batch % ( \"Batch 1\", 22  ))\ncursor.execute(second_query)\ncnx.commit()\nbatch_id = cursor.execute('select last_insert_id() from batch')\ncursor.close()\n\nbatch_id\n</code></pre>\n<p>Out:\n5<br/>\n... or whatever the correct Batch_ID value actually is</p>\n", "abstract": "This might be just a requirement of PyMySql in Python, but I found that I had to name the exact table that I wanted the ID for: In: Out:\n5\n... or whatever the correct Batch_ID value actually is"}]}, {"link": "https://stackoverflow.com/questions/3659142/bulk-insert-with-sqlalchemy-orm", "question": {"id": "3659142", "title": "Bulk insert with SQLAlchemy ORM", "content": "<p>Is there any way to get SQLAlchemy to do a bulk insert rather than inserting each individual object. i.e.,</p>\n<p>doing:</p>\n<pre><code class=\"python\">INSERT INTO `foo` (`bar`) VALUES (1), (2), (3)\n</code></pre>\n<p>rather than:</p>\n<pre><code class=\"python\">INSERT INTO `foo` (`bar`) VALUES (1)\nINSERT INTO `foo` (`bar`) VALUES (2)\nINSERT INTO `foo` (`bar`) VALUES (3)\n</code></pre>\n<p>I've just converted some code to use sqlalchemy rather than raw sql and although it is now much nicer to work with it seems to be slower now (up to a factor of 10), I'm wondering if this is the reason.</p>\n<p>May be I could improve the situation using sessions more efficiently. At the moment I have <code>autoCommit=False</code> and do a <code>session.commit()</code> after I've added some stuff. Although this seems to cause the data to go stale if the DB is changed elsewhere, like even if I do a new query I still get old results back?</p>\n<p>Thanks for your help!</p>\n", "abstract": "Is there any way to get SQLAlchemy to do a bulk insert rather than inserting each individual object. i.e., doing: rather than: I've just converted some code to use sqlalchemy rather than raw sql and although it is now much nicer to work with it seems to be slower now (up to a factor of 10), I'm wondering if this is the reason. May be I could improve the situation using sessions more efficiently. At the moment I have autoCommit=False and do a session.commit() after I've added some stuff. Although this seems to cause the data to go stale if the DB is changed elsewhere, like even if I do a new query I still get old results back? Thanks for your help!"}, "answers": [{"id": 31205155, "score": 254, "vote": 0, "content": "<p>SQLAlchemy introduced that in version <code>1.0.0</code>:</p>\n<p><a href=\"https://docs.sqlalchemy.org/en/latest/orm/persistence_techniques.html#bulk-operations\" rel=\"noreferrer\">Bulk operations - SQLAlchemy docs</a></p>\n<p>With these operations, you can now do bulk inserts or updates!</p>\n<p>For instance, you can do:</p>\n<pre><code class=\"python\">s = Session()\nobjects = [\n    User(name=\"u1\"),\n    User(name=\"u2\"),\n    User(name=\"u3\")\n]\ns.bulk_save_objects(objects)\ns.commit()\n</code></pre>\n<p>Here, a bulk insert will be made.</p>\n", "abstract": "SQLAlchemy introduced that in version 1.0.0: Bulk operations - SQLAlchemy docs With these operations, you can now do bulk inserts or updates! For instance, you can do: Here, a bulk insert will be made."}, {"id": 34344200, "score": 72, "vote": 0, "content": "<p>The sqlalchemy docs have a <a href=\"https://docs.sqlalchemy.org/en/13/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow\" rel=\"noreferrer\">writeup</a> on the performance of various techniques that can be used for bulk inserts:</p>\n<blockquote>\n<p>ORMs are basically not intended for high-performance bulk inserts -\n  this is the whole reason SQLAlchemy offers the Core in addition to the\n  ORM as a first-class component.</p>\n<p>For the use case of fast bulk inserts, the SQL generation and\n  execution system that the ORM builds on top of is part of the Core.\n  Using this system directly, we can produce an INSERT that is\n  competitive with using the raw database API directly.</p>\n<p>Alternatively, the SQLAlchemy ORM offers the Bulk Operations suite of\n  methods, which provide hooks into subsections of the unit of work\n  process in order to emit Core-level INSERT and UPDATE constructs with\n  a small degree of ORM-based automation.</p>\n<p>The example below illustrates time-based tests for several different\n  methods of inserting rows, going from the most automated to the least.\n  With cPython 2.7, runtimes observed:</p>\n<pre><code class=\"python\">classics-MacBook-Pro:sqlalchemy classic$ python test.py\nSQLAlchemy ORM: Total time for 100000 records 12.0471920967 secs\nSQLAlchemy ORM pk given: Total time for 100000 records 7.06283402443 secs\nSQLAlchemy ORM bulk_save_objects(): Total time for 100000 records 0.856323003769 secs\nSQLAlchemy Core: Total time for 100000 records 0.485800027847 secs\nsqlite3: Total time for 100000 records 0.487842082977 sec\n</code></pre>\n<p>Script:</p>\n<pre><code class=\"python\">import time\nimport sqlite3\n\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String,  create_engine\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\nBase = declarative_base()\nDBSession = scoped_session(sessionmaker())\nengine = None\n\n\nclass Customer(Base):\n    __tablename__ = \"customer\"\n    id = Column(Integer, primary_key=True)\n    name = Column(String(255))\n\n\ndef init_sqlalchemy(dbname='sqlite:///sqlalchemy.db'):\n    global engine\n    engine = create_engine(dbname, echo=False)\n    DBSession.remove()\n    DBSession.configure(bind=engine, autoflush=False, expire_on_commit=False)\n    Base.metadata.drop_all(engine)\n    Base.metadata.create_all(engine)\n\n\ndef test_sqlalchemy_orm(n=100000):\n    init_sqlalchemy()\n    t0 = time.time()\n    for i in xrange(n):\n        customer = Customer()\n        customer.name = 'NAME ' + str(i)\n        DBSession.add(customer)\n        if i % 1000 == 0:\n            DBSession.flush()\n    DBSession.commit()\n    print(\n        \"SQLAlchemy ORM: Total time for \" + str(n) +\n        \" records \" + str(time.time() - t0) + \" secs\")\n\n\ndef test_sqlalchemy_orm_pk_given(n=100000):\n    init_sqlalchemy()\n    t0 = time.time()\n    for i in xrange(n):\n        customer = Customer(id=i+1, name=\"NAME \" + str(i))\n        DBSession.add(customer)\n        if i % 1000 == 0:\n            DBSession.flush()\n    DBSession.commit()\n    print(\n        \"SQLAlchemy ORM pk given: Total time for \" + str(n) +\n        \" records \" + str(time.time() - t0) + \" secs\")\n\n\ndef test_sqlalchemy_orm_bulk_insert(n=100000):\n    init_sqlalchemy()\n    t0 = time.time()\n    n1 = n\n    while n1 &gt; 0:\n        n1 = n1 - 10000\n        DBSession.bulk_insert_mappings(\n            Customer,\n            [\n                dict(name=\"NAME \" + str(i))\n                for i in xrange(min(10000, n1))\n            ]\n        )\n    DBSession.commit()\n    print(\n        \"SQLAlchemy ORM bulk_save_objects(): Total time for \" + str(n) +\n        \" records \" + str(time.time() - t0) + \" secs\")\n\n\ndef test_sqlalchemy_core(n=100000):\n    init_sqlalchemy()\n    t0 = time.time()\n    engine.execute(\n        Customer.__table__.insert(),\n        [{\"name\": 'NAME ' + str(i)} for i in xrange(n)]\n    )\n    print(\n        \"SQLAlchemy Core: Total time for \" + str(n) +\n        \" records \" + str(time.time() - t0) + \" secs\")\n\n\ndef init_sqlite3(dbname):\n    conn = sqlite3.connect(dbname)\n    c = conn.cursor()\n    c.execute(\"DROP TABLE IF EXISTS customer\")\n    c.execute(\n        \"CREATE TABLE customer (id INTEGER NOT NULL, \"\n        \"name VARCHAR(255), PRIMARY KEY(id))\")\n    conn.commit()\n    return conn\n\n\ndef test_sqlite3(n=100000, dbname='sqlite3.db'):\n    conn = init_sqlite3(dbname)\n    c = conn.cursor()\n    t0 = time.time()\n    for i in xrange(n):\n        row = ('NAME ' + str(i),)\n        c.execute(\"INSERT INTO customer (name) VALUES (?)\", row)\n    conn.commit()\n    print(\n        \"sqlite3: Total time for \" + str(n) +\n        \" records \" + str(time.time() - t0) + \" sec\")\n\nif __name__ == '__main__':\n    test_sqlalchemy_orm(100000)\n    test_sqlalchemy_orm_pk_given(100000)\n    test_sqlalchemy_orm_bulk_insert(100000)\n    test_sqlalchemy_core(100000)\n    test_sqlite3(100000)\n</code></pre>\n</blockquote>\n", "abstract": "The sqlalchemy docs have a writeup on the performance of various techniques that can be used for bulk inserts: ORMs are basically not intended for high-performance bulk inserts -\n  this is the whole reason SQLAlchemy offers the Core in addition to the\n  ORM as a first-class component. For the use case of fast bulk inserts, the SQL generation and\n  execution system that the ORM builds on top of is part of the Core.\n  Using this system directly, we can produce an INSERT that is\n  competitive with using the raw database API directly. Alternatively, the SQLAlchemy ORM offers the Bulk Operations suite of\n  methods, which provide hooks into subsections of the unit of work\n  process in order to emit Core-level INSERT and UPDATE constructs with\n  a small degree of ORM-based automation. The example below illustrates time-based tests for several different\n  methods of inserting rows, going from the most automated to the least.\n  With cPython 2.7, runtimes observed: Script:"}, {"id": 3663101, "score": 35, "vote": 0, "content": "<p>As far as I know, there is no way to get the ORM to issue bulk inserts. I believe the underlying reason is that SQLAlchemy needs to keep track of each object's identity (i.e., new primary keys), and bulk inserts interfere with that. For example, assuming your <code>foo</code> table contains an <code>id</code> column and is mapped to a <code>Foo</code> class:</p>\n<pre><code class=\"python\">x = Foo(bar=1)\nprint x.id\n# None\nsession.add(x)\nsession.flush()\n# BEGIN\n# INSERT INTO foo (bar) VALUES(1)\n# COMMIT\nprint x.id\n# 1\n</code></pre>\n<p>Since SQLAlchemy picked up the value for <code>x.id</code> without issuing another query, we can infer that it got the value directly from the <code>INSERT</code> statement. If you don't need subsequent access to the created objects via the <em>same</em> instances, you can skip the ORM layer for your insert:</p>\n<pre><code class=\"python\">Foo.__table__.insert().execute([{'bar': 1}, {'bar': 2}, {'bar': 3}])\n# INSERT INTO foo (bar) VALUES ((1,), (2,), (3,))\n</code></pre>\n<p>SQLAlchemy can't match these new rows with any existing objects, so you'll have to query them anew for any subsequent operations.</p>\n<p>As far as stale data is concerned, it's helpful to remember that the session has no built-in way to know when the database is changed outside of the session. In order to access externally modified data through existing instances, the instances must be marked as <em>expired</em>. This happens by default on <code>session.commit()</code>, but can be done manually by calling <code>session.expire_all()</code> or <code>session.expire(instance)</code>. An example (SQL omitted):</p>\n<pre><code class=\"python\">x = Foo(bar=1)\nsession.add(x)\nsession.commit()\nprint x.bar\n# 1\nfoo.update().execute(bar=42)\nprint x.bar\n# 1\nsession.expire(x)\nprint x.bar\n# 42\n</code></pre>\n<p><code>session.commit()</code> expires <code>x</code>, so the first print statement implicitly opens a new transaction and re-queries <code>x</code>'s attributes. If you comment out the first print statement, you'll notice that the second one now picks up the correct value, because the new query isn't emitted until after the update.</p>\n<p>This makes sense from the point of view of transactional isolation - you should only pick up external modifications between transactions. If this is causing you trouble, I'd suggest clarifying or  re-thinking your application's transaction boundaries instead of immediately reaching for <code>session.expire_all()</code>.</p>\n", "abstract": "As far as I know, there is no way to get the ORM to issue bulk inserts. I believe the underlying reason is that SQLAlchemy needs to keep track of each object's identity (i.e., new primary keys), and bulk inserts interfere with that. For example, assuming your foo table contains an id column and is mapped to a Foo class: Since SQLAlchemy picked up the value for x.id without issuing another query, we can infer that it got the value directly from the INSERT statement. If you don't need subsequent access to the created objects via the same instances, you can skip the ORM layer for your insert: SQLAlchemy can't match these new rows with any existing objects, so you'll have to query them anew for any subsequent operations. As far as stale data is concerned, it's helpful to remember that the session has no built-in way to know when the database is changed outside of the session. In order to access externally modified data through existing instances, the instances must be marked as expired. This happens by default on session.commit(), but can be done manually by calling session.expire_all() or session.expire(instance). An example (SQL omitted): session.commit() expires x, so the first print statement implicitly opens a new transaction and re-queries x's attributes. If you comment out the first print statement, you'll notice that the second one now picks up the correct value, because the new query isn't emitted until after the update. This makes sense from the point of view of transactional isolation - you should only pick up external modifications between transactions. If this is causing you trouble, I'd suggest clarifying or  re-thinking your application's transaction boundaries instead of immediately reaching for session.expire_all()."}, {"id": 43632261, "score": 29, "vote": 0, "content": "<p>I usually do it using <a href=\"http://docs.sqlalchemy.org/en/latest/orm/session_api.html#sqlalchemy.orm.session.Session.add_all\" rel=\"noreferrer\"><code>add_all</code></a>.</p>\n<pre><code class=\"python\">from app import session\nfrom models import User\n\nobjects = [User(name=\"u1\"), User(name=\"u2\"), User(name=\"u3\")]\nsession.add_all(objects)\nsession.commit()\n</code></pre>\n", "abstract": "I usually do it using add_all."}, {"id": 27235131, "score": 23, "vote": 0, "content": "<p>Direct support was added to SQLAlchemy as of version 0.8</p>\n<p>As per the <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/dml.html?highlight=insert%20values#sqlalchemy.sql.expression.Insert.values\" rel=\"noreferrer\">docs</a>, <code>connection.execute(table.insert().values(data))</code> should do the trick. (Note that this is <em>not</em> the same as <code>connection.execute(table.insert(), data)</code> which results in many individual row inserts via a call to <code>executemany</code>). On anything but a local connection the difference in performance can be enormous.</p>\n", "abstract": "Direct support was added to SQLAlchemy as of version 0.8 As per the docs, connection.execute(table.insert().values(data)) should do the trick. (Note that this is not the same as connection.execute(table.insert(), data) which results in many individual row inserts via a call to executemany). On anything but a local connection the difference in performance can be enormous."}, {"id": 35495327, "score": 20, "vote": 0, "content": "<p>SQLAlchemy introduced that in version <code>1.0.0</code>:</p>\n<p><a href=\"http://docs.sqlalchemy.org/en/rel_1_0/orm/persistence_techniques.html#bulk-operations\" rel=\"noreferrer\">Bulk operations - SQLAlchemy docs</a></p>\n<p>With these operations, you can now do bulk inserts or updates!</p>\n<p>For instance (if you want the lowest overhead for simple table INSERTs), you can use <a href=\"http://docs.sqlalchemy.org/en/rel_1_0/orm/session_api.html#sqlalchemy.orm.session.Session.bulk_insert_mappings\" rel=\"noreferrer\"><code>Session.bulk_insert_mappings()</code></a>:</p>\n<pre><code class=\"python\">loadme = [(1, 'a'),\n          (2, 'b'),\n          (3, 'c')]\ndicts = [dict(bar=t[0], fly=t[1]) for t in loadme]\n\ns = Session()\ns.bulk_insert_mappings(Foo, dicts)\ns.commit()\n</code></pre>\n<p>Or, if you want, skip the <code>loadme</code> tuples and write the dictionaries directly into <code>dicts</code> (but I find it easier to leave all the wordiness out of the data and load up a list of dictionaries in a loop).</p>\n", "abstract": "SQLAlchemy introduced that in version 1.0.0: Bulk operations - SQLAlchemy docs With these operations, you can now do bulk inserts or updates! For instance (if you want the lowest overhead for simple table INSERTs), you can use Session.bulk_insert_mappings(): Or, if you want, skip the loadme tuples and write the dictionaries directly into dicts (but I find it easier to leave all the wordiness out of the data and load up a list of dictionaries in a loop)."}, {"id": 37515965, "score": 8, "vote": 0, "content": "<p>Piere's answer is correct but one issue is that <code>bulk_save_objects</code> by default does not return the primary keys of the objects, if that is of concern to you. Set <code>return_defaults</code> to <code>True</code> to get this behavior.</p>\n<p>The documentation is <a href=\"http://docs.sqlalchemy.org/en/latest/orm/session_api.html#sqlalchemy.orm.session.Session.bulk_save_objects\" rel=\"noreferrer\">here</a>.</p>\n<pre><code class=\"python\">foos = [Foo(bar='a',), Foo(bar='b'), Foo(bar='c')]\nsession.bulk_save_objects(foos, return_defaults=True)\nfor foo in foos:\n    assert foo.id is not None\nsession.commit()\n</code></pre>\n", "abstract": "Piere's answer is correct but one issue is that bulk_save_objects by default does not return the primary keys of the objects, if that is of concern to you. Set return_defaults to True to get this behavior. The documentation is here."}, {"id": 29311778, "score": 6, "vote": 0, "content": "<p>This is a way:</p>\n<pre><code class=\"python\">values = [1, 2, 3]\nFoo.__table__.insert().execute([{'bar': x} for x in values])\n</code></pre>\n<p>This will insert like this:</p>\n<pre><code class=\"python\">INSERT INTO `foo` (`bar`) VALUES (1), (2), (3)\n</code></pre>\n<p>Reference: The SQLAlchemy <a href=\"http://docs.sqlalchemy.org/en/rel_0_8/faq.html\" rel=\"noreferrer\">FAQ</a> includes benchmarks for various commit methods.</p>\n", "abstract": "This is a way: This will insert like this: Reference: The SQLAlchemy FAQ includes benchmarks for various commit methods."}, {"id": 50839429, "score": 6, "vote": 0, "content": "<p><em>All Roads Lead to Rome</em>, but some of them crosses mountains, requires ferries but if you want to get there quickly just take the motorway.</p>\n<hr/>\n<p>In this case the motorway is to use the <a href=\"http://initd.org/psycopg/docs/extras.html#fast-execution-helpers\" rel=\"noreferrer\">execute_batch()</a> feature of <a href=\"http://initd.org/psycopg/\" rel=\"noreferrer\">psycopg2</a>. The documentation says it the best:</p>\n<p>The current implementation of <code>executemany()</code> is (using an extremely charitable understatement) not particularly performing. These functions can be used to speed up the repeated execution of a statement against a set of parameters. By reducing the number of server roundtrips the performance can be orders of magnitude better than using <code>executemany()</code>.</p>\n<p>In my own test <code>execute_batch()</code> is <strong>approximately twice as fast</strong> as <code>executemany()</code>, and gives the option to configure the page_size for further tweaking (if you want to squeeze the last 2-3% of performance out of the driver).</p>\n<p>The same feature can easily be enabled if you are using SQLAlchemy by setting <code>use_batch_mode=True</code> as a parameter when you instantiate the engine with <code>create_engine()</code></p>\n", "abstract": "All Roads Lead to Rome, but some of them crosses mountains, requires ferries but if you want to get there quickly just take the motorway. In this case the motorway is to use the execute_batch() feature of psycopg2. The documentation says it the best: The current implementation of executemany() is (using an extremely charitable understatement) not particularly performing. These functions can be used to speed up the repeated execution of a statement against a set of parameters. By reducing the number of server roundtrips the performance can be orders of magnitude better than using executemany(). In my own test execute_batch() is approximately twice as fast as executemany(), and gives the option to configure the page_size for further tweaking (if you want to squeeze the last 2-3% of performance out of the driver). The same feature can easily be enabled if you are using SQLAlchemy by setting use_batch_mode=True as a parameter when you instantiate the engine with create_engine()"}, {"id": 50022299, "score": 4, "vote": 0, "content": "<p>The best answer I found so far was in sqlalchemy documentation:</p>\n<p><a href=\"http://docs.sqlalchemy.org/en/latest/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow\" rel=\"nofollow noreferrer\">http://docs.sqlalchemy.org/en/latest/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow</a></p>\n<p>There is a complete example of a benchmark of possible solutions.</p>\n<p>As shown in the documentation:</p>\n<p>bulk_save_objects is not the best solution but it performance are correct.</p>\n<p>The second best implementation in terms of readability I think was with the SQLAlchemy Core:</p>\n<pre><code class=\"python\">def test_sqlalchemy_core(n=100000):\n    init_sqlalchemy()\n    t0 = time.time()\n    engine.execute(\n        Customer.__table__.insert(),\n            [{\"name\": 'NAME ' + str(i)} for i in xrange(n)]\n    )\n</code></pre>\n<p>The context of this function is given in the documentation article.</p>\n", "abstract": "The best answer I found so far was in sqlalchemy documentation: http://docs.sqlalchemy.org/en/latest/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow There is a complete example of a benchmark of possible solutions. As shown in the documentation: bulk_save_objects is not the best solution but it performance are correct. The second best implementation in terms of readability I think was with the SQLAlchemy Core: The context of this function is given in the documentation article."}, {"id": 69909827, "score": 1, "vote": 0, "content": "<p>Sqlalchemy supports bulk insert</p>\n<pre><code class=\"python\">bulk_list = [\n    Foo(\n        bar=1,\n    ),\n    Foo(\n        bar=2,\n    ),\n    Foo(\n        bar=3,\n    ),\n]\ndb.session.bulk_save_objects(bulk_list)\ndb.session.commit()\n</code></pre>\n", "abstract": "Sqlalchemy supports bulk insert"}]}, {"link": "https://stackoverflow.com/questions/5033547/sqlalchemy-cascade-delete", "question": {"id": "5033547", "title": "SQLAlchemy: cascade delete", "content": "<p>I must be missing something trivial with SQLAlchemy's cascade options because I cannot get a simple cascade delete to operate correctly -- if a parent element is a deleted, the children persist, with <code>null</code> foreign keys. </p>\n<p>I've put a concise test case here:</p>\n<pre><code class=\"python\">from sqlalchemy import Column, Integer, ForeignKey\nfrom sqlalchemy.orm import relationship\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass Parent(Base):\n    __tablename__ = \"parent\"\n    id = Column(Integer, primary_key = True)\n\nclass Child(Base):\n    __tablename__ = \"child\"\n    id = Column(Integer, primary_key = True)\n    parentid = Column(Integer, ForeignKey(Parent.id))\n    parent = relationship(Parent, cascade = \"all,delete\", backref = \"children\")\n\nengine = create_engine(\"sqlite:///:memory:\")\nBase.metadata.create_all(engine)\nSession = sessionmaker(bind=engine)\n\nsession = Session()\n\nparent = Parent()\nparent.children.append(Child())\nparent.children.append(Child())\nparent.children.append(Child())\n\nsession.add(parent)\nsession.commit()\n\nprint \"Before delete, children = {0}\".format(session.query(Child).count())\nprint \"Before delete, parent = {0}\".format(session.query(Parent).count())\n\nsession.delete(parent)\nsession.commit()\n\nprint \"After delete, children = {0}\".format(session.query(Child).count())\nprint \"After delete parent = {0}\".format(session.query(Parent).count())\n\nsession.close()\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">Before delete, children = 3\nBefore delete, parent = 1\nAfter delete, children = 3\nAfter delete parent = 0\n</code></pre>\n<p>There is a simple, one-to-many relationship between Parent and Child. The script creates a parent, adds 3 children, then commits. Next, it deletes the parent, but the children persist. Why? How do I make the children cascade delete?</p>\n", "abstract": "I must be missing something trivial with SQLAlchemy's cascade options because I cannot get a simple cascade delete to operate correctly -- if a parent element is a deleted, the children persist, with null foreign keys.  I've put a concise test case here: Output: There is a simple, one-to-many relationship between Parent and Child. The script creates a parent, adds 3 children, then commits. Next, it deletes the parent, but the children persist. Why? How do I make the children cascade delete?"}, "answers": [{"id": 5034070, "score": 254, "vote": 0, "content": "<p>The problem is that sqlalchemy considers <code>Child</code> as the parent, because that is where you defined your relationship (it doesn't care that you called it \"Child\" of course).</p>\n<p>If you define the relationship on the <code>Parent</code> class instead, it will work:</p>\n<pre><code class=\"python\">children = relationship(\"Child\", cascade=\"all,delete\", backref=\"parent\")\n</code></pre>\n<p>(note <code>\"Child\"</code> as a string: this is allowed when using the declarative style, so that you are able to refer to a class that is not yet defined)</p>\n<p>You might want to add <code>delete-orphan</code> as well (<code>delete</code> causes children to be deleted when the parent gets deleted, <code>delete-orphan</code> also deletes any children that were \"removed\" from the parent, even if the parent is not deleted)</p>\n<p>EDIT: just found out: if you <em>really</em> want to define the relationship on the <code>Child</code> class, you can do so, but you will have to define the cascade <strong>on the backref</strong> (by creating the backref explicitly), like this:</p>\n<pre><code class=\"python\">parent = relationship(Parent, backref=backref(\"children\", cascade=\"all,delete\"))\n</code></pre>\n<p>(implying <code>from sqlalchemy.orm import backref</code>)</p>\n", "abstract": "The problem is that sqlalchemy considers Child as the parent, because that is where you defined your relationship (it doesn't care that you called it \"Child\" of course). If you define the relationship on the Parent class instead, it will work: (note \"Child\" as a string: this is allowed when using the declarative style, so that you are able to refer to a class that is not yet defined) You might want to add delete-orphan as well (delete causes children to be deleted when the parent gets deleted, delete-orphan also deletes any children that were \"removed\" from the parent, even if the parent is not deleted) EDIT: just found out: if you really want to define the relationship on the Child class, you can do so, but you will have to define the cascade on the backref (by creating the backref explicitly), like this: (implying from sqlalchemy.orm import backref)"}, {"id": 12801654, "score": 156, "vote": 0, "content": "<p>@Steven's asnwer is good when you are deleting through <code>session.delete()</code> which never happens in my case. I noticed that most of the time I delete through <code>session.query().filter().delete()</code> (which doesn't put elements in the memory and deletes directly from db).\nUsing this method sqlalchemy's <code>cascade='all, delete'</code> doesn't work. There is a solution though: <code>ON DELETE CASCADE</code> through db (note: not all databases support it). </p>\n<pre><code class=\"python\">class Child(Base):\n    __tablename__ = \"children\"\n\n    id = Column(Integer, primary_key=True)\n    parent_id = Column(Integer, ForeignKey(\"parents.id\", ondelete='CASCADE'))\n\nclass Parent(Base):\n    __tablename__ = \"parents\"\n\n    id = Column(Integer, primary_key=True)\n    child = relationship(Child, backref=\"parent\", passive_deletes=True)\n</code></pre>\n", "abstract": "@Steven's asnwer is good when you are deleting through session.delete() which never happens in my case. I noticed that most of the time I delete through session.query().filter().delete() (which doesn't put elements in the memory and deletes directly from db).\nUsing this method sqlalchemy's cascade='all, delete' doesn't work. There is a solution though: ON DELETE CASCADE through db (note: not all databases support it). "}, {"id": 38770040, "score": 154, "vote": 0, "content": "<p>Pretty old post, but I just spent an hour or two on this, so I wanted to share my finding, especially since some of the other comments listed aren't quite right.</p>\n<h2>TL;DR</h2>\n<p>Give the child table a foreign or modify the existing one, adding <code>ondelete='CASCADE'</code>:</p>\n<pre><code class=\"python\">parent_id = db.Column(db.Integer, db.ForeignKey('parent.id', ondelete='CASCADE'))\n</code></pre>\n<p>And <em>one</em> of the following relationships:</p>\n<p>a) This on the parent table:</p>\n<pre><code class=\"python\">children = db.relationship('Child', backref='parent', passive_deletes=True)\n</code></pre>\n<p>b) <em>Or</em> this on the child table:</p>\n<pre><code class=\"python\">parent = db.relationship('Parent', backref=backref('children', passive_deletes=True))\n</code></pre>\n<hr/>\n<h2>Details</h2>\n<p>First off, despite what the accepted answer says, the parent/child relationship is not established by using <code>relationship</code>, it's established by using <code>ForeignKey</code>. You can put the <code>relationship</code> on either the parent or child tables and it will work fine. Although, apparently on the child tables, you have to use the <code>backref</code> function in addition to the keyword argument.</p>\n<h3>Option 1 (preferred)</h3>\n<p>Second, SqlAlchemy supports two different kinds of cascading. The first, and the one I recommend, is built into your database and usually takes the form of a constraint on the foreign key declaration. In PostgreSQL it looks like this:</p>\n<pre><code class=\"python\">CONSTRAINT child_parent_id_fkey FOREIGN KEY (parent_id)\nREFERENCES parent_table(id) MATCH SIMPLE\nON DELETE CASCADE\n</code></pre>\n<p>This means that when you delete a record from <code>parent_table</code>, then all the corresponding rows in <code>child_table</code> will be deleted for you by the database. It's fast and reliable and probably your best bet. You set this up in SqlAlchemy through <code>ForeignKey</code> like this (part of the child table definition):</p>\n<pre><code class=\"python\">parent_id = db.Column(db.Integer, db.ForeignKey('parent.id', ondelete='CASCADE'))\nparent = db.relationship('Parent', backref=backref('children', passive_deletes=True))\n</code></pre>\n<p>The <code>ondelete='CASCADE'</code> is the part that creates the <code>ON DELETE CASCADE</code> on the table.</p>\n<h3>Gotcha!</h3>\n<p>There's an important caveat here. Notice how I have a <code>relationship</code> specified with <code>passive_deletes=True</code>? If you don't have that, the entire thing will not work. This is because by default when you delete a parent record SqlAlchemy does something really weird. It sets the foreign keys of all child rows to <code>NULL</code>. So if you delete a row from <code>parent_table</code> where <code>id</code> = 5, then it will basically execute</p>\n<pre><code class=\"python\">UPDATE child_table SET parent_id = NULL WHERE parent_id = 5\n</code></pre>\n<p>Why you would want this I have no idea. I'd be surprised if many database engines even allowed you to set a valid foreign key to <code>NULL</code>, creating an orphan. Seems like a bad idea, but maybe there's a use case. Anyway, if you let SqlAlchemy do this, you will prevent the database from being able to clean up the children using the <code>ON DELETE CASCADE</code> that you set up. This is because it relies on those foreign keys to know which child rows to delete. Once SqlAlchemy has set them all to <code>NULL</code>, the database can't delete them. Setting the <code>passive_deletes=True</code> prevents SqlAlchemy from <code>NULL</code>ing out the foreign keys.</p>\n<p>You can read more about passive deletes in the <a href=\"http://docs.sqlalchemy.org/en/latest/orm/collections.html#using-passive-deletes\" rel=\"noreferrer\">SqlAlchemy docs</a>.</p>\n<h3>Option 2</h3>\n<p>The other way you can do it is to let SqlAlchemy do it for you. This is set up using the <code>cascade</code> argument of the <code>relationship</code>. If you have the relationship defined on the parent table, it looks like this:</p>\n<pre><code class=\"python\">children = relationship('Child', cascade='all,delete', backref='parent')\n</code></pre>\n<p>If the relationship is on the child, you do it like this:</p>\n<pre><code class=\"python\">parent = relationship('Parent', backref=backref('children', cascade='all,delete'))\n</code></pre>\n<p>Again, this is the child so you have to call a method called <code>backref</code> and putting the cascade data in there.</p>\n<p>With this in place, when you delete a parent row, SqlAlchemy will actually run delete statements for you to clean up the child rows. This will likely not be as efficient as letting this database handle if for you so I don't recommend it.</p>\n<p>Here are the <a href=\"http://docs.sqlalchemy.org/en/latest/orm/cascades.html?highlight=cascade#delete\" rel=\"noreferrer\">SqlAlchemy docs</a> on the cascading features it supports.</p>\n", "abstract": "Pretty old post, but I just spent an hour or two on this, so I wanted to share my finding, especially since some of the other comments listed aren't quite right. Give the child table a foreign or modify the existing one, adding ondelete='CASCADE': And one of the following relationships: a) This on the parent table: b) Or this on the child table: First off, despite what the accepted answer says, the parent/child relationship is not established by using relationship, it's established by using ForeignKey. You can put the relationship on either the parent or child tables and it will work fine. Although, apparently on the child tables, you have to use the backref function in addition to the keyword argument. Second, SqlAlchemy supports two different kinds of cascading. The first, and the one I recommend, is built into your database and usually takes the form of a constraint on the foreign key declaration. In PostgreSQL it looks like this: This means that when you delete a record from parent_table, then all the corresponding rows in child_table will be deleted for you by the database. It's fast and reliable and probably your best bet. You set this up in SqlAlchemy through ForeignKey like this (part of the child table definition): The ondelete='CASCADE' is the part that creates the ON DELETE CASCADE on the table. There's an important caveat here. Notice how I have a relationship specified with passive_deletes=True? If you don't have that, the entire thing will not work. This is because by default when you delete a parent record SqlAlchemy does something really weird. It sets the foreign keys of all child rows to NULL. So if you delete a row from parent_table where id = 5, then it will basically execute Why you would want this I have no idea. I'd be surprised if many database engines even allowed you to set a valid foreign key to NULL, creating an orphan. Seems like a bad idea, but maybe there's a use case. Anyway, if you let SqlAlchemy do this, you will prevent the database from being able to clean up the children using the ON DELETE CASCADE that you set up. This is because it relies on those foreign keys to know which child rows to delete. Once SqlAlchemy has set them all to NULL, the database can't delete them. Setting the passive_deletes=True prevents SqlAlchemy from NULLing out the foreign keys. You can read more about passive deletes in the SqlAlchemy docs. The other way you can do it is to let SqlAlchemy do it for you. This is set up using the cascade argument of the relationship. If you have the relationship defined on the parent table, it looks like this: If the relationship is on the child, you do it like this: Again, this is the child so you have to call a method called backref and putting the cascade data in there. With this in place, when you delete a parent row, SqlAlchemy will actually run delete statements for you to clean up the child rows. This will likely not be as efficient as letting this database handle if for you so I don't recommend it. Here are the SqlAlchemy docs on the cascading features it supports."}, {"id": 62327279, "score": 8, "vote": 0, "content": "<p>Alex Okrushko answer almost worked best for me. Used ondelete='CASCADE' and passive_deletes=True combined. But I had to do something extra to make it work for sqlite.</p>\n<pre><code class=\"python\">Base = declarative_base()\nROOM_TABLE = \"roomdata\"\nFURNITURE_TABLE = \"furnituredata\"\n\nclass DBFurniture(Base):\n    __tablename__ = FURNITURE_TABLE\n    id = Column(Integer, primary_key=True)\n    room_id = Column(Integer, ForeignKey('roomdata.id', ondelete='CASCADE'))\n\n\nclass DBRoom(Base):\n    __tablename__ = ROOM_TABLE\n    id = Column(Integer, primary_key=True)\n    furniture = relationship(\"DBFurniture\", backref=\"room\", passive_deletes=True)\n</code></pre>\n<p>Make sure to add this code to ensure it works for sqlite.</p>\n<pre><code class=\"python\">from sqlalchemy import event\nfrom sqlalchemy.engine import Engine\nfrom sqlite3 import Connection as SQLite3Connection\n\n@event.listens_for(Engine, \"connect\")\ndef _set_sqlite_pragma(dbapi_connection, connection_record):\n    if isinstance(dbapi_connection, SQLite3Connection):\n        cursor = dbapi_connection.cursor()\n        cursor.execute(\"PRAGMA foreign_keys=ON;\")\n        cursor.close()\n</code></pre>\n<p>Stolen from here: <a href=\"https://stackoverflow.com/questions/57726047/sqlalchemy-expression-language-and-sqlites-on-delete-cascade\">SQLAlchemy expression language and SQLite's on delete cascade</a></p>\n", "abstract": "Alex Okrushko answer almost worked best for me. Used ondelete='CASCADE' and passive_deletes=True combined. But I had to do something extra to make it work for sqlite. Make sure to add this code to ensure it works for sqlite. Stolen from here: SQLAlchemy expression language and SQLite's on delete cascade"}, {"id": 16131906, "score": 7, "vote": 0, "content": "<p>Steven is correct in that you need to explicitly create the backref, this results in the cascade being applied on the parent (as opposed to it being applied to the child like in the test scenario).</p>\n<p>However, defining the relationship on the Child does NOT make sqlalchemy consider Child the parent. It doesn't matter where the relationship is defined (child or parent), its the foreign key that links the two tables that determines which is the parent and which is the child.</p>\n<p>It makes sense to stick to one convention though, and based on Steven's response, I'm defining all my child relationships on the parent.</p>\n", "abstract": "Steven is correct in that you need to explicitly create the backref, this results in the cascade being applied on the parent (as opposed to it being applied to the child like in the test scenario). However, defining the relationship on the Child does NOT make sqlalchemy consider Child the parent. It doesn't matter where the relationship is defined (child or parent), its the foreign key that links the two tables that determines which is the parent and which is the child. It makes sense to stick to one convention though, and based on Steven's response, I'm defining all my child relationships on the parent."}, {"id": 46458115, "score": 7, "vote": 0, "content": "<p>Steven's answer is solid.  I'd like to point out an additional implication.</p>\n<p>By using <code>relationship</code>, you're making the app layer (Flask) responsible for referential integrity. That means other processes that access the database not through Flask, like a database utility or a person connecting to the database directly, will not experience those constraints and could change your data in a way that breaks the logical data model you worked so hard to design.</p>\n<p>Whenever possible, use the <code>ForeignKey</code> approach described by d512 and Alex. The DB engine is very good at truly enforcing constraints (in an unavoidable way), so this is by far the best strategy for maintaining data integrity.  The only time you need to rely on an app to handle data integrity is when the database can't handle them, e.g. versions of SQLite that don't support foreign keys.</p>\n<p>If you need to create further linkage among entities to enable app behaviors like navigating parent-child object relationships, use <code>backref</code> in conjunction with <code>ForeignKey</code>. </p>\n", "abstract": "Steven's answer is solid.  I'd like to point out an additional implication. By using relationship, you're making the app layer (Flask) responsible for referential integrity. That means other processes that access the database not through Flask, like a database utility or a person connecting to the database directly, will not experience those constraints and could change your data in a way that breaks the logical data model you worked so hard to design. Whenever possible, use the ForeignKey approach described by d512 and Alex. The DB engine is very good at truly enforcing constraints (in an unavoidable way), so this is by far the best strategy for maintaining data integrity.  The only time you need to rely on an app to handle data integrity is when the database can't handle them, e.g. versions of SQLite that don't support foreign keys. If you need to create further linkage among entities to enable app behaviors like navigating parent-child object relationships, use backref in conjunction with ForeignKey. "}, {"id": 6226404, "score": 6, "vote": 0, "content": "<p>I struggled with the documentation as well, but found that the docstrings themselves tend to be easier than the manual.  For example, if you import relationship from sqlalchemy.orm and do help(relationship), it will give you all the options you can specify for cascade.  The bullet for <code>delete-orphan</code> says:</p>\n<blockquote>\n<p>if an item of the child's type with no parent is detected, mark it for deletion.<br/>\n  Note that this option prevents a pending item of the child's class from being\n  persisted without a parent present.</p>\n</blockquote>\n<p>I realize your issue was more with the way the documentation for defining parent-child relationships.  But it seemed that you might also be having a problem with the cascade options, because <code>\"all\"</code> includes <code>\"delete\"</code>.  <code>\"delete-orphan\"</code> is the only option that's not included in <code>\"all\"</code>.</p>\n", "abstract": "I struggled with the documentation as well, but found that the docstrings themselves tend to be easier than the manual.  For example, if you import relationship from sqlalchemy.orm and do help(relationship), it will give you all the options you can specify for cascade.  The bullet for delete-orphan says: if an item of the child's type with no parent is detected, mark it for deletion.\n  Note that this option prevents a pending item of the child's class from being\n  persisted without a parent present. I realize your issue was more with the way the documentation for defining parent-child relationships.  But it seemed that you might also be having a problem with the cascade options, because \"all\" includes \"delete\".  \"delete-orphan\" is the only option that's not included in \"all\"."}, {"id": 68144359, "score": 3, "vote": 0, "content": "<p>Even tho this question is very old, it comes up first when searched for in Google so I'll post my solution to add up to what others said (I've spent few hours even after reading all the answers in here).</p>\n<p>As d512 explained, it is all about Foreign Keys. It was quite a surprise to me but not all databases / engines support Foreign Keys. I'm running a MySQL database. After long investigation, I noticed that when I create new table it defaults to an engine (MyISAM) that doesn't support Foreign Keys. All I had to do was to set it to InnoDB by adding <code>mysql_engine='InnoDB'</code> when defining a Table. In my project I'm using an imperative mapping and it looks like so:</p>\n<pre><code class=\"python\">db.Table('child',\n    Column('id', Integer, primary_key=True),\n    # other columns\n    Column('parent_id',\n           ForeignKey('parent.id', ondelete=\"CASCADE\")),\n    mysql_engine='InnoDB')\n</code></pre>\n", "abstract": "Even tho this question is very old, it comes up first when searched for in Google so I'll post my solution to add up to what others said (I've spent few hours even after reading all the answers in here). As d512 explained, it is all about Foreign Keys. It was quite a surprise to me but not all databases / engines support Foreign Keys. I'm running a MySQL database. After long investigation, I noticed that when I create new table it defaults to an engine (MyISAM) that doesn't support Foreign Keys. All I had to do was to set it to InnoDB by adding mysql_engine='InnoDB' when defining a Table. In my project I'm using an imperative mapping and it looks like so:"}, {"id": 56694733, "score": 2, "vote": 0, "content": "<p>Answer by Stevan is perfect. But if you are still getting the error. Other possible try on top of that would be -</p>\n<p><a href=\"http://vincentaudebert.github.io/python/sql/2015/10/09/cascade-delete-sqlalchemy/\" rel=\"nofollow noreferrer\">http://vincentaudebert.github.io/python/sql/2015/10/09/cascade-delete-sqlalchemy/</a></p>\n<p>Copied from the link-</p>\n<p>Quick tip if you get in trouble with a foreign key dependency even if you have specified a cascade delete in your models.</p>\n<p>Using SQLAlchemy, to specify a cascade delete you should have <code>cascade='all, delete'</code> on your parent table. Ok but then when you execute something like:</p>\n<pre><code class=\"python\">session.query(models.yourmodule.YourParentTable).filter(conditions).delete()\n</code></pre>\n<p>It actually triggers an error about a foreign key used in your children tables.</p>\n<p>The solution I used it to query the object and then delete it:</p>\n<pre><code class=\"python\">session = models.DBSession()\nyour_db_object = session.query(models.yourmodule.YourParentTable).filter(conditions).first()\nif your_db_object is not None:\n    session.delete(your_db_object)\n</code></pre>\n<p>This should delete your parent record AND all the children associated with it.</p>\n", "abstract": "Answer by Stevan is perfect. But if you are still getting the error. Other possible try on top of that would be - http://vincentaudebert.github.io/python/sql/2015/10/09/cascade-delete-sqlalchemy/ Copied from the link- Quick tip if you get in trouble with a foreign key dependency even if you have specified a cascade delete in your models. Using SQLAlchemy, to specify a cascade delete you should have cascade='all, delete' on your parent table. Ok but then when you execute something like: It actually triggers an error about a foreign key used in your children tables. The solution I used it to query the object and then delete it: This should delete your parent record AND all the children associated with it."}, {"id": 58439224, "score": 1, "vote": 0, "content": "<p><strong>TLDR:</strong> If the above solutions don't work, try adding nullable=False to your column.</p>\n<p>I'd like to add a small point here for some people who may not get the cascade function to work with the existing solutions (which are great). The main difference between my work and the example was that I used automap. I do not know exactly how that might interfere with the setup of cascades, but I want to note that I used it. I am also working with a SQLite database. </p>\n<p>I tried every solution described here, but rows in my child table continued to have their foreign key set to null when the parent row was deleted. I'd tried all the solutions here to no avail. However, the cascade worked once I set the child column with the foreign key to nullable = False.</p>\n<p>On the child table, I added:</p>\n<pre><code class=\"python\">Column('parent_id', Integer(), ForeignKey('parent.id', ondelete=\"CASCADE\"), nullable=False)\nChild.parent = relationship(\"parent\", backref=backref(\"children\", passive_deletes=True)\n</code></pre>\n<p>With this setup, the cascade functioned as expected. </p>\n", "abstract": "TLDR: If the above solutions don't work, try adding nullable=False to your column. I'd like to add a small point here for some people who may not get the cascade function to work with the existing solutions (which are great). The main difference between my work and the example was that I used automap. I do not know exactly how that might interfere with the setup of cascades, but I want to note that I used it. I am also working with a SQLite database.  I tried every solution described here, but rows in my child table continued to have their foreign key set to null when the parent row was deleted. I'd tried all the solutions here to no avail. However, the cascade worked once I set the child column with the foreign key to nullable = False. On the child table, I added: With this setup, the cascade functioned as expected. "}]}, {"link": "https://stackoverflow.com/questions/25648393/how-to-move-a-model-between-two-django-apps-django-1-7", "question": {"id": "25648393", "title": "How to move a model between two Django apps (Django 1.7)", "content": "<p>So about a year ago I started a project and like all new developers I didn't really focus too much on the structure, however now I am further along with Django it has started to appear that my project layout mainly my models are horrible in structure.</p>\n<p>I have models mainly held in a single app and really most of these models should be in their own individual apps, I did try and resolve this and move them with south however I found it tricky and really difficult due to foreign keys ect.</p>\n<p>However due to Django 1.7 and built in support for migrations is there a better way to do this now?</p>\n", "abstract": "So about a year ago I started a project and like all new developers I didn't really focus too much on the structure, however now I am further along with Django it has started to appear that my project layout mainly my models are horrible in structure. I have models mainly held in a single app and really most of these models should be in their own individual apps, I did try and resolve this and move them with south however I found it tricky and really difficult due to foreign keys ect. However due to Django 1.7 and built in support for migrations is there a better way to do this now?"}, "answers": [{"id": 26472482, "score": 365, "vote": 0, "content": "<p>This can be done fairly easily using <code>migrations.SeparateDatabaseAndState</code>. Basically, we use a database operation to rename the table concurrently with two state operations to remove the model from one app's history and create it in another's.</p>\n<h2>Remove from old app</h2>\n<pre><code class=\"python\">python manage.py makemigrations old_app --empty\n</code></pre>\n<p>In the migration:</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    dependencies = []\n\n    database_operations = [\n        migrations.AlterModelTable('TheModel', 'newapp_themodel')\n    ]\n\n    state_operations = [\n        migrations.DeleteModel('TheModel')\n    ]\n\n    operations = [\n        migrations.SeparateDatabaseAndState(\n            database_operations=database_operations,\n            state_operations=state_operations)\n    ]\n</code></pre>\n<h2>Add to new app</h2>\n<p>First, copy the model to the new app's model.py, then:</p>\n<pre><code class=\"python\">python manage.py makemigrations new_app\n</code></pre>\n<p>This will generate a migration with a naive <code>CreateModel</code> operation as the sole operation. Wrap that in a <code>SeparateDatabaseAndState</code> operation such that we don't try to recreate the table. Also include the prior migration as a dependency:</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    dependencies = [\n        ('old_app', 'above_migration')\n    ]\n\n    state_operations = [\n        migrations.CreateModel(\n            name='TheModel',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n            ],\n            options={\n                'db_table': 'newapp_themodel',\n            },\n            bases=(models.Model,),\n        )\n    ]\n\n    operations = [\n        migrations.SeparateDatabaseAndState(state_operations=state_operations)\n    ]\n</code></pre>\n", "abstract": "This can be done fairly easily using migrations.SeparateDatabaseAndState. Basically, we use a database operation to rename the table concurrently with two state operations to remove the model from one app's history and create it in another's. In the migration: First, copy the model to the new app's model.py, then: This will generate a migration with a naive CreateModel operation as the sole operation. Wrap that in a SeparateDatabaseAndState operation such that we don't try to recreate the table. Also include the prior migration as a dependency:"}, {"id": 25652003, "score": 29, "vote": 0, "content": "<p>I am removing the old answer as may result in data loss. As <a href=\"https://stackoverflow.com/a/26472482/2698552\">ozan mentioned</a>, we can create 2 migrations one in each app. The comments below this post refer to my old answer.</p>\n<p>First migration to remove model from 1st app.</p>\n<pre><code class=\"python\">$ python manage.py makemigrations old_app --empty\n</code></pre>\n<p>Edit migration file to include these operations.</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    database_operations = [migrations.AlterModelTable('TheModel', 'newapp_themodel')]\n\n    state_operations = [migrations.DeleteModel('TheModel')]\n\n    operations = [\n      migrations.SeparateDatabaseAndState(\n        database_operations=database_operations,\n        state_operations=state_operations)\n    ]\n</code></pre>\n<p>Second migration which depends on first migration and create the new table in 2nd app. After moving model code to 2nd app</p>\n<pre><code class=\"python\">$ python manage.py makemigrations new_app \n</code></pre>\n<p>and edit migration file to something like this.</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    dependencies = [\n        ('old_app', 'above_migration')\n    ]\n\n    state_operations = [\n        migrations.CreateModel(\n            name='TheModel',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n            ],\n            options={\n                'db_table': 'newapp_themodel',\n            },\n            bases=(models.Model,),\n        )\n    ]\n\n    operations = [\n        migrations.SeparateDatabaseAndState(state_operations=state_operations)\n    ]\n</code></pre>\n", "abstract": "I am removing the old answer as may result in data loss. As ozan mentioned, we can create 2 migrations one in each app. The comments below this post refer to my old answer. First migration to remove model from 1st app. Edit migration file to include these operations. Second migration which depends on first migration and create the new table in 2nd app. After moving model code to 2nd app and edit migration file to something like this."}, {"id": 29622570, "score": 28, "vote": 0, "content": "<p>I encountered the same problem.\n<a href=\"https://stackoverflow.com/a/26472482/2698552\">Ozan's answer</a> helped me a lot but unfortunately was not enough. Indeed I had several ForeignKey linking to the model I wanted to move. After some headache I found the solution so decided to post it to solve people time.</p>\n<p>You need 2 more steps:</p>\n<ol>\n<li>Before doing anything, change all your <code>ForeignKey</code> linking to <code>TheModel</code> into <code>Integerfield</code>. Then run <code>python manage.py makemigrations</code></li>\n<li>After doing Ozan's steps, re-convert your foreign keys: put back <code>ForeignKey(TheModel)</code>instead of <code>IntegerField()</code>. Then make the migrations again (<code>python manage.py makemigrations</code>). You can then migrate and it should work (<code>python manage.py migrate</code>)</li>\n</ol>\n<p>Hope it helps. Of course test it in local before trying in production to avoid bad suprises :)</p>\n", "abstract": "I encountered the same problem.\nOzan's answer helped me a lot but unfortunately was not enough. Indeed I had several ForeignKey linking to the model I wanted to move. After some headache I found the solution so decided to post it to solve people time. You need 2 more steps: Hope it helps. Of course test it in local before trying in production to avoid bad suprises :)"}, {"id": 30784483, "score": 15, "vote": 0, "content": "<p>How I did it (tested on Django==1.8, with postgres, so probably also 1.7)</p>\n<p>Situation</p>\n<p><strong>app1.YourModel</strong></p>\n<p>but you want it to go to:\n<strong>app2.YourModel</strong></p>\n<ol>\n<li>Copy YourModel (the code) from app1 to app2.</li>\n<li><p>add this to app2.YourModel:</p>\n<pre><code class=\"python\">Class Meta:\n    db_table = 'app1_yourmodel'\n</code></pre></li>\n<li><p>$ python manage.py makemigrations app2</p></li>\n<li><p>A new migration (e.g. 0009_auto_something.py) is made in app2 with a migrations.CreateModel() statement, move this statement to the initial migration of app2 (e.g. 0001_initial.py) (it will be just like it always have been there). And now remove the created migration = 0009_auto_something.py</p></li>\n<li><p>Just as you act, like app2.YourModel always has been there, now remove the existence of app1.YourModel from your migrations. Meaning: comment out the CreateModel statements, and every adjustment or datamigration you used after that.</p></li>\n<li><p>And of course, every reference to app1.YourModel has to be changed to app2.YourModel through your project. Also, don't forget that all possible foreign keys to app1.YourModel in migrations have to be changed to app2.YourModel</p></li>\n<li><p>Now if you do $ python manage.py migrate, nothing has changed, also when you do $ python manage.py makemigrations, nothing new has been detected.</p></li>\n<li><p>Now the finishing touch: remove the Class Meta from app2.YourModel and do $ python manage.py makemigrations app2 &amp;&amp; python manage.py migrate app2 (if you look into this migration you'll see something like this:)</p>\n<pre><code class=\"python\">    migrations.AlterModelTable(\n    name='yourmodel',\n    table=None,\n),\n</code></pre></li>\n</ol>\n<p>table=None, means it will take the default table-name, which in this case will be app2_yourmodel.</p>\n<ol start=\"9\">\n<li>DONE, with data saved.</li>\n</ol>\n<p>P.S during the migration it will see that that content_type app1.yourmodel has been removed and can be deleted. You can say yes to that but only if you don't use it. In case you heavily depend on it to have FKs to that content-type be intact, don't answer yes or no yet, but go into the db that time manually, and remove the contentype app2.yourmodel, and rename the contenttype app1.yourmodel to app2.yourmodel, and then continue by answering no.</p>\n", "abstract": "How I did it (tested on Django==1.8, with postgres, so probably also 1.7) Situation app1.YourModel but you want it to go to:\napp2.YourModel add this to app2.YourModel: $ python manage.py makemigrations app2 A new migration (e.g. 0009_auto_something.py) is made in app2 with a migrations.CreateModel() statement, move this statement to the initial migration of app2 (e.g. 0001_initial.py) (it will be just like it always have been there). And now remove the created migration = 0009_auto_something.py Just as you act, like app2.YourModel always has been there, now remove the existence of app1.YourModel from your migrations. Meaning: comment out the CreateModel statements, and every adjustment or datamigration you used after that. And of course, every reference to app1.YourModel has to be changed to app2.YourModel through your project. Also, don't forget that all possible foreign keys to app1.YourModel in migrations have to be changed to app2.YourModel Now if you do $ python manage.py migrate, nothing has changed, also when you do $ python manage.py makemigrations, nothing new has been detected. Now the finishing touch: remove the Class Meta from app2.YourModel and do $ python manage.py makemigrations app2 && python manage.py migrate app2 (if you look into this migration you'll see something like this:) table=None, means it will take the default table-name, which in this case will be app2_yourmodel. P.S during the migration it will see that that content_type app1.yourmodel has been removed and can be deleted. You can say yes to that but only if you don't use it. In case you heavily depend on it to have FKs to that content-type be intact, don't answer yes or no yet, but go into the db that time manually, and remove the contentype app2.yourmodel, and rename the contenttype app1.yourmodel to app2.yourmodel, and then continue by answering no."}, {"id": 48998655, "score": 15, "vote": 0, "content": "<p>I get nervous hand-coding migrations (as is required by <a href=\"https://stackoverflow.com/a/26472482/1978687\">Ozan's</a> answer) so the following combines Ozan's and <a href=\"https://stackoverflow.com/a/30784483/1978687\">Michael's</a> strategies to minimize the amount of hand-coding required:</p>\n<ol>\n<li>Before moving any models, make sure you're working with a clean baseline by running <code>makemigrations</code>.</li>\n<li>Move the code for the Model from <code>app1</code> to <code>app2</code></li>\n<li><p>As recommended by @Michael, we point the new model to the old database table using the <code>db_table</code> Meta option on the \"new\" model:</p>\n<pre><code class=\"python\">class Meta:\n    db_table = 'app1_yourmodel'\n</code></pre></li>\n<li><p>Run <code>makemigrations</code>.  This will generate <code>CreateModel</code> in <code>app2</code> and <code>DeleteModel</code> in <code>app1</code>.  Technically, these migrations refer to the exact same table and would remove (including all data) and re-create the table.</p></li>\n<li><p>In reality, we don't want (or need) to do anything to the table.  We just need Django to believe that the change has been made.  Per @Ozan's answer, the <code>state_operations</code> flag in <code>SeparateDatabaseAndState</code> does this.  So we wrap all of the <code>migrations</code> entries <strong>IN BOTH MIGRATIONS FILES</strong> with <code>SeparateDatabaseAndState(state_operations=[...])</code>.  For example,</p>\n<pre><code class=\"python\">operations = [\n    ...\n    migrations.DeleteModel(\n        name='YourModel',\n    ),\n    ...\n]\n</code></pre>\n<p>becomes</p>\n<pre><code class=\"python\">operations = [\n    migrations.SeparateDatabaseAndState(state_operations=[\n        ...\n        migrations.DeleteModel(\n            name='YourModel',\n        ),\n        ...\n    ])\n]\n</code></pre></li>\n<li><p>You also need to make sure the new \"virtual\" <code>CreateModel</code> migration depends on any migration that <strong>actually created or altered the original table</strong>.  For example, if your new migrations are <code>app2.migrations.0004_auto_&lt;date&gt;</code> (for the <code>Create</code>) and <code>app1.migrations.0007_auto_&lt;date&gt;</code> (for the <code>Delete</code>), the simplest thing to do is:</p>\n<ul>\n<li>Open <code>app1.migrations.0007_auto_&lt;date&gt;</code> and copy its <code>app1</code> dependency (e.g.  <code>('app1', '0006...'),</code>).  This is the \"immediately prior\" migration in <code>app1</code> and should include dependencies on all of the actual model building logic.</li>\n<li>Open  <code>app2.migrations.0004_auto_&lt;date&gt;</code> and add the dependency you just copied to its <code>dependencies</code> list.</li>\n</ul></li>\n</ol>\n<p>If you have <code>ForeignKey</code> relationship(s) to the model you're moving, the above may not work.  This happens because:</p>\n<ul>\n<li>Dependencies are not automatically created for the <code>ForeignKey</code> changes</li>\n<li>We do not want to wrap the <code>ForeignKey</code> changes in <code>state_operations</code> so we need to ensure they are separate from the table operations.</li>\n</ul>\n<p><strong><em>NOTE: Django 2.2 added a warning (<code>models.E028</code>) that breaks this method.  You may be able to work around it with <code>managed=False</code> but I have not tested it.</em></strong></p>\n<p>The \"minimum\" set of operations differ depending on the situation, but the following procedure should work for most/all <code>ForeignKey</code> migrations:</p>\n<ol>\n<li><strong>COPY</strong> the model from <code>app1</code> to <code>app2</code>, set <code>db_table</code>, but DON'T change any FK references.</li>\n<li>Run <code>makemigrations</code> and wrap all <code>app2</code> migration in <code>state_operations</code> (see above)\n\n<ul>\n<li>As above, add a dependency in the <code>app2</code> <code>CreateTable</code> to the latest <code>app1</code> migration</li>\n</ul></li>\n<li>Point all of the FK references to the new model.  If you aren't using string references, move the old model to the bottom of <code>models.py</code> (DON'T remove it) so it doesn't compete with the imported class.</li>\n<li><p>Run <code>makemigrations</code> but DON'T wrap anything in <code>state_operations</code> (the FK changes should actually happen).  Add a dependency in all the <code>ForeignKey</code> migrations (i.e. <code>AlterField</code>) to the <code>CreateTable</code> migration in <code>app2</code> (you'll need this list for the next step so keep track of them).  For example:</p>\n<ul>\n<li>Find the migration that includes the <code>CreateModel</code> e.g. <code>app2.migrations.0002_auto_&lt;date&gt;</code> and copy the name of that migration.</li>\n<li><p>Find all migrations that have a ForeignKey to that model (e.g. by searching <code>app2.YourModel</code> to find migrations like:</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    dependencies = [\n        ('otherapp', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='relatedmodel',\n            name='fieldname',\n            field=models.ForeignKey(... to='app2.YourModel'),\n        ),\n    ]\n</code></pre></li>\n<li><p>Add the <code>CreateModel</code> migration as as a dependency:</p>\n<pre><code class=\"python\">class Migration(migrations.Migration):\n\n    dependencies = [\n        ('otherapp', '0001_initial'),\n        ('app2', '0002_auto_&lt;date&gt;'),\n    ]  \n</code></pre></li>\n</ul></li>\n<li><p>Remove the models from <code>app1</code></p></li>\n<li>Run <code>makemigrations</code> and wrap the <code>app1</code> migration in <code>state_operations</code>.\n\n<ul>\n<li>Add a dependency to all of the <code>ForeignKey</code> migrations (i.e. <code>AlterField</code>) from the previous step (may include migrations in <code>app1</code> and <code>app2</code>).</li>\n<li>When I built these migrations, the <code>DeleteTable</code> already depended on the <code>AlterField</code> migrations so I didn't need to manually enforce it (i.e. <code>Alter</code> before <code>Delete</code>).</li>\n</ul></li>\n</ol>\n<p>At this point, Django is good to go.  The new model points to the old table and Django's migrations have convinced it that everything has been relocated appropriately.  The big caveat (from @Michael's answer) is that a new <code>ContentType</code> is created for the new model.  If you link (e.g. by <code>ForeignKey</code>) to content types, you'll need to create a migration to update the <code>ContentType</code> table.</p>\n<p>I wanted to cleanup after myself (Meta options and table names) so I used the following procedure (from @Michael):</p>\n<ol>\n<li>Remove the <code>db_table</code> Meta entry</li>\n<li>Run <code>makemigrations</code> again to generate the database rename</li>\n<li>Edit this last migration and make sure it depends on the <code>DeleteTable</code> migration.  It doesn't seem like it should be necessary as the <code>Delete</code> should be purely logical, but I've run into errors (e.g. <code>app1_yourmodel</code> doesn't exist) if I don't.</li>\n</ol>\n", "abstract": "I get nervous hand-coding migrations (as is required by Ozan's answer) so the following combines Ozan's and Michael's strategies to minimize the amount of hand-coding required: As recommended by @Michael, we point the new model to the old database table using the db_table Meta option on the \"new\" model: Run makemigrations.  This will generate CreateModel in app2 and DeleteModel in app1.  Technically, these migrations refer to the exact same table and would remove (including all data) and re-create the table. In reality, we don't want (or need) to do anything to the table.  We just need Django to believe that the change has been made.  Per @Ozan's answer, the state_operations flag in SeparateDatabaseAndState does this.  So we wrap all of the migrations entries IN BOTH MIGRATIONS FILES with SeparateDatabaseAndState(state_operations=[...]).  For example, becomes You also need to make sure the new \"virtual\" CreateModel migration depends on any migration that actually created or altered the original table.  For example, if your new migrations are app2.migrations.0004_auto_<date> (for the Create) and app1.migrations.0007_auto_<date> (for the Delete), the simplest thing to do is: If you have ForeignKey relationship(s) to the model you're moving, the above may not work.  This happens because: NOTE: Django 2.2 added a warning (models.E028) that breaks this method.  You may be able to work around it with managed=False but I have not tested it. The \"minimum\" set of operations differ depending on the situation, but the following procedure should work for most/all ForeignKey migrations: Run makemigrations but DON'T wrap anything in state_operations (the FK changes should actually happen).  Add a dependency in all the ForeignKey migrations (i.e. AlterField) to the CreateTable migration in app2 (you'll need this list for the next step so keep track of them).  For example: Find all migrations that have a ForeignKey to that model (e.g. by searching app2.YourModel to find migrations like: Add the CreateModel migration as as a dependency: Remove the models from app1 At this point, Django is good to go.  The new model points to the old table and Django's migrations have convinced it that everything has been relocated appropriately.  The big caveat (from @Michael's answer) is that a new ContentType is created for the new model.  If you link (e.g. by ForeignKey) to content types, you'll need to create a migration to update the ContentType table. I wanted to cleanup after myself (Meta options and table names) so I used the following procedure (from @Michael):"}, {"id": 44645691, "score": 3, "vote": 0, "content": "<p>Another hacky alternative if the data is not big or too complicated, but still important to maintain, is to:</p>\n<ul>\n<li>Get data fixtures using <a href=\"https://docs.djangoproject.com/en/1.11/ref/django-admin/#dumpdata\" rel=\"nofollow noreferrer\">manage.py dumpdata</a></li>\n<li>Proceed to model changes and migrations properly, without relating the changes</li>\n<li>Global replace the fixtures from the old model and app names to the new</li>\n<li>Load data using <a href=\"https://docs.djangoproject.com/en/1.11/ref/django-admin/#loaddata\" rel=\"nofollow noreferrer\">manage.py loaddata</a></li>\n</ul>\n", "abstract": "Another hacky alternative if the data is not big or too complicated, but still important to maintain, is to:"}, {"id": 26340949, "score": 1, "vote": 0, "content": "<p>You can try the following (untested):</p>\n<ol>\n<li>move the model from <code>src_app</code> to <code>dest_app</code></li>\n<li>migrate <code>dest_app</code>; make sure the schema migration depends on the latest <code>src_app</code> migration (<a href=\"https://docs.djangoproject.com/en/dev/topics/migrations/#migration-files\" rel=\"nofollow\">https://docs.djangoproject.com/en/dev/topics/migrations/#migration-files</a>)</li>\n<li>add a data migration to <code>dest_app</code>, that copies all data from <code>src_app</code></li>\n<li>migrate <code>src_app</code>; make sure the schema migration depends on the latest (data) migration of <code>dest_app</code> -- that is: the migration of step 3</li>\n</ol>\n<p>Note that you will be <em>copying</em> the whole table, instead of <em>moving</em> it, but that way both apps don't have to touch a table that belongs to the other app, which I think is more important.</p>\n", "abstract": "You can try the following (untested): Note that you will be copying the whole table, instead of moving it, but that way both apps don't have to touch a table that belongs to the other app, which I think is more important."}, {"id": 47393073, "score": 1, "vote": 0, "content": "<p>Copied from my answer at <a href=\"https://stackoverflow.com/a/47392970/8971048\">https://stackoverflow.com/a/47392970/8971048</a></p>\n<p>In case you need to move the model and you don't have access to the app anymore (or you don't want the access), you can create a new Operation and consider to create a new model only if the migrated model does not exist.</p>\n<p>In this example I am passing 'MyModel' from old_app to myapp. </p>\n<pre><code class=\"python\">class MigrateOrCreateTable(migrations.CreateModel):\n    def __init__(self, source_table, dst_table, *args, **kwargs):\n        super(MigrateOrCreateTable, self).__init__(*args, **kwargs)\n        self.source_table = source_table\n        self.dst_table = dst_table\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        table_exists = self.source_table in schema_editor.connection.introspection.table_names()\n        if table_exists:\n            with schema_editor.connection.cursor() as cursor:\n                cursor.execute(\"RENAME TABLE {} TO {};\".format(self.source_table, self.dst_table))\n        else:\n            return super(MigrateOrCreateTable, self).database_forwards(app_label, schema_editor, from_state, to_state)\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('myapp', '0002_some_migration'),\n    ]\n\n    operations = [\n        MigrateOrCreateTable(\n            source_table='old_app_mymodel',\n            dst_table='myapp_mymodel',\n            name='MyModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=18))\n            ],\n        ),\n    ]\n</code></pre>\n", "abstract": "Copied from my answer at https://stackoverflow.com/a/47392970/8971048 In case you need to move the model and you don't have access to the app anymore (or you don't want the access), you can create a new Operation and consider to create a new model only if the migrated model does not exist. In this example I am passing 'MyModel' from old_app to myapp. "}, {"id": 25808797, "score": 0, "vote": 0, "content": "<p><strong>This is tested roughly, so do not forget to backup your DB!!!</strong> </p>\n<p>For example, there are two apps: <code>src_app</code> and <code>dst_app</code>, we want to move model <code>MoveMe</code> from <code>src_app</code> to <code>dst_app</code>.</p>\n<p>Create empty migrations for both apps:</p>\n<pre><code class=\"python\">python manage.py makemigrations --empty src_app\npython manage.py makemigrations --empty dst_app\n</code></pre>\n<p>Let's assume, that new migrations are <code>XXX1_src_app_new</code> and <code>XXX1_dst_app_new</code>, previuos top migrations are <code>XXX0_src_app_old</code> and <code>XXX0_dst_app_old</code>.</p>\n<p>Add an operation that renames table for <code>MoveMe</code> model and renames its app_label in ProjectState to <code>XXX1_dst_app_new</code>. Do not forget to add dependency on <code>XXX0_src_app_old</code> migration. The resulting <code>XXX1_dst_app_new</code> migration is:</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\"># -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n# this operations is almost the same as RenameModel\n# https://github.com/django/django/blob/1.7/django/db/migrations/operations/models.py#L104\nclass MoveModelFromOtherApp(migrations.operations.base.Operation):\n\n    def __init__(self, name, old_app_label):\n        self.name = name\n        self.old_app_label = old_app_label\n\n    def state_forwards(self, app_label, state):\n\n        # Get all of the related objects we need to repoint\n        apps = state.render(skip_cache=True)\n        model = apps.get_model(self.old_app_label, self.name)\n        related_objects = model._meta.get_all_related_objects()\n        related_m2m_objects = model._meta.get_all_related_many_to_many_objects()\n        # Rename the model\n        state.models[app_label, self.name.lower()] = state.models.pop(\n            (self.old_app_label, self.name.lower())\n        )\n        state.models[app_label, self.name.lower()].app_label = app_label\n        for model_state in state.models.values():\n            try:\n                i = model_state.bases.index(\"%s.%s\" % (self.old_app_label, self.name.lower()))\n                model_state.bases = model_state.bases[:i] + (\"%s.%s\" % (app_label, self.name.lower()),) + model_state.bases[i+1:]\n            except ValueError:\n                pass\n        # Repoint the FKs and M2Ms pointing to us\n        for related_object in (related_objects + related_m2m_objects):\n            # Use the new related key for self referential related objects.\n            if related_object.model == model:\n                related_key = (app_label, self.name.lower())\n            else:\n                related_key = (\n                    related_object.model._meta.app_label,\n                    related_object.model._meta.object_name.lower(),\n                )\n            new_fields = []\n            for name, field in state.models[related_key].fields:\n                if name == related_object.field.name:\n                    field = field.clone()\n                    field.rel.to = \"%s.%s\" % (app_label, self.name)\n                new_fields.append((name, field))\n            state.models[related_key].fields = new_fields\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        old_apps = from_state.render()\n        new_apps = to_state.render()\n        old_model = old_apps.get_model(self.old_app_label, self.name)\n        new_model = new_apps.get_model(app_label, self.name)\n        if self.allowed_to_migrate(schema_editor.connection.alias, new_model):\n            # Move the main table\n            schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            related_objects = old_model._meta.get_all_related_objects()\n            related_m2m_objects = old_model._meta.get_all_related_many_to_many_objects()\n            for related_object in (related_objects + related_m2m_objects):\n                if related_object.model == old_model:\n                    model = new_model\n                    related_key = (app_label, self.name.lower())\n                else:\n                    model = related_object.model\n                    related_key = (\n                        related_object.model._meta.app_label,\n                        related_object.model._meta.object_name.lower(),\n                    )\n                to_field = new_apps.get_model(\n                    *related_key\n                )._meta.get_field_by_name(related_object.field.name)[0]\n                schema_editor.alter_field(\n                    model,\n                    related_object.field,\n                    to_field,\n                )\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        self.old_app_label, app_label = app_label, self.old_app_label\n        self.database_forwards(app_label, schema_editor, from_state, to_state)\n        app_label, self.old_app_label = self.old_app_label, app_label\n\n    def describe(self):\n        return \"Move %s from %s\" % (self.name, self.old_app_label)\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n       ('dst_app', 'XXX0_dst_app_old'),\n       ('src_app', 'XXX0_src_app_old'),\n    ]\n\n    operations = [\n        MoveModelFromOtherApp('MoveMe', 'src_app'),\n    ]\n</code></pre>\n<p>Add dependency on <code>XXX1_dst_app_new</code> to <code>XXX1_src_app_new</code>. <code>XXX1_src_app_new</code> is no-op migration that is needed to make sure that future <code>src_app</code> migrations will be executed after <code>XXX1_dst_app_new</code>.</p>\n<p>Move <code>MoveMe</code> from <code>src_app/models.py</code> to <code>dst_app/models.py</code>. Then run:</p>\n<pre><code class=\"python\">python manage.py migrate\n</code></pre>\n<p>That's all!</p>\n", "abstract": "This is tested roughly, so do not forget to backup your DB!!!  For example, there are two apps: src_app and dst_app, we want to move model MoveMe from src_app to dst_app. Create empty migrations for both apps: Let's assume, that new migrations are XXX1_src_app_new and XXX1_dst_app_new, previuos top migrations are XXX0_src_app_old and XXX0_dst_app_old. Add an operation that renames table for MoveMe model and renames its app_label in ProjectState to XXX1_dst_app_new. Do not forget to add dependency on XXX0_src_app_old migration. The resulting XXX1_dst_app_new migration is: Add dependency on XXX1_dst_app_new to XXX1_src_app_new. XXX1_src_app_new is no-op migration that is needed to make sure that future src_app migrations will be executed after XXX1_dst_app_new. Move MoveMe from src_app/models.py to dst_app/models.py. Then run: That's all!"}, {"id": 33096296, "score": 0, "vote": 0, "content": "<p>Lets say you are moving model TheModel from app_a to app_b.</p>\n<p>An alternate solution is to alter the existing migrations by hand. The idea is that each time you see an operation altering TheModel in app_a's migrations, you copy that operation to the end of app_b's initial migration. And each time you see a reference 'app_a.TheModel' in app_a's migrations, you change it to 'app_b.TheModel'.</p>\n<p>I just did this for an existing project, where I wanted to extract a certain model to an reusable app. The procedure went smoothly. I guess things would be much harder if there were references from app_b to app_a. Also, I had a manually defined Meta.db_table for my model which might have helped.</p>\n<p>Notably you will end up with altered migration history. This doesn't matter, even if you have a database with the original migrations applied. If both the original and the rewritten migrations end up with the same database schema, then such rewrite should be OK.</p>\n", "abstract": "Lets say you are moving model TheModel from app_a to app_b. An alternate solution is to alter the existing migrations by hand. The idea is that each time you see an operation altering TheModel in app_a's migrations, you copy that operation to the end of app_b's initial migration. And each time you see a reference 'app_a.TheModel' in app_a's migrations, you change it to 'app_b.TheModel'. I just did this for an existing project, where I wanted to extract a certain model to an reusable app. The procedure went smoothly. I guess things would be much harder if there were references from app_b to app_a. Also, I had a manually defined Meta.db_table for my model which might have helped. Notably you will end up with altered migration history. This doesn't matter, even if you have a database with the original migrations applied. If both the original and the rewritten migrations end up with the same database schema, then such rewrite should be OK."}, {"id": 43198881, "score": 0, "vote": 0, "content": "<ol>\n<li>change the names of old models to \u2018model_name_old\u2019</li>\n<li>makemigrations</li>\n<li>make new models named \u2018model_name_new\u2019 with identical relationships on the related models\n(eg. user model now has user.blog_old and user.blog_new)</li>\n<li>makemigrations</li>\n<li>write a custom migration that migrates all the data to the new model tables</li>\n<li>test the hell out of these migrations by comparing backups with new db copies before and after running the migrations</li>\n<li>when all is satisfactory, delete the old models</li>\n<li>makemigrations</li>\n<li>change the new models to the correct name \u2018model_name_new\u2019 -&gt; \u2018model_name\u2019</li>\n<li>test the whole slew of migrations on a staging server</li>\n<li>take your production site down for a few minutes in order to run all migrations without users interfering</li>\n</ol>\n<p>Do this individually for each model that needs to be moved.\nI wouldn\u2019t suggest doing what the other answer says by changing to integers and back to foreign keys\nThere is a chance that new foreign keys will be different and rows may have different IDs after the migrations and I didn\u2019t want to run any risk of mismatching ids when switching back to foreign keys.</p>\n", "abstract": "Do this individually for each model that needs to be moved.\nI wouldn\u2019t suggest doing what the other answer says by changing to integers and back to foreign keys\nThere is a chance that new foreign keys will be different and rows may have different IDs after the migrations and I didn\u2019t want to run any risk of mismatching ids when switching back to foreign keys."}]}, {"link": "https://stackoverflow.com/questions/2887878/importing-a-csv-file-into-a-sqlite3-database-table-using-python", "question": {"id": "2887878", "title": "Importing a CSV file into a sqlite3 database table using Python", "content": "<p>I have a CSV file and I want to bulk-import this file into my sqlite3 database using Python. the command is \".import .....\". but it seems that it cannot work like this. Can anyone give me an example of how to do it in sqlite3? I am using windows just in case.\nThanks</p>\n", "abstract": "I have a CSV file and I want to bulk-import this file into my sqlite3 database using Python. the command is \".import .....\". but it seems that it cannot work like this. Can anyone give me an example of how to do it in sqlite3? I am using windows just in case.\nThanks"}, "answers": [{"id": 2888042, "score": 174, "vote": 0, "content": "\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import csv, sqlite3\n\ncon = sqlite3.connect(\":memory:\") # change to 'sqlite:///your_filename.db'\ncur = con.cursor()\ncur.execute(\"CREATE TABLE t (col1, col2);\") # use your column names here\n\nwith open('data.csv','r') as fin: # `with` statement available in 2.5+\n    # csv.DictReader uses first line in file for column headings by default\n    dr = csv.DictReader(fin) # comma is default delimiter\n    to_db = [(i['col1'], i['col2']) for i in dr]\n\ncur.executemany(\"INSERT INTO t (col1, col2) VALUES (?, ?);\", to_db)\ncon.commit()\ncon.close()\n</code></pre>\n", "abstract": ""}, {"id": 28802613, "score": 134, "vote": 0, "content": "<p>Creating an sqlite connection to a file on disk is left as an exercise for the reader ... but there is now a two-liner made possible by the pandas library</p>\n<pre><code class=\"python\">df = pandas.read_csv(csvfile)\ndf.to_sql(table_name, conn, if_exists='append', index=False)\n</code></pre>\n", "abstract": "Creating an sqlite connection to a file on disk is left as an exercise for the reader ... but there is now a two-liner made possible by the pandas library"}, {"id": 59671652, "score": 32, "vote": 0, "content": "<p>You're right that <code>.import</code> is the way to go, but that's a command from the SQLite3 command line program. A lot of the top answers to this question involve native python loops, but if your files are large (mine are 10^6 to 10^7 records), you want to avoid reading everything into pandas or using a native python list comprehension/loop (though I did not time them for comparison).</p>\n<p>For large files, I believe the best option is to use <code>subprocess.run()</code> to execute sqlite's import command.  In the example below, I assume the table already exists, but the csv file has headers in the first row.  See <a href=\"https://sqlite.org/cli.html#importing_csv_files\" rel=\"noreferrer\"><code>.import</code> docs</a> for more info.</p>\n<h2><code>subprocess.run()</code></h2>\n<pre><code class=\"python\">from pathlib import Path\ndb_name = Path('my.db').resolve()\ncsv_file = Path('file.csv').resolve()\nresult = subprocess.run(['sqlite3',\n                         str(db_name),\n                         '-cmd',\n                         '.mode csv',\n                         '.import --skip 1 ' + str(csv_file).replace('\\\\','\\\\\\\\')\n                                 +' &lt;table_name&gt;'],\n                        capture_output=True)\n</code></pre>\n<p><em>edit note: sqlite3's <code>.import</code> command has improved so that it can treat the first row as header names or even skip the first <em>x</em> rows (requires version &gt;=3.32, as noted in <a href=\"https://stackoverflow.com/a/61981659/534674\">this answer</a>.  If you have an older version of sqlite3, you may need to first create the table, then strip off the first row of the csv before importing.  The <code>--skip 1</code> argument will give an error prior to 3.32</em></p>\n<p><strong>Explanation</strong><br/>\nFrom the command line, the command you're looking for is <code>sqlite3 my.db -cmd \".mode csv\" \".import file.csv table\"</code>.  <code>subprocess.run()</code> runs a command line process.  The argument to <code>subprocess.run()</code> is a sequence of strings which are interpreted as a command followed by all of it's arguments.</p>\n<ul>\n<li><code>sqlite3 my.db</code> opens the database</li>\n<li><code>-cmd</code> flag after the database allows you to pass multiple follow on commands to the sqlite program.  In the shell, each command has to be in quotes, but here, they just need to be their own element of the sequence</li>\n<li><code>'.mode csv'</code> does what you'd expect</li>\n<li><code>'.import --skip 1'+str(csv_file).replace('\\\\','\\\\\\\\')+' &lt;table_name&gt;'</code> is the import command.<br/>\nUnfortunately, since subprocess passes all follow-ons to <code>-cmd</code> as quoted strings, you need to double up your backslashes if you have a windows directory path.</li>\n</ul>\n<h2>Stripping Headers</h2>\n<p>Not really the main point of the question, but here's what I used.  Again, I didn't want to read the whole files into memory at any point:</p>\n<pre><code class=\"python\">with open(csv, \"r\") as source:\n    source.readline()\n    with open(str(csv)+\"_nohead\", \"w\") as target:\n        shutil.copyfileobj(source, target)\n\n</code></pre>\n", "abstract": "You're right that .import is the way to go, but that's a command from the SQLite3 command line program. A lot of the top answers to this question involve native python loops, but if your files are large (mine are 10^6 to 10^7 records), you want to avoid reading everything into pandas or using a native python list comprehension/loop (though I did not time them for comparison). For large files, I believe the best option is to use subprocess.run() to execute sqlite's import command.  In the example below, I assume the table already exists, but the csv file has headers in the first row.  See .import docs for more info. edit note: sqlite3's .import command has improved so that it can treat the first row as header names or even skip the first x rows (requires version >=3.32, as noted in this answer.  If you have an older version of sqlite3, you may need to first create the table, then strip off the first row of the csv before importing.  The --skip 1 argument will give an error prior to 3.32 Explanation\nFrom the command line, the command you're looking for is sqlite3 my.db -cmd \".mode csv\" \".import file.csv table\".  subprocess.run() runs a command line process.  The argument to subprocess.run() is a sequence of strings which are interpreted as a command followed by all of it's arguments. Not really the main point of the question, but here's what I used.  Again, I didn't want to read the whole files into memory at any point:"}, {"id": 30734789, "score": 14, "vote": 0, "content": "<p>My 2 cents (more generic):</p>\n<pre><code class=\"python\">import csv, sqlite3\nimport logging\n\ndef _get_col_datatypes(fin):\n    dr = csv.DictReader(fin) # comma is default delimiter\n    fieldTypes = {}\n    for entry in dr:\n        feildslLeft = [f for f in dr.fieldnames if f not in fieldTypes.keys()]\n        if not feildslLeft: break # We're done\n        for field in feildslLeft:\n            data = entry[field]\n\n            # Need data to decide\n            if len(data) == 0:\n                continue\n\n            if data.isdigit():\n                fieldTypes[field] = \"INTEGER\"\n            else:\n                fieldTypes[field] = \"TEXT\"\n        # TODO: Currently there's no support for DATE in sqllite\n\n    if len(feildslLeft) &gt; 0:\n        raise Exception(\"Failed to find all the columns data types - Maybe some are empty?\")\n\n    return fieldTypes\n\n\ndef escapingGenerator(f):\n    for line in f:\n        yield line.encode(\"ascii\", \"xmlcharrefreplace\").decode(\"ascii\")\n\n\ndef csvToDb(csvFile, outputToFile = False):\n    # TODO: implement output to file\n\n    with open(csvFile,mode='r', encoding=\"ISO-8859-1\") as fin:\n        dt = _get_col_datatypes(fin)\n\n        fin.seek(0)\n\n        reader = csv.DictReader(fin)\n\n        # Keep the order of the columns name just as in the CSV\n        fields = reader.fieldnames\n        cols = []\n\n        # Set field and type\n        for f in fields:\n            cols.append(\"%s %s\" % (f, dt[f]))\n\n        # Generate create table statement:\n        stmt = \"CREATE TABLE ads (%s)\" % \",\".join(cols)\n\n        con = sqlite3.connect(\":memory:\")\n        cur = con.cursor()\n        cur.execute(stmt)\n\n        fin.seek(0)\n\n\n        reader = csv.reader(escapingGenerator(fin))\n\n        # Generate insert statement:\n        stmt = \"INSERT INTO ads VALUES(%s);\" % ','.join('?' * len(cols))\n\n        cur.executemany(stmt, reader)\n        con.commit()\n\n    return con\n</code></pre>\n", "abstract": "My 2 cents (more generic):"}, {"id": 2887897, "score": 13, "vote": 0, "content": "<p>The <code>.import</code> command is a feature of the sqlite3 command-line tool. To do it in Python, you should simply load the data using whatever facilities Python has, such as the <a href=\"http://docs.python.org/library/csv.html\" rel=\"noreferrer\">csv module</a>, and inserting the data as per usual.</p>\n<p>This way, you also have control over what types are inserted, rather than relying on sqlite3's seemingly undocumented behaviour.</p>\n", "abstract": "The .import command is a feature of the sqlite3 command-line tool. To do it in Python, you should simply load the data using whatever facilities Python has, such as the csv module, and inserting the data as per usual. This way, you also have control over what types are inserted, rather than relying on sqlite3's seemingly undocumented behaviour."}, {"id": 12432311, "score": 10, "vote": 0, "content": "<p>Many thanks for bernie's <a href=\"https://stackoverflow.com/a/2888042/12892\">answer</a>!  Had to tweak it a bit - here's what worked for me:</p>\n<pre><code class=\"python\">import csv, sqlite3\nconn = sqlite3.connect(\"pcfc.sl3\")\ncurs = conn.cursor()\ncurs.execute(\"CREATE TABLE PCFC (id INTEGER PRIMARY KEY, type INTEGER, term TEXT, definition TEXT);\")\nreader = csv.reader(open('PC.txt', 'r'), delimiter='|')\nfor row in reader:\n    to_db = [unicode(row[0], \"utf8\"), unicode(row[1], \"utf8\"), unicode(row[2], \"utf8\")]\n    curs.execute(\"INSERT INTO PCFC (type, term, definition) VALUES (?, ?, ?);\", to_db)\nconn.commit()\n</code></pre>\n<p>My text file (PC.txt) looks like this:</p>\n<pre><code class=\"python\">1 | Term 1 | Definition 1\n2 | Term 2 | Definition 2\n3 | Term 3 | Definition 3\n</code></pre>\n", "abstract": "Many thanks for bernie's answer!  Had to tweak it a bit - here's what worked for me: My text file (PC.txt) looks like this:"}, {"id": 19730169, "score": 9, "vote": 0, "content": "<pre><code class=\"python\">#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport sys, csv, sqlite3\n\ndef main():\n    con = sqlite3.connect(sys.argv[1]) # database file input\n    cur = con.cursor()\n    cur.executescript(\"\"\"\n        DROP TABLE IF EXISTS t;\n        CREATE TABLE t (COL1 TEXT, COL2 TEXT);\n        \"\"\") # checks to see if table exists and makes a fresh table.\n\n    with open(sys.argv[2], \"rb\") as f: # CSV file input\n        reader = csv.reader(f, delimiter=',') # no header information with delimiter\n        for row in reader:\n            to_db = [unicode(row[0], \"utf8\"), unicode(row[1], \"utf8\")] # Appends data from CSV file representing and handling of text\n            cur.execute(\"INSERT INTO neto (COL1, COL2) VALUES(?, ?);\", to_db)\n            con.commit()\n    con.close() # closes connection to database\n\nif __name__=='__main__':\n    main()\n</code></pre>\n", "abstract": ""}, {"id": 68756927, "score": 6, "vote": 0, "content": "<pre><code class=\"python\">\"\"\"\ncd Final_Codes\npython csv_to_db.py\nCSV to SQL DB\n\"\"\"\n\nimport csv\nimport sqlite3\nimport os\nimport fnmatch\n\nUP_FOLDER = os.path.dirname(os.getcwd())\nDATABASE_FOLDER = os.path.join(UP_FOLDER, \"Databases\")\nDBNAME = \"allCompanies_database.db\"\n\n\ndef getBaseNameNoExt(givenPath):\n    \"\"\"Returns the basename of the file without the extension\"\"\"\n    filename = os.path.splitext(os.path.basename(givenPath))[0]\n    return filename\n\n\ndef find(pattern, path):\n    \"\"\"Utility to find files wrt a regex search\"\"\"\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result\n\n\nif __name__ == \"__main__\":\n    Database_Path = os.path.join(DATABASE_FOLDER, DBNAME)\n    # change to 'sqlite:///your_filename.db'\n    csv_files = find('*.csv', DATABASE_FOLDER)\n\n    con = sqlite3.connect(Database_Path)\n    cur = con.cursor()\n    for each in csv_files:\n        with open(each, 'r') as fin:  # `with` statement available in 2.5+\n            # csv.DictReader uses first line in file for column headings by default\n            dr = csv.DictReader(fin)  # comma is default delimiter\n            TABLE_NAME = getBaseNameNoExt(each)\n            Cols = dr.fieldnames\n            numCols = len(Cols)\n            \"\"\"\n            for i in dr:\n                print(i.values())\n            \"\"\"\n            to_db = [tuple(i.values()) for i in dr]\n            print(TABLE_NAME)\n            # use your column names here\n            ColString = ','.join(Cols)\n            QuestionMarks = [\"?\"] * numCols\n            ToAdd = ','.join(QuestionMarks)\n            cur.execute(f\"CREATE TABLE {TABLE_NAME} ({ColString});\")\n            cur.executemany(\n                f\"INSERT INTO {TABLE_NAME} ({ColString}) VALUES ({ToAdd});\", to_db)\n            con.commit()\n    con.close()\n    print(\"Execution Complete!\")\n\n</code></pre>\n<p>This should come in handy when you have a lot of csv files in a folder which you wish to convert to a single .db file in a go!</p>\n<p>Notice that you dont have to know the filenames, tablenames or fieldnames (column names) beforehand!</p>\n", "abstract": "This should come in handy when you have a lot of csv files in a folder which you wish to convert to a single .db file in a go! Notice that you dont have to know the filenames, tablenames or fieldnames (column names) beforehand!"}, {"id": 58849240, "score": 5, "vote": 0, "content": "<p>If the CSV file must be imported as part of a python program, then for simplicity and efficiency, you could use <code>os.system</code> along the lines suggested by the following:</p>\n<pre><code class=\"python\">import os\n\ncmd = \"\"\"sqlite3 database.db &lt;&lt;&lt; \".import input.csv mytable\" \"\"\"\n\nrc = os.system(cmd)\n\nprint(rc)\n\n</code></pre>\n<p>The point is that by specifying the filename of the database, the data will automatically be saved, assuming there are no errors reading it.</p>\n", "abstract": "If the CSV file must be imported as part of a python program, then for simplicity and efficiency, you could use os.system along the lines suggested by the following: The point is that by specifying the filename of the database, the data will automatically be saved, assuming there are no errors reading it."}, {"id": 41584832, "score": 4, "vote": 0, "content": "<p>You can do this using <code>blaze</code> &amp; <code>odo</code> efficiently</p>\n<pre><code class=\"python\">import blaze as bz\ncsv_path = 'data.csv'\nbz.odo(csv_path, 'sqlite:///data.db::data')\n</code></pre>\n<p>Odo will store the csv file to <code>data.db</code> (sqlite database) under the schema <code>data</code></p>\n<p>Or you use <code>odo</code> directly, without <code>blaze</code>. Either ways is fine. Read this <a href=\"http://odo.pydata.org/en/latest/perf.html#csv-sqlite3-57m-31s\" rel=\"nofollow noreferrer\">documentation</a></p>\n", "abstract": "You can do this using blaze & odo efficiently Odo will store the csv file to data.db (sqlite database) under the schema data Or you use odo directly, without blaze. Either ways is fine. Read this documentation"}, {"id": 50798644, "score": 4, "vote": 0, "content": "<p>Based on Guy L solution (Love it) but can handle escaped fields.</p>\n<pre><code class=\"python\">import csv, sqlite3\n\ndef _get_col_datatypes(fin):\n    dr = csv.DictReader(fin) # comma is default delimiter\n    fieldTypes = {}\n    for entry in dr:\n        feildslLeft = [f for f in dr.fieldnames if f not in fieldTypes.keys()]        \n        if not feildslLeft: break # We're done\n        for field in feildslLeft:\n            data = entry[field]\n\n            # Need data to decide\n            if len(data) == 0:\n                continue\n\n            if data.isdigit():\n                fieldTypes[field] = \"INTEGER\"\n            else:\n                fieldTypes[field] = \"TEXT\"\n        # TODO: Currently there's no support for DATE in sqllite\n\n    if len(feildslLeft) &gt; 0:\n        raise Exception(\"Failed to find all the columns data types - Maybe some are empty?\")\n\n    return fieldTypes\n\n\ndef escapingGenerator(f):\n    for line in f:\n        yield line.encode(\"ascii\", \"xmlcharrefreplace\").decode(\"ascii\")\n\n\ndef csvToDb(csvFile,dbFile,tablename, outputToFile = False):\n\n    # TODO: implement output to file\n\n    with open(csvFile,mode='r', encoding=\"ISO-8859-1\") as fin:\n        dt = _get_col_datatypes(fin)\n\n        fin.seek(0)\n\n        reader = csv.DictReader(fin)\n\n        # Keep the order of the columns name just as in the CSV\n        fields = reader.fieldnames\n        cols = []\n\n        # Set field and type\n        for f in fields:\n            cols.append(\"\\\"%s\\\" %s\" % (f, dt[f]))\n\n        # Generate create table statement:\n        stmt = \"create table if not exists \\\"\" + tablename + \"\\\" (%s)\" % \",\".join(cols)\n        print(stmt)\n        con = sqlite3.connect(dbFile)\n        cur = con.cursor()\n        cur.execute(stmt)\n\n        fin.seek(0)\n\n\n        reader = csv.reader(escapingGenerator(fin))\n\n        # Generate insert statement:\n        stmt = \"INSERT INTO \\\"\" + tablename + \"\\\" VALUES(%s);\" % ','.join('?' * len(cols))\n\n        cur.executemany(stmt, reader)\n        con.commit()\n        con.close()\n</code></pre>\n", "abstract": "Based on Guy L solution (Love it) but can handle escaped fields."}, {"id": 63912093, "score": 4, "vote": 0, "content": "<p>Here are solutions that'll work if your CSV file is really big.  Use <code>to_sql</code> as suggested by another answer, but set chunksize so it doesn't try to process the whole file at once.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect('my_data.db')\nc = conn.cursor()\nusers = pd.read_csv('users.csv')\nusers.to_sql('users', conn, if_exists='append', index = False, chunksize = 10000)\n</code></pre>\n<p>You can also use Dask, as described <a href=\"https://stackoverflow.com/a/54349509/1125159\">here</a> to write a lot of Pandas DataFrames in parallel:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">dto_sql = dask.delayed(pd.DataFrame.to_sql)\nout = [dto_sql(d, 'table_name', db_url, if_exists='append', index=True)\n       for d in ddf.to_delayed()]\ndask.compute(*out)\n</code></pre>\n<p>See <a href=\"https://mungingdata.com/sqlite/create-database-load-csv-python/\" rel=\"nofollow noreferrer\">here</a> for more details.</p>\n", "abstract": "Here are solutions that'll work if your CSV file is really big.  Use to_sql as suggested by another answer, but set chunksize so it doesn't try to process the whole file at once. You can also use Dask, as described here to write a lot of Pandas DataFrames in parallel: See here for more details."}, {"id": 64082233, "score": 3, "vote": 0, "content": "<p>The following can also add fields' name based on the CSV header:</p>\n<pre><code class=\"python\">import sqlite3\n\ndef csv_sql(file_dir,table_name,database_name):\n    con = sqlite3.connect(database_name)\n    cur = con.cursor()\n    # Drop the current table by: \n    # cur.execute(\"DROP TABLE IF EXISTS %s;\" % table_name)\n\n    with open(file_dir, 'r') as fl:\n        hd = fl.readline()[:-1].split(',')\n        ro = fl.readlines()\n        db = [tuple(ro[i][:-1].split(',')) for i in range(len(ro))]\n\n    header = ','.join(hd)\n    cur.execute(\"CREATE TABLE IF NOT EXISTS %s (%s);\" % (table_name,header))\n    cur.executemany(\"INSERT INTO %s (%s) VALUES (%s);\" % (table_name,header,('?,'*len(hd))[:-1]), db)\n    con.commit()\n    con.close()\n\n# Example:\ncsv_sql('./surveys.csv','survey','eco.db')\n</code></pre>\n", "abstract": "The following can also add fields' name based on the CSV header:"}, {"id": 64309962, "score": 2, "vote": 0, "content": "<p>With this you can do joins on CSVs as well:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import sqlite3\nimport os\nimport pandas as pd\nfrom typing import List\n\nclass CSVDriver:\n    def __init__(self, table_dir_path: str):\n        self.table_dir_path = table_dir_path  # where tables (ie. csv files) are located\n        self._con = None\n\n    @property\n    def con(self) -&gt; sqlite3.Connection:\n        \"\"\"Make a singleton connection to an in-memory SQLite database\"\"\"\n        if not self._con:\n            self._con = sqlite3.connect(\":memory:\")\n        return self._con\n    \n    def _exists(self, table: str) -&gt; bool:\n        query = \"\"\"\n        SELECT name\n        FROM sqlite_master \n        WHERE type ='table'\n        AND name NOT LIKE 'sqlite_%';\n        \"\"\"\n        tables = self.con.execute(query).fetchall()\n        return table in tables\n\n    def _load_table_to_mem(self, table: str, sep: str = None) -&gt; None:\n        \"\"\"\n        Load a CSV into an in-memory SQLite database\n        sep is set to None in order to force pandas to auto-detect the delimiter\n        \"\"\"\n        if self._exists(table):\n            return\n        file_name = table + \".csv\"\n        path = os.path.join(self.table_dir_path, file_name)\n        if not os.path.exists(path):\n            raise ValueError(f\"CSV table {table} does not exist in {self.table_dir_path}\")\n        df = pd.read_csv(path, sep=sep, engine=\"python\")  # set engine to python to skip pandas' warning\n        df.to_sql(table, self.con, if_exists='replace', index=False, chunksize=10000)\n\n    def query(self, query: str) -&gt; List[tuple]:\n        \"\"\"\n        Run an SQL query on CSV file(s). \n        Tables are loaded from table_dir_path\n        \"\"\"\n        tables = extract_tables(query)\n        for table in tables:\n            self._load_table_to_mem(table)\n        cursor = self.con.cursor()\n        cursor.execute(query)\n        records = cursor.fetchall()\n        return records\n</code></pre>\n<p>extract_tables():</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import sqlparse\nfrom sqlparse.sql import IdentifierList, Identifier,  Function\nfrom sqlparse.tokens import Keyword, DML\nfrom collections import namedtuple\nimport itertools\n\nclass Reference(namedtuple('Reference', ['schema', 'name', 'alias', 'is_function'])):\n    __slots__ = ()\n\n    def has_alias(self):\n        return self.alias is not None\n\n    @property\n    def is_query_alias(self):\n        return self.name is None and self.alias is not None\n\n    @property\n    def is_table_alias(self):\n        return self.name is not None and self.alias is not None and not self.is_function\n\n    @property\n    def full_name(self):\n        if self.schema is None:\n            return self.name\n        else:\n            return self.schema + '.' + self.name\n\ndef _is_subselect(parsed):\n    if not parsed.is_group:\n        return False\n    for item in parsed.tokens:\n        if item.ttype is DML and item.value.upper() in ('SELECT', 'INSERT',\n                                                        'UPDATE', 'CREATE', 'DELETE'):\n            return True\n    return False\n\n\ndef _identifier_is_function(identifier):\n    return any(isinstance(t, Function) for t in identifier.tokens)\n\n\ndef _extract_from_part(parsed):\n    tbl_prefix_seen = False\n    for item in parsed.tokens:\n        if item.is_group:\n            for x in _extract_from_part(item):\n                yield x\n        if tbl_prefix_seen:\n            if _is_subselect(item):\n                for x in _extract_from_part(item):\n                    yield x\n            # An incomplete nested select won't be recognized correctly as a\n            # sub-select. eg: 'SELECT * FROM (SELECT id FROM user'. This causes\n            # the second FROM to trigger this elif condition resulting in a\n            # StopIteration. So we need to ignore the keyword if the keyword\n            # FROM.\n            # Also 'SELECT * FROM abc JOIN def' will trigger this elif\n            # condition. So we need to ignore the keyword JOIN and its variants\n            # INNER JOIN, FULL OUTER JOIN, etc.\n            elif item.ttype is Keyword and (\n                    not item.value.upper() == 'FROM') and (\n                    not item.value.upper().endswith('JOIN')):\n                tbl_prefix_seen = False\n            else:\n                yield item\n        elif item.ttype is Keyword or item.ttype is Keyword.DML:\n            item_val = item.value.upper()\n            if (item_val in ('COPY', 'FROM', 'INTO', 'UPDATE', 'TABLE') or\n                    item_val.endswith('JOIN')):\n                tbl_prefix_seen = True\n        # 'SELECT a, FROM abc' will detect FROM as part of the column list.\n        # So this check here is necessary.\n        elif isinstance(item, IdentifierList):\n            for identifier in item.get_identifiers():\n                if (identifier.ttype is Keyword and\n                        identifier.value.upper() == 'FROM'):\n                    tbl_prefix_seen = True\n                    break\n\n\ndef _extract_table_identifiers(token_stream):\n    for item in token_stream:\n        if isinstance(item, IdentifierList):\n            for ident in item.get_identifiers():\n                try:\n                    alias = ident.get_alias()\n                    schema_name = ident.get_parent_name()\n                    real_name = ident.get_real_name()\n                except AttributeError:\n                    continue\n                if real_name:\n                    yield Reference(schema_name, real_name,\n                                    alias, _identifier_is_function(ident))\n        elif isinstance(item, Identifier):\n            yield Reference(item.get_parent_name(), item.get_real_name(),\n                            item.get_alias(), _identifier_is_function(item))\n        elif isinstance(item, Function):\n            yield Reference(item.get_parent_name(), item.get_real_name(),\n                            item.get_alias(), _identifier_is_function(item))\n\n\ndef extract_tables(sql):\n    # let's handle multiple statements in one sql string\n    extracted_tables = []\n    statements = list(sqlparse.parse(sql))\n    for statement in statements:\n        stream = _extract_from_part(statement)\n        extracted_tables.append([ref.name for ref in _extract_table_identifiers(stream)])\n    return list(itertools.chain(*extracted_tables))\n</code></pre>\n<p>Example (assuming <code>account.csv</code> and <code>tojoin.csv</code> exist in <code>/path/to/files</code>):</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">db_path = r\"/path/to/files\"\ndriver = CSVDriver(db_path)\nquery = \"\"\"\nSELECT tojoin.col_to_join \nFROM account\nLEFT JOIN tojoin\nON account.a = tojoin.a\n\"\"\"\ndriver.query(query)\n</code></pre>\n", "abstract": "With this you can do joins on CSVs as well: extract_tables(): Example (assuming account.csv and tojoin.csv exist in /path/to/files):"}, {"id": 53912346, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">import csv, sqlite3\n\ndef _get_col_datatypes(fin):\n    dr = csv.DictReader(fin) # comma is default delimiter\n    fieldTypes = {}\n    for entry in dr:\n        feildslLeft = [f for f in dr.fieldnames if f not in fieldTypes.keys()]        \n        if not feildslLeft: break # We're done\n        for field in feildslLeft:\n            data = entry[field]\n\n        # Need data to decide\n        if len(data) == 0:\n            continue\n\n        if data.isdigit():\n            fieldTypes[field] = \"INTEGER\"\n        else:\n            fieldTypes[field] = \"TEXT\"\n    # TODO: Currently there's no support for DATE in sqllite\n\nif len(feildslLeft) &gt; 0:\n    raise Exception(\"Failed to find all the columns data types - Maybe some are empty?\")\n\nreturn fieldTypes\n\n\ndef escapingGenerator(f):\n    for line in f:\n        yield line.encode(\"ascii\", \"xmlcharrefreplace\").decode(\"ascii\")\n\n\ndef csvToDb(csvFile,dbFile,tablename, outputToFile = False):\n\n    # TODO: implement output to file\n\n    with open(csvFile,mode='r', encoding=\"ISO-8859-1\") as fin:\n        dt = _get_col_datatypes(fin)\n\n        fin.seek(0)\n\n        reader = csv.DictReader(fin)\n\n        # Keep the order of the columns name just as in the CSV\n        fields = reader.fieldnames\n        cols = []\n\n        # Set field and type\n        for f in fields:\n            cols.append(\"\\\"%s\\\" %s\" % (f, dt[f]))\n\n        # Generate create table statement:\n        stmt = \"create table if not exists \\\"\" + tablename + \"\\\" (%s)\" % \",\".join(cols)\n        print(stmt)\n        con = sqlite3.connect(dbFile)\n        cur = con.cursor()\n        cur.execute(stmt)\n\n        fin.seek(0)\n\n\n        reader = csv.reader(escapingGenerator(fin))\n\n        # Generate insert statement:\n        stmt = \"INSERT INTO \\\"\" + tablename + \"\\\" VALUES(%s);\" % ','.join('?' * len(cols))\n\n        cur.executemany(stmt, reader)\n        con.commit()\n        con.close()\n</code></pre>\n", "abstract": ""}, {"id": 55407488, "score": 1, "vote": 0, "content": "<p>in the interest of simplicity, you could use the sqlite3 command line tool from the Makefile of your project.</p>\n<pre><code class=\"python\">%.sql3: %.csv\n    rm -f $@\n    sqlite3 $@ -echo -cmd \".mode csv\" \".import $&lt; $*\"\n%.dump: %.sql3\n    sqlite3 $&lt; \"select * from $*\"\n</code></pre>\n<p><code>make test.sql3</code> then creates the sqlite database from an existing test.csv file, with a single table \"test\". you can then <code>make test.dump</code> to verify the contents.</p>\n", "abstract": "in the interest of simplicity, you could use the sqlite3 command line tool from the Makefile of your project. make test.sql3 then creates the sqlite database from an existing test.csv file, with a single table \"test\". you can then make test.dump to verify the contents."}, {"id": 60187194, "score": 1, "vote": 0, "content": "<p>I've found that it can be necessary to break up the transfer of data from the csv to the database in chunks as to not run out of memory. This can be done like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import csv\nimport sqlite3\nfrom operator import itemgetter\n\n# Establish connection\nconn = sqlite3.connect(\"mydb.db\")\n\n# Create the table \nconn.execute(\n    \"\"\"\n    CREATE TABLE persons(\n        person_id INTEGER,\n        last_name TEXT, \n        first_name TEXT, \n        address TEXT\n    )\n    \"\"\"\n)\n\n# These are the columns from the csv that we want\ncols = [\"person_id\", \"last_name\", \"first_name\", \"address\"]\n\n# If the csv file is huge, we instead add the data in chunks\nchunksize = 10000\n\n# Parse csv file and populate db in chunks\nwith conn, open(\"persons.csv\") as f:\n    reader = csv.DictReader(f)\n\n    chunk = []\n    for i, row in reader: \n\n        if i % chunksize == 0 and i &gt; 0:\n            conn.executemany(\n                \"\"\"\n                INSERT INTO persons\n                    VALUES(?, ?, ?, ?)\n                \"\"\", chunk\n            )\n            chunk = []\n\n        items = itemgetter(*cols)(row)\n        chunk.append(items)\n\n</code></pre>\n", "abstract": "I've found that it can be necessary to break up the transfer of data from the csv to the database in chunks as to not run out of memory. This can be done like this:"}, {"id": 73029204, "score": 0, "vote": 0, "content": "<p>Here is my version, works already by asking you to select the '.csv' file you want to convert</p>\n<pre><code class=\"python\">from multiprocessing import current_process\nimport pandas as pd\nimport sqlite3 \nimport os\nfrom tkinter import Tk\nfrom tkinter.filedialog import askopenfilename\nfrom pathlib import Path\n\ndef csv_to_db(csv_filedir):\n\n    if not Path(csv_filedir).is_file():                         # if needed ask for user input of CVS file\n        current_path = os.getcwd()\n        Tk().withdraw()                                     \n        csv_filedir = askopenfilename(initialdir=current_path) \n\n    try:\n        data = pd.read_csv(csv_filedir)                             # load CSV file\n    except:\n        print(\"Something went wrong when opening to the file\")\n        print(csv_filedir)\n\n    csv_df = pd.DataFrame(data)\n    csv_df = csv_df.fillna('NULL')                              # make NaN = to 'NULL' for SQL format\n\n    [path,filename] = os.path.split(csv_filedir)                # define path and filename \n    [filename,_] = os.path.splitext(filename)\n    database_filedir = os.path.join(path, filename + '.db')\n\n    conn = sqlite3.connect(database_filedir)                    # connect to SQL server\n\n    [fields_sql, header_sql_string] = create_sql_fields(csv_df)\n\n    # CREATE EMPTY DATABASE\n    create_sql = ''.join(['CREATE TABLE IF NOT EXISTS ' + filename + ' (' + fields_sql + ')'])\n    cursor = conn.cursor()\n    cursor.execute(create_sql)\n    \n    # INSERT EACH ROW IN THE SQL DATABASE\n    for irow in csv_df.itertuples():\n        insert_values_string = ''.join(['INSERT INTO ', filename, header_sql_string, ' VALUES ('])\n        insert_sql = f\"{insert_values_string} {irow[1]}, '{irow[2]}','{irow[3]}', {irow[4]}, '{irow[5]}' )\"\n        print(insert_sql)\n        cursor.execute(insert_sql)\n\n    # COMMIT CHANGES TO DATABASE AND CLOSE CONNECTION\n    conn.commit()\n    conn.close()\n\n    print('\\n' + csv_filedir + ' \\n converted to \\n' + database_filedir)\n\n    return database_filedir\n\n\ndef create_sql_fields(df):                                          # gather the headers of the CSV and create two strings \n    fields_sql = []                                                 # str1 = var1 TYPE, va2, TYPE ...\n    header_names = []                                               # str2 = var1, var2, var3, var4\n    for col in range(0,len(df.columns)):\n        fields_sql.append(df.columns[col])\n        fields_sql.append(str(df.dtypes[col]))\n\n        header_names.append(df.columns[col])\n        if col != len(df.columns)-1:\n            fields_sql.append(',')\n            header_names.append(',')\n\n    fields_sql = ' '.join(fields_sql)\n    fields_sql = fields_sql.replace('int64','integer')\n    fields_sql = fields_sql.replace('float64','integer')\n    fields_sql = fields_sql.replace('object','text')\n\n    header_sql_string = '(' + ''.join(header_names) + ')'\n    \n    return fields_sql, header_sql_string\n\n\ncsv_to_db('')\n</code></pre>\n", "abstract": "Here is my version, works already by asking you to select the '.csv' file you want to convert"}]}, {"link": "https://stackoverflow.com/questions/8895208/sqlalchemy-how-to-filter-date-field", "question": {"id": "8895208", "title": "SQLAlchemy: how to filter date field?", "content": "<p>Here is model:</p>\n<pre><code class=\"python\">class User(Base):\n    ...\n    birthday = Column(Date, index=True)   #in database it's like '1987-01-17'\n    ...\n</code></pre>\n<p>I want to filter between two dates, for example to choose all users in interval 18-30 years.</p>\n<p>How to implement it with SQLAlchemy?</p>\n<p>I think of:</p>\n<pre><code class=\"python\">query = DBSession.query(User).filter(\n    and_(User.birthday &gt;= '1988-01-17', User.birthday &lt;= '1985-01-17')\n) \n\n# means age &gt;= 24 and age &lt;= 27\n</code></pre>\n<p>I know this is not correct, but how to do correct?</p>\n", "abstract": "Here is model: I want to filter between two dates, for example to choose all users in interval 18-30 years. How to implement it with SQLAlchemy? I think of: I know this is not correct, but how to do correct?"}, "answers": [{"id": 8898533, "score": 243, "vote": 0, "content": "<p>In fact, your query is right except for the typo: your filter is excluding all records: you should change the <code>&lt;=</code> for <code>&gt;=</code> and vice versa:</p>\n<pre><code class=\"python\">qry = DBSession.query(User).filter(\n        and_(User.birthday &lt;= '1988-01-17', User.birthday &gt;= '1985-01-17'))\n# or same:\nqry = DBSession.query(User).filter(User.birthday &lt;= '1988-01-17').\\\n        filter(User.birthday &gt;= '1985-01-17')\n</code></pre>\n<p>Also you can use <code>between</code>:</p>\n<pre><code class=\"python\">qry = DBSession.query(User).filter(User.birthday.between('1985-01-17', '1988-01-17'))\n</code></pre>\n", "abstract": "In fact, your query is right except for the typo: your filter is excluding all records: you should change the <= for >= and vice versa: Also you can use between:"}, {"id": 54901334, "score": 12, "vote": 0, "content": "<p>if you want to get the <strong>whole</strong> period:</p>\n<pre><code class=\"python\">    from sqlalchemy import and_, func\n\n    query = DBSession.query(User).filter(and_(func.date(User.birthday) &gt;= '1985-01-17'),\\\n                                              func.date(User.birthday) &lt;= '1988-01-17'))\n</code></pre>\n<p>That means range: <em>1985-01-17</em> <strong>00:00</strong> - <em>1988-01-17</em> <strong>23:59</strong></p>\n", "abstract": "if you want to get the whole period: That means range: 1985-01-17 00:00 - 1988-01-17 23:59"}, {"id": 39364708, "score": 7, "vote": 0, "content": "<pre><code class=\"python\">from app import SQLAlchemyDB as db\n\nChance.query.filter(Chance.repo_id==repo_id, \n                    Chance.status==\"1\", \n                    db.func.date(Chance.apply_time)&lt;=end, \n                    db.func.date(Chance.apply_time)&gt;=start).count()\n</code></pre>\n<p>it is equal to:</p>\n<pre><code class=\"python\">select\n   count(id)\nfrom\n   Chance\nwhere\n   repo_id=:repo_id \n   and status='1'\n   and date(apple_time) &lt;= end\n   and date(apple_time) &gt;= start\n</code></pre>\n<p>wish can help you.</p>\n", "abstract": "it is equal to: wish can help you."}, {"id": 69267747, "score": 1, "vote": 0, "content": "<p>Here is a version which works with the latest <code>Flask</code> version and uses <a href=\"https://flask-marshmallow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\"><code>flask-marshmallow</code></a>.</p>\n<pre><code class=\"python\">from datetime import date, timedelta\n\nfrom flask import jsonify\n\nfrom app import db, ma\nfrom app.models import User\n\nfrom . import main\n\nclass UserSchema(ma.Schema):\n    class Meta:\n        fields = ('forename', 'surname', 'birthday', ...)\n\n\n@main.route('/', methods=('GET',))\ndef get_users():\n\n    start_range = date.today() + timedelta(years=-30)\n    end_range = date.today() + timedelta(years=-18)\n\n    users = db.session.query(User).filter(User.birthday.between(start_range, end_range)).all()\n\n    users_schema = UserSchema(many=True)\n\n    return jsonify(users_schema.dump(users))     \n</code></pre>\n", "abstract": "Here is a version which works with the latest Flask version and uses flask-marshmallow."}]}, {"link": "https://stackoverflow.com/questions/43102442/whats-the-difference-between-mysqldb-mysqlclient-and-mysql-connector-python", "question": {"id": "43102442", "title": "What&#39;s the difference between MySQLdb, mysqlclient and MySQL connector/Python?", "content": "<p>So I've been trying to do some database update with python and while setting up the whole dev environment, I came across these three things which made me dizzy.</p>\n<ol>\n<li><p>There's <a href=\"http://mysql-python.sourceforge.net/MySQLdb.html\" rel=\"noreferrer\">MySQLdb</a></p></li>\n<li><p>There's <a href=\"https://pypi.python.org/pypi/mysqlclient\" rel=\"noreferrer\">mysqlclient</a></p></li>\n<li>And then there's a <a href=\"https://dev.mysql.com/doc/connector-python/en/\" rel=\"noreferrer\">mysql connector python</a></li>\n</ol>\n<p>What's each of them, the difference and where to use them? Thanks</p>\n", "abstract": "So I've been trying to do some database update with python and while setting up the whole dev environment, I came across these three things which made me dizzy. There's MySQLdb There's mysqlclient What's each of them, the difference and where to use them? Thanks"}, "answers": [{"id": 43102794, "score": 107, "vote": 0, "content": "<p><a href=\"http://mysql-python.sourceforge.net/MySQLdb.html\" rel=\"noreferrer\">MySQLdb</a> is a thin python wrapper around C module which implements API for MySQL database. </p>\n<p>There was <a href=\"https://github.com/farcepest/MySQLdb1\" rel=\"noreferrer\">MySQLDb1</a> version of wrapper used some time ago and now it is considered to be a legacy. As MySQLDb1 started evolving to <a href=\"https://github.com/farcepest/moist\" rel=\"noreferrer\">MySQLDb2</a> with bug fixes and Python3 support, a MySQLDb1 was forked and here is how <a href=\"https://github.com/PyMySQL/mysqlclient-python\" rel=\"noreferrer\">mysqlclient</a> appeared, with bugfixes and Python3 support. Sum up, so now we have MySQLDb2 which is not ready for production use, MySQLDb1 as an outdated driver and a community supported mysqlclient with bug fixes and Python3 support.</p>\n<p>Now, to solve that mess, MySQL provides their own version of MySQL adapter - <a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-introduction.html\" rel=\"noreferrer\">mysql connector</a>, an all-in python module that uses MySQL API with <strong><em>no C modules dependencies</em></strong> and only standard python modules used.</p>\n<p>So now the question comes down to: mysqlclient vs mysql connector.</p>\n<p>As for me, I would go with officially supported library, however <code>mysqlclient</code> should be a good choice as well.\nBoth of them are being actively updated with fixes and new features which you can see by active commits in last days.</p>\n<p><em>Note: I did not have much experience with them, so there might be cases when one or another does not suite your needs. Both libraries follow <a href=\"https://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP-249</a> standard which means you should be fine with at least base functionality everywhere.</em> </p>\n<p><strong>Installation and Dependencies</strong></p>\n<ul>\n<li>mysqlclient</li>\n</ul>\n<p>As a fork of C wrapper it requires C modules to work with MySQL which adds python header files to build these extensions (read python-dev). Installation depends on the system you use, just make sure you aware of package names and can install them.</p>\n<ul>\n<li>mysql connector\n<a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-installation.html\" rel=\"noreferrer\">Main documentation</a> is pretty clear, however you should be aware of <a href=\"https://developers.google.com/protocol-buffers/docs/downloads\" rel=\"noreferrer\">Protobuf C++</a> dependency (for mysql connector <a href=\"https://github.com/sanpingz/mysql-connector\" rel=\"noreferrer\">versions &gt;= 2.2.3</a>).</li>\n</ul>\n", "abstract": "MySQLdb is a thin python wrapper around C module which implements API for MySQL database.  There was MySQLDb1 version of wrapper used some time ago and now it is considered to be a legacy. As MySQLDb1 started evolving to MySQLDb2 with bug fixes and Python3 support, a MySQLDb1 was forked and here is how mysqlclient appeared, with bugfixes and Python3 support. Sum up, so now we have MySQLDb2 which is not ready for production use, MySQLDb1 as an outdated driver and a community supported mysqlclient with bug fixes and Python3 support. Now, to solve that mess, MySQL provides their own version of MySQL adapter - mysql connector, an all-in python module that uses MySQL API with no C modules dependencies and only standard python modules used. So now the question comes down to: mysqlclient vs mysql connector. As for me, I would go with officially supported library, however mysqlclient should be a good choice as well.\nBoth of them are being actively updated with fixes and new features which you can see by active commits in last days. Note: I did not have much experience with them, so there might be cases when one or another does not suite your needs. Both libraries follow PEP-249 standard which means you should be fine with at least base functionality everywhere.  Installation and Dependencies As a fork of C wrapper it requires C modules to work with MySQL which adds python header files to build these extensions (read python-dev). Installation depends on the system you use, just make sure you aware of package names and can install them."}, {"id": 46396881, "score": 103, "vote": 0, "content": "<p>There are thee MySQL adapters for Python that are currently maintained:</p>\n<ul>\n<li><p><a href=\"https://github.com/PyMySQL/mysqlclient-python\" rel=\"noreferrer\"><code>mysqlclient</code></a> - By far the fastest MySQL connector for CPython. Requires the <code>mysql-connector-c</code> C library to work.</p></li>\n<li><p><a href=\"https://github.com/PyMySQL/PyMySQL\" rel=\"noreferrer\"><code>PyMySQL</code></a> - Pure Python MySQL client. <a href=\"https://github.com/PyMySQL/PyMySQL/issues/342#issuecomment-104939515\" rel=\"noreferrer\">According to the maintainer of both <code>mysqlclient</code> and <code>PyMySQL</code></a>, you should use <code>PyMySQL</code> if:</p>\n<ul>\n<li>You can't use <code>libmysqlclient</code> for some reason.</li>\n<li>You want to use monkeypatched socket of gevent or eventlet.</li>\n<li>You wan't to hack mysql protocol.</li>\n</ul></li>\n<li><p><a href=\"https://github.com/mysql/mysql-connector-python\" rel=\"noreferrer\"><code>mysql-connector-python</code></a> - MySQL connector developed by the MySQL group at Oracle, also written entirely in Python. It's performance appears to be the worst out of the three. Also, due to some licensing issues, you can't download it from PyPI (but it's now available through conda).</p></li>\n</ul>\n<h3>Benchmarks</h3>\n<p>According to the following benchmarks, <code>mysqlclient</code> is faster (sometimes &gt; 10x faster) than the pure Python clients.</p>\n<ul>\n<li><a href=\"https://gist.github.com/methane/90ec97dda7fa9c7c4ef1\" rel=\"noreferrer\">Benchmarking MySQL drivers (Python 3.4)</a></li>\n<li><a href=\"https://wiki.openstack.org/wiki/PyMySQL_evaluation\" rel=\"noreferrer\">PyMySQL Evaluation</a></li>\n<li><a href=\"http://charlesnagy.info/it/python/python-mysqldb-vs-mysql-connector-query-performance\" rel=\"noreferrer\">Python MySQLdb vs mysql-connector query performance</a></li>\n</ul>\n", "abstract": "There are thee MySQL adapters for Python that are currently maintained: mysqlclient - By far the fastest MySQL connector for CPython. Requires the mysql-connector-c C library to work. PyMySQL - Pure Python MySQL client. According to the maintainer of both mysqlclient and PyMySQL, you should use PyMySQL if: mysql-connector-python - MySQL connector developed by the MySQL group at Oracle, also written entirely in Python. It's performance appears to be the worst out of the three. Also, due to some licensing issues, you can't download it from PyPI (but it's now available through conda). According to the following benchmarks, mysqlclient is faster (sometimes > 10x faster) than the pure Python clients."}, {"id": 52685419, "score": 25, "vote": 0, "content": "<blockquote>\n<p>A lot of options provided by users. Little late to party. But my <strong>2\n  cents</strong> in on with benchmarking for pypy 3.7 version.</p>\n</blockquote>\n<h2>Stick to mysqlclient if you want faster access and repetitive access</h2>\n<pre><code class=\"python\">MySQL Connector/Python: 23.096168518066406 [sec]\nmysqlclient: 6.815327882766724 [sec]\nPyMySQL: 24.616853952407837 [sec]\nMySQL Connector/Python: 22.619106769561768 [sec]\nmysqlclient: 6.607790231704712 [sec]\nPyMySQL: 24.410773038864136 [sec]\n</code></pre>\n<p>Loop... from previous benchmarking...</p>\n<pre><code class=\"python\">def q100k(cur):\n    t = time.time()\n    for _ in range(100000):\n        cur.execute(\"SELECT 1,2,3,4,5,6\")\n        res = cur.fetchall()\n        assert len(res) == 1\n        assert res[0] == (1, 2, 3, 4, 5, 6)\n    return time.time() - t\n</code></pre>\n", "abstract": "A lot of options provided by users. Little late to party. But my 2\n  cents in on with benchmarking for pypy 3.7 version. Loop... from previous benchmarking..."}, {"id": 69632486, "score": 2, "vote": 0, "content": "<p><strong>For developers using SQLAlchemy</strong></p>\n<p>This question was a good starting point for learning what DBAPIs are available for Python and their tradeoffs. However, the libraries mentioned above are ever-changing, and their performance and issues are not set in stone. Hence, testing the individual dialects and evaluating their performance for yourself would be a good approach.</p>\n<p>I have provided below links listing the available DBAPIs. Those links include references to each library's documentation and a commentary on their features and support.</p>\n<ul>\n<li>For MariaDB: <a href=\"https://docs.sqlalchemy.org/en/14/dialects/mysql.html#module-sqlalchemy.dialects.mysql.base\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/14/dialects/mysql.html#module-sqlalchemy.dialects.mysql.base</a></li>\n<li>PostgreSQL: <a href=\"https://docs.sqlalchemy.org/en/14/dialects/postgresql.html#:%7E:text=DBAPI%20Support-,%C2%B6,-The%20following%20dialect\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/14/dialects/postgresql.html#:~:text=DBAPI%20Support-,%C2%B6,-The%20following%20dialect</a></li>\n<li>More Database dialects can be found in the side menu <a href=\"https://docs.sqlalchemy.org/en/14/core/engines.html#mysql\" rel=\"nofollow noreferrer\">here</a>.</li>\n</ul>\n<p>To understand what DBAPIs are in general, use this <a href=\"https://docs.sqlalchemy.org/en/14/glossary.html#term-DBAPI\" rel=\"nofollow noreferrer\">link</a>, as well as reading the above mentioned <a href=\"https://www.python.org/dev/peps/pep-0249/\" rel=\"nofollow noreferrer\">PEP-249</a>. In addition, the diagram below can help you visualise where DBAPIs belong <a href=\"https://docs.sqlalchemy.org/en/14/core/engines.html\" rel=\"nofollow noreferrer\">architecturally</a>.\n<a href=\"https://i.stack.imgur.com/A9E32.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/A9E32.png\"/></a></p>\n", "abstract": "For developers using SQLAlchemy This question was a good starting point for learning what DBAPIs are available for Python and their tradeoffs. However, the libraries mentioned above are ever-changing, and their performance and issues are not set in stone. Hence, testing the individual dialects and evaluating their performance for yourself would be a good approach. I have provided below links listing the available DBAPIs. Those links include references to each library's documentation and a commentary on their features and support. To understand what DBAPIs are in general, use this link, as well as reading the above mentioned PEP-249. In addition, the diagram below can help you visualise where DBAPIs belong architecturally.\n"}]}, {"link": "https://stackoverflow.com/questions/3172929/operationalerror-database-is-locked", "question": {"id": "3172929", "title": "OperationalError: database is locked", "content": "<p>I have made some repetitive operations in my application (testing it), and suddenly I\u2019m getting a weird error: </p>\n<pre><code class=\"python\">OperationalError: database is locked\n</code></pre>\n<p>I've restarted the server, but the error persists. What can it be all about?</p>\n", "abstract": "I have made some repetitive operations in my application (testing it), and suddenly I\u2019m getting a weird error:  I've restarted the server, but the error persists. What can it be all about?"}, "answers": [{"id": 3172950, "score": 132, "vote": 0, "content": "<p>From django doc:</p>\n<blockquote>\n<p>SQLite is meant to be a lightweight\n  database, and thus can't support a\n  high level of concurrency.\n  OperationalError: database is locked\n  errors indicate that your application\n  is experiencing more concurrency than\n  sqlite can handle in default\n  configuration. This error means that\n  one thread or process has an exclusive\n  lock on the database connection and\n  another thread timed out waiting for\n  the lock the be released.</p>\n<p>Python's SQLite wrapper has a default\n  timeout value that determines how long\n  the second thread is allowed to wait\n  on the lock before it times out and\n  raises the OperationalError: database\n  is locked error.</p>\n<p>If you're getting this error, you can\n  solve it by:</p>\n<ul>\n<li>Switching to another database backend. At a certain point SQLite becomes too \"lite\" for real-world applications, and these sorts of concurrency errors indicate you've reached that point.</li>\n<li>Rewriting your code to reduce concurrency and ensure that database transactions are short-lived.</li>\n<li>Increase the default timeout value by setting the timeout database option</li>\n</ul>\n</blockquote>\n<p><a href=\"http://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errorsoption\" rel=\"noreferrer\">http://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errorsoption</a></p>\n", "abstract": "From django doc: SQLite is meant to be a lightweight\n  database, and thus can't support a\n  high level of concurrency.\n  OperationalError: database is locked\n  errors indicate that your application\n  is experiencing more concurrency than\n  sqlite can handle in default\n  configuration. This error means that\n  one thread or process has an exclusive\n  lock on the database connection and\n  another thread timed out waiting for\n  the lock the be released. Python's SQLite wrapper has a default\n  timeout value that determines how long\n  the second thread is allowed to wait\n  on the lock before it times out and\n  raises the OperationalError: database\n  is locked error. If you're getting this error, you can\n  solve it by: http://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errorsoption"}, {"id": 39449185, "score": 69, "vote": 0, "content": "<p>In my case, It was because I open the database from SQLite Browser. When I close it from the browser, the problem is gone.</p>\n", "abstract": "In my case, It was because I open the database from SQLite Browser. When I close it from the browser, the problem is gone."}, {"id": 53470179, "score": 48, "vote": 0, "content": "<p>I slightly disagree with the accepted answer which, by quoting this doc, implicitly links OP's problem (<code>Database is locked</code>) to this:</p>\n<blockquote>\n<p>Switching to another database backend. At a certain point SQLite becomes too \"lite\" for real-world applications, and these sorts of concurrency errors indicate you've reached that point.</p>\n</blockquote>\n<p>This is a bit \"too easy\" to incriminate SQlite for this problem (which is <a href=\"https://www.sqlite.org/whentouse.html\" rel=\"nofollow noreferrer\">very powerful</a> when correctly used; it's not only a toy for small databases, fun fact: <code>An SQLite database is limited in size to 140 terabytes </code>).</p>\n<p>Unless you have a very busy server with thousands of connections at the same second, <strong>the reason for this <code>Database is locked</code> error is probably more a bad use of the API, than a problem inherent to SQlite which would be \"too light\"</strong>. Here are more informations about <a href=\"https://www.sqlite.org/limits.html\" rel=\"nofollow noreferrer\">Implementation Limits for SQLite</a>.</p>\n<hr/>\n<p>Now the solution:</p>\n<p>I had the same problem when I was using two scripts using the same database at the same time:</p>\n<ul>\n<li>one was accessing the DB with write operations</li>\n<li>the other was accessing the DB in read-only</li>\n</ul>\n<p>Solution: <strong>always do <code>cursor.close()</code> as soon as possible after having done a (even read-only) query.</strong></p>\n<p><a href=\"https://stackoverflow.com/questions/53270520/how-to-know-which-process-is-responsible-for-a-operationalerror-database-is-lo/53470118#53470118\">Here are more details</a>.</p>\n", "abstract": "I slightly disagree with the accepted answer which, by quoting this doc, implicitly links OP's problem (Database is locked) to this: Switching to another database backend. At a certain point SQLite becomes too \"lite\" for real-world applications, and these sorts of concurrency errors indicate you've reached that point. This is a bit \"too easy\" to incriminate SQlite for this problem (which is very powerful when correctly used; it's not only a toy for small databases, fun fact: An SQLite database is limited in size to 140 terabytes ). Unless you have a very busy server with thousands of connections at the same second, the reason for this Database is locked error is probably more a bad use of the API, than a problem inherent to SQlite which would be \"too light\". Here are more informations about Implementation Limits for SQLite. Now the solution: I had the same problem when I was using two scripts using the same database at the same time: Solution: always do cursor.close() as soon as possible after having done a (even read-only) query. Here are more details."}, {"id": 19517097, "score": 46, "vote": 0, "content": "<p>The practical reason for this is often that the python or django shells have opened a request to the DB and it wasn't closed properly; killing your terminal access often frees it up.  I had this error on running command line tests today.  </p>\n<p>Edit:  I get periodic upvotes on this.  If you'd like to kill access without rebooting the terminal, then from commandline you can do: </p>\n<pre><code class=\"python\">from django import db\ndb.connections.close_all()\n</code></pre>\n", "abstract": "The practical reason for this is often that the python or django shells have opened a request to the DB and it wasn't closed properly; killing your terminal access often frees it up.  I had this error on running command line tests today.   Edit:  I get periodic upvotes on this.  If you'd like to kill access without rebooting the terminal, then from commandline you can do: "}, {"id": 56000682, "score": 7, "vote": 0, "content": "<p>As others have told, there is another process that is using the SQLite file and has not closed the connection. In case you are using Linux, you can see which processes are using the file (for example <code>db.sqlite3</code>) using the <code>fuser</code> command as follows:</p>\n<pre class=\"lang-sh prettyprint-override\"><code class=\"python\">$ sudo fuser -v db.sqlite3\n                     USER        PID ACCESS COMMAND\n/path/to/db.sqlite3:\n                     user        955 F....  apache2\n</code></pre>\n<p>If you want to stop the processes to release the lock, use <code>fuser -k</code> which sends the <code>KILL</code> signal to all processes accessing the file:</p>\n<pre class=\"lang-sh prettyprint-override\"><code class=\"python\">sudo fuser -k db.sqlite3\n</code></pre>\n<p>Note that this is dangerous as it might stop the web server process in a production server.</p>\n<p>Thanks to @cz-game for pointing out <code>fuser</code>!</p>\n", "abstract": "As others have told, there is another process that is using the SQLite file and has not closed the connection. In case you are using Linux, you can see which processes are using the file (for example db.sqlite3) using the fuser command as follows: If you want to stop the processes to release the lock, use fuser -k which sends the KILL signal to all processes accessing the file: Note that this is dangerous as it might stop the web server process in a production server. Thanks to @cz-game for pointing out fuser!"}, {"id": 57826827, "score": 5, "vote": 0, "content": "<p>I encountered this error message in a situation that is not (clearly) addressed by the help info linked in patrick's answer.</p>\n<p>When I used <code>transaction.atomic()</code> to wrap a call to <code>FooModel.objects.get_or_create()</code> and called that code simultaneously from two different threads, only one thread would succeed, while the other would get the \"database is locked\" error. Changing the timeout database option had no effect on the behavior.</p>\n<p>I think this is due to the fact that sqlite <a href=\"https://sqlite.org/whentouse.html\" rel=\"noreferrer\">cannot handle multiple simultaneous writers</a>, so the application must serialize writes on their own.</p>\n<p>I solved the problem by using a <code>threading.RLock</code> object instead of <code>transaction.atomic()</code> when my Django app is running with a sqlite backend. That's not entirely equivalent, so you may need to do something else in your application.</p>\n<p>Here's my code that runs <code>FooModel.objects.get_or_create</code> simultaneously from two different threads, in case it is helpful:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from concurrent.futures import ThreadPoolExecutor\n\nimport configurations\nconfigurations.setup()\n\nfrom django.db import transaction\nfrom submissions.models import ExerciseCollectionSubmission\n\ndef makeSubmission(user_id):\n    try:\n        with transaction.atomic():\n            e, _ = ExerciseCollectionSubmission.objects.get_or_create(\n                student_id=user_id, exercise_collection_id=172)\n    except Exception as e:\n        return f'failed: {e}'\n\n    e.delete()\n\n    return 'success'\n\n\nfutures = []\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    futures.append(executor.submit(makeSubmission, 296))\n    futures.append(executor.submit(makeSubmission, 297))\n\nfor future in futures:\n    print(future.result())\n</code></pre>\n", "abstract": "I encountered this error message in a situation that is not (clearly) addressed by the help info linked in patrick's answer. When I used transaction.atomic() to wrap a call to FooModel.objects.get_or_create() and called that code simultaneously from two different threads, only one thread would succeed, while the other would get the \"database is locked\" error. Changing the timeout database option had no effect on the behavior. I think this is due to the fact that sqlite cannot handle multiple simultaneous writers, so the application must serialize writes on their own. I solved the problem by using a threading.RLock object instead of transaction.atomic() when my Django app is running with a sqlite backend. That's not entirely equivalent, so you may need to do something else in your application. Here's my code that runs FooModel.objects.get_or_create simultaneously from two different threads, in case it is helpful:"}, {"id": 67547949, "score": 5, "vote": 0, "content": "<p>I got this error when using a database file saved under WSL (\\\\wsl$ ...) and running a windows python interpreter.</p>\n<p>You can either not save the database in your WSL-tree or use a linux based interpreter in your distro.</p>\n", "abstract": "I got this error when using a database file saved under WSL (\\\\wsl$ ...) and running a windows python interpreter. You can either not save the database in your WSL-tree or use a linux based interpreter in your distro."}, {"id": 69635230, "score": 4, "vote": 0, "content": "<p>I was facing this issue in my flask app because I opened the database in SQLite Browser and forgot to write the changes.</p>\n<p>If you have also made any changes in SQLite Browser, then <strong>click on write changes and everything will be fine</strong>.</p>\n<p><a href=\"https://i.stack.imgur.com/0WU4v.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/0WU4v.png\"/></a></p>\n", "abstract": "I was facing this issue in my flask app because I opened the database in SQLite Browser and forgot to write the changes. If you have also made any changes in SQLite Browser, then click on write changes and everything will be fine. "}, {"id": 54077133, "score": 2, "vote": 0, "content": "<p>This also could happen if you are connected to your sqlite db via dbbrowser plugin through pycharm. Disconnection will solve the problem</p>\n", "abstract": "This also could happen if you are connected to your sqlite db via dbbrowser plugin through pycharm. Disconnection will solve the problem"}, {"id": 54546021, "score": 2, "vote": 0, "content": "<p>For me it gets resolved once I closed the django shell which was opened using <code>python manage.py shell</code></p>\n", "abstract": "For me it gets resolved once I closed the django shell which was opened using python manage.py shell"}, {"id": 57757570, "score": 2, "vote": 0, "content": "<p>I've got the same error! One of the reasons was the DB connection was not closed. \nTherefore, check for <strong>unclosed DB connections</strong>. Also, check if you have <strong>committed</strong> the DB before closing the connection.</p>\n", "abstract": "I've got the same error! One of the reasons was the DB connection was not closed. \nTherefore, check for unclosed DB connections. Also, check if you have committed the DB before closing the connection."}, {"id": 60234436, "score": 2, "vote": 0, "content": "<p>I had a similar error, right after the first instantiation of Django (v3.0.3). All recommendations here did not work apart from:</p>\n<ul>\n<li>deleted the <code>db.sqlite3</code> file and lose the data there, if any,</li>\n<li><code>python manage.py makemigrations</code></li>\n<li><code>python manage.py migrate</code></li>\n</ul>\n<hr/>\n<p>Btw, if you want to just test PostgreSQL:</p>\n<pre class=\"lang-sh prettyprint-override\"><code class=\"python\">docker run --rm --name django-postgres \\\n  -e POSTGRES_PASSWORD=mypassword \\\n  -e PGPORT=5432 \\\n  -e POSTGRES_DB=myproject \\\n  -p 5432:5432 \\\n  postgres:9.6.17-alpine\n</code></pre>\n<p>Change the <code>settings.py</code> to add this <code>DATABASES</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': 'myproject',\n        'USER': 'postgres',\n        'PASSWORD': 'mypassword',\n        'HOST': 'localhost',\n        'PORT': '5432',\n    }\n}\n</code></pre>\n<p>...and add database adapter:</p>\n<pre><code class=\"python\">pip install psycopg2-binary\n</code></pre>\n<p>Then the usual:</p>\n<pre><code class=\"python\">python manage.py makemigrations\npython manage.py migrate\n</code></pre>\n", "abstract": "I had a similar error, right after the first instantiation of Django (v3.0.3). All recommendations here did not work apart from: Btw, if you want to just test PostgreSQL: Change the settings.py to add this DATABASES: ...and add database adapter: Then the usual:"}, {"id": 67847988, "score": 2, "vote": 0, "content": "<p>Check if your database is opened on another DB Browser.</p>\n<p>If it is opened on an other application, then close the application and run the program again.</p>\n", "abstract": "Check if your database is opened on another DB Browser. If it is opened on an other application, then close the application and run the program again."}, {"id": 61645838, "score": 1, "vote": 0, "content": "<p>Just close (stop) and open (start) the database. This solved my problem.</p>\n", "abstract": "Just close (stop) and open (start) the database. This solved my problem."}, {"id": 63461708, "score": 1, "vote": 0, "content": "<p>I found this worked for my needs. (thread locking) YMMV\nconn = sqlite3.connect(database, timeout=10)</p>\n<p><a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"nofollow noreferrer\">https://docs.python.org/3/library/sqlite3.html</a></p>\n<p>sqlite3.connect(database[, timeout, detect_types, isolation_level, check_same_thread, factory, cached_statements, uri])</p>\n<p>When a database is accessed by multiple connections, and one of the processes modifies the database, the SQLite database is locked until that transaction is committed. The timeout parameter specifies how long the connection should wait for the lock to go away until raising an exception. The default for the timeout parameter is 5.0 (five seconds).</p>\n", "abstract": "I found this worked for my needs. (thread locking) YMMV\nconn = sqlite3.connect(database, timeout=10) https://docs.python.org/3/library/sqlite3.html sqlite3.connect(database[, timeout, detect_types, isolation_level, check_same_thread, factory, cached_statements, uri]) When a database is accessed by multiple connections, and one of the processes modifies the database, the SQLite database is locked until that transaction is committed. The timeout parameter specifies how long the connection should wait for the lock to go away until raising an exception. The default for the timeout parameter is 5.0 (five seconds)."}, {"id": 64941354, "score": 1, "vote": 0, "content": "<p>In my case, I added a new record manually saved and again through shell tried to add new record this time it works perfectly check it out.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">In [7]: from main.models import Flight\n\nIn [8]: f = Flight(origin=\"Florida\", destination=\"Alaska\", duration=10)\n\nIn [9]: f.save()\n\nIn [10]: Flight.objects.all() \nOut[10]: &lt;QuerySet [&lt;Flight: Flight object (1)&gt;, &lt;Flight: Flight object (2)&gt;, &lt;Flight: Flight object (3)&gt;, &lt;Flight: Flight object (4)&gt;]&gt;\n</code></pre>\n", "abstract": "In my case, I added a new record manually saved and again through shell tried to add new record this time it works perfectly check it out."}, {"id": 52322178, "score": 0, "vote": 0, "content": "<p>In my case, I had not saved a database operation I performed within the SQLite Browser. Saving it solved the issue.</p>\n", "abstract": "In my case, I had not saved a database operation I performed within the SQLite Browser. Saving it solved the issue."}, {"id": 61311274, "score": 0, "vote": 0, "content": "<p>A very unusual scenario, which happened to me.</p>\n<p>There was infinite recursion, which kept creating the objects.</p>\n<p>More specifically, using DRF, I was overriding create method in a view, and I did</p>\n<pre><code class=\"python\">def create(self, request, *args, **kwargs):\n    ....\n    ....\n\n    return self.create(request, *args, **kwargs)\n</code></pre>\n", "abstract": "A very unusual scenario, which happened to me. There was infinite recursion, which kept creating the objects. More specifically, using DRF, I was overriding create method in a view, and I did"}, {"id": 62607952, "score": 0, "vote": 0, "content": "<p>Already lot of Answers are available here, even I want to share my case , this may help someone..</p>\n<p>I have opened the connection in Python API to update values, I'll close connection only after receiving server response. Here what I did was I have opened connection to do some other operation in server as well before closing the connection in Python API.</p>\n", "abstract": "Already lot of Answers are available here, even I want to share my case , this may help someone.. I have opened the connection in Python API to update values, I'll close connection only after receiving server response. Here what I did was I have opened connection to do some other operation in server as well before closing the connection in Python API."}, {"id": 63247618, "score": 0, "vote": 0, "content": "<p>If you get this error while using <code>manage.py shell</code>, one possible reason is that you have a development server running (<code>manage.py runserver</code>) which is locking the database. Stoping the server while using the shell has always fixed the problem for me.</p>\n", "abstract": "If you get this error while using manage.py shell, one possible reason is that you have a development server running (manage.py runserver) which is locking the database. Stoping the server while using the shell has always fixed the problem for me."}, {"id": 66757275, "score": 0, "vote": 0, "content": "<p>actually I have faced same problem , when I use \"transaction.atomic() with select_for_update() \" i got error message \"the OperationalError: database is locked\" ,</p>\n<p>and after many tries / searching / read django docs ,\ni found the problem from SQLite itself it is not support select_for_update method as django DOCs says , kindly have a look at the following url and read it deeply:</p>\n<p><a href=\"https://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errors\" rel=\"nofollow noreferrer\">https://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errors</a></p>\n<p>, and when i moved to MySQL everything goes fine .</p>\n<p>as django DOCs also says \"database is locked\" may happen when database timeout occur ,\nthey recommend you to change database timeout by setting up the following option :</p>\n<pre><code class=\"python\">'OPTIONS': {\n    # ...\n    'timeout': 20,\n    # ...\n}\n</code></pre>\n<p>finally, I recommend you to use MySQL/PostgreSQL even if you working on development environment  .</p>\n<p>I hope this helpful for you .</p>\n", "abstract": "actually I have faced same problem , when I use \"transaction.atomic() with select_for_update() \" i got error message \"the OperationalError: database is locked\" , and after many tries / searching / read django docs ,\ni found the problem from SQLite itself it is not support select_for_update method as django DOCs says , kindly have a look at the following url and read it deeply: https://docs.djangoproject.com/en/dev/ref/databases/#database-is-locked-errors , and when i moved to MySQL everything goes fine . as django DOCs also says \"database is locked\" may happen when database timeout occur ,\nthey recommend you to change database timeout by setting up the following option : finally, I recommend you to use MySQL/PostgreSQL even if you working on development environment  . I hope this helpful for you ."}, {"id": 67483290, "score": 0, "vote": 0, "content": "<p>I got this error when attempting to create a new table in SQLite but the <code>session</code> object contained uncommitted (though flushed) changes.</p>\n<p>Make sure to either:</p>\n<ol>\n<li>Commit the session(s) before creating a new table</li>\n<li>Close all sessions and perform the table creation in a new connection</li>\n<li>...</li>\n</ol>\n", "abstract": "I got this error when attempting to create a new table in SQLite but the session object contained uncommitted (though flushed) changes. Make sure to either:"}, {"id": 70801590, "score": 0, "vote": 0, "content": "<p><a href=\"https://stackoverflow.com/users/15030095/shilp-thapak\">@Shilp Thapak</a>'s answer is correct: the reason for the error is that you did not write your manual changes to the data in your DB Browser for SQLite before running your application.</p>\n<p>If you didn't write the changes in whatever SQL client you are using, you can still create the engine but</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code class=\"python\">engine.connect()</code></pre>\n</div>\n</div>\n</p>\n<p>will throw the operational error about the database being locked.</p>\n<p>You can check whether your engine can connect by checking the existence of a rollback journal. The default mode of a rollback journal is to be created and deleted at the start and end of a transaction.</p>\n<p>It is exists in the same directory where your database is, it has the same name as the database file and the suffix \"-journal\" appended.</p>\n<p>If the mode is not changed, at <a href=\"https://i.stack.imgur.com/MNaiZ.png\" rel=\"nofollow noreferrer\">Journal mode in Edit pragmas panel in DB Browser for SQLite</a>.</p>\n<p>You can check the existence of the temp file like so:\n<div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code class=\"python\">if os.path.isfile('your-database.sqlite-journal'):\n    print(\"The database is locked. Please write your changes in your SQL client before proceeding.\\n\")</code></pre>\n</div>\n</div>\n</p>\n<p>Read more about temporary files <a href=\"https://sqlite.org/tempfiles.html\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>So no need to close the server or DB Browser for SQLite for that sake. In fact, as long as all the changes are written, you can have several clients connected to the database simultaneously and still run your application at the same time.</p>\n", "abstract": "@Shilp Thapak's answer is correct: the reason for the error is that you did not write your manual changes to the data in your DB Browser for SQLite before running your application. If you didn't write the changes in whatever SQL client you are using, you can still create the engine but \n\nengine.connect()\n\n\n will throw the operational error about the database being locked. You can check whether your engine can connect by checking the existence of a rollback journal. The default mode of a rollback journal is to be created and deleted at the start and end of a transaction. It is exists in the same directory where your database is, it has the same name as the database file and the suffix \"-journal\" appended. If the mode is not changed, at Journal mode in Edit pragmas panel in DB Browser for SQLite. You can check the existence of the temp file like so:\n\n\nif os.path.isfile('your-database.sqlite-journal'):\n    print(\"The database is locked. Please write your changes in your SQL client before proceeding.\\n\")\n\n\n Read more about temporary files here. So no need to close the server or DB Browser for SQLite for that sake. In fact, as long as all the changes are written, you can have several clients connected to the database simultaneously and still run your application at the same time."}, {"id": 71392326, "score": 0, "vote": 0, "content": "<p>For me it was simply because I was accessing the database in SQLite app at the same time of running my Python code to create a new table.\nClosing SQLite until the code is done solved my issue.</p>\n", "abstract": "For me it was simply because I was accessing the database in SQLite app at the same time of running my Python code to create a new table.\nClosing SQLite until the code is done solved my issue."}, {"id": 74001297, "score": 0, "vote": 0, "content": "<p>I just needed to add <code>alias sqlite='sqlite3'</code> to my <code>~/.zshrc</code></p>\n<p>I then deleted the partially-failed creation of the <code>virtualenv</code> in <code>~/.pyenv/versions/new-virtualenv</code> and reran <code>pyenv virtualenv &lt;name&gt;</code> and it worked swimmingly</p>\n", "abstract": "I just needed to add alias sqlite='sqlite3' to my ~/.zshrc I then deleted the partially-failed creation of the virtualenv in ~/.pyenv/versions/new-virtualenv and reran pyenv virtualenv <name> and it worked swimmingly"}, {"id": 55736345, "score": -2, "vote": 0, "content": "<p><strong>UPDATE</strong> django version 2.1.7</p>\n<p>I got this error <code>sqlite3.OperationalError: database is locked</code> using <code>pytest</code> with <code>django</code>.</p>\n<p>Solution:</p>\n<p>If we are using <code>@pytest.mark.django_db</code> decorator. What it does is create a <code>in-memory-db</code> for testing. </p>\n<p>Named: <code>file:memorydb_default?mode=memory&amp;cache=shared</code> We can get this name with:</p>\n<pre><code class=\"python\">from django.db import connection\ndb_path = connection.settings_dict['NAME']\n</code></pre>\n<p>To access this database and also edit it, do:</p>\n<p>Connect to the data base:</p>\n<pre><code class=\"python\">with sqlite3.connect(db_path, uri=True) as conn:\n    c = conn.cursor()\n</code></pre>\n<p>Use <code>uri=True</code> to specifies the disk file that is the SQLite database to be opened.</p>\n<p>To avoid the error activate transactions in the decorator:</p>\n<pre><code class=\"python\">@pytest.mark.django_db(transaction=True)\n</code></pre>\n<p>Final function:</p>\n<pre><code class=\"python\">from django.db import connection\n\n@pytest.mark.django_db(transaction=True)\ndef test_mytest():\n    db_path = connection.settings_dict['NAME']\n    with sqlite3.connect(db_path, uri=True) as conn:\n        c = conn.cursor()\n        c.execute('my amazing query')\n        conn.commit()\n    assert ... == ....\n</code></pre>\n", "abstract": "UPDATE django version 2.1.7 I got this error sqlite3.OperationalError: database is locked using pytest with django. Solution: If we are using @pytest.mark.django_db decorator. What it does is create a in-memory-db for testing.  Named: file:memorydb_default?mode=memory&cache=shared We can get this name with: To access this database and also edit it, do: Connect to the data base: Use uri=True to specifies the disk file that is the SQLite database to be opened. To avoid the error activate transactions in the decorator: Final function:"}, {"id": 63255938, "score": -2, "vote": 0, "content": "<p>Just reboot your server, it will clear all current processes that have your database locked.</p>\n", "abstract": "Just reboot your server, it will clear all current processes that have your database locked."}, {"id": 48262525, "score": -10, "vote": 0, "content": "<p>try this command:</p>\n<pre><code class=\"python\">sudo fuser -k 8000/tcp\n</code></pre>\n", "abstract": "try this command:"}]}, {"link": "https://stackoverflow.com/questions/15454008/how-to-reset-db-in-django-i-get-a-command-reset-not-found-error", "question": {"id": "15454008", "title": "How to reset db in Django? I get a command &#39;reset&#39; not found error", "content": "<p>Following this Django by Example tutotrial here: <a href=\"http://lightbird.net/dbe/todo_list.html\">http://lightbird.net/dbe/todo_list.html</a></p>\n<p>The tutorial says:</p>\n<blockquote>\n<p>\"This changes our table layout and we\u2019ll have to ask Django to reset\n  and recreate tables:</p>\n<p><code>manage.py reset todo; manage.py syncdb</code>\"</p>\n</blockquote>\n<p>though, when I run <code>manage.py reset todo</code>, I get the error:</p>\n<pre><code class=\"python\">$ python manage.py reset todo                                       \n- Unknown command: 'reset'\n</code></pre>\n<p>Is this because I am using sqlite3 and not postgresql?</p>\n<p><strong>Can somebody tell me what the command is to reset the database?</strong></p>\n<p>The command: <code>python manage.py sqlclear todo</code> returns the error:</p>\n<pre><code class=\"python\">$ python manage.py sqlclear todo    \nCommandError: App with label todo could not be found.    \nAre you sure your INSTALLED_APPS setting is correct?\n</code></pre>\n<p>So I added 'todo' to my INSTALLED_APPS in settings.py, and ran <code>python manage.py sqlclear todo</code> again, resulting in this error:</p>\n<pre><code class=\"python\">$ python manage.py sqlclear todo                                      \n- NameError: name 'admin' is not defined\n</code></pre>\n", "abstract": "Following this Django by Example tutotrial here: http://lightbird.net/dbe/todo_list.html The tutorial says: \"This changes our table layout and we\u2019ll have to ask Django to reset\n  and recreate tables: manage.py reset todo; manage.py syncdb\" though, when I run manage.py reset todo, I get the error: Is this because I am using sqlite3 and not postgresql? Can somebody tell me what the command is to reset the database? The command: python manage.py sqlclear todo returns the error: So I added 'todo' to my INSTALLED_APPS in settings.py, and ran python manage.py sqlclear todo again, resulting in this error:"}, "answers": [{"id": 15454063, "score": 179, "vote": 0, "content": "<p><code>reset</code> has been replaced by <code>flush</code> with Django 1.5, see:\n</p>\n<pre><code class=\"python\">python manage.py help flush\n</code></pre>\n", "abstract": "reset has been replaced by flush with Django 1.5, see:\n"}, {"id": 18409489, "score": 39, "vote": 0, "content": "<p>It looks like the 'flush' answer will work for some, but not all cases.  I needed not just to flush the values in the database, but to recreate the tables properly.  I'm not using migrations yet (early days) so I really needed to drop all the tables.  </p>\n<p>Two ways I've found to drop all tables, both require something other than core django.</p>\n<p>If you're on Heroku, drop all the tables with pg:reset:</p>\n<pre><code class=\"python\">heroku pg:reset DATABASE_URL\nheroku run python manage.py syncdb\n</code></pre>\n<p>If you can install Django Extensions, it has a way to do a complete reset:  </p>\n<pre><code class=\"python\">python ./manage.py reset_db --router=default\n</code></pre>\n", "abstract": "It looks like the 'flush' answer will work for some, but not all cases.  I needed not just to flush the values in the database, but to recreate the tables properly.  I'm not using migrations yet (early days) so I really needed to drop all the tables.   Two ways I've found to drop all tables, both require something other than core django. If you're on Heroku, drop all the tables with pg:reset: If you can install Django Extensions, it has a way to do a complete reset:  "}, {"id": 18542131, "score": 27, "vote": 0, "content": "<p>Similar to LisaD's answer, <a href=\"https://django-extensions.readthedocs.io/en/latest/\" rel=\"noreferrer\">Django Extensions</a> has a great reset_db command that totally drops everything, instead of just truncating the tables like \"flush\" does.</p>\n<p><code>python ./manage.py reset_db</code></p>\n<p>Merely flushing the tables wasn't fixing a persistent error that occurred when I was deleting objects. Doing a reset_db fixed the problem.</p>\n", "abstract": "Similar to LisaD's answer, Django Extensions has a great reset_db command that totally drops everything, instead of just truncating the tables like \"flush\" does. python ./manage.py reset_db Merely flushing the tables wasn't fixing a persistent error that occurred when I was deleting objects. Doing a reset_db fixed the problem."}, {"id": 47844848, "score": 22, "vote": 0, "content": "<p>if you are using Django 2.0\nThen</p>\n<pre><code class=\"python\">python manage.py flush \n</code></pre>\n<p>will work</p>\n", "abstract": "if you are using Django 2.0\nThen will work"}, {"id": 37217527, "score": 17, "vote": 0, "content": "<p>If you want to clean the whole database, you can use:\n<strong>python manage.py flush</strong>\nIf you want to clean database table of a Django app, you can use:\n<strong>python manage.py migrate appname zero</strong></p>\n", "abstract": "If you want to clean the whole database, you can use:\npython manage.py flush\nIf you want to clean database table of a Django app, you can use:\npython manage.py migrate appname zero"}, {"id": 46871597, "score": 11, "vote": 0, "content": "<p>With django 1.11, simply delete all migration files from the <code>migrations</code> folder of each application (all files except <code>__init__.py</code>). Then</p>\n<ol>\n<li>Manually drop database.</li>\n<li>Manually create database.</li>\n<li>Run <code>python3 manage.py makemigrations</code>.</li>\n<li>Run <code>python3 manage.py migrate</code>.</li>\n</ol>\n<p>And voilla, your database has been completely reset.</p>\n", "abstract": "With django 1.11, simply delete all migration files from the migrations folder of each application (all files except __init__.py). Then And voilla, your database has been completely reset."}, {"id": 57087139, "score": 5, "vote": 0, "content": "<pre><code class=\"python\">python manage.py flush\n</code></pre>\n<p>deleted old db contents, </p>\n<p>Don't forget to create new superuser:</p>\n<pre><code class=\"python\">python manage.py createsuperuser\n</code></pre>\n", "abstract": "deleted old db contents,  Don't forget to create new superuser:"}, {"id": 31729589, "score": 4, "vote": 0, "content": "<p>For me this solved the problem.</p>\n<pre><code class=\"python\">heroku pg:reset DATABASE_URL\n\nheroku run bash\n&gt;&gt; Inside heroku bash\ncd app_name &amp;&amp; rm -rf migrations &amp;&amp; cd ..\n./manage.py makemigrations app_name\n./manage.py migrate\n</code></pre>\n", "abstract": "For me this solved the problem."}, {"id": 36890649, "score": 3, "vote": 0, "content": "<p>Just a follow up to <a href=\"https://stackoverflow.com/users/971577/lisad\">@LisaD's</a> answer.<br/>\nAs of 2016 (<code>Django 1.9</code>), you need to type:</p>\n<pre><code class=\"python\">heroku pg:reset DATABASE_URL\nheroku run python manage.py makemigrations\nheroku run python manage.py migrate\n</code></pre>\n<p>This will give you a fresh new database within Heroku.</p>\n", "abstract": "Just a follow up to @LisaD's answer.\nAs of 2016 (Django 1.9), you need to type: This will give you a fresh new database within Heroku."}, {"id": 54559301, "score": 3, "vote": 0, "content": "<ol>\n<li><p>Just manually delete you database. Ensure you create backup first (in my case <strong>db.sqlite3</strong> is my database)</p></li>\n<li><p>Run this command <code>manage.py migrate</code></p></li>\n</ol>\n", "abstract": "Just manually delete you database. Ensure you create backup first (in my case db.sqlite3 is my database) Run this command manage.py migrate"}, {"id": 73439018, "score": 0, "vote": 0, "content": "<p>I use <code>python manage.py flush</code> on django 4.1 with postreSQL</p>\n", "abstract": "I use python manage.py flush on django 4.1 with postreSQL"}]}, {"link": "https://stackoverflow.com/questions/6626810/multiple-columns-index-when-using-the-declarative-orm-extension-of-sqlalchemy", "question": {"id": "6626810", "title": "Multiple columns index when using the declarative ORM extension of sqlalchemy", "content": "<p>According to <a href=\"http://docs.sqlalchemy.org/en/latest/core/constraints.html#indexes\" rel=\"noreferrer\">the documentation</a> and the comments in the <code>sqlalchemy.Column</code> class, we should use the class <code>sqlalchemy.schema.Index</code> to specify an index that contains multiple columns.</p>\n<p>However, the example shows how to do it by directly using the Table object like this:</p>\n<pre><code class=\"python\">meta = MetaData()\nmytable = Table('mytable', meta,\n    # an indexed column, with index \"ix_mytable_col1\"\n    Column('col1', Integer, index=True),\n\n    # a uniquely indexed column with index \"ix_mytable_col2\"\n    Column('col2', Integer, index=True, unique=True),\n\n    Column('col3', Integer),\n    Column('col4', Integer),\n\n    Column('col5', Integer),\n    Column('col6', Integer),\n    )\n\n# place an index on col3, col4\nIndex('idx_col34', mytable.c.col3, mytable.c.col4)\n</code></pre>\n<p>How should we do it if we use the declarative ORM extension?</p>\n<pre><code class=\"python\">class A(Base):\n    __tablename__ = 'table_A'\n    id = Column(Integer, , primary_key=True)\n    a = Column(String(32))\n    b = Column(String(32))\n</code></pre>\n<p>I  would like an index on column \"a\" and \"b\".</p>\n", "abstract": "According to the documentation and the comments in the sqlalchemy.Column class, we should use the class sqlalchemy.schema.Index to specify an index that contains multiple columns. However, the example shows how to do it by directly using the Table object like this: How should we do it if we use the declarative ORM extension? I  would like an index on column \"a\" and \"b\"."}, "answers": [{"id": 6627154, "score": 171, "vote": 0, "content": "<p>those are just <code>Column</code> objects, index=True flag works normally:</p>\n<pre><code class=\"python\">class A(Base):\n    __tablename__ = 'table_A'\n    id = Column(Integer, primary_key=True)\n    a = Column(String(32), index=True)\n    b = Column(String(32), index=True)\n</code></pre>\n<p>if you'd like a composite index, again <code>Table</code> is present here as usual you just don't have to declare it, everything works the same (make sure you're on recent 0.6 or 0.7 for the declarative A.a wrapper to be interpreted as a <code>Column</code> after the class declaration is complete):</p>\n<pre><code class=\"python\">class A(Base):\n    __tablename__ = 'table_A'\n    id = Column(Integer, primary_key=True)\n    a = Column(String(32))\n    b = Column(String(32))\n\nIndex('my_index', A.a, A.b)\n</code></pre>\n<p>In 0.7 the <code>Index</code> can be in the <code>Table</code> arguments too, which with declarative is via <code>__table_args__</code>:</p>\n<pre><code class=\"python\">class A(Base):\n    __tablename__ = 'table_A'\n    id = Column(Integer, primary_key=True)\n    a = Column(String(32))\n    b = Column(String(32))\n    __table_args__ = (Index('my_index', \"a\", \"b\"), )\n</code></pre>\n", "abstract": "those are just Column objects, index=True flag works normally: if you'd like a composite index, again Table is present here as usual you just don't have to declare it, everything works the same (make sure you're on recent 0.6 or 0.7 for the declarative A.a wrapper to be interpreted as a Column after the class declaration is complete): In 0.7 the Index can be in the Table arguments too, which with declarative is via __table_args__:"}, {"id": 52689029, "score": 20, "vote": 0, "content": "<p>To complete @zzzeek's <a href=\"https://stackoverflow.com/a/6627154\">answer</a>.</p>\n<p>If you like to add a composite index with DESC and use the ORM declarative method you can do as follows.</p>\n<p>Furthermore, I was struggling with the <a href=\"https://docs.sqlalchemy.org/en/latest/core/constraints.html#functional-indexes\" rel=\"nofollow noreferrer\">Functional Indexes</a> documentation of SQLAlchemy, trying to figure out a how to substitute <code>mytable.c.somecol</code>.</p>\n<blockquote>\n<pre><code class=\"python\">from sqlalchemy import Index\n\nIndex('someindex', mytable.c.somecol.desc())\n</code></pre>\n</blockquote>\n<p>We can just use the model property and call <code>.desc()</code> on it:</p>\n<pre><code class=\"python\">from flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\nclass GpsReport(db.Model):\n    __tablename__ = 'gps_report'\n\n    id = db.Column(db.Integer, db.Sequence('gps_report_id_seq'), nullable=False, autoincrement=True, server_default=db.text(\"nextval('gps_report_id_seq'::regclass)\"))\n\n    timestamp = db.Column(db.DateTime, nullable=False, primary_key=True)\n\n    device_id = db.Column(db.Integer, db.ForeignKey('device.id'), primary_key=True, autoincrement=False)\n    device = db.relationship(\"Device\", back_populates=\"gps_reports\")\n\n\n    # Indexes\n\n    __table_args__ = (\n        db.Index('gps_report_timestamp_device_id_idx', timestamp.desc(), device_id),\n    )\n</code></pre>\n<p>If you use Alembic, I'm using Flask-Migrate, it generates something like:</p>\n<pre><code class=\"python\">from alembic import op  \nimport sqlalchemy as sa\n# Added manually this import\nfrom sqlalchemy.schema import Sequence, CreateSequence\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    # Manually added the Sequence creation\n    op.execute(CreateSequence(Sequence('gps_report_id_seq')))\n\n    op.create_table('gps_report',\n    sa.Column('id', sa.Integer(), server_default=sa.text(\"nextval('gps_report_id_seq'::regclass)\"), nullable=False),\n    sa.Column('timestamp', sa.DateTime(), nullable=False))\n    sa.Column('device_id', sa.Integer(), autoincrement=False, nullable=False),\n    op.create_index('gps_report_timestamp_device_id_idx', 'gps_report', [sa.text('timestamp DESC'), 'device_id'], unique=False)\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index('gps_report_timestamp_device_id_idx', table_name='gps_report')\n    op.drop_table('gps_report')\n\n    # Manually added the Sequence removal\n    op.execute(sa.schema.DropSequence(sa.Sequence('gps_report_id_seq'))) \n    # ### end Alembic commands ###\n</code></pre>\n<p>Finally, you should have the following table and indexes in your PostgreSQL database:</p>\n<pre><code class=\"python\">psql&gt; \\d gps_report;\n                                           Table \"public.gps_report\"\n     Column      |            Type             | Collation | Nullable |                Default                 \n-----------------+-----------------------------+-----------+----------+----------------------------------------\n id              | integer                     |           | not null | nextval('gps_report_id_seq'::regclass)\n timestamp       | timestamp without time zone |           | not null | \n device_id       | integer                     |           | not null | \nIndexes:\n    \"gps_report_pkey\" PRIMARY KEY, btree (\"timestamp\", device_id)\n    \"gps_report_timestamp_device_id_idx\" btree (\"timestamp\" DESC, device_id)\nForeign-key constraints:\n    \"gps_report_device_id_fkey\" FOREIGN KEY (device_id) REFERENCES device(id)\n</code></pre>\n", "abstract": "To complete @zzzeek's answer. If you like to add a composite index with DESC and use the ORM declarative method you can do as follows. Furthermore, I was struggling with the Functional Indexes documentation of SQLAlchemy, trying to figure out a how to substitute mytable.c.somecol. We can just use the model property and call .desc() on it: If you use Alembic, I'm using Flask-Migrate, it generates something like: Finally, you should have the following table and indexes in your PostgreSQL database:"}]}, {"link": "https://stackoverflow.com/questions/3332991/sqlalchemy-filter-multiple-columns", "question": {"id": "3332991", "title": "sqlalchemy filter multiple columns", "content": "<p>How do I combine two columns and apply filter? For example, I want to search in both the \"firstname\" and \"lastname\" columns at the same time.  Here is how I have been doing it if searching only one column:</p>\n<pre><code class=\"python\">query = meta.Session.query(User).filter(User.firstname.like(searchVar))\n</code></pre>\n", "abstract": "How do I combine two columns and apply filter? For example, I want to search in both the \"firstname\" and \"lastname\" columns at the same time.  Here is how I have been doing it if searching only one column:"}, "answers": [{"id": 44702268, "score": 153, "vote": 0, "content": "<p>There are number of ways to do it:</p>\n<p>Using <code>filter()</code> (<em>and</em> operator)</p>\n<pre><code class=\"python\">query = meta.Session.query(User).filter(\n    User.firstname.like(search_var1),\n    User.lastname.like(search_var2)\n    )\n</code></pre>\n<p>Using <code>filter_by()</code> (<em>and</em> operator)</p>\n<pre><code class=\"python\">query = meta.Session.query(User).filter_by(\n    firstname.like(search_var1),\n    lastname.like(search_var2)\n    )\n</code></pre>\n<p>Chaining <code>filter()</code> or <code>filter_by()</code> (<em>and</em> operator)</p>\n<pre><code class=\"python\">query = meta.Session.query(User).\\\n    filter_by(firstname.like(search_var1)).\\\n    filter_by(lastname.like(search_var2))\n</code></pre>\n<p>Using <code>or_()</code>, <code>and_()</code>, and <code>not()</code></p>\n<pre><code class=\"python\">from sqlalchemy import and_, or_, not_\n\nquery = meta.Session.query(User).filter(\n    and_(\n        User.firstname.like(search_var1),\n        User.lastname.like(search_var2)\n    )\n)\n</code></pre>\n", "abstract": "There are number of ways to do it: Using filter() (and operator) Using filter_by() (and operator) Chaining filter() or filter_by() (and operator) Using or_(), and_(), and not()"}, {"id": 3792292, "score": 86, "vote": 0, "content": "<p>You can simply call <code>filter</code> multiple times:</p>\n<pre><code class=\"python\">query = meta.Session.query(User).filter(User.firstname.like(searchVar1)). \\\n                                 filter(User.lastname.like(searchVar2))\n</code></pre>\n", "abstract": "You can simply call filter multiple times:"}, {"id": 3353200, "score": 72, "vote": 0, "content": "<p>You can use SQLAlchemy's <a href=\"http://docs.sqlalchemy.org/en/latest/core/sqlelement.html?highlight=or_#sqlalchemy.sql.expression.or_\" rel=\"noreferrer\"><code>or_</code> function</a> to search in more than one column (the underscore is necessary to distinguish it from Python's own <code>or</code>).</p>\n<p>Here's an example:</p>\n<pre><code class=\"python\">from sqlalchemy import or_\nquery = meta.Session.query(User).filter(or_(User.firstname.like(searchVar),\n                                            User.lastname.like(searchVar)))\n</code></pre>\n", "abstract": "You can use SQLAlchemy's or_ function to search in more than one column (the underscore is necessary to distinguish it from Python's own or). Here's an example:"}, {"id": 61350372, "score": 5, "vote": 0, "content": "<p>A generic piece of code that will work for multiple columns.\nThis can also be used if there is a need to conditionally implement search functionality in the application.</p>\n<pre><code class=\"python\">search_key = 'abc'\nsearch_args = [col.ilike('%%%s%%' % search_key) for col in ['col1', 'col2', 'col3']]\nquery = Query(table).filter(or_(*search_args))\nsession.execute(query).fetchall()\n</code></pre>\n<p>Note: the <code>%%</code> are important to skip % formatting  the query.</p>\n", "abstract": "A generic piece of code that will work for multiple columns.\nThis can also be used if there is a need to conditionally implement search functionality in the application. Note: the %% are important to skip % formatting  the query."}]}, {"link": "https://stackoverflow.com/questions/12604909/pandas-how-to-change-all-the-values-of-a-column", "question": {"id": "12604909", "title": "Pandas: how to change all the values of a column?", "content": "<p>I have a data frame with a column called <code>\"Date\"</code> and want all the values from this column to have the same value (the year only). Example:</p>\n<pre><code class=\"python\">City     Date\nParis    01/04/2004\nLisbon   01/09/2004\nMadrid   2004\nPekin    31/2004\n</code></pre>\n<p>What I want is:</p>\n<pre><code class=\"python\">City     Date\nParis    2004\nLisbon   2004\nMadrid   2004\nPekin    2004\n</code></pre>\n<p>Here is my code:</p>\n<pre><code class=\"python\">fr61_70xls = pd.ExcelFile('AMADEUS FRANCE 1961-1970.xlsx')\n\n#Here we import the individual sheets and clean the sheets    \nyears=(['1961','1962','1963','1964','1965','1966','1967','1968','1969','1970'])\n\nfr={}\n\nheader=(['City','Country','NACE','Cons','Last_year','Op_Rev_EUR_Last_avail_yr','BvD_Indep_Indic','GUO_Name','Legal_status','Date_of_incorporation','Legal_status_date'])\n\nfor year in years:\n    # save every sheet in variable fr['1961'], fr['1962'] and so on\n    fr[year]=fr61_70xls.parse(year,header=0,parse_cols=10)\n    fr[year].columns=header\n    # drop the entire Legal status date column\n    fr[year]=fr[year].drop(['Legal_status_date','Date_of_incorporation'],axis=1)\n    # drop every row where GUO Name is empty\n    fr[year]=fr[year].dropna(axis=0,how='all',subset=[['GUO_Name']])\n    fr[year]=fr[year].set_index(['GUO_Name','Date_of_incorporation'])\n</code></pre>\n<p><em>It happens that in my DataFrames, called for example <code>fr['1961']</code> the values of <code>Date_of_incorporation</code> can be anything (strings, integer, and so on), so maybe it would be best to completely erase this column and then attach another column with only the year to the DataFrames?</em></p>\n", "abstract": "I have a data frame with a column called \"Date\" and want all the values from this column to have the same value (the year only). Example: What I want is: Here is my code: It happens that in my DataFrames, called for example fr['1961'] the values of Date_of_incorporation can be anything (strings, integer, and so on), so maybe it would be best to completely erase this column and then attach another column with only the year to the DataFrames?"}, "answers": [{"id": 12605055, "score": 160, "vote": 0, "content": "<p>As @DSM points out, you can do this more directly using the <a href=\"http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods\" rel=\"noreferrer\">vectorised string methods</a>:</p>\n<pre><code class=\"python\">df['Date'].str[-4:].astype(int)\n</code></pre>\n<p>Or using extract (assuming there is only one set of digits of length 4 somewhere in each string):</p>\n<pre><code class=\"python\">df['Date'].str.extract('(?P&lt;year&gt;\\d{4})').astype(int)\n</code></pre>\n<p>An alternative slightly more flexible way, might be to use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html\" rel=\"noreferrer\"><code>apply</code></a> (or equivalently <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\" rel=\"noreferrer\"><code>map</code></a>) to do this:</p>\n<pre><code class=\"python\">df['Date'] = df['Date'].apply(lambda x: int(str(x)[-4:]))\n             #  converts the last 4 characters of the string to an integer\n</code></pre>\n<p>The lambda function, is taking the input from the <code>Date</code> and converting it to a year.<br/>\nYou could (and perhaps should) write this more verbosely as:</p>\n<pre><code class=\"python\">def convert_to_year(date_in_some_format):\n    date_as_string = str(date_in_some_format)  # cast to string\n    year_as_string = date_in_some_format[-4:] # last four characters\n    return int(year_as_string)\n\ndf['Date'] = df['Date'].apply(convert_to_year)\n</code></pre>\n<p><em>Perhaps 'Year' is a better name for this column...</em></p>\n", "abstract": "As @DSM points out, you can do this more directly using the vectorised string methods: Or using extract (assuming there is only one set of digits of length 4 somewhere in each string): An alternative slightly more flexible way, might be to use apply (or equivalently map) to do this: The lambda function, is taking the input from the Date and converting it to a year.\nYou could (and perhaps should) write this more verbosely as: Perhaps 'Year' is a better name for this column..."}, {"id": 48736861, "score": 30, "vote": 0, "content": "<p>You can do a column transformation by using <code>apply</code> </p>\n<p>Define a clean function to remove the dollar and commas and convert your data to float.</p>\n<pre><code class=\"python\">def clean(x):\n    x = x.replace(\"$\", \"\").replace(\",\", \"\").replace(\" \", \"\")\n    return float(x)\n</code></pre>\n<p>Next, call it on your column like this.</p>\n<pre><code class=\"python\">data['Revenue'] = data['Revenue'].apply(clean)\n</code></pre>\n", "abstract": "You can do a column transformation by using apply  Define a clean function to remove the dollar and commas and convert your data to float. Next, call it on your column like this."}, {"id": 52913006, "score": 7, "vote": 0, "content": "<p>Or if one want to use <code>lambda</code> function in the <code>apply</code> function:</p>\n<pre><code class=\"python\">data['Revenue']=data['Revenue'].apply(lambda x:float(x.replace(\"$\",\"\").replace(\",\", \"\").replace(\" \", \"\")))\n</code></pre>\n", "abstract": "Or if one want to use lambda function in the apply function:"}]}, {"link": "https://stackoverflow.com/questions/22255589/get-all-keys-in-redis-database-with-python", "question": {"id": "22255589", "title": "Get all keys in Redis database with python", "content": "<p>There is a post about a Redis command to get all available keys, but I would like to do it with Python.</p>\n<p>Any way to do this?</p>\n", "abstract": "There is a post about a Redis command to get all available keys, but I would like to do it with Python. Any way to do this?"}, "answers": [{"id": 34166690, "score": 156, "vote": 0, "content": "<p>Use <code>scan_iter()</code></p>\n<p><code>scan_iter()</code> is superior to <code>keys()</code> for large numbers of keys because it gives you an iterator you can use rather than trying to load all the keys into memory.</p>\n<p>I had a 1B records in my redis and I could never get enough memory to return all the keys at once.</p>\n<p><strong>SCANNING KEYS ONE-BY-ONE</strong></p>\n<p>Here is a python snippet using <code>scan_iter()</code> to get all keys from the store matching a pattern and delete them one-by-one:</p>\n<pre><code class=\"python\">import redis\nr = redis.StrictRedis(host='localhost', port=6379, db=0)\nfor key in r.scan_iter(\"user:*\"):\n    # delete the key\n    r.delete(key)\n</code></pre>\n<p><strong>SCANNING IN BATCHES</strong></p>\n<p>If you have a very large list of keys to scan - for example, larger than &gt;100k keys - it will be more efficient to scan them in batches, like this:</p>\n<pre><code class=\"python\">import redis\nfrom itertools import izip_longest\n\nr = redis.StrictRedis(host='localhost', port=6379, db=0)\n\n# iterate a list in batches of size n\ndef batcher(iterable, n):\n    args = [iter(iterable)] * n\n    return izip_longest(*args)\n\n# in batches of 500 delete keys matching user:*\nfor keybatch in batcher(r.scan_iter('user:*'),500):\n    r.delete(*keybatch)\n</code></pre>\n<p>I benchmarked this script and found that using a batch size of 500 was 5 times faster than scanning keys one-by-one. I tested different batch sizes (3,50,500,1000,5000) and found that a batch size of 500 seems to be optimal.</p>\n<p>Note that whether you use  the <code>scan_iter()</code> or <code>keys()</code> method, the operation is not atomic and could fail part way through. </p>\n<p><strong>DEFINITELY AVOID USING XARGS ON THE COMMAND-LINE</strong></p>\n<p>I do not recommend this example I found repeated elsewhere. It will fail for unicode keys and is incredibly slow for even moderate numbers of keys:</p>\n<pre><code class=\"python\">redis-cli --raw keys \"user:*\"| xargs redis-cli del\n</code></pre>\n<p>In this example xargs creates a new redis-cli process for every key! that's bad.</p>\n<p>I benchmarked this approach to be 4 times slower than the first python example where it deleted every key one-by-one and 20 times slower than deleting in batches of 500.</p>\n", "abstract": "Use scan_iter() scan_iter() is superior to keys() for large numbers of keys because it gives you an iterator you can use rather than trying to load all the keys into memory. I had a 1B records in my redis and I could never get enough memory to return all the keys at once. SCANNING KEYS ONE-BY-ONE Here is a python snippet using scan_iter() to get all keys from the store matching a pattern and delete them one-by-one: SCANNING IN BATCHES If you have a very large list of keys to scan - for example, larger than >100k keys - it will be more efficient to scan them in batches, like this: I benchmarked this script and found that using a batch size of 500 was 5 times faster than scanning keys one-by-one. I tested different batch sizes (3,50,500,1000,5000) and found that a batch size of 500 seems to be optimal. Note that whether you use  the scan_iter() or keys() method, the operation is not atomic and could fail part way through.  DEFINITELY AVOID USING XARGS ON THE COMMAND-LINE I do not recommend this example I found repeated elsewhere. It will fail for unicode keys and is incredibly slow for even moderate numbers of keys: In this example xargs creates a new redis-cli process for every key! that's bad. I benchmarked this approach to be 4 times slower than the first python example where it deleted every key one-by-one and 20 times slower than deleting in batches of 500."}, {"id": 22255608, "score": 72, "vote": 0, "content": "<p>Yes, use <a href=\"http://redis-py.readthedocs.org/en/latest/#redis.StrictRedis.keys\"><code>keys()</code></a> from the StrictRedis module:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import redis\n&gt;&gt;&gt; r = redis.StrictRedis(host=YOUR_HOST, port=YOUR_PORT, db=YOUR_DB)\n&gt;&gt;&gt; r.keys()\n</code></pre>\n<p>Giving a null pattern will fetch all of them. As per the page linked:</p>\n<blockquote>\n<p>keys(pattern='*')</p>\n<p>Returns a list of keys matching pattern</p>\n</blockquote>\n", "abstract": "Yes, use keys() from the StrictRedis module: Giving a null pattern will fetch all of them. As per the page linked: keys(pattern='*') Returns a list of keys matching pattern"}, {"id": 30098050, "score": 14, "vote": 0, "content": "<pre><code class=\"python\">import redis\nr = redis.Redis(\"localhost\", 6379)\nfor key in r.scan_iter():\n       print key\n</code></pre>\n<p>using Pyredis library</p>\n<p><a href=\"http://redis.io/commands/scan\" rel=\"noreferrer\">scan command</a> </p>\n<p>Available since 2.8.0.</p>\n<p>Time complexity: O(1) for every call. O(N) for a complete iteration, including enough command calls for the cursor to return back to 0. N is the number of elements inside the collection..</p>\n", "abstract": "using Pyredis library scan command  Available since 2.8.0. Time complexity: O(1) for every call. O(N) for a complete iteration, including enough command calls for the cursor to return back to 0. N is the number of elements inside the collection.."}, {"id": 64560504, "score": 2, "vote": 0, "content": "<p>I'd like to add some example code to go with Patrick's answer and others.<br/>\nThis shows results both using keys and the scan_iter technique.\nAnd please note that Python3 uses zip_longest instead of izip_longest. The code below loops through all the keys and displays them. I set the batchsize as a variable to 12, to make the output smaller.</p>\n<p>I wrote this to better understand how the batching of keys worked.</p>\n<pre><code class=\"python\">import redis\nfrom itertools import zip_longest\n\n\\# connection/building of my redisObj omitted here\n\n\\# iterate a list in batches of size n\ndef batcher(iterable, n):\n    args = [iter(iterable)] * n\n    return zip_longest(*args)\n    \nresult1 = redisObj.get(\"TestEN\")\nprint(result1)\nresult2 = redisObj.get(\"TestES\")\nprint(result2)\n\nprint(\"\\n\\nLoop through all keys:\")\nkeys = redisObj.keys('*')\ncounter = 0\nprint(\"len(keys)=\", len(keys))\nfor key in keys:\n    counter +=1\n    print (counter, \"key=\" +key, \" value=\" + redisObj.get(key))\n\nprint(\"\\n\\nLoop through all keys in batches (using itertools)\")\n\\# in batches of 500 delete keys matching user:*\ncounter = 0\nbatch_counter = 0\nprint(\"Try scan_iter:\")\nfor keybatch in batcher(redisObj.scan_iter('*'), 12):\n    batch_counter +=1\n    print(batch_counter, \"keybatch=\", keybatch)\n    for key in keybatch:\n        if key != None:\n            counter += 1\n            print(\"  \", counter, \"key=\" + key, \" value=\" + redisObj.get(key))\n</code></pre>\n<p>Example output:</p>\n<pre><code class=\"python\">Loop through all keys:\nlen(keys)= 2\n1 key=TestES  value=Ola Mundo\n2 key=TestEN  value=Hello World\n\n\nLoop through all keys in batches (using itertools)\nTry scan_iter:\n1 keybatch= ('TestES', 'TestEN', None, None, None, None, None, None, None, None, None, None)\n   1 key=TestES  value=Ola Mundo\n   2 key=TestEN  value=Hello World\n</code></pre>\n<p>Note redis comamnds are single threaded, so doing a keys() can block other redis activity. See excellent post here that explains that in more detail: <a href=\"https://stackoverflow.com/questions/32603964/scan-vs-keys-performance-in-redis\">SCAN vs KEYS performance in Redis</a></p>\n", "abstract": "I'd like to add some example code to go with Patrick's answer and others.\nThis shows results both using keys and the scan_iter technique.\nAnd please note that Python3 uses zip_longest instead of izip_longest. The code below loops through all the keys and displays them. I set the batchsize as a variable to 12, to make the output smaller. I wrote this to better understand how the batching of keys worked. Example output: Note redis comamnds are single threaded, so doing a keys() can block other redis activity. See excellent post here that explains that in more detail: SCAN vs KEYS performance in Redis"}, {"id": 65406259, "score": 2, "vote": 0, "content": "<p>An addition to the accepted answer above.</p>\n<p><code>scan_iter</code> can be used with a <code>count</code> parameter in order to tell redis to search through a number of keys during a single iteration. This can speed up keys fetching significantly, especially when used with matching pattern and on big key spaces.</p>\n<p>Be careful tough when using very high values for the count since that may ruin the performance for other concurrent queries.</p>\n<p><a href=\"https://docs.keydb.dev/blog/2020/08/10/blog-post/\" rel=\"nofollow noreferrer\">https://docs.keydb.dev/blog/2020/08/10/blog-post/</a> Here's an article with more details and some benchmarks.</p>\n", "abstract": "An addition to the accepted answer above. scan_iter can be used with a count parameter in order to tell redis to search through a number of keys during a single iteration. This can speed up keys fetching significantly, especially when used with matching pattern and on big key spaces. Be careful tough when using very high values for the count since that may ruin the performance for other concurrent queries. https://docs.keydb.dev/blog/2020/08/10/blog-post/ Here's an article with more details and some benchmarks."}, {"id": 72472050, "score": 1, "vote": 0, "content": "<p>I have improved on Patrick's and Neal's code and added export to csv:</p>\n<pre><code class=\"python\">import csv\nimport redis\nfrom itertools import zip_longest\n\nredisObj = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\nsearchStr = \"\"\n\n# iterate a list in batches of size n\ndef batcher(iterable, n):\n    args = [iter(iterable)] * n\n    return zip_longest(*args)\n\nwith open('redis.csv', 'w', newline='') as csvfile:\n    fieldnames = ['key', 'value']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n\n    print(\"\\n\\nLoop through all keys in batches (using itertools)\")\n    counter = 0\n    batch_counter = 0\n    print(\"Try scan_iter:\")\n    for keybatch in batcher(redisObj.scan_iter('*'), 500):\n        batch_counter +=1\n        #print(batch_counter, \"keybatch=\", keybatch)\n        for key in keybatch:\n            if key != None:\n                counter += 1\n                val = \"\"\n                if (searchStr in key):\n                    valType = redisObj.type(key)\n                    print(valType)\n                    match valType:\n                        case \"string\":\n                            val = redisObj.get(key)\n                        case \"list\":\n                            valList = redisObj.lrange(key, 0, -1)\n                            val = '\\n'.join(valList)\n                        case \"set\":\n                            valList = redisObj.smembers(key)\n                            val = '\\n'.join(valList)\n                        case \"zset\":\n                            valDict = redisObj.zrange(key, 0, -1, False, True)\n                            val = '\\n'.join(['='.join(i) for i in valDict.items()])\n                        case \"hash\":\n                            valDict = redisObj.hgetall(key)\n                            val = '\\n'.join(['='.join(i) for i in valDict.items()])\n                        case \"stream\":\n                            val = \"\"\n                        case _:\n                            val = \"\"\n                print(\"  \", counter, \"key=\" + key, \" value=\" + val)\n                writer.writerow({'key': key, 'value': val})\n</code></pre>\n", "abstract": "I have improved on Patrick's and Neal's code and added export to csv:"}]}, {"link": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo", "question": {"id": "15312732", "title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb", "content": "<p>The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.</p>\n<pre><code class=\"python\"> Traceback (most recent call last):\n File \"manage.py\", line 10, in &lt;module&gt;\n execute_from_command_line(sys.argv)\n File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line\nutility.execute()\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute\nself.fetch_command(subcommand).run_from_argv(self.argv)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command\nklass = load_command_class(app_name, subcommand)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class\nmodule = import_module('%s.management.commands.%s' % (app_name, name))\nFile \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module\n__import__(name)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in &lt;module&gt;\nfrom django.core.management.sql import custom_sql_for_model, emit_post_sync_signal\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in &lt;module&gt;\nfrom django.db import models\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in &lt;module&gt;\nbackend = load_backend(connection.settings_dict['ENGINE'])\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__\nreturn getattr(connections[DEFAULT_DB_ALIAS], item)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__\nbackend = load_backend(db['ENGINE'])\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend\nreturn import_module('.base', backend_name)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module\n__import__(name)\nFile \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in &lt;module&gt;\nraise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e)\ndjango.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb\n</code></pre>\n<p>Databse Settings::</p>\n<pre><code class=\"python\">DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.\n        'NAME': 'ar_test_db',                      # Or path to database file if using \n        # The following settings are not used with sqlite3:\n        'USER': '',\n        'PASSWORD': '',\n        'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.\n        'PORT': '',                      # Set to empty string for default.\n    }\n}\n</code></pre>\n<p>Thanks a lot for the help !!</p>\n", "abstract": "The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used. Databse Settings:: Thanks a lot for the help !!"}, "answers": [{"id": 15312750, "score": 140, "vote": 0, "content": "<p>It looks like you don't have the python mysql package installed, try:</p>\n<pre><code class=\"python\">pip install mysql-python\n</code></pre>\n<p>or if not using a virtual environment (on *nix hosts):</p>\n<pre><code class=\"python\">sudo pip install mysql-python\n</code></pre>\n", "abstract": "It looks like you don't have the python mysql package installed, try: or if not using a virtual environment (on *nix hosts):"}, {"id": 34365315, "score": 47, "vote": 0, "content": "<p>If you get errors trying to install mysqlclient with pip, you may lack the mysql dev library. Install it by running:</p>\n<pre><code class=\"python\">apt-get install libmysqlclient-dev\n</code></pre>\n<p>and try again to install mysqlclient:</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n", "abstract": "If you get errors trying to install mysqlclient with pip, you may lack the mysql dev library. Install it by running: and try again to install mysqlclient:"}, {"id": 16734735, "score": 40, "vote": 0, "content": "<p>you have to install python-mysqldb - Python interface to MySQL</p>\n<p>Try <br/>\n<code>sudo apt-get install python-mysqldb</code></p>\n", "abstract": "you have to install python-mysqldb - Python interface to MySQL Try \nsudo apt-get install python-mysqldb"}, {"id": 23681230, "score": 14, "vote": 0, "content": "<p>My answer is similar to @Ron-E, but I got a few more errors/corrections so I'm putting my steps below for Mac OSX on Mavericks and Python 2.7.6.</p>\n<ol>\n<li><p>Install Python mysql package (if you get a success message, then ignore the below steps)</p>\n<pre><code class=\"python\">pip install mysql-python\n</code></pre></li>\n<li><p>When I did the above, I got the error \"EnvironmentError: mysql_config not found\"\n    <img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/LuVXK.png\"/>\n    To fix this, I did the below in terminal:</p>\n<pre><code class=\"python\">export PATH=$PATH:/usr/local/mysql/bin\n</code></pre></li>\n<li><p>When I reran step 1, I get a new error \"error: command 'cc' failed with exit status 1\"\n<img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/Z4NKE.png\"/>\n    To fix this, I did the below in terminal:</p>\n<pre><code class=\"python\"> export CFLAGS=-Qunused-arguments\n export CPPFLAGS=-Qunused-arguments\n</code></pre></li>\n<li><p>I reran step 1 and got the success message 'Successfully installed mysql-python'!</p></li>\n</ol>\n", "abstract": "My answer is similar to @Ron-E, but I got a few more errors/corrections so I'm putting my steps below for Mac OSX on Mavericks and Python 2.7.6. Install Python mysql package (if you get a success message, then ignore the below steps) When I did the above, I got the error \"EnvironmentError: mysql_config not found\"\n    \n    To fix this, I did the below in terminal: When I reran step 1, I get a new error \"error: command 'cc' failed with exit status 1\"\n\n    To fix this, I did the below in terminal: I reran step 1 and got the success message 'Successfully installed mysql-python'!"}, {"id": 19133958, "score": 12, "vote": 0, "content": "<p>When I set up Django development environment for PyCharm in Mac OS X Mountain Lion with python, mysql, sequel pro application I got error same as owner of this thread.\nHowever, my answer for them who is running python-mysqldb under Mac OS Mountain Lion x86_x64 (MySql and Python also should be same architecture) and already tried everything like pip and etc. In order fix this problem do following steps:</p>\n<ol>\n<li>Download MySql for Python from <a href=\"http://sourceforge.net/projects/mysql-python/\">here</a></li>\n<li>Untar downloaded file. In terminal window do following: tar xvfz downloade.tar.</li>\n<li>cd /to untared directory</li>\n<li>Run sudo python setup.py install</li>\n<li><p>If you get error something like this: \"Environment Error: /usr/local/bin/mysql_config not found\" then try to add path ass follows: \"export PATH=$PATH:/usr/local/mysql/bin\". But id did not helped to me and I found another solution. In the end of command execution error output which looks like this:</p>\n<p>File \"/path_to_file/MySQL-python-1.2.4b4/setup_posix.py\", line 25, in mysql_config\nraise EnvironmentError(\"%s not found\" % (mysql_config.path,))</p></li>\n<li><p>Open setup_posix.py with vim and go to line 25 (In your case it can be different unless if it is same version). </p></li>\n<li><p>Line 25 should look like this after your editing unless your mysql have symbolic link like follows '/usr/local/mysql/bin/': </p>\n<p><code>f = popen(\"%s --%s\" % ('/usr/local/mysql/bin/mysql_config', what))</code></p></li>\n<li><p>After this I got another error as following:</p>\n<p>django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: dlopen(/Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so, 2): Library not loaded: libmysqlclient.18.dylib\nReferenced from: /Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so\nReason: image not found</p></li>\n<li><p>Finally I did following in console:</p>\n<p>sudo ln -s /usr/local/mysql/lib/libmysqlclient.18.dylib /usr/lib/libmysqlclient.18.dylib</p></li>\n</ol>\n<p>Currently everything works fine. So I hope it will be helpful for somebody who uses Mac. :)</p>\n", "abstract": "When I set up Django development environment for PyCharm in Mac OS X Mountain Lion with python, mysql, sequel pro application I got error same as owner of this thread.\nHowever, my answer for them who is running python-mysqldb under Mac OS Mountain Lion x86_x64 (MySql and Python also should be same architecture) and already tried everything like pip and etc. In order fix this problem do following steps: If you get error something like this: \"Environment Error: /usr/local/bin/mysql_config not found\" then try to add path ass follows: \"export PATH=$PATH:/usr/local/mysql/bin\". But id did not helped to me and I found another solution. In the end of command execution error output which looks like this: File \"/path_to_file/MySQL-python-1.2.4b4/setup_posix.py\", line 25, in mysql_config\nraise EnvironmentError(\"%s not found\" % (mysql_config.path,)) Open setup_posix.py with vim and go to line 25 (In your case it can be different unless if it is same version).  Line 25 should look like this after your editing unless your mysql have symbolic link like follows '/usr/local/mysql/bin/':  f = popen(\"%s --%s\" % ('/usr/local/mysql/bin/mysql_config', what)) After this I got another error as following: django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: dlopen(/Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so, 2): Library not loaded: libmysqlclient.18.dylib\nReferenced from: /Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so\nReason: image not found Finally I did following in console: sudo ln -s /usr/local/mysql/lib/libmysqlclient.18.dylib /usr/lib/libmysqlclient.18.dylib Currently everything works fine. So I hope it will be helpful for somebody who uses Mac. :)"}, {"id": 55086635, "score": 9, "vote": 0, "content": "<p>I've <strong>solved</strong> this issue in this environment:</p>\n<pre><code class=\"python\">$ pyenv --version\npyenv 1.2.9\n\n$ python --version\nPython 3.7.2\n\n$ python -m django --version\n2.1.7\n</code></pre>\n<p><strong><em>./settings.py</em></strong></p>\n<pre><code class=\"python\">DATABASES = {\n    'default': {\n        'NAME': 'my_db_name',\n        'ENGINE': 'mysql.connector.django',   # 'django.db.backends.mysql'\n        'USER': '&lt;user&gt;',\n        'PASSWORD': '&lt;pass&gt;',\n        'HOST': 'localhost',\n        'PORT': 3306,\n        'OPTIONS': {\n            'autocommit': True,\n        },\n    }\n}\n</code></pre>\n<p>If you use <code>'ENGINE': 'mysql.connector.django'</code> , install driver executing:  </p>\n<pre><code class=\"python\">$ pip install mysql-connector-python\nSuccessfully installed mysql-connector-python-8.0.15 protobuf-3.7.0 six-1.12.0\n</code></pre>\n<p><strong>Note that <code>$ pip install mysql-python</code>didn't work for me.</strong> </p>\n<p>Note that if you use  <code>'ENGINE': 'django.db.backends.mysql'</code> , you should install driver executing:<br/>\n<code>$ pip install mysqlclient</code></p>\n<p>Finally execute:<br/>\n<code>$ python manage.py migrate</code></p>\n<p>If it's all right, python creates all these tables id database: </p>\n<pre><code class=\"python\">auth_group\nauth_group_permissions\nauth_permission\nauth_user\nauth_user_groups\nauth_user_user_permissions\ndjango_admin_log\ndjango_content_type\ndjango_migrations\ndjango_session\n</code></pre>\n", "abstract": "I've solved this issue in this environment: ./settings.py If you use 'ENGINE': 'mysql.connector.django' , install driver executing:   Note that $ pip install mysql-pythondidn't work for me.  Note that if you use  'ENGINE': 'django.db.backends.mysql' , you should install driver executing:\n$ pip install mysqlclient Finally execute:\n$ python manage.py migrate If it's all right, python creates all these tables id database: "}, {"id": 21419619, "score": 8, "vote": 0, "content": "<p>You are missing the python <code>mysqldb</code> library. Use this command (for Debian/Ubuntu) to install it:\n<code>sudo apt-get install python-mysqldb</code></p>\n", "abstract": "You are missing the python mysqldb library. Use this command (for Debian/Ubuntu) to install it:\nsudo apt-get install python-mysqldb"}, {"id": 52020641, "score": 8, "vote": 0, "content": "<p>It is because it did not find sql connector. try:</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n", "abstract": "It is because it did not find sql connector. try:"}, {"id": 54730700, "score": 8, "vote": 0, "content": "<p>If you are using Python version 3.4 or above, you have to install</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">sudo apt-get install python3-dev libmysqlclient-dev \n</code></pre>\n<p>in your terminal. Then install <code>pip install mysqlclient</code> on your virtual env or where you installed pip.</p>\n", "abstract": "If you are using Python version 3.4 or above, you have to install in your terminal. Then install pip install mysqlclient on your virtual env or where you installed pip."}, {"id": 20091657, "score": 6, "vote": 0, "content": "<p>Download and install Mysql-python from here for windows environment.  <a href=\"http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python\" rel=\"noreferrer\">http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python</a>. </p>\n", "abstract": "Download and install Mysql-python from here for windows environment.  http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python. "}, {"id": 47766023, "score": 5, "vote": 0, "content": "<p>For Ubuntu 16.04 and 18.04 or python 3 versions</p>\n<pre><code class=\"python\">sudo apt-get install python3-mysqldb\n</code></pre>\n", "abstract": "For Ubuntu 16.04 and 18.04 or python 3 versions"}, {"id": 21210005, "score": 3, "vote": 0, "content": "<p>On Ubuntu it is advised to use the distributions repository.</p>\n<pre><code class=\"python\">sudo apt-get install python-mysqldb\n</code></pre>\n", "abstract": "On Ubuntu it is advised to use the distributions repository."}, {"id": 58418894, "score": 3, "vote": 0, "content": "<p>try:</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n<p>I recommend to use it in a virtual environment.</p>\n", "abstract": "try: I recommend to use it in a virtual environment."}, {"id": 30059559, "score": 2, "vote": 0, "content": "<p>With the same error message as Will, it worked for me to install mysql first as the missing file will be added during the installation.\nSo after</p>\n<pre><code class=\"python\">brew install mysql\n\npip install mysql-python\n</code></pre>\n<p>ran without errors.</p>\n", "abstract": "With the same error message as Will, it worked for me to install mysql first as the missing file will be added during the installation.\nSo after ran without errors."}, {"id": 40741218, "score": 2, "vote": 0, "content": "<p>This happened with me as well and I believe this has become a common error not only for Django developers but also for Flask as well, so the way I solved this issue was using brew.</p>\n<ol>\n<li><code>brew install mysql</code></li>\n<li><code>sudo pip install mysql-python</code></li>\n</ol>\n<p>This way every single issue was solved and both frameworks work absolutely fine.</p>\n<p>P.S.: For those who use macports (such as myself), this can be an issue as brew works in a different level, my advice is to use brew instead of macports</p>\n<p>I hope I could be helpful.</p>\n", "abstract": "This happened with me as well and I believe this has become a common error not only for Django developers but also for Flask as well, so the way I solved this issue was using brew. This way every single issue was solved and both frameworks work absolutely fine. P.S.: For those who use macports (such as myself), this can be an issue as brew works in a different level, my advice is to use brew instead of macports I hope I could be helpful."}, {"id": 64658301, "score": 2, "vote": 0, "content": "<p>I think it is the version error.</p>\n<p>try installing this in following order:</p>\n<ol>\n<li><p><code>sudo apt-get install python3-mysqldb</code></p>\n</li>\n<li><p><code>pip3 install mysqlclient</code></p>\n</li>\n<li><p><code>python3 manage.py makemigrations</code></p>\n</li>\n<li><p><code>python3 manage.py migrate</code></p>\n</li>\n</ol>\n", "abstract": "I think it is the version error. try installing this in following order: sudo apt-get install python3-mysqldb pip3 install mysqlclient python3 manage.py makemigrations python3 manage.py migrate"}, {"id": 54662949, "score": 1, "vote": 0, "content": "<p>I was having the same problem.The following solved my issue</p>\n<p>Run <strong>pip install pymysql</strong> in your shell</p>\n<p>Then, edit the init.py file in your project origin directory(the same as settings.py)\nand then</p>\n<p>add:</p>\n<p><strong>import pymysql</strong></p>\n<p><strong>pymysql.install_as_MySQLdb()</strong></p>\n<p>this should solve the problem.</p>\n", "abstract": "I was having the same problem.The following solved my issue Run pip install pymysql in your shell Then, edit the init.py file in your project origin directory(the same as settings.py)\nand then add: import pymysql pymysql.install_as_MySQLdb() this should solve the problem."}, {"id": 71764343, "score": 1, "vote": 0, "content": "<p>I was stuck in this for a very long and then I realized what's wrong django checks for mysqlclient if it's not installed before django it throws this error so simple solution is</p>\n<pre><code class=\"python\">pip uninstall django\npip install mysqlclient\npip install django\n</code></pre>\n", "abstract": "I was stuck in this for a very long and then I realized what's wrong django checks for mysqlclient if it's not installed before django it throws this error so simple solution is"}, {"id": 18330186, "score": 0, "vote": 0, "content": "<p>Just to add to other answers, if you're using Django, it is advisable that you install mysql-python BEFORE installing Django.</p>\n", "abstract": "Just to add to other answers, if you're using Django, it is advisable that you install mysql-python BEFORE installing Django."}, {"id": 39810042, "score": 0, "vote": 0, "content": "<p>I wasted a lot of time on this. Turns out that the default database library is not supported for Python 3. <a href=\"https://stackoverflow.com/questions/35152201/django-1-8-throwing-importerror-no-module-named-mysqldb\">You have to use a different one</a>.</p>\n", "abstract": "I wasted a lot of time on this. Turns out that the default database library is not supported for Python 3. You have to use a different one."}, {"id": 40446881, "score": 0, "vote": 0, "content": "<p>if the error looks like this </p>\n<pre><code class=\"python\">django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module:\ndlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/_mysql.so, 2): Library not loaded: \n/usr/local/opt/mysql/lib/libmysqlclient.20.dylib\nReferenced from: /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/_mysql.so\n</code></pre>\n<p>then try :</p>\n<pre><code class=\"python\">pip install python-mysqldb\n</code></pre>\n", "abstract": "if the error looks like this  then try :"}, {"id": 47436915, "score": 0, "vote": 0, "content": "<p>Faced similar issue. I tried installing <code>mysql-python</code> using pip, but it failed due to gcc dependency errors.</p>\n<p>The solution that worked for me</p>\n<pre><code class=\"python\">conda install mysql-python\n</code></pre>\n<p>Please note that I already had anaconda installed, which didn't had gcc dependency.</p>\n", "abstract": "Faced similar issue. I tried installing mysql-python using pip, but it failed due to gcc dependency errors. The solution that worked for me Please note that I already had anaconda installed, which didn't had gcc dependency."}, {"id": 66485836, "score": 0, "vote": 0, "content": "<p><strong>I faced this similar error to when I wanted to use MySQL as the default database.</strong><br/>\nThese error might be as a result of the fact that <a href=\"https://pypi.org/project/mysqlclient/\" rel=\"nofollow noreferrer\">mysqlclient</a> hasn't been installed.<br/>\nAnd this can be installed from the command line using;<br/>\n<code>pip install mysqlclient</code>  (for recent versions of python)<br/>\nor   <code>pip install mysql-python</code> (for older versions of python)<br/>\n<strong>In my case, after meeting this requirement, I still had this same error and it was solved as below;</strong>\nIn your <em>django project directory</em>, <em><strong>select venv directory</strong></em> and under these directory,<br/>\nselect the <em><strong>pyvenv.cfg</strong></em> file and click allow on the dialog box which ask you if you want to edit this file.  In this file,<br/>\n<code>include-system-site-packages</code> is set to <code>false</code>. So change this line of code to true like this;</p>\n<pre><code class=\"python\">include-system-site-packages = true\n</code></pre>\n<p>Everything now works well</p>\n", "abstract": "I faced this similar error to when I wanted to use MySQL as the default database.\nThese error might be as a result of the fact that mysqlclient hasn't been installed.\nAnd this can be installed from the command line using;\npip install mysqlclient  (for recent versions of python)\nor   pip install mysql-python (for older versions of python)\nIn my case, after meeting this requirement, I still had this same error and it was solved as below;\nIn your django project directory, select venv directory and under these directory,\nselect the pyvenv.cfg file and click allow on the dialog box which ask you if you want to edit this file.  In this file,\ninclude-system-site-packages is set to false. So change this line of code to true like this; Everything now works well"}, {"id": 69045409, "score": 0, "vote": 0, "content": "<p>try this, simply using pip:\nOn Windows the recommended command is:</p>\n<pre><code class=\"python\">python -m pip install mysqlclient\n</code></pre>\n<p>use it in a virtual environment</p>\n", "abstract": "try this, simply using pip:\nOn Windows the recommended command is: use it in a virtual environment"}, {"id": 74084837, "score": 0, "vote": 0, "content": "<p>I had tried running different commands in this question. However none of them work for me in MXLinux. It's a Debian 11 based distro (As of writing this). So, this answer should be applicable to Debian also.</p>\n<pre><code class=\"python\">sudo apt-get install python3-dev default-libmysqlclient-dev\n</code></pre>\n<p>Then install mysql in your python environment</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n<p>I have tested this against, Python==3.8.13. AFAIK, this will work with Pyenv, Conda, Pipenv, Poetry etc. too.</p>\n<p>Note:- This may work with Latest Version of Debian Based distros like LinuxMint, KDE Neon, Zorin OS, Kubuntu, elementary OS, Pop!_OS, Linux Lite, Linuxfx, etc. too.</p>\n", "abstract": "I had tried running different commands in this question. However none of them work for me in MXLinux. It's a Debian 11 based distro (As of writing this). So, this answer should be applicable to Debian also. Then install mysql in your python environment I have tested this against, Python==3.8.13. AFAIK, this will work with Pyenv, Conda, Pipenv, Poetry etc. too. Note:- This may work with Latest Version of Debian Based distros like LinuxMint, KDE Neon, Zorin OS, Kubuntu, elementary OS, Pop!_OS, Linux Lite, Linuxfx, etc. too."}, {"id": 65490570, "score": -1, "vote": 0, "content": "<p><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-django-backend.html\" rel=\"nofollow noreferrer\">Source</a></p>\n<p>Install (if not found):</p>\n<pre><code class=\"python\">pip install mysql.connector\n</code></pre>\n<p>Settings:</p>\n<pre><code class=\"python\">'default': {\n    'ENGINE': 'mysql.connector.django',\n    'NAME': '...',\n    'HOST': '...',\n    'PORT': '3306',\n    'USER': '...',\n    'PASSWORD': '...',\n}\n</code></pre>\n", "abstract": "Source Install (if not found): Settings:"}, {"id": 71891791, "score": -1, "vote": 0, "content": "<p>Try this</p>\n<pre><code class=\"python\">apt-get install libmysqlclient-dev\n</code></pre>\n<p>and try again to install mysqlclient:</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n", "abstract": "Try this and try again to install mysqlclient:"}, {"id": 45318470, "score": -3, "vote": 0, "content": "<p>Maybe you can try the following mode of operation:<br/>\n<code>sudo python manage.py runserver 0.0.0.0:8000</code></p>\n", "abstract": "Maybe you can try the following mode of operation:\nsudo python manage.py runserver 0.0.0.0:8000"}]}, {"link": "https://stackoverflow.com/questions/60805/getting-random-row-through-sqlalchemy", "question": {"id": "60805", "title": "Getting random row through SQLAlchemy", "content": "<p>How do I select one or more random rows from a table using SQLAlchemy? </p>\n", "abstract": "How do I select one or more random rows from a table using SQLAlchemy? "}, "answers": [{"id": 60815, "score": 150, "vote": 0, "content": "<p>This is very much a database-specific issue.</p>\n<p>I know that PostgreSQL, SQLite, MySQL, and Oracle have the ability to order by a random function, so you can use this in SQLAlchemy:</p>\n<pre><code class=\"python\">from  sqlalchemy.sql.expression import func, select\n\nselect.order_by(func.random()) # for PostgreSQL, SQLite\n\nselect.order_by(func.rand()) # for MySQL\n\nselect.order_by('dbms_random.value') # For Oracle\n</code></pre>\n<p>Next, you need to limit the query by the number of records you need (for example using <code>.limit()</code>).</p>\n<p>Bear in mind that at least in PostgreSQL, selecting random record has severe perfomance issues; <a href=\"http://www.depesz.com/index.php/2007/09/16/my-thoughts-on-getting-random-row/\" rel=\"noreferrer\">here</a> is good article about it.</p>\n", "abstract": "This is very much a database-specific issue. I know that PostgreSQL, SQLite, MySQL, and Oracle have the ability to order by a random function, so you can use this in SQLAlchemy: Next, you need to limit the query by the number of records you need (for example using .limit()). Bear in mind that at least in PostgreSQL, selecting random record has severe perfomance issues; here is good article about it."}, {"id": 33583008, "score": 28, "vote": 0, "content": "<p>Here's four different variations, ordered from slowest to fastest. <code>timeit</code> results at the bottom:</p>\n<pre><code class=\"python\">from sqlalchemy.sql import func\nfrom sqlalchemy.orm import load_only\n\ndef simple_random():\n    return random.choice(model_name.query.all())\n\ndef load_only_random():\n    return random.choice(model_name.query.options(load_only('id')).all())\n\ndef order_by_random():\n    return model_name.query.order_by(func.random()).first()\n\ndef optimized_random():\n    return model_name.query.options(load_only('id')).offset(\n            func.floor(\n                func.random() *\n                db.session.query(func.count(model_name.id))\n            )\n        ).limit(1).all()\n</code></pre>\n<p><code>timeit</code> results for 10,000 runs on my Macbook against a PostgreSQL table with 300 rows:</p>\n<pre><code class=\"python\">simple_random(): \n    90.09954111799925\nload_only_random():\n    65.94714171699889\norder_by_random():\n    23.17819356000109\noptimized_random():\n    19.87806927999918\n</code></pre>\n<p>You can easily see that using <code>func.random()</code> is far faster than returning all results to Python's <code>random.choice()</code>. </p>\n<p>Additionally, as the size of the table increases, the performance of <code>order_by_random()</code> will degrade significantly because an <code>ORDER BY</code> requires a full table scan versus the <code>COUNT</code> in <code>optimized_random()</code> can use an index.</p>\n", "abstract": "Here's four different variations, ordered from slowest to fastest. timeit results at the bottom: timeit results for 10,000 runs on my Macbook against a PostgreSQL table with 300 rows: You can easily see that using func.random() is far faster than returning all results to Python's random.choice().  Additionally, as the size of the table increases, the performance of order_by_random() will degrade significantly because an ORDER BY requires a full table scan versus the COUNT in optimized_random() can use an index."}, {"id": 390676, "score": 27, "vote": 0, "content": "<p>If you are using the orm and the table is not big (or you have its amount of rows cached) and you want it to be database independent the really simple approach is.</p>\n<pre><code class=\"python\">import random\nrand = random.randrange(0, session.query(Table).count()) \nrow = session.query(Table)[rand]\n</code></pre>\n<p>This is cheating slightly but thats why you use an orm.</p>\n", "abstract": "If you are using the orm and the table is not big (or you have its amount of rows cached) and you want it to be database independent the really simple approach is. This is cheating slightly but thats why you use an orm."}, {"id": 14906244, "score": 23, "vote": 0, "content": "<p>There is a simple way to pull a random row that IS database independent.\nJust use .offset() . No need to pull all rows:</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">import random\nquery = DBSession.query(Table)\nrowCount = int(query.count())\nrandomRow = query.offset(int(rowCount*random.random())).first()\n</code></pre>\n<p>Where Table is your table (or you could put any query there).\nIf you want a few rows, then you can just run this multiple times, and make sure that each row is not identical to the previous.</p>\n", "abstract": "There is a simple way to pull a random row that IS database independent.\nJust use .offset() . No need to pull all rows: Where Table is your table (or you could put any query there).\nIf you want a few rows, then you can just run this multiple times, and make sure that each row is not identical to the previous."}, {"id": 52692368, "score": 8, "vote": 0, "content": "<p>Some SQL DBMS, namely Microsoft SQL Server, DB2, and <a href=\"https://www.postgresql.org/docs/current/static/sql-select.html#SQL-FROM\" rel=\"nofollow noreferrer\">PostgreSQL</a> have implemented the SQL:2003 <code>TABLESAMPLE</code> clause. Support was added to SQLAlchemy <a href=\"https://bitbucket.org/zzzeek/sqlalchemy/issues/3718\" rel=\"nofollow noreferrer\">in version 1.1</a>. It allows returning a sample of a table using different sampling methods \u2013 the standard requires <code>SYSTEM</code> and <code>BERNOULLI</code>, which return a desired approximate percentage of a table.</p>\n<p>In SQLAlchemy <a href=\"https://docs.sqlalchemy.org/en/latest/core/selectable.html#sqlalchemy.sql.expression.FromClause.tablesample\" rel=\"nofollow noreferrer\"><code>FromClause.tablesample()</code></a> and <a href=\"https://docs.sqlalchemy.org/en/latest/core/selectable.html#sqlalchemy.sql.expression.tablesample\" rel=\"nofollow noreferrer\"><code>tablesample()</code></a> are used to produce a <a href=\"https://docs.sqlalchemy.org/en/latest/core/selectable.html#sqlalchemy.sql.expression.TableSample\" rel=\"nofollow noreferrer\"><code>TableSample</code></a> construct:</p>\n<pre><code class=\"python\"># Approx. 1%, using SYSTEM method\nsample1 = mytable.tablesample(1)\n\n# Approx. 1%, using BERNOULLI method\nsample2 = mytable.tablesample(func.bernoulli(1))\n</code></pre>\n<p>There's a slight gotcha when used with mapped classes: the produced <code>TableSample</code> object must be aliased in order to be used to query model objects:</p>\n<pre><code class=\"python\">sample = aliased(MyModel, tablesample(MyModel, 1))\nres = session.query(sample).all()\n</code></pre>\n<hr/>\n<p>Since many of the answers contain performance benchmarks, I'll include some simple tests here as well. Using a simple table in PostgreSQL with about a  million rows and a single integer column, select (approx.) 1% sample:</p>\n<pre><code class=\"python\">In [24]: %%timeit\n    ...: foo.select().\\\n    ...:     order_by(func.random()).\\\n    ...:     limit(select([func.round(func.count() * 0.01)]).\n    ...:           select_from(foo).\n    ...:           as_scalar()).\\\n    ...:     execute().\\\n    ...:     fetchall()\n    ...: \n307 ms \u00b1 5.72 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [25]: %timeit foo.tablesample(1).select().execute().fetchall()\n6.36 ms \u00b1 188 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [26]: %timeit foo.tablesample(func.bernoulli(1)).select().execute().fetchall()\n19.8 ms \u00b1 381 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>\n<p>Before rushing to use <code>SYSTEM</code> sampling method one should know that it samples <em>pages</em>, not individual tuples, so it might not be suitable for small tables, for example, and may not produce as random results, if the table is clustered.</p>\n<hr/>\n<p>If using a dialect that does not allow passing the sample percentage / number of rows and seed as parameters, and a driver that does not inline values, then either pass the values as literal SQL text <em>if they are static</em>, or inline them using a custom SQLA compiler extension:</p>\n<pre><code class=\"python\">from sqlalchemy.ext.compiler import compiles\nfrom sqlalchemy.sql import TableSample\n\n@compiles(TableSample)\ndef visit_tablesample(tablesample, self, asfrom=False, **kw):\n    \"\"\" Compile `TableSample` with values inlined.\n    \"\"\"\n    kw_literal_binds = {**kw, \"literal_binds\": True}\n    text = \"%s TABLESAMPLE %s\" % (\n        self.visit_alias(tablesample, asfrom=True, **kw),\n        tablesample._get_method()._compiler_dispatch(self, **kw_literal_binds),\n    )\n\n    if tablesample.seed is not None:\n        text += \" REPEATABLE (%s)\" % (\n            tablesample.seed._compiler_dispatch(self, **kw_literal_binds)\n        )\n\n    return text\n\nfrom sqlalchemy import table, literal, text\n\n# Static percentage\nprint(table(\"tbl\").tablesample(text(\"5 PERCENT\")))\n# Compiler inlined values\nprint(table(\"tbl\").tablesample(5, seed=literal(42)))\n</code></pre>\n", "abstract": "Some SQL DBMS, namely Microsoft SQL Server, DB2, and PostgreSQL have implemented the SQL:2003 TABLESAMPLE clause. Support was added to SQLAlchemy in version 1.1. It allows returning a sample of a table using different sampling methods \u2013 the standard requires SYSTEM and BERNOULLI, which return a desired approximate percentage of a table. In SQLAlchemy FromClause.tablesample() and tablesample() are used to produce a TableSample construct: There's a slight gotcha when used with mapped classes: the produced TableSample object must be aliased in order to be used to query model objects: Since many of the answers contain performance benchmarks, I'll include some simple tests here as well. Using a simple table in PostgreSQL with about a  million rows and a single integer column, select (approx.) 1% sample: Before rushing to use SYSTEM sampling method one should know that it samples pages, not individual tuples, so it might not be suitable for small tables, for example, and may not produce as random results, if the table is clustered. If using a dialect that does not allow passing the sample percentage / number of rows and seed as parameters, and a driver that does not inline values, then either pass the values as literal SQL text if they are static, or inline them using a custom SQLA compiler extension:"}, {"id": 50345203, "score": 2, "vote": 0, "content": "<p><strong>This is my function to select random row(s) of a table:</strong> </p>\n<pre><code class=\"python\">from sqlalchemy.sql.expression import func\n\ndef random_find_rows(sample_num):\n    if not sample_num:\n        return []\n\n    session = DBSession()\n    return session.query(Table).order_by(func.random()).limit(sample_num).all()\n</code></pre>\n", "abstract": "This is my function to select random row(s) of a table: "}, {"id": 42780139, "score": 0, "vote": 0, "content": "<p>This is the solution I use:</p>\n<pre><code class=\"python\">from random import randint\n\nrows_query = session.query(Table)                # get all rows\nif rows_query.count() &gt; 0:                       # make sure there's at least 1 row\n    rand_index = randint(0,rows_query.count()-1) # get random index to rows \n    rand_row   = rows_query.all()[rand_index]    # use random index to get random row\n</code></pre>\n", "abstract": "This is the solution I use:"}, {"id": 21242325, "score": -3, "vote": 0, "content": "<h2>this solution will select a single random row</h2>\n<p>This solution requires that the primary key is named id, it should be if its not already:   </p>\n<pre><code class=\"python\">import random\nmax_model_id = YourModel.query.order_by(YourModel.id.desc())[0].id\nrandom_id = random.randrange(0,max_model_id)\nrandom_row = YourModel.query.get(random_id)\nprint random_row\n</code></pre>\n", "abstract": "This solution requires that the primary key is named id, it should be if its not already:   "}, {"id": 62321734, "score": -4, "vote": 0, "content": "<p>Use this simplest method\nthis example on choosing a random question from the database:-</p>\n<pre><code class=\"python\">#first import the random module\nimport random\n\n#then choose what ever Model you want inside random.choise() method\nget_questions = random.choice(Question.query.all())\n</code></pre>\n", "abstract": "Use this simplest method\nthis example on choosing a random question from the database:-"}, {"id": 60811, "score": -8, "vote": 0, "content": "<p>Theres a couple of ways through SQL, depending on which data base is being used.</p>\n<p>(I think SQLAlchemy can use all these anyways)</p>\n<p>mysql:</p>\n<pre><code class=\"python\">SELECT colum FROM table\nORDER BY RAND()\nLIMIT 1\n</code></pre>\n<p>PostgreSQL:</p>\n<pre><code class=\"python\">SELECT column FROM table\nORDER BY RANDOM()\nLIMIT 1\n</code></pre>\n<p>MSSQL:</p>\n<pre><code class=\"python\">SELECT TOP 1 column FROM table\nORDER BY NEWID()\n</code></pre>\n<p>IBM DB2:</p>\n<pre><code class=\"python\">SELECT column, RAND() as IDX\nFROM table\nORDER BY IDX FETCH FIRST 1 ROWS ONLY\n</code></pre>\n<p>Oracle:</p>\n<pre><code class=\"python\">SELECT column FROM\n(SELECT column FROM table\nORDER BY dbms_random.value)\nWHERE rownum = 1\n</code></pre>\n<p>However I don't know of any standard way</p>\n", "abstract": "Theres a couple of ways through SQL, depending on which data base is being used. (I think SQLAlchemy can use all these anyways) mysql: PostgreSQL: MSSQL: IBM DB2: Oracle: However I don't know of any standard way"}]}, {"link": "https://stackoverflow.com/questions/36028759/how-to-open-and-convert-sqlite-database-to-pandas-dataframe", "question": {"id": "36028759", "title": "How to open and convert sqlite database to pandas dataframe", "content": "<p>I have downloaded some datas as a sqlite database (data.db) and I want to open this database in python and then convert it into pandas dataframe.</p>\n<p>This is so far I have done</p>\n<pre><code class=\"python\">import sqlite3\nimport pandas    \ndat = sqlite3.connect('data.db') #connected to database with out error\npandas.DataFrame.from_records(dat, index=None, exclude=None, columns=None, coerce_float=False, nrows=None)\n</code></pre>\n<p>But its throwing this error</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py\", line 980, in from_records\n    coerce_float=coerce_float)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py\", line 5353, in _to_arrays\n    if not len(data):\nTypeError: object of type 'sqlite3.Connection' has no len()\n</code></pre>\n<p>How to convert sqlite database to pandas dataframe</p>\n", "abstract": "I have downloaded some datas as a sqlite database (data.db) and I want to open this database in python and then convert it into pandas dataframe. This is so far I have done But its throwing this error How to convert sqlite database to pandas dataframe"}, "answers": [{"id": 36029761, "score": 168, "vote": 0, "content": "<p>Despite sqlite being part of the Python Standard Library and is a nice and easy interface to SQLite databases, the Pandas tutorial <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-tables\" rel=\"noreferrer\">states</a>:</p>\n<blockquote>\n<p>Note In order to use read_sql_table(), you must have the SQLAlchemy\noptional dependency installed.</p>\n</blockquote>\n<p>But Pandas still supports sqlite3 access if you want to avoid installing SQLAlchemy:</p>\n<pre><code class=\"python\">import sqlite3\nimport pandas as pd\n# Create your connection.\ncnx = sqlite3.connect('file.db')\n\ndf = pd.read_sql_query(\"SELECT * FROM table_name\", cnx)\n</code></pre>\n<p>As stated <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sqlite-fallback\" rel=\"noreferrer\">here</a>, but you need to know the name of the used table in advance.</p>\n", "abstract": "Despite sqlite being part of the Python Standard Library and is a nice and easy interface to SQLite databases, the Pandas tutorial states: Note In order to use read_sql_table(), you must have the SQLAlchemy\noptional dependency installed. But Pandas still supports sqlite3 access if you want to avoid installing SQLAlchemy: As stated here, but you need to know the name of the used table in advance."}, {"id": 36029723, "score": 13, "vote": 0, "content": "<p>The line</p>\n<pre><code class=\"python\">data = sqlite3.connect('data.db')\n</code></pre>\n<p>opens a connection to the database. There are no records queried up to this. So you have to execute a query afterward and provide this to the pandas <code>DataFrame</code> constructor.</p>\n<p>It should look similar to this</p>\n<pre><code class=\"python\">import sqlite3\nimport pandas as pd\n\ndat = sqlite3.connect('data.db')\nquery = dat.execute(\"SELECT * From &lt;TABLENAME&gt;\")\ncols = [column[0] for column in query.description]\nresults= pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\n</code></pre>\n<p>I am not really firm with SQL commands, so you should check the correctness of the query.  should be the name of the table in your database.</p>\n", "abstract": "The line opens a connection to the database. There are no records queried up to this. So you have to execute a query afterward and provide this to the pandas DataFrame constructor. It should look similar to this I am not really firm with SQL commands, so you should check the correctness of the query.  should be the name of the table in your database."}, {"id": 67938218, "score": 11, "vote": 0, "content": "<p>Parsing a sqlite .db into a dictionary of dataframes without knowing the table names:</p>\n<pre><code class=\"python\">def read_sqlite(dbfile):\n    import sqlite3\n    from pandas import read_sql_query, read_sql_table\n\n    with sqlite3.connect(dbfile) as dbcon:\n        tables = list(read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", dbcon)['name'])\n        out = {tbl : read_sql_query(f\"SELECT * from {tbl}\", dbcon) for tbl in tables}\n\n   return out\n</code></pre>\n", "abstract": "Parsing a sqlite .db into a dictionary of dataframes without knowing the table names:"}, {"id": 46111478, "score": 6, "vote": 0, "content": "<p>Search <code>sqlalchemy</code>, <code>engine</code> and database name in google (sqlite in this case):</p>\n<pre><code class=\"python\">import pandas as pd\nimport sqlalchemy\n\ndb_name = \"data.db\"\ntable_name = \"LITTLE_BOBBY_TABLES\"\n\nengine = sqlalchemy.create_engine(\"sqlite:///%s\" % db_name, execution_options={\"sqlite_raw_colnames\": True})\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n", "abstract": "Search sqlalchemy, engine and database name in google (sqlite in this case):"}, {"id": 63092048, "score": 4, "vote": 0, "content": "<p>I wrote a piece of code up that saves tables in a database file such as .sqlite or .db and creates an excel file out of it with each table as a sheet or makes individual tables into csvs.</p>\n<p>Note: You don't need to know the table names in advance!</p>\n<pre><code class=\"python\">import os, fnmatch\nimport sqlite3\nimport pandas as pd\n\n#creates a directory without throwing an error\ndef create_dir(dir):\n  if not os.path.exists(dir):\n    os.makedirs(dir)\n    print(\"Created Directory : \", dir)\n  else:\n    print(\"Directory already existed : \", dir)\n  return dir\n\n#finds files in a directory corresponding to a regex query\ndef find(pattern, path):\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result\n\n\n\n#convert sqlite databases(.db,.sqlite) to pandas dataframe(excel with each table as a different sheet or individual csv sheets)\ndef save_db(dbpath=None,excel_path=None,csv_path=None,extension=\"*.sqlite\",csvs=True,excels=True):\n    if (excels==False and csvs==False):\n      print(\"Atleast one of the parameters need to be true: csvs or excels\")\n      return -1\n\n    #little code to find files by extension\n    if dbpath==None:\n      files=find(extension,os.getcwd())\n      if len(files)&gt;1:\n        print(\"Multiple files found! Selecting the first one found!\")\n        print(\"To locate your file, set dbpath=&lt;yourpath&gt;\")\n      dbpath = find(extension,os.getcwd())[0] if dbpath==None else dbpath\n      print(\"Reading database file from location :\",dbpath)\n\n    #path handling\n\n    external_folder,base_name=os.path.split(os.path.abspath(dbpath))\n    file_name=os.path.splitext(base_name)[0] #firstname without .\n    exten=os.path.splitext(base_name)[-1]   #.file_extension\n\n    internal_folder=\"Saved_Dataframes_\"+file_name\n    main_path=os.path.join(external_folder,internal_folder)\n    create_dir(main_path)\n\n\n    excel_path=os.path.join(main_path,\"Excel_Multiple_Sheets.xlsx\") if excel_path==None else excel_path\n    csv_path=main_path if csv_path==None else csv_path\n\n    db = sqlite3.connect(dbpath)\n    cursor = db.cursor()\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = cursor.fetchall()\n    print(len(tables),\"Tables found :\")\n\n    if excels==True:\n      #for writing to excel(xlsx) we will be needing this!\n      try:\n        import XlsxWriter\n      except ModuleNotFoundError:\n        !pip install XlsxWriter\n\n    if (excels==True and csvs==True):\n      writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n      i=0\n      for table_name in tables:\n          table_name = table_name[0]\n          table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n          i+=1\n          print(\"Parsing Excel Sheet \",i,\" : \",table_name)\n          table.to_excel(writer, sheet_name=table_name, index=False)\n          print(\"Parsing CSV File \",i,\" : \",table_name)\n          table.to_csv(os.path.join(csv_path,table_name + '.csv'), index_label='index')\n\n      writer.save()\n\n\n    elif excels==True:\n      writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n      i=0\n      for table_name in tables:\n          table_name = table_name[0]\n          table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n          i+=1\n          print(\"Parsing Excel Sheet \",i,\" : \",table_name)\n          table.to_excel(writer, sheet_name=table_name, index=False)\n\n      writer.save()\n\n    elif csvs==True:\n      i=0\n      for table_name in tables:\n          table_name = table_name[0]\n          table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n          i+=1\n          print(\"Parsing CSV File \",i,\" : \",table_name)\n          table.to_csv(os.path.join(csv_path,table_name + '.csv'), index_label='index')\n    cursor.close()\n    db.close()\n    return 0\nsave_db(); \n</code></pre>\n", "abstract": "I wrote a piece of code up that saves tables in a database file such as .sqlite or .db and creates an excel file out of it with each table as a sheet or makes individual tables into csvs. Note: You don't need to know the table names in advance!"}, {"id": 68202654, "score": 3, "vote": 0, "content": "<p>If <code>data.db</code> is your SQLite database and <code>table_name</code> is one of its tables, then you can do:</p>\n<pre><code class=\"python\">import pandas as pd\ndf = pd.read_sql_table('table_name', 'sqlite:///data.db')\n</code></pre>\n<p>No other imports needed.</p>\n", "abstract": "If data.db is your SQLite database and table_name is one of its tables, then you can do: No other imports needed."}, {"id": 52158203, "score": -1, "vote": 0, "content": "<p>i have stored my data in database.sqlite table name is Reviews</p>\n<pre><code class=\"python\">import sqlite3\ncon=sqlite3.connect(\"database.sqlite\")\n\ndata=pd.read_sql_query(\"SELECT * FROM Reviews\",con)\nprint(data)\n</code></pre>\n", "abstract": "i have stored my data in database.sqlite table name is Reviews"}]}, {"link": "https://stackoverflow.com/questions/5503925/how-do-i-use-a-dictionary-to-update-fields-in-django-models", "question": {"id": "5503925", "title": "How do I use a dictionary to update fields in Django models?", "content": "<p>Suppose I have a model like this:</p>\n<pre><code class=\"python\">class Book(models.Model):\n    num_pages = ...\n    author = ...\n    date = ...\n</code></pre>\n<p>Can I create a dictionary, and then insert or update the model using it?</p>\n<pre><code class=\"python\">d = {\"num_pages\":40, author:\"Jack\", date:\"3324\"}\n</code></pre>\n", "abstract": "Suppose I have a model like this: Can I create a dictionary, and then insert or update the model using it?"}, "answers": [{"id": 5506790, "score": 121, "vote": 0, "content": "<p>Here's an example of create using your dictionary d:</p>\n<pre><code class=\"python\">Book.objects.create(**d)\n</code></pre>\n<p>To update an existing model, you will need to use the QuerySet <code>filter</code> method.  Assuming you know the <code>pk</code> of the Book you want to update:</p>\n<pre><code class=\"python\">Book.objects.filter(pk=pk).update(**d)\n</code></pre>\n", "abstract": "Here's an example of create using your dictionary d: To update an existing model, you will need to use the QuerySet filter method.  Assuming you know the pk of the Book you want to update:"}, {"id": 5503999, "score": 86, "vote": 0, "content": "<p>Use <code>**</code> for creating a new model. Loop through the dictionary and use <code>setattr()</code> in order to update an existing model.</p>\n<p>From Tom Christie's Django Rest Framework</p>\n<p><a href=\"https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/serializers.py\" rel=\"noreferrer\">https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/serializers.py</a></p>\n<pre><code class=\"python\">for attr, value in validated_data.items():\n    setattr(instance, attr, value)\ninstance.save()\n</code></pre>\n", "abstract": "Use ** for creating a new model. Loop through the dictionary and use setattr() in order to update an existing model. From Tom Christie's Django Rest Framework https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/serializers.py"}, {"id": 7535133, "score": 75, "vote": 0, "content": "<p>If you know you would like to create it:</p>\n<pre><code class=\"python\">Book.objects.create(**d)\n</code></pre>\n<p>Assuming you need to check for an existing instance, you can find it with get or create:</p>\n<pre><code class=\"python\">instance, created = Book.objects.get_or_create(slug=slug, defaults=d)\nif not created:\n    for attr, value in d.items(): \n        setattr(instance, attr, value)\n    instance.save()\n</code></pre>\n<p>As mentioned in another answer, you can also use the <code>update</code> function on the queryset manager, but i believe that will not send any signals out (which may not matter to you if you aren't using them). However, you probably shouldn't use it to alter a single object:</p>\n<pre><code class=\"python\">Book.objects.filter(id=id).update()\n</code></pre>\n", "abstract": "If you know you would like to create it: Assuming you need to check for an existing instance, you can find it with get or create: As mentioned in another answer, you can also use the update function on the queryset manager, but i believe that will not send any signals out (which may not matter to you if you aren't using them). However, you probably shouldn't use it to alter a single object:"}, {"id": 63121756, "score": 9, "vote": 0, "content": "<p>if you have already Django object and you want to update it's field, you may do it without filter. because you have it already, in this case, yoy may :</p>\n<pre><code class=\"python\">your_obj.__dict__.update(your_dict)\nyour_obj.save()\n</code></pre>\n", "abstract": "if you have already Django object and you want to update it's field, you may do it without filter. because you have it already, in this case, yoy may :"}, {"id": 53659240, "score": 1, "vote": 0, "content": "<p>Adding on top of other answers, here's a bit more secure version to prevent messing up with related fields:</p>\n<pre><code class=\"python\">def is_simple_editable_field(field):\n    return (\n            field.editable\n            and not field.primary_key\n            and not isinstance(field, (ForeignObjectRel, RelatedField))\n    )\n\ndef update_from_dict(instance, attrs, commit):\n    allowed_field_names = {\n        f.name for f in instance._meta.get_fields()\n        if is_simple_editable_field(f)\n    }\n\n    for attr, val in attrs.items():\n        if attr in allowed_field_names:\n            setattr(instance, attr, val)\n\n    if commit:\n        instance.save()\n</code></pre>\n<p>It checks, that field you're trying to update is editable, is not primary key and is not one of related fields.</p>\n<p>Example usage:</p>\n<pre><code class=\"python\">book = Book.objects.first()\nupdate_from_dict(book, {\"num_pages\":40, author:\"Jack\", date:\"3324\"})\n</code></pre>\n<p>The luxury DRF serializers <code>.create</code> and <code>.update</code> methods have is that there is limited and validated set of fields, which is not the case for manual update.</p>\n", "abstract": "Adding on top of other answers, here's a bit more secure version to prevent messing up with related fields: It checks, that field you're trying to update is editable, is not primary key and is not one of related fields. Example usage: The luxury DRF serializers .create and .update methods have is that there is limited and validated set of fields, which is not the case for manual update."}, {"id": 68866906, "score": 1, "vote": 0, "content": "<p>To update one record you can use very handy function:</p>\n<pre><code class=\"python\">class Book(models.Model):\n    num_pages = ...\n    author = ...\n    date = ...\n\n    def update(self,*args, **kwargs):\n            for name,values in kwargs.items():\n                try:\n                    setattr(self,name,values)\n                except KeyError:\n                    pass\n            self.save()\n</code></pre>\n<p>and then:</p>\n<pre><code class=\"python\">d = {\"num_pages\":40, author:\"Jack\", date:\"3324\"}\nbook = Book.objects.first()\nbook.update(**d)\n</code></pre>\n", "abstract": "To update one record you can use very handy function: and then:"}]}, {"link": "https://stackoverflow.com/questions/11714536/check-if-an-object-exists", "question": {"id": "11714536", "title": "Check if an object exists", "content": "<p>I need to check if <code>Model.objects.filter(...)</code> turned up anything, but do not need to insert anything. My code so far is:</p>\n<pre><code class=\"python\">user_pass = log_in(request.POST)  # form class\nif user_pass.is_valid():\n    cleaned_info = user_pass.cleaned_data\n    user_object = User.objects.filter(email = cleaned_info['username'])\n</code></pre>\n", "abstract": "I need to check if Model.objects.filter(...) turned up anything, but do not need to insert anything. My code so far is:"}, "answers": [{"id": 21750566, "score": 187, "vote": 0, "content": "<p>I think the easiest from a logical and efficiency point of view is using the queryset's <strong>exists()</strong> function, documented here:</p>\n<p><a href=\"https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.exists\" rel=\"noreferrer\">https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.exists</a></p>\n<p>So in your example above I would simply write:</p>\n<pre><code class=\"python\">if User.objects.filter(email = cleaned_info['username']).exists():\n    # at least one object satisfying query exists\nelse:\n    # no object satisfying query exists\n</code></pre>\n", "abstract": "I think the easiest from a logical and efficiency point of view is using the queryset's exists() function, documented here: https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.exists So in your example above I would simply write:"}, {"id": 11714635, "score": 77, "vote": 0, "content": "<p>Since <code>filter</code> returns a <code>QuerySet</code>, you can use <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#count\" rel=\"noreferrer\">count</a> to check how many results were returned.  This is assuming you don't actually need the results.</p>\n<pre><code class=\"python\">num_results = User.objects.filter(email = cleaned_info['username']).count()\n</code></pre>\n<p>After looking at the documentation though, it's better to just call len on your filter if you are planning on using the results later, as you'll only be making one sql query:</p>\n<blockquote>\n<p>A count() call performs a SELECT COUNT(*) behind the scenes, so you should always use count() rather than loading all of the record into Python objects and calling len() on the result (unless you need to load the objects into memory anyway, in which case len() will be faster).</p>\n</blockquote>\n<pre><code class=\"python\">num_results = len(user_object)\n</code></pre>\n", "abstract": "Since filter returns a QuerySet, you can use count to check how many results were returned.  This is assuming you don't actually need the results. After looking at the documentation though, it's better to just call len on your filter if you are planning on using the results later, as you'll only be making one sql query: A count() call performs a SELECT COUNT(*) behind the scenes, so you should always use count() rather than loading all of the record into Python objects and calling len() on the result (unless you need to load the objects into memory anyway, in which case len() will be faster)."}, {"id": 54742930, "score": 8, "vote": 0, "content": "<p>If the user exists you can get the user in <code>user_object</code> else <code>user_object</code> will be <code>None</code>.</p>\n<pre><code class=\"python\">try:\n    user_object = User.objects.get(email = cleaned_info['username'])\nexcept User.DoesNotExist:\n    user_object = None\nif user_object:\n    # user exist\n    pass\nelse:\n    # user does not exist\n    pass\n</code></pre>\n", "abstract": "If the user exists you can get the user in user_object else user_object will be None."}, {"id": 11715285, "score": 7, "vote": 0, "content": "<p>the boolean value of an empty QuerySet is also False, so you could also just do...</p>\n<pre><code class=\"python\">...\nif not user_object:\n   do insert or whatever etc.\n</code></pre>\n", "abstract": "the boolean value of an empty QuerySet is also False, so you could also just do..."}, {"id": 11714677, "score": 6, "vote": 0, "content": "<p>You can also use <a href=\"https://docs.djangoproject.com/en/dev/topics/http/shortcuts/#get-object-or-404\" rel=\"noreferrer\">get_object_or_404()</a>, it will raise a <code>Http404</code> if the object wasn't found:</p>\n<pre><code class=\"python\">user_pass = log_in(request.POST) #form class\nif user_pass.is_valid():\n    cleaned_info = user_pass.cleaned_data\n    user_object = get_object_or_404(User, email=cleaned_info['username'])\n    # User object found, you are good to go!\n    ...\n</code></pre>\n", "abstract": "You can also use get_object_or_404(), it will raise a Http404 if the object wasn't found:"}, {"id": 32884494, "score": 4, "vote": 0, "content": "<p>You can use:</p>\n<pre><code class=\"python\">try:\n   # get your models\nexcept ObjectDoesNotExist:\n   # do something\n</code></pre>\n", "abstract": "You can use:"}]}, {"link": "https://stackoverflow.com/questions/4372797/how-do-i-update-a-mongo-document-after-inserting-it", "question": {"id": "4372797", "title": "How do I update a Mongo document after inserting it?", "content": "<p>Let's say I insert the document.</p>\n<pre><code class=\"python\">post = { some dictionary }\nmongo_id = mycollection.insert(post)\n</code></pre>\n<p>Now, let's say I want to add a field and update it. How do I do that? This doesn't seem to work.....</p>\n<pre><code class=\"python\">post = mycollection.find_one({\"_id\":mongo_id}) \npost['newfield'] = \"abc\"\nmycollection.save(post)\n</code></pre>\n", "abstract": "Let's say I insert the document. Now, let's say I want to add a field and update it. How do I do that? This doesn't seem to work....."}, "answers": [{"id": 4374288, "score": 119, "vote": 0, "content": "<p>In pymongo you can update with:</p>\n<pre><code class=\"python\">mycollection.update({'_id':mongo_id}, {\"$set\": post}, upsert=False)\n</code></pre>\n<p>Upsert parameter will insert instead of updating if the post is not found in the database.<br/>\nDocumentation is available at <a href=\"http://www.mongodb.org/display/DOCS/Updating\" rel=\"nofollow noreferrer\">mongodb site</a>.</p>\n<p><strong>UPDATE</strong> For version &gt; 3 use <code>update_one</code> instead of <code>update</code>:</p>\n<pre><code class=\"python\">mycollection.update_one({'_id':mongo_id}, {\"$set\": post}, upsert=False)\n</code></pre>\n", "abstract": "In pymongo you can update with: Upsert parameter will insert instead of updating if the post is not found in the database.\nDocumentation is available at mongodb site. UPDATE For version > 3 use update_one instead of update:"}, {"id": 46608956, "score": 35, "vote": 0, "content": "<pre><code class=\"python\">mycollection.find_one_and_update({\"_id\": mongo_id}, \n                                 {\"$set\": {\"newfield\": \"abc\"}})\n</code></pre>\n<p>should work splendidly for you. If there is no document of id <code>mongo_id</code>, it will fail, unless you also use <code>upsert=True</code>. This returns the old document by default. To get the new one, pass <code>return_document=ReturnDocument.AFTER</code>. All parameters are described in <a href=\"http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find_one_and_update\" rel=\"noreferrer\">the API</a>.</p>\n<p>The method was introduced for MongoDB 3.0. It was extended for 3.2, 3.4, and 3.6.</p>\n", "abstract": "should work splendidly for you. If there is no document of id mongo_id, it will fail, unless you also use upsert=True. This returns the old document by default. To get the new one, pass return_document=ReturnDocument.AFTER. All parameters are described in the API. The method was introduced for MongoDB 3.0. It was extended for 3.2, 3.4, and 3.6."}, {"id": 11948570, "score": 22, "vote": 0, "content": "<p>I will use <code>collection.save(the_changed_dict)</code> this way. I've just tested this, and it still works for me. The following is quoted directly from <code>pymongo doc.</code>:</p>\n<p><code>save(to_save[, manipulate=True[, safe=False[, **kwargs]]])</code></p>\n<blockquote>\n<p>Save a document in this collection.</p>\n<p>If to_save already has an \"_id\" then \n  an update() (upsert) operation is performed and \n  any existing document with that \"_id\" is\n  overwritten. Otherwise an insert() operation is performed. In this\n  case if manipulate is True an \"_id\" will be added to to_save and this\n  method returns the \"_id\" of the saved document. If manipulate is False\n  the \"_id\" will be added by the server but this method will return\n  None.</p>\n</blockquote>\n", "abstract": "I will use collection.save(the_changed_dict) this way. I've just tested this, and it still works for me. The following is quoted directly from pymongo doc.: save(to_save[, manipulate=True[, safe=False[, **kwargs]]]) Save a document in this collection. If to_save already has an \"_id\" then \n  an update() (upsert) operation is performed and \n  any existing document with that \"_id\" is\n  overwritten. Otherwise an insert() operation is performed. In this\n  case if manipulate is True an \"_id\" will be added to to_save and this\n  method returns the \"_id\" of the saved document. If manipulate is False\n  the \"_id\" will be added by the server but this method will return\n  None."}, {"id": 40958666, "score": 12, "vote": 0, "content": "<p>According to the latest documentation about PyMongo titled <a href=\"https://docs.mongodb.com/getting-started/python/insert/#insert-a-document\" rel=\"noreferrer\">Insert a Document</a> (insert is deprecated) and following defensive approach, you should insert and update as follows:</p>\n<pre><code class=\"python\">result = mycollection.insert_one(post)\npost = mycollection.find_one({'_id': result.inserted_id})\n\nif post is not None:\n    post['newfield'] = \"abc\"\n    mycollection.save(post)\n</code></pre>\n", "abstract": "According to the latest documentation about PyMongo titled Insert a Document (insert is deprecated) and following defensive approach, you should insert and update as follows:"}, {"id": 32128091, "score": 11, "vote": 0, "content": "<p>This is an old question, but I stumbled onto this when looking for the answer so I wanted to give the update to the answer for reference.</p>\n<p>The methods <code>save</code> and <code>update</code> are deprecated.</p>\n<blockquote>\n<p>save(to_save, manipulate=True, check_keys=True, **kwargs)\u00b6 Save a\n  document in this collection.</p>\n<p>DEPRECATED - Use insert_one() or replace_one() instead.</p>\n<p>Changed in version 3.0: Removed the safe parameter. Pass w=0 for\n  unacknowledged write operations.</p>\n<p>update(spec, document, upsert=False, manipulate=False, multi=False,\n  check_keys=True, **kwargs) Update a document(s) in this collection.</p>\n<p>DEPRECATED - Use replace_one(), update_one(), or update_many()\n  instead.</p>\n<p>Changed in version 3.0: Removed the safe parameter. Pass w=0 for\n  unacknowledged write operations.</p>\n</blockquote>\n<p>in the OPs particular case, it's better to use <a href=\"http://api.mongodb.org/python/current/api/pymongo/collection.html#pymongo.collection.Collection.replace_one\" rel=\"noreferrer\"><code>replace_one</code></a>.</p>\n", "abstract": "This is an old question, but I stumbled onto this when looking for the answer so I wanted to give the update to the answer for reference. The methods save and update are deprecated. save(to_save, manipulate=True, check_keys=True, **kwargs)\u00b6 Save a\n  document in this collection. DEPRECATED - Use insert_one() or replace_one() instead. Changed in version 3.0: Removed the safe parameter. Pass w=0 for\n  unacknowledged write operations. update(spec, document, upsert=False, manipulate=False, multi=False,\n  check_keys=True, **kwargs) Update a document(s) in this collection. DEPRECATED - Use replace_one(), update_one(), or update_many()\n  instead. Changed in version 3.0: Removed the safe parameter. Pass w=0 for\n  unacknowledged write operations. in the OPs particular case, it's better to use replace_one."}]}, {"link": "https://stackoverflow.com/questions/42392600/oserror-errno-18-invalid-cross-device-link", "question": {"id": "42392600", "title": "OSError: [Errno 18] Invalid cross-device link", "content": "<p>I'm working with django 1.6.5 and python 2.7.\nI have import feature in my app and I get error:</p>\n<pre><code class=\"python\">OSError: [Errno 18] Invalid cross-device link\n</code></pre>\n<p>I have problem with this part of code:</p>\n<pre><code class=\"python\">os.rename(db_temp, settings.DATABASES['bookmat']['NAME'])\n</code></pre>\n<p>code in settings:</p>\n<pre><code class=\"python\">'bookmat': {\n    'ENGINE': 'django.db.backends.sqlite3',\n    'NAME': '/my_projects/book/db/bookmat.sqlite3',\n},\n</code></pre>\n", "abstract": "I'm working with django 1.6.5 and python 2.7.\nI have import feature in my app and I get error: I have problem with this part of code: code in settings:"}, "answers": [{"id": 43967659, "score": 131, "vote": 0, "content": "<p><code>os.rename</code> only works if source and destination are on the same file system. You should use <code>shutil.move</code> instead.</p>\n", "abstract": "os.rename only works if source and destination are on the same file system. You should use shutil.move instead."}, {"id": 42400063, "score": 10, "vote": 0, "content": "<p>I think rename only works when the source and target names are on the same file system. You probably have different mounts. Otherwise you get that error. You can implement the same effect with a copy and a delete.</p>\n<p>Hope it helps</p>\n", "abstract": "I think rename only works when the source and target names are on the same file system. You probably have different mounts. Otherwise you get that error. You can implement the same effect with a copy and a delete. Hope it helps"}]}, {"link": "https://stackoverflow.com/questions/7831371/is-there-a-way-to-get-a-list-of-column-names-in-sqlite", "question": {"id": "7831371", "title": "Is there a way to get a list of column names in sqlite?", "content": "<p>I want to get a list of column names from a table in a database. Using pragma I get a list of tuples with a lot of unneeded information. Is there a way to get only the column names? So I might end up with something like this:</p>\n<blockquote>\n<p>[Column1, Column2, Column3, Column4]</p>\n</blockquote>\n<p>The reason why I absolutely need this list is because I want to search for a column name in the list and get the index because the index is used in a lot of my code.</p>\n<p>Is there a way of getting a list like this?</p>\n<p>Thanks</p>\n", "abstract": "I want to get a list of column names from a table in a database. Using pragma I get a list of tuples with a lot of unneeded information. Is there a way to get only the column names? So I might end up with something like this: [Column1, Column2, Column3, Column4] The reason why I absolutely need this list is because I want to search for a column name in the list and get the index because the index is used in a lot of my code. Is there a way of getting a list like this? Thanks"}, "answers": [{"id": 7831685, "score": 201, "vote": 0, "content": "<p>You can use sqlite3 and <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">pep-249</a></p>\n<pre><code class=\"python\">import sqlite3\nconnection = sqlite3.connect('~/foo.sqlite')\ncursor = connection.execute('select * from bar')\n</code></pre>\n<p><b>cursor.description</b> is description of columns</p>\n<pre><code class=\"python\">names = list(map(lambda x: x[0], cursor.description))\n</code></pre>\n<p>Alternatively you could use a list comprehension:</p>\n<pre><code class=\"python\">names = [description[0] for description in cursor.description]\n</code></pre>\n", "abstract": "You can use sqlite3 and pep-249 cursor.description is description of columns Alternatively you could use a list comprehension:"}, {"id": 18788347, "score": 39, "vote": 0, "content": "<p>An alternative to the <strong>cursor.description</strong> solution from <a href=\"https://stackoverflow.com/users/975918/smallredstone\">smallredstone</a> could be to use <strong>row.keys()</strong>:</p>\n<pre><code class=\"python\">import sqlite3\nconnection = sqlite3.connect('~/foo.sqlite')\nconnection.row_factory = sqlite3.Row\ncursor = connection.execute('select * from bar')\n# instead of cursor.description:\nrow = cursor.fetchone()\nnames = row.keys()\n</code></pre>\n<p>The drawback: it only works if there is at least a row returned from the query.</p>\n<p>The benefit: you can access the columns by their name (row['your_column_name'])</p>\n<p>Read more about the <a href=\"http://docs.python.org/3/library/sqlite3.html#sqlite3.Row\" rel=\"nofollow noreferrer\">Row objects in the python documentation</a>.</p>\n", "abstract": "An alternative to the cursor.description solution from smallredstone could be to use row.keys(): The drawback: it only works if there is at least a row returned from the query. The benefit: you can access the columns by their name (row['your_column_name']) Read more about the Row objects in the python documentation."}, {"id": 7831585, "score": 15, "vote": 0, "content": "<p>As far as I can tell Sqlite doesn't support INFORMATION_SCHEMA. Instead it has sqlite_master.</p>\n<p>I don't think you can get the list you want in just one command. You can get the information you need using sql or pragma, then use regex to split it into the format you need</p>\n<pre><code class=\"python\">SELECT sql FROM sqlite_master WHERE name='tablename';\n</code></pre>\n<p>gives you something like</p>\n<pre><code class=\"python\">CREATE TABLE tablename(\n        col1 INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n        col2 NVARCHAR(100) NOT NULL,\n        col3 NVARCHAR(100) NOT NULL,\n)\n</code></pre>\n<p>Or using pragma</p>\n<pre><code class=\"python\">PRAGMA table_info(tablename);\n</code></pre>\n<p>gives you something like</p>\n<pre><code class=\"python\">0|col1|INTEGER|1||1\n1|col2|NVARCHAR(100)|1||0\n2|col3|NVARCHAR(100)|1||0\n</code></pre>\n", "abstract": "As far as I can tell Sqlite doesn't support INFORMATION_SCHEMA. Instead it has sqlite_master. I don't think you can get the list you want in just one command. You can get the information you need using sql or pragma, then use regex to split it into the format you need gives you something like Or using pragma gives you something like"}, {"id": 44184517, "score": 10, "vote": 0, "content": "<p><strong>Quick, interactive way to see column names</strong></p>\n<p>If you're working interactively in Python and just want to quickly 'see' the column names, I found cursor.description to work.</p>\n<pre><code class=\"python\">import sqlite3\nconn = sqlite3.connect('test-db.db')\ncursor = conn.execute('select * from mytable')\ncursor.description\n</code></pre>\n<p><em>Outputs something like this:</em></p>\n<pre><code class=\"python\">(('Date', None, None, None, None, None, None),\n ('Object-Name', None, None, None, None, None, None),\n ('Object-Count', None, None, None, None, None, None))\n</code></pre>\n<p>Or, quick way to access and print them out.</p>\n<pre><code class=\"python\">colnames = cursor.description\nfor row in colnames:\n    print row[0]\n</code></pre>\n<p><em>Outputs something like this:</em></p>\n<pre><code class=\"python\">Date\nObject-Name\nObject-Count\n</code></pre>\n", "abstract": "Quick, interactive way to see column names If you're working interactively in Python and just want to quickly 'see' the column names, I found cursor.description to work. Outputs something like this: Or, quick way to access and print them out. Outputs something like this:"}, {"id": 54962890, "score": 9, "vote": 0, "content": "<p>You can get a list of column names by running:</p>\n<pre><code class=\"python\">SELECT name FROM PRAGMA_TABLE_INFO('your_table');\nname      \ntbl_name  \nrootpage  \nsql\n</code></pre>\n<p>You can check if a certain column exists by running:</p>\n<pre><code class=\"python\">SELECT 1 FROM PRAGMA_TABLE_INFO('your_table') WHERE name='sql';\n1\n</code></pre>\n<p>Reference:</p>\n<p><a href=\"https://www.sqlite.org/pragma.html#pragfunc\" rel=\"noreferrer\">https://www.sqlite.org/pragma.html#pragfunc</a></p>\n", "abstract": "You can get a list of column names by running: You can check if a certain column exists by running: Reference: https://www.sqlite.org/pragma.html#pragfunc"}, {"id": 38854129, "score": 8, "vote": 0, "content": "<p>Assuming that you know the table name, and want the names of the data columns you can use the listed code will do it in a simple and elegant way to my taste: </p>\n<pre><code class=\"python\">import sqlite3\n\ndef get_col_names():\n#this works beautifully given that you know the table name\n    conn = sqlite3.connect(\"t.db\")\n    c = conn.cursor()\n    c.execute(\"select * from tablename\")\n    return [member[0] for member in c.description]\n</code></pre>\n", "abstract": "Assuming that you know the table name, and want the names of the data columns you can use the listed code will do it in a simple and elegant way to my taste: "}, {"id": 54971823, "score": 8, "vote": 0, "content": "<p>It is very easy.<br/>\nFirst create a connection , lets name it, <code>con</code>.\nThen run the following code.</p>\n<pre class=\"lang-sql prettyprint-override\"><code class=\"python\">cur =con.cursor()\ncur.execute(\"select * from table_name limit 1\")\ncol_name=[i[0] for i in cur.description]\nprint(col_name)\n</code></pre>\n<p>You will get column name as a list</p>\n", "abstract": "It is very easy.\nFirst create a connection , lets name it, con.\nThen run the following code. You will get column name as a list"}, {"id": 66550574, "score": 3, "vote": 0, "content": "<p>Well, I may be very late to answer this but since people still follow this thread, I just wanted to share how I use to get the list of column names in <code>python sqlite3</code>.</p>\n<pre><code class=\"python\">import sqlite3\n\ndef getVarList(con, tableName)\n    return [fields[1] for fields in con.execute(f\"PRAGMA table_info({tableName})\").fetchall()]\n    \nconn = sqlite3.connect('foo.db')\nvarList = getVarList(conn, 'bar')\n</code></pre>\n", "abstract": "Well, I may be very late to answer this but since people still follow this thread, I just wanted to share how I use to get the list of column names in python sqlite3."}, {"id": 40444394, "score": 2, "vote": 0, "content": "<p>I use this:</p>\n<pre><code class=\"python\">import sqlite3\n\n    db = sqlite3.connect('~/foo.sqlite')\n    dbc = db.cursor()\n    dbc.execute(\"PRAGMA table_info('bar')\"\n    ciao = dbc.fetchall()\n\n    HeaderList=[]\n    for i in ciao:\n        counter=0\n        for a in i:\n            counter+=1\n            if( counter==2):\n                HeaderList.append(a)\n\nprint(HeaderList)\n</code></pre>\n", "abstract": "I use this:"}, {"id": 69391462, "score": 2, "vote": 0, "content": "<p>Since the question has a python flag. I feel free to post a python specific answer with pandas:</p>\n<pre><code class=\"python\">import sqlite3\nimport pandas as pd\n\npath_to_db = 'path/to/db'\nconnect = sqlite3.connect(path_to_db, isolation_level=None)\ntable = 'table_name'\n\ncolumn_list = list(pd.read_sql_query(f\"SELECT * FROM {table} limit 1\", connect).columns)\n</code></pre>\n", "abstract": "Since the question has a python flag. I feel free to post a python specific answer with pandas:"}, {"id": 50775904, "score": 1, "vote": 0, "content": "<p>I like the answer by @thebeancounter, but prefer to parameterize the unknowns, the only problem being a vulnerability to exploits on the table name.  If you're sure it's okay, then this works:</p>\n<pre><code class=\"python\">def get_col_names(cursor, tablename):\n    \"\"\"Get column names of a table, given its name and a cursor\n       (or connection) to the database.\n    \"\"\"\n    reader=cursor.execute(\"SELECT * FROM {}\".format(tablename))\n    return [x[0] for x in reader.description] \n</code></pre>\n<p>If it's a problem, you could add code to sanitize the tablename.</p>\n", "abstract": "I like the answer by @thebeancounter, but prefer to parameterize the unknowns, the only problem being a vulnerability to exploits on the table name.  If you're sure it's okay, then this works: If it's a problem, you could add code to sanitize the tablename."}, {"id": 58311698, "score": 1, "vote": 0, "content": "<p>Another way of using pragma:</p>\n<pre><code class=\"python\">&gt; table = \"foo\"\n&gt; cur.execute(\"SELECT group_concat(name, ', ') FROM pragma_table_info(?)\", (table,))\n&gt; cur.fetchone()\n('foo', 'bar', ...,)\n</code></pre>\n", "abstract": "Another way of using pragma:"}, {"id": 72896126, "score": 0, "vote": 0, "content": "<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import sqlite3\n\nwith sqlite3.connect('statics.db') as cur:\n    cur.execute(\"CREATE TABLE IF NOT EXISTS data(id INT PRIMARY KEY NOT NULL)\")\n    pragmas = cur.execute(\"PRAGMA table_info(data);\")\n    columns = [n for _, n, *_ in pragmas.fetchall()]\n    print(columns)\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/5602918/select-null-values-in-sqlalchemy", "question": {"id": "5602918", "title": "Select NULL Values in SQLAlchemy", "content": "<p>Here's my (PostgreSQL) table --</p>\n<pre><code class=\"python\">test=&gt; create table people (name varchar primary key,\n                            marriage_status varchar) ; \n\ntest=&gt; insert into people values ('Ken', 'married');\ntest=&gt; insert into people values ('May', 'single');\ntest=&gt; insert into people values ('Joe', NULL);\n</code></pre>\n<p>I want to select all people that are <strong>not</strong> known to be married, i.e., including those with NULL marriage_status.</p>\n<p>This does <strong>not</strong> work --</p>\n<pre><code class=\"python\">test=&gt; select * from people where marriage_status != 'married' ; \n name | marriage_status \n------+-----------------\n May  | single\n(1 row)\n</code></pre>\n<p>Of course this does --</p>\n<pre><code class=\"python\">test=&gt; select * from people where marriage_status != 'married'\n       or marriage_status is NULL ; \n name | marriage_status \n------+-----------------\n May  | single\n Joe  | \n</code></pre>\n<p>The problem is that I'm accessing it from SQLAlchemy with --</p>\n<pre><code class=\"python\">...filter(or_(people.marriage_status!='married',\n              people.marriage_status is None))\n</code></pre>\n<p>which gets translated to --</p>\n<pre><code class=\"python\">SELECT people.name as name,\n       people.marriage_status as marriage_status\nFROM people \nWHERE people.marriage_status != %(status_1)s OR False\nsqlalchemy.engine.base.Engine.... {'status_1': 'married'}\n</code></pre>\n<p>And does <strong>not</strong> work --</p>\n<pre><code class=\"python\">test=&gt; select * from people where marriage_status != 'married'\n       or False; \n name | marriage_status \n------+-----------------\n May  | single\n(1 row)\n</code></pre>\n<p>neither does --</p>\n<pre><code class=\"python\">test=&gt; select * from people where marriage_status != 'married'\n       or NULL; \n name | marriage_status \n------+-----------------\n May  | single\n(1 row)\n</code></pre>\n<p>How should I select NULL values through SQLAlchemy?</p>\n", "abstract": "Here's my (PostgreSQL) table -- I want to select all people that are not known to be married, i.e., including those with NULL marriage_status. This does not work -- Of course this does -- The problem is that I'm accessing it from SQLAlchemy with -- which gets translated to -- And does not work -- neither does -- How should I select NULL values through SQLAlchemy?"}, "answers": [{"id": 5632224, "score": 177, "vote": 0, "content": "<p>For <strong>SQLAlchemy 0.7.9 and newer</strong>\nPlease use the <a href=\"https://stackoverflow.com/a/44679356/99594\">answer from @jsnow</a>. !!!</p>\n<p>For <strong>SQLAlchemy 0.7.8 and older</strong></p>\n<p>(as indicated by <a href=\"https://stackoverflow.com/users/2572431/augurar\">@augurar</a>): Because <em>sqlalchemy</em> uses <em>magic methods (operator overloading)</em> to create <code>SQL</code> constructs, it can only handle operator such as <code>!=</code> or <code>==</code>, but is not able to work with <code>is</code> (which is a very valid Python construct).</p>\n<p>Therefore, to make it work with sqlalchemy, you should use:</p>\n<pre><code class=\"python\">...filter(or_(people.marriage_status!='married', people.marriage_status == None))\n</code></pre>\n<p>, basically replace the <code>is None</code> with <code>== None</code>. In this case your query will be translated properly to the following SQL:</p>\n<pre><code class=\"python\">SELECT people.name AS people_name, people.marriage_status AS people_marriage_status \nFROM people \nWHERE people.marriage_status IS NULL OR people.marriage_status != ?\n</code></pre>\n<p>See <code>IS NULL</code> in the <a href=\"http://docs.sqlalchemy.org/en/latest/orm/tutorial.html#common-filter-operators\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n", "abstract": "For SQLAlchemy 0.7.9 and newer\nPlease use the answer from @jsnow. !!! For SQLAlchemy 0.7.8 and older (as indicated by @augurar): Because sqlalchemy uses magic methods (operator overloading) to create SQL constructs, it can only handle operator such as != or ==, but is not able to work with is (which is a very valid Python construct). Therefore, to make it work with sqlalchemy, you should use: , basically replace the is None with == None. In this case your query will be translated properly to the following SQL: See IS NULL in the documentation."}, {"id": 44679356, "score": 96, "vote": 0, "content": "<p>Since SQLAlchemy 0.7.9 you may use the <code>is_</code> (or <code>is_not</code>) method of the column.</p>\n<p>A filter expression like:</p>\n<p><code>filter(or_(people.marriage_status!='married', people.marriage_status.is_(None)))</code></p>\n<p>will generate the parameterized SQL:</p>\n<p><code>WHERE people.marriage_status != %(status_1)s OR people.marriage_status IS NULL</code></p>\n", "abstract": "Since SQLAlchemy 0.7.9 you may use the is_ (or is_not) method of the column. A filter expression like: filter(or_(people.marriage_status!='married', people.marriage_status.is_(None))) will generate the parameterized SQL: WHERE people.marriage_status != %(status_1)s OR people.marriage_status IS NULL"}, {"id": 69658685, "score": 18, "vote": 0, "content": "<p>An elegant way to write it would be to use is_ and is_not, as shown in the following example:</p>\n<pre><code class=\"python\">query.filter(people.student.is_not(None)) \nquery.filter(people.student.is_(None)) \n</code></pre>\n", "abstract": "An elegant way to write it would be to use is_ and is_not, as shown in the following example:"}, {"id": 72866292, "score": 4, "vote": 0, "content": "<p>The question is quite old, but for completeness, here the version I prefer for readability:</p>\n<pre><code class=\"python\">import sqlalchemy as sa\n\nquery.filter(people.marriage_status == sa.null()) \nquery.filter(people.marriage_status != sa.null())\n</code></pre>\n", "abstract": "The question is quite old, but for completeness, here the version I prefer for readability:"}, {"id": 12014797, "score": 1, "vote": 0, "content": "<p>i ran into a similar problem</p>\n<p><a href=\"https://groups.google.com/forum/?fromgroups#!topic/sqlalchemy/EVpxsNp5Ifg%5B1-25%5D\" rel=\"nofollow\">https://groups.google.com/forum/?fromgroups#!topic/sqlalchemy/EVpxsNp5Ifg%5B1-25%5D</a></p>\n<p>short answer:\n - there is not a column operator for IS (NOT) NULL now, but there will be</p>\n<p>in the meantime you can use either:</p>\n<p>filter(tablename.is_deleted.op(\"IS NOT\")(True)) </p>\n<p>filter(coalesce(tablename.is_deleted, False) != True) </p>\n", "abstract": "i ran into a similar problem https://groups.google.com/forum/?fromgroups#!topic/sqlalchemy/EVpxsNp5Ifg%5B1-25%5D short answer:\n - there is not a column operator for IS (NOT) NULL now, but there will be in the meantime you can use either: filter(tablename.is_deleted.op(\"IS NOT\")(True))  filter(coalesce(tablename.is_deleted, False) != True) "}]}, {"link": "https://stackoverflow.com/questions/9353822/connecting-postgresql-with-sqlalchemy", "question": {"id": "9353822", "title": "Connecting postgresql with sqlalchemy", "content": "<p>I know this might be really a simple question but I don't know the solution. What is happening here when I try to connect to postgresql? I am self learner in this field of database and programming so please be gentle with me.\nWhen I try following code:</p>\n<pre><code class=\"python\">import sqlalchemy\ndb = sqlalchemy.create_engine('postgresql:///tutorial.db')\n</code></pre>\n<p>I get this error:</p>\n<blockquote>\n<p>Traceback (most recent call last):\n     File \"\", line 1, in \n      db = sqlalchemy.create_engine('postgresql:///tutorial.db')\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\engine__init__.py\", line 327, in create_engine\n      return strategy.create(*args, **kwargs)\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\engine\\strategies.py\", line 64, in create\n      dbapi = dialect_cls.dbapi(**dbapi_args)\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\dialects\\postgresql\\psycopg2.py\", line 289, in dbapi\n      psycopg = <strong>import</strong>('psycopg2')\n  ImportError: No module named psycopg2</p>\n</blockquote>\n<p>Do I need to install psycopg2 separately? What is the correct connection string for postgresql?</p>\n", "abstract": "I know this might be really a simple question but I don't know the solution. What is happening here when I try to connect to postgresql? I am self learner in this field of database and programming so please be gentle with me.\nWhen I try following code: I get this error: Traceback (most recent call last):\n     File \"\", line 1, in \n      db = sqlalchemy.create_engine('postgresql:///tutorial.db')\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\engine__init__.py\", line 327, in create_engine\n      return strategy.create(*args, **kwargs)\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\engine\\strategies.py\", line 64, in create\n      dbapi = dialect_cls.dbapi(**dbapi_args)\n    File \"C:\\Python27\\lib\\site-packages\\sqlalchemy-0.7.5dev-py2.7.egg\\sqlalchemy\\dialects\\postgresql\\psycopg2.py\", line 289, in dbapi\n      psycopg = import('psycopg2')\n  ImportError: No module named psycopg2 Do I need to install psycopg2 separately? What is the correct connection string for postgresql?"}, "answers": [{"id": 42587012, "score": 96, "vote": 0, "content": "<p>You would need to <code>pip install SQLAlchemy</code> and <code>pip install psycopg2</code>.\nAn example of a SQLAlchemy connection string that uses psycopg2:</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine\nengine = create_engine('postgresql+psycopg2://user:password@hostname/database_name')\n</code></pre>\n<p>You could also connect to your database using the psycopg2 driver exclusively:</p>\n<pre><code class=\"python\">import psycopg2\nconn_string = \"host='localhost' dbname='my_database' user='postgres' password='secret'\"\nconn = psycopg2.connect(conn_string)\n</code></pre>\n<p>However, using the psycopg2 driver to connect does not take advantage of SQLAlchemy.</p>\n", "abstract": "You would need to pip install SQLAlchemy and pip install psycopg2.\nAn example of a SQLAlchemy connection string that uses psycopg2: You could also connect to your database using the psycopg2 driver exclusively: However, using the psycopg2 driver to connect does not take advantage of SQLAlchemy."}, {"id": 9353830, "score": 67, "vote": 0, "content": "<p>Yes, psycopg2 are basically the Python drivers for PostgreSQL that need to be installed separately.</p>\n<p>A list of valid connection strings can be found here, yours is a bit off (you need to the username, the password and hostname as specified in the link below):</p>\n<p><a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#postgresql\" rel=\"noreferrer\">http://docs.sqlalchemy.org/en/latest/core/engines.html#postgresql</a></p>\n", "abstract": "Yes, psycopg2 are basically the Python drivers for PostgreSQL that need to be installed separately. A list of valid connection strings can be found here, yours is a bit off (you need to the username, the password and hostname as specified in the link below): http://docs.sqlalchemy.org/en/latest/core/engines.html#postgresql"}, {"id": 48060270, "score": 3, "vote": 0, "content": "<p>Yes, you need to install psycopg2 separately, if you're using linux you can simply enter the following line to the terminal: <code>$pip install psycopg2</code> if this doesn't work try using sudo: <code>$sudo pip install psycopg2</code></p>\n", "abstract": "Yes, you need to install psycopg2 separately, if you're using linux you can simply enter the following line to the terminal: $pip install psycopg2 if this doesn't work try using sudo: $sudo pip install psycopg2"}]}, {"link": "https://stackoverflow.com/questions/27766794/switching-from-sqlite-to-mysql-with-flask-sqlalchemy", "question": {"id": "27766794", "title": "Switching from SQLite to MySQL with Flask SQLAlchemy", "content": "<p>I have a site that I've built with Flask SQLAlchemy and SQLite, and need to switch to MySQL. I have migrated the database itself and have it running under MySQL, but </p>\n<ol>\n<li>Can't figure out how to connect to the MySQL database (that is, what the <code>SQLALCHEMY_DATABASE_URI</code> should be) and</li>\n<li>Am unclear if any of my existing SQLAlchemy SQLite code will work with MySQL.</li>\n</ol>\n<p>I suspect that (1) is fairly simple and just a matter of being shown how to map, for example, the contents of the connection dialog I use in my MySQL database tool to an appropriately formatted URL. But I'm worried about (2), I had assumed that SQLAlchemy provided an abstraction layer so that <a href=\"https://pythonhosted.org/Flask-SQLAlchemy/quickstart.html#a-minimal-application\">simple SQLAlchemy code such as</a> </p>\n<pre><code class=\"python\">from flask import Flask\nfrom flask.ext.sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'\ndb = SQLAlchemy(app)\n\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True)\n    email = db.Column(db.String(120), unique=True)\n\n    def __init__(self, username, email):\n        self.username = username\n        self.email = email\n\n    def __repr__(self):\n        return '&lt;User %r&gt;' % self.username\n\nadmin = User('admin', 'admin@example.com')\n\ndb.session.add(admin)\n\nUser.query.all()\n\nUser.query.filter_by(username='admin').first()\n</code></pre>\n<p>wold work without any modifications other than an appropriate change to the database URI; but the <a href=\"http://pylonsdevelopment.blogspot.com/2010/07/connecting-to-mysql-database-using.html\">examples I've found</a> for using SQLAlchemy with MySQL seem to use a completely different API.</p>\n<p>Can I (2) migrate my Flask SQLAlchemy code to work with a MySQL database by simply changing the database URI and if so (1) what should that URI be?</p>\n", "abstract": "I have a site that I've built with Flask SQLAlchemy and SQLite, and need to switch to MySQL. I have migrated the database itself and have it running under MySQL, but  I suspect that (1) is fairly simple and just a matter of being shown how to map, for example, the contents of the connection dialog I use in my MySQL database tool to an appropriately formatted URL. But I'm worried about (2), I had assumed that SQLAlchemy provided an abstraction layer so that simple SQLAlchemy code such as  wold work without any modifications other than an appropriate change to the database URI; but the examples I've found for using SQLAlchemy with MySQL seem to use a completely different API. Can I (2) migrate my Flask SQLAlchemy code to work with a MySQL database by simply changing the database URI and if so (1) what should that URI be?"}, "answers": [{"id": 28391385, "score": 121, "vote": 0, "content": "<p>The tutorial pointed by you shows the right way of connecting to MySQL using SQLAlchemy. Below is your code with very little changes:</p>\n<p>My assumptions are your MySQL server is running on the same machine where Flask is running and the database name is db_name. In case your server is not same machine, put the server IP in place of <code>localhost</code>.</p>\n<pre><code class=\"python\">from flask import Flask\nfrom flask.ext.sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://username:password@localhost/db_name'\ndb = SQLAlchemy(app)\n\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(80), unique=True)\n    email = db.Column(db.String(120), unique=True)\n\n    def __init__(self, username, email):\n        self.username = username\n        self.email = email\n\n    def __repr__(self):\n        return '&lt;User %r&gt;' % self.username\n\nadmin = User('admin', 'admin@example.com')\n\ndb.create_all() # In case user table doesn't exists already. Else remove it.    \n\ndb.session.add(admin)\n\ndb.session.commit() # This is needed to write the changes to database\n\nUser.query.all()\n\nUser.query.filter_by(username='admin').first()\n</code></pre>\n<p>It happened to me that the default driver used by <code>SQLAlchemy</code> (<code>mqsqldb</code>), doesn't get compiled for me in my virtual environments. So I have opted for a MySQL driver with full python implementation <code>pymysql</code>. Once you install it using <code>pip install pymysql</code>, the SQLALCHEMY_DATABASE_URI will change to:</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://username:password@localhost/db_name'\n</code></pre>\n<p>The purpose of using ORM like SQLAlchemy is that , you can use different database with little or no change in most cases. So, my answer is yes. You should be able to use your sqlite code to work with MySQL with the URI mapped as in above code.</p>\n", "abstract": "The tutorial pointed by you shows the right way of connecting to MySQL using SQLAlchemy. Below is your code with very little changes: My assumptions are your MySQL server is running on the same machine where Flask is running and the database name is db_name. In case your server is not same machine, put the server IP in place of localhost. It happened to me that the default driver used by SQLAlchemy (mqsqldb), doesn't get compiled for me in my virtual environments. So I have opted for a MySQL driver with full python implementation pymysql. Once you install it using pip install pymysql, the SQLALCHEMY_DATABASE_URI will change to: The purpose of using ORM like SQLAlchemy is that , you can use different database with little or no change in most cases. So, my answer is yes. You should be able to use your sqlite code to work with MySQL with the URI mapped as in above code."}, {"id": 55775096, "score": 11, "vote": 0, "content": "<p>The accepted answer was correct at the time, but the syntax in the import statement has been deprecated. </p>\n<p>This:</p>\n<pre><code class=\"python\">from flask.ext.sqlalchemy import SQLAlchemy\n</code></pre>\n<p>Should be replaced with:</p>\n<pre><code class=\"python\">import flask_sqlalchemy\n</code></pre>\n<p>Since questions regarding database connections tend to get traffic and stay relevant for a long time, it's worth having on the record.</p>\n<p>The deprecation is in the <a href=\"http://flask.pocoo.org/docs/1.0/changelog/\" rel=\"noreferrer\">Flask Version 1.0 Changelog</a>, which actually uses this module in the example:</p>\n<blockquote>\n<p>flask.ext - import extensions directly by their name instead of\n  through the flask.ext namespace. For example, import\n  flask.ext.sqlalchemy becomes import flask_sqlalchemy.</p>\n</blockquote>\n", "abstract": "The accepted answer was correct at the time, but the syntax in the import statement has been deprecated.  This: Should be replaced with: Since questions regarding database connections tend to get traffic and stay relevant for a long time, it's worth having on the record. The deprecation is in the Flask Version 1.0 Changelog, which actually uses this module in the example: flask.ext - import extensions directly by their name instead of\n  through the flask.ext namespace. For example, import\n  flask.ext.sqlalchemy becomes import flask_sqlalchemy."}]}, {"link": "https://stackoverflow.com/questions/4384098/in-django-models-py-whats-the-difference-between-default-null-and-blank", "question": {"id": "4384098", "title": "In Django models.py, what&#39;s the difference between default, null, and blank?", "content": "<ul>\n<li><code>null=True</code></li>\n<li><code>blank=True</code></li>\n<li><code>default = 0</code></li>\n</ul>\n<p>What's the difference? When do you use what?</p>\n", "abstract": "What's the difference? When do you use what?"}, "answers": [{"id": 4384131, "score": 82, "vote": 0, "content": "<p>Direct from <a href=\"http://docs.djangoproject.com/en/dev/ref/models/fields/\" rel=\"noreferrer\">Django model field reference</a>:</p>\n<blockquote>\n<p><strong><code>Field.null</code></strong></p>\n<p>If <code>True</code>, Django will store empty values as <code>NULL</code> in the database. Default is <code>False</code>.</p>\n<p>Note that empty string values will always get stored as empty strings, not as <code>NULL</code>. Only use <code>null=True</code> for non-string fields such as integers, booleans and dates. For both types of fields, you will also need to set <code>blank=True</code> if you wish to permit empty values in forms, as the <code>null</code> parameter only affects database storage (see <code>blank</code>).</p>\n<p>Avoid using <code>null</code> on string-based fields such as <code>CharField</code> and <code>TextField</code> unless you have an excellent reason. If a string-based field has <code>null=True</code>, that means it has two possible values for \u201cno data\u201d: NULL, and the empty string. In most cases, it\u2019s redundant to have two possible values for \u201cno data;\u201d Django convention is to use the empty string, not <code>NULL</code>.</p>\n</blockquote>\n<p> </p>\n<blockquote>\n<p><strong><code>Field.blank</code></strong></p>\n<p>If <code>True</code>, the field is allowed to be blank. Default is <code>False</code>.</p>\n<p>Note that this is different than <code>null</code>. <code>null</code> is purely database-related, whereas <code>blank</code> is validation-related. If a field has <code>blank=True</code>, validation on Django\u2019s admin site will allow entry of an empty value. If a field has <code>blank=False</code>, the field will be required.</p>\n</blockquote>\n<p> </p>\n<blockquote>\n<p><strong><code>Field.default</code></strong></p>\n<p>The default value for the field. This can be a value or a callable object. If callable it will be called every time a new object is created.</p>\n</blockquote>\n", "abstract": "Direct from Django model field reference: Field.null If True, Django will store empty values as NULL in the database. Default is False. Note that empty string values will always get stored as empty strings, not as NULL. Only use null=True for non-string fields such as integers, booleans and dates. For both types of fields, you will also need to set blank=True if you wish to permit empty values in forms, as the null parameter only affects database storage (see blank). Avoid using null on string-based fields such as CharField and TextField unless you have an excellent reason. If a string-based field has null=True, that means it has two possible values for \u201cno data\u201d: NULL, and the empty string. In most cases, it\u2019s redundant to have two possible values for \u201cno data;\u201d Django convention is to use the empty string, not NULL.   Field.blank If True, the field is allowed to be blank. Default is False. Note that this is different than null. null is purely database-related, whereas blank is validation-related. If a field has blank=True, validation on Django\u2019s admin site will allow entry of an empty value. If a field has blank=False, the field will be required.   Field.default The default value for the field. This can be a value or a callable object. If callable it will be called every time a new object is created."}, {"id": 4384133, "score": 20, "vote": 0, "content": "<p>From <a href=\"http://docs.djangoproject.com/en/1.2/topics/db/models/\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p><code>null</code> If True, Django will store empty\n  values as NULL in the database.\n  Default is False.</p>\n<p><code>blank</code> If True, the field is allowed to\n  be blank. Default is False.</p>\n<p><code>default</code> The default value for the\n  field.</p>\n</blockquote>\n<p>You can use \"<code>default</code>\" to set the value that will be used for the field in question should your code not explicitly set it to a value.</p>\n<p>Use \"<code>blank</code>\" for form validation purposes - blank=True will allow the field to be set to an empty value</p>\n<p>Use \"<code>null</code>\" if you would like to store an empty value as \"null\" in the DB.  Often it's preferred, however, to set blank values to an empty string or to 0 as appropriate for a given field.</p>\n", "abstract": "From docs: null If True, Django will store empty\n  values as NULL in the database.\n  Default is False. blank If True, the field is allowed to\n  be blank. Default is False. default The default value for the\n  field. You can use \"default\" to set the value that will be used for the field in question should your code not explicitly set it to a value. Use \"blank\" for form validation purposes - blank=True will allow the field to be set to an empty value Use \"null\" if you would like to store an empty value as \"null\" in the DB.  Often it's preferred, however, to set blank values to an empty string or to 0 as appropriate for a given field."}, {"id": 52217039, "score": 18, "vote": 0, "content": "<p>I know you already have your answer however till this day it's difficult to judge whether to put <code>null=True</code> or <code>blank=True</code> or <code>both</code> to a field. I personally think it's pretty useless and confusing to provide so many options to developers. Let the handle the nulls or blanks however they want. </p>\n<p>I follow this table (from the book \"Two Scoops of Django\"):\n<a href=\"https://i.stack.imgur.com/dob5K.png\" rel=\"noreferrer\"><img alt=\"When to use null and blank\" src=\"https://i.stack.imgur.com/dob5K.png\"/></a></p>\n<p><a href=\"https://i.stack.imgur.com/IsrFh.png\" rel=\"noreferrer\"><img alt=\"When to use null and blank\" src=\"https://i.stack.imgur.com/IsrFh.png\"/></a></p>\n", "abstract": "I know you already have your answer however till this day it's difficult to judge whether to put null=True or blank=True or both to a field. I personally think it's pretty useless and confusing to provide so many options to developers. Let the handle the nulls or blanks however they want.  I follow this table (from the book \"Two Scoops of Django\"):\n "}, {"id": 28214919, "score": 2, "vote": 0, "content": "<p>In implementation terms:</p>\n<p>The 'blank' field corresponds to all forms. Specifies if this value is required in form and corresponding form validation is done.\n'True' allows empty values.</p>\n<p>The 'null' field corresponds to DB level. It will be set as NULL or NOT NULL at the DB.</p>\n<p>Hence if leave a field empty in admin with blank=true, NULL is fed into the DB. Now this might throw an error if that particular column in the DB is specified as NOT NULL.</p>\n", "abstract": "In implementation terms: The 'blank' field corresponds to all forms. Specifies if this value is required in form and corresponding form validation is done.\n'True' allows empty values. The 'null' field corresponds to DB level. It will be set as NULL or NOT NULL at the DB. Hence if leave a field empty in admin with blank=true, NULL is fed into the DB. Now this might throw an error if that particular column in the DB is specified as NOT NULL."}, {"id": 57703695, "score": 2, "vote": 0, "content": "<p><strong>Null</strong>: It is database-related. Defines if a given database column will accept null values or not.</p>\n<p><strong>Blank</strong>: It is validation-related. It will be used during forms validation, when calling form.is_valid().</p>\n<p><strong>Default</strong>: All Time it store the given value(default value) to the field if one doesn't provide any value for this field.</p>\n<p>The default values of null and blank are <strong>False</strong>.</p>\n<p>That being said, it is perfectly fine to have a field with null=True and blank=False. Meaning on the database level the field can be NULL, but in the application level it is a required field.</p>\n<p>Now, where most developers get it wrong: Defining null=True for string-based fields such as CharField and TextField. Avoid doing that. Otherwise, you will end up having two possible values for \u201cno data\u201d, that is: <strong>None</strong> and an empty string. Having two possible values for \u201cno data\u201d is redundant. The Django convention is to use the empty string, not NULL.</p>\n", "abstract": "Null: It is database-related. Defines if a given database column will accept null values or not. Blank: It is validation-related. It will be used during forms validation, when calling form.is_valid(). Default: All Time it store the given value(default value) to the field if one doesn't provide any value for this field. The default values of null and blank are False. That being said, it is perfectly fine to have a field with null=True and blank=False. Meaning on the database level the field can be NULL, but in the application level it is a required field. Now, where most developers get it wrong: Defining null=True for string-based fields such as CharField and TextField. Avoid doing that. Otherwise, you will end up having two possible values for \u201cno data\u201d, that is: None and an empty string. Having two possible values for \u201cno data\u201d is redundant. The Django convention is to use the empty string, not NULL."}]}, {"link": "https://stackoverflow.com/questions/2511679/python-number-of-rows-affected-by-cursor-executeselect", "question": {"id": "2511679", "title": "Python: Number of rows affected by cursor.execute(&quot;SELECT ...)", "content": "<p>How can I access the number of rows affected by:</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT COUNT(*) from result where server_state='2' AND name LIKE '\"+digest+\"_\"+charset+\"_%'\")\n</code></pre>\n", "abstract": "How can I access the number of rows affected by:"}, "answers": [{"id": 2512521, "score": 101, "vote": 0, "content": "<p>Try using <code>fetchone</code>:</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT COUNT(*) from result where server_state='2' AND name LIKE '\"+digest+\"_\"+charset+\"_%'\")\nresult=cursor.fetchone()\n</code></pre>\n<p><code>result</code> will hold a tuple with one element, the value of <code>COUNT(*)</code>.\nSo to find the number of rows:</p>\n<pre><code class=\"python\">number_of_rows=result[0]\n</code></pre>\n<p>Or, if you'd rather do it in one fell swoop:</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT COUNT(*) from result where server_state='2' AND name LIKE '\"+digest+\"_\"+charset+\"_%'\")\n(number_of_rows,)=cursor.fetchone()\n</code></pre>\n<p>PS. It's also good practice to use parametrized arguments whenever possible, because it can automatically quote arguments for you when needed, and protect against sql injection.</p>\n<p>The correct syntax for parametrized arguments depends on your python/database adapter (e.g. mysqldb, psycopg2 or sqlite3). It would look something like</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT COUNT(*) from result where server_state= %s AND name LIKE %s\",[2,digest+\"_\"+charset+\"_%\"])\n(number_of_rows,)=cursor.fetchone()\n</code></pre>\n", "abstract": "Try using fetchone: result will hold a tuple with one element, the value of COUNT(*).\nSo to find the number of rows: Or, if you'd rather do it in one fell swoop: PS. It's also good practice to use parametrized arguments whenever possible, because it can automatically quote arguments for you when needed, and protect against sql injection. The correct syntax for parametrized arguments depends on your python/database adapter (e.g. mysqldb, psycopg2 or sqlite3). It would look something like"}, {"id": 2511718, "score": 100, "vote": 0, "content": "<p>From <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP 249</a>, which is usually implemented by Python database APIs:</p>\n<blockquote>\n<p>Cursor Objects should respond to the following methods and attributes:</p>\n</blockquote>\n<p>[\u2026]</p>\n<blockquote>\n<p><a href=\"https://www.python.org/dev/peps/pep-0249/#rowcount\" rel=\"noreferrer\"><strong><code>.rowcount</code></strong></a><br/> \n             This read-only attribute specifies the number of rows that the last .execute*() produced (for DQL statements like 'select') or affected (for DML statements like 'update' or 'insert').</p>\n</blockquote>\n<p>But be careful\u2014it goes on to say:</p>\n<blockquote>\n<p>The attribute is -1 in case no <a href=\"https://www.python.org/dev/peps/pep-0249/#id14\" rel=\"noreferrer\"><code>.execute*()</code></a> has been performed on the cursor or the rowcount of the last operation is cannot be determined by the interface. <a href=\"https://www.python.org/dev/peps/pep-0249/#id46\" rel=\"noreferrer\">[7]</a></p>\n<p><strong>Note:</strong><br/>\n  Future versions of the DB API specification could redefine the latter case to have the object return <code>None</code> instead of -1.</p>\n</blockquote>\n<p>So if you've executed your statement, <em>and</em> it works, <em>and</em> you're certain your code will always be run against the same version of the same DBMS, this is a reasonable solution.</p>\n", "abstract": "From PEP 249, which is usually implemented by Python database APIs: Cursor Objects should respond to the following methods and attributes: [\u2026] .rowcount \n             This read-only attribute specifies the number of rows that the last .execute*() produced (for DQL statements like 'select') or affected (for DML statements like 'update' or 'insert'). But be careful\u2014it goes on to say: The attribute is -1 in case no .execute*() has been performed on the cursor or the rowcount of the last operation is cannot be determined by the interface. [7] Note:\n  Future versions of the DB API specification could redefine the latter case to have the object return None instead of -1. So if you've executed your statement, and it works, and you're certain your code will always be run against the same version of the same DBMS, this is a reasonable solution."}, {"id": 2834349, "score": 42, "vote": 0, "content": "<p>The number of rows effected is returned from execute:</p>\n<pre><code class=\"python\">rows_affected=cursor.execute(\"SELECT ... \")\n</code></pre>\n<p>of course, as AndiDog already mentioned, you can get the row count by accessing the rowcount property of the cursor at any time to get the count for the last execute:</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT ... \")\nrows_affected=cursor.rowcount\n</code></pre>\n<p>From the inline documentation of python MySQLdb:</p>\n<pre><code class=\"python\"> def execute(self, query, args=None):\n\n    \"\"\"Execute a query.\n\n    query -- string, query to execute on server\n    args -- optional sequence or mapping, parameters to use with query.\n\n    Note: If args is a sequence, then %s must be used as the\n    parameter placeholder in the query. If a mapping is used,\n    %(key)s must be used as the placeholder.\n\n    Returns long integer rows affected, if any\n\n    \"\"\"\n</code></pre>\n", "abstract": "The number of rows effected is returned from execute: of course, as AndiDog already mentioned, you can get the row count by accessing the rowcount property of the cursor at any time to get the count for the last execute: From the inline documentation of python MySQLdb:"}, {"id": 19854354, "score": 19, "vote": 0, "content": "<p>In my opinion, the simplest way to get the amount of selected rows is the following:</p>\n<p>The cursor object returns a list with the results when using the fetch commands (fetchall(), fetchone(), fetchmany()). To get the selected rows just print the length of this list. But it just makes sense for fetchall(). ;-)</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">print len(cursor.fetchall)\n\n# python3\nprint(len(cur.fetchall()))\n</code></pre>\n", "abstract": "In my opinion, the simplest way to get the amount of selected rows is the following: The cursor object returns a list with the results when using the fetch commands (fetchall(), fetchone(), fetchmany()). To get the selected rows just print the length of this list. But it just makes sense for fetchall(). ;-)"}, {"id": 63736245, "score": 2, "vote": 0, "content": "<p>To get the number of selected rows I usually use the following:</p>\n<pre><code class=\"python\">cursor.execute(sql)\ncount = len(cursor.fetchall())\n</code></pre>\n", "abstract": "To get the number of selected rows I usually use the following:"}, {"id": 64637724, "score": 0, "vote": 0, "content": "<p>when using <code>count(*)</code> the result is <code>{'count(*)': 9}</code></p>\n<p>-- where 9 represents the number of rows in the table, for the instance.</p>\n<p>So, in order to fetch the just the number, this worked in my case, using mysql 8.</p>\n<pre><code class=\"python\">cursor.fetchone()['count(*)']\n</code></pre>\n", "abstract": "when using count(*) the result is {'count(*)': 9} -- where 9 represents the number of rows in the table, for the instance. So, in order to fetch the just the number, this worked in my case, using mysql 8."}]}, {"link": "https://stackoverflow.com/questions/51335298/concepts-of-backref-and-back-populate-in-sqlalchemy", "question": {"id": "51335298", "title": "Concepts of backref and back_populate in SQLalchemy?", "content": "<p>Can anyone explain the concepts of these two ideas and how they relate to making relationships between tables? I can't really seem to find anything that explains it clearly and the documentation feels like there's too much jargon to understand in easy concepts. For instance, in this example of a one to many relationship in the documentation:</p>\n<pre><code class=\"python\">class Parent(Base):\n    __tablename__ = 'parent'\n    id = Column(Integer, primary_key=True)\n    children = relationship(\"Child\", back_populates=\"parent\")\n\nclass Child(Base):\n    __tablename__ = 'child'\n    id = Column(Integer, primary_key=True)\n    parent_id = Column(Integer, ForeignKey('parent.id'))\n    parent = relationship(\"Parent\", back_populates=\"children\")\n</code></pre>\n<p>Why does the <code>relationship()</code> go inside the parent class while <code>ForeignKey</code> goes inside the child class? And what does having <code>back_populates</code> exactly do to one another? Does having the placement of which class the <code>relationship()</code> function exist in matter?</p>\n", "abstract": "Can anyone explain the concepts of these two ideas and how they relate to making relationships between tables? I can't really seem to find anything that explains it clearly and the documentation feels like there's too much jargon to understand in easy concepts. For instance, in this example of a one to many relationship in the documentation: Why does the relationship() go inside the parent class while ForeignKey goes inside the child class? And what does having back_populates exactly do to one another? Does having the placement of which class the relationship() function exist in matter?"}, "answers": [{"id": 59920780, "score": 129, "vote": 0, "content": "<p><code>backref</code> is a shortcut for configuring both <code>parent.children</code> and <code>child.parent</code> <code>relationship</code>s at one place only on the parent or the child class (not both). That is, instead of having  </p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">children = relationship(\"Child\", back_populates=\"parent\")  # on the parent class\n</code></pre>\n<p>and</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">parent = relationship(\"Parent\", back_populates=\"children\")  # on the child class\n</code></pre>\n<p>you only need one of this: </p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">children = relationship(\"Child\", backref=\"parent\")  # only on the parent class\n</code></pre>\n<p>or</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">parent = relationship(\"Parent\", backref=\"children\")  # only on the child class\n</code></pre>\n<p><code>children = relationship(\"Child\", backref=\"parent\")</code> will create the <code>.parent</code> relationship on the child class automatically. On the other hand, if you use <code>back_populates</code> you <strong>must</strong> explicitly create the <code>relationship</code>s in both parent and child classes. </p>\n<p><strong>Why does the relationship() go inside the parent class while ForeignKey goes inside the child class?</strong></p>\n<p>As I said above, if you use <code>back_populates</code>, it needs to go on both parent and child classes. If you use <code>backref</code>, it needs to go on one of them only. <code>ForeignKey</code> needs to go on the child class, no matter where the <code>relationship</code> is placed, this is a fundamental concept of relational databases.</p>\n<p><strong>And what does having back_populates exactly do to one another?</strong></p>\n<p><code>back_populates</code> informs each relationship about the other, so that they are kept in sync. For example if you do</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">p1 = Parent()\nc1 = Child()\np1.children.append(c1)\nprint(p1.children)  # will print a list of Child instances with one element: c1\nprint(c1.parent)  # will print Parent instance: p1\n</code></pre>\n<p>As you can see, <code>p1</code> was set as parent of <code>c1</code> even when you didn't set it explicitly.</p>\n<p><strong>Does having the placement of which class the relationship() function exist in matter?</strong> </p>\n<p>This only applies to <code>backref</code>, and no, you can place the relationship on the parent class (<code>children = relationship(\"Child\", backref=\"parent\")</code>) or on the child class (<code>parent = relationship(\"Parent\", backref=\"children\")</code>) and have the exact same effect.</p>\n", "abstract": "backref is a shortcut for configuring both parent.children and child.parent relationships at one place only on the parent or the child class (not both). That is, instead of having   and you only need one of this:  or children = relationship(\"Child\", backref=\"parent\") will create the .parent relationship on the child class automatically. On the other hand, if you use back_populates you must explicitly create the relationships in both parent and child classes.  Why does the relationship() go inside the parent class while ForeignKey goes inside the child class? As I said above, if you use back_populates, it needs to go on both parent and child classes. If you use backref, it needs to go on one of them only. ForeignKey needs to go on the child class, no matter where the relationship is placed, this is a fundamental concept of relational databases. And what does having back_populates exactly do to one another? back_populates informs each relationship about the other, so that they are kept in sync. For example if you do As you can see, p1 was set as parent of c1 even when you didn't set it explicitly. Does having the placement of which class the relationship() function exist in matter?  This only applies to backref, and no, you can place the relationship on the parent class (children = relationship(\"Child\", backref=\"parent\")) or on the child class (parent = relationship(\"Parent\", backref=\"children\")) and have the exact same effect."}, {"id": 72952407, "score": 4, "vote": 0, "content": "<p>Apparently, use of <strong>back_populates</strong> with explicit <strong>relationship()</strong> constructs should be preferred as\nexplained at :</p>\n<p><a href=\"https://docs.sqlalchemy.org/en/14/orm/backref.html\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/14/orm/backref.html</a></p>\n", "abstract": "Apparently, use of back_populates with explicit relationship() constructs should be preferred as\nexplained at : https://docs.sqlalchemy.org/en/14/orm/backref.html"}]}, {"link": "https://stackoverflow.com/questions/35359969/typeerror-tuple-indices-must-be-integers-not-str", "question": {"id": "35359969", "title": "TypeError: tuple indices must be integers, not str", "content": "<p>I am trying to pull data from a database and assign them to different lists.\nThis specific error is giving me a lot of trouble \"TypeError: tuple indices must be integers, not str\" \nI tried converting it to float and etc, but to no success.</p>\n<p>The code goes as below</p>\n<pre><code class=\"python\">conn=MySQLdb.connect(*details*)\ncursor=conn.cursor()\nocs={}\noltv={}\nquery=\"select pool_number, average_credit_score as waocs, average_original_ltv as waoltv from *tablename* where as_of_date= *date*\"\ncursor.execute(query)\nresult=cursor.fetchall()\n\nfor row in result:\n print row\n ocs[row[\"pool_number\"]]=int(row[\"waocs\"])\n oltv[row[\"pool_number\"]]=int(row[\"waoltv\"])\n</code></pre>\n<p>Sample output of print statement is as follows :</p>\n<pre><code class=\"python\">('MA3146', 711L, 81L)\n('MA3147', 679L, 83L)\n('MA3148', 668L, 86L)\n</code></pre>\n<p>And this is the exact error I am getting:</p>\n<pre><code class=\"python\">ocs[row[\"pool_number\"]]=int(row[\"waocs\"])\nTypeError: tuple indices must be integers, not str\n</code></pre>\n<p>Any help would be appreciated! Thanks people!</p>\n", "abstract": "I am trying to pull data from a database and assign them to different lists.\nThis specific error is giving me a lot of trouble \"TypeError: tuple indices must be integers, not str\" \nI tried converting it to float and etc, but to no success. The code goes as below Sample output of print statement is as follows : And this is the exact error I am getting: Any help would be appreciated! Thanks people!"}, "answers": [{"id": 35360064, "score": 85, "vote": 0, "content": "<p>Like the error says, <code>row</code> is a tuple, so you can't do <code>row[\"pool_number\"]</code>. You need to use the index: <code>row[0]</code>.</p>\n", "abstract": "Like the error says, row is a tuple, so you can't do row[\"pool_number\"]. You need to use the index: row[0]."}, {"id": 58741127, "score": 42, "vote": 0, "content": "<p>I think you should do </p>\n<pre><code class=\"python\">for index, row in result: \n</code></pre>\n<p>If you wanna access by name. </p>\n", "abstract": "I think you should do  If you wanna access by name. "}, {"id": 50223906, "score": 11, "vote": 0, "content": "<p>TL;DR: add the parameter <code>cursorclass=MySQLdb.cursors.DictCursor</code> at the end of your <code>MySQLdb.connect</code>.</p>\n<hr/>\n<p>I had a working code and the DB moved, I had to change the host/user/pass. After this change, my code stopped working and I started getting this error. Upon closer inspection, I copy-pasted the connection string on a place that had an extra directive. The old code read like:</p>\n<pre><code class=\"python\"> conn = MySQLdb.connect(host=\"oldhost\",\n                        user=\"olduser\",\n                        passwd=\"oldpass\",\n                        db=\"olddb\", \n                        cursorclass=MySQLdb.cursors.DictCursor)\n</code></pre>\n<p>Which was replaced by:</p>\n<pre><code class=\"python\"> conn = MySQLdb.connect(host=\"newhost\",\n                        user=\"newuser\",\n                        passwd=\"newpass\",\n                        db=\"newdb\")\n</code></pre>\n<p>The parameter <code>cursorclass=MySQLdb.cursors.DictCursor</code> at the end was making python allow me to access the rows using the column names as index. But the poor copy-paste eliminated that, yielding the error.</p>\n<p>So, as an alternative to the solutions already presented, you can also add this parameter and access the rows in the way you originally wanted. ^_^ I hope this helps others.</p>\n", "abstract": "TL;DR: add the parameter cursorclass=MySQLdb.cursors.DictCursor at the end of your MySQLdb.connect. I had a working code and the DB moved, I had to change the host/user/pass. After this change, my code stopped working and I started getting this error. Upon closer inspection, I copy-pasted the connection string on a place that had an extra directive. The old code read like: Which was replaced by: The parameter cursorclass=MySQLdb.cursors.DictCursor at the end was making python allow me to access the rows using the column names as index. But the poor copy-paste eliminated that, yielding the error. So, as an alternative to the solutions already presented, you can also add this parameter and access the rows in the way you originally wanted. ^_^ I hope this helps others."}, {"id": 67232026, "score": 7, "vote": 0, "content": "<p>I know it is not specific to this question, but for anyone coming in from a Google search: this error is also caused by a comma behind an object that creates a tuple rather than a dictionary</p>\n<pre><code class=\"python\">&gt;&gt;&gt;dict = {}\n&gt;&gt;&gt;tuple = {},\n</code></pre>\n<h2>Tuple</h2>\n<pre><code class=\"python\">&gt;&gt;&gt;tuple_ = {'key' : 'value'},\n&gt;&gt;&gt;type(tuple_)\n&lt;class 'tuple'&gt;\n</code></pre>\n<h2>Dictionary</h2>\n<pre><code class=\"python\">&gt;&gt;&gt;dict_ = {'key' : 'value'} \n&gt;&gt;&gt;type(dict_)        \n&lt;class 'dict'&gt;\n</code></pre>\n", "abstract": "I know it is not specific to this question, but for anyone coming in from a Google search: this error is also caused by a comma behind an object that creates a tuple rather than a dictionary"}, {"id": 64409980, "score": 6, "vote": 0, "content": "<p>Just adding a parameter like the below worked for me.</p>\n<pre><code class=\"python\">cursor=conn.cursor(dictionary=True)\n</code></pre>\n<p>I hope this would be helpful either.</p>\n", "abstract": "Just adding a parameter like the below worked for me. I hope this would be helpful either."}, {"id": 35360065, "score": 4, "vote": 0, "content": "<p>The Problem is how you access <code>row</code> </p>\n<p>Specifically <code>row[\"waocs\"]</code> and <code>row[\"pool_number\"]</code> of <code>ocs[row[\"pool_number\"]]=int(row[\"waocs\"])</code></p>\n<p>If you look up the <a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor-fetchall.html\" rel=\"nofollow\">official-documentation</a> of <code>fetchall()</code> you find.</p>\n<blockquote>\n<p>The method fetches all (or all remaining) rows of a query result set and returns a list of tuples.</p>\n</blockquote>\n<p>Therefore you have to access the values of rows with <code>row[__integer__]</code> like <code>row[0]</code></p>\n", "abstract": "The Problem is how you access row  Specifically row[\"waocs\"] and row[\"pool_number\"] of ocs[row[\"pool_number\"]]=int(row[\"waocs\"]) If you look up the official-documentation of fetchall() you find. The method fetches all (or all remaining) rows of a query result set and returns a list of tuples. Therefore you have to access the values of rows with row[__integer__] like row[0]"}, {"id": 66309100, "score": 0, "vote": 0, "content": "<p>SQlite3 has a method named row_factory. This method would allow you to access the values by column name.</p>\n<p><a href=\"https://www.kite.com/python/examples/3884/sqlite3-use-a-row-factory-to-access-values-by-column-name\" rel=\"nofollow noreferrer\">https://www.kite.com/python/examples/3884/sqlite3-use-a-row-factory-to-access-values-by-column-name</a></p>\n", "abstract": "SQlite3 has a method named row_factory. This method would allow you to access the values by column name. https://www.kite.com/python/examples/3884/sqlite3-use-a-row-factory-to-access-values-by-column-name"}, {"id": 67407160, "score": 0, "vote": 0, "content": "<p>I see that you're trying to identify by the name of a row. If you are looking for a specific column within the row, you can do [integer][column name]</p>\n<p>For example, to iterate through each row and only pull out the value from the row with the column header of \"pool number\", you can do this:</p>\n<pre><code class=\"python\">for row in df_updated.iterrows():\n    cell = row[1]['pool number']\n    print(cell)\n</code></pre>\n<p>The code will then iterate through each row but only print out the value that matches the \"pool number\" column</p>\n", "abstract": "I see that you're trying to identify by the name of a row. If you are looking for a specific column within the row, you can do [integer][column name] For example, to iterate through each row and only pull out the value from the row with the column header of \"pool number\", you can do this: The code will then iterate through each row but only print out the value that matches the \"pool number\" column"}]}, {"link": "https://stackoverflow.com/questions/30449960/django-save-vs-update-to-update-the-database", "question": {"id": "30449960", "title": "Django: save() vs update() to update the database?", "content": "<p>I'm writing a Django app, and I need a function to update a field in the database.  Is there any reason to do one of these methods rather than the other?</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">def save_db_field(name,field,value):\n    obj = MyModel.objects.get(name=name)\n    obj.field = value\n    obj.save()\n\ndef update_db_field(name,field,value):\n    MyModel.objects.get(name=name).update(field=value)\n</code></pre>\n<p>It seems like the second is better because it does it in one DB call instead of two.  Is there a reason why fetching, then updating is any better?</p>\n", "abstract": "I'm writing a Django app, and I need a function to update a field in the database.  Is there any reason to do one of these methods rather than the other? It seems like the second is better because it does it in one DB call instead of two.  Is there a reason why fetching, then updating is any better?"}, "answers": [{"id": 30453181, "score": 67, "vote": 0, "content": "<p>There are several key differences.</p>\n<p><code>update</code> is used on a queryset, so it is possible to update multiple objects at once.</p>\n<p>As <a href=\"https://stackoverflow.com/users/257972/fallenangel\">@FallenAngel</a> pointed out, there are differences in how custom <code>save()</code> method triggers, but it is also important to keep in mind <code>signals</code> and <code>ModelManagers</code>. I have build a small testing app to show some valuable differencies. I am using Python 2.7.5, Django==1.7.7 and SQLite, note that the final SQLs may vary on different versions of Django and different database engines.</p>\n<p>Ok, here's the example code.</p>\n<p><code>models.py</code>:</p>\n<pre><code class=\"python\">from __future__ import print_function\nfrom django.db import models\nfrom django.db.models import signals\nfrom django.db.models.signals import pre_save, post_save\nfrom django.dispatch import receiver\n\n__author__ = 'sobolevn'\n\nclass CustomManager(models.Manager):\n    def get_queryset(self):\n        super_query = super(models.Manager, self).get_queryset()\n        print('Manager is called', super_query)\n        return super_query\n\n\nclass ExtraObject(models.Model):\n    name = models.CharField(max_length=30)\n\n    def __unicode__(self):\n        return self.name\n\n\nclass TestModel(models.Model):\n\n    name = models.CharField(max_length=30)\n    key = models.ForeignKey('ExtraObject')\n    many = models.ManyToManyField('ExtraObject', related_name='extras')\n\n    objects = CustomManager()\n\n    def save(self, *args, **kwargs):\n        print('save() is called.')\n        super(TestModel, self).save(*args, **kwargs)\n\n    def __unicode__(self):\n        # Never do such things (access by foreing key) in real life,\n        # because it hits the database.\n        return u'{} {} {}'.format(self.name, self.key.name, self.many.count())\n\n\n@receiver(pre_save, sender=TestModel)\n@receiver(post_save, sender=TestModel)\ndef reicever(*args, **kwargs):\n    print('signal dispatched')\n</code></pre>\n<p><code>views.py</code>:</p>\n<pre><code class=\"python\">def index(request):\n    if request and request.method == 'GET':\n\n        from models import ExtraObject, TestModel\n\n        # Create exmple data if table is empty:\n        if TestModel.objects.count() == 0:\n            for i in range(15):\n                extra = ExtraObject.objects.create(name=str(i))\n                test = TestModel.objects.create(key=extra, name='test_%d' % i)\n                test.many.add(test)\n                print test\n\n        to_edit = TestModel.objects.get(id=1)\n        to_edit.name = 'edited_test'\n        to_edit.key = ExtraObject.objects.create(name='new_for')\n        to_edit.save()\n\n        new_key = ExtraObject.objects.create(name='new_for_update')\n        to_update = TestModel.objects.filter(id=2).update(name='updated_name', key=new_key)\n        # return any kind of HttpResponse\n</code></pre>\n<p>That resuled in these SQL queries:</p>\n<pre><code class=\"python\"># to_edit = TestModel.objects.get(id=1):\nQUERY = u'SELECT \"main_testmodel\".\"id\", \"main_testmodel\".\"name\", \"main_testmodel\".\"key_id\" \nFROM \"main_testmodel\" \nWHERE \"main_testmodel\".\"id\" = %s LIMIT 21' \n- PARAMS = (u'1',)\n\n# to_edit.save():\nQUERY = u'UPDATE \"main_testmodel\" SET \"name\" = %s, \"key_id\" = %s \nWHERE \"main_testmodel\".\"id\" = %s' \n- PARAMS = (u\"'edited_test'\", u'2', u'1')\n\n# to_update = TestModel.objects.filter(id=2).update(name='updated_name', key=new_key):\nQUERY = u'UPDATE \"main_testmodel\" SET \"name\" = %s, \"key_id\" = %s \nWHERE \"main_testmodel\".\"id\" = %s' \n- PARAMS = (u\"'updated_name'\", u'3', u'2')\n</code></pre>\n<p>We have just one query for <code>update()</code> and two for <code>save()</code>.</p>\n<p>Next, lets talk about overriding <code>save()</code> method. It is called only once for <code>save()</code> method obviously. It is worth mentioning, that <code>.objects.create()</code> also calls <code>save()</code> method. </p>\n<p>But <code>update()</code> does not call <code>save()</code> on models. And if no <code>save()</code> method is called for <code>update()</code>, so the signals are not triggered either. Output:</p>\n<pre><code class=\"python\">Starting development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n\n# TestModel.objects.get(id=1):\nManager is called [&lt;TestModel: edited_test new_for 0&gt;]\nManager is called [&lt;TestModel: edited_test new_for 0&gt;]\nsave() is called.\nsignal dispatched\nsignal dispatched\n\n# to_update = TestModel.objects.filter(id=2).update(name='updated_name', key=new_key):\nManager is called [&lt;TestModel: edited_test new_for 0&gt;]\n</code></pre>\n<p>As you can see <code>save()</code> triggers <code>Manager</code>'s <code>get_queryset()</code> twice. When <code>update()</code> only once. </p>\n<p>Resolution. If you need to \"silently\" update your values, without <code>save()</code> been called - use <code>update</code>. Usecases: <code>last_seen</code> user's field. When you need to update your model properly use <code>save()</code>.</p>\n", "abstract": "There are several key differences. update is used on a queryset, so it is possible to update multiple objects at once. As @FallenAngel pointed out, there are differences in how custom save() method triggers, but it is also important to keep in mind signals and ModelManagers. I have build a small testing app to show some valuable differencies. I am using Python 2.7.5, Django==1.7.7 and SQLite, note that the final SQLs may vary on different versions of Django and different database engines. Ok, here's the example code. models.py: views.py: That resuled in these SQL queries: We have just one query for update() and two for save(). Next, lets talk about overriding save() method. It is called only once for save() method obviously. It is worth mentioning, that .objects.create() also calls save() method.  But update() does not call save() on models. And if no save() method is called for update(), so the signals are not triggered either. Output: As you can see save() triggers Manager's get_queryset() twice. When update() only once.  Resolution. If you need to \"silently\" update your values, without save() been called - use update. Usecases: last_seen user's field. When you need to update your model properly use save()."}, {"id": 30451569, "score": 43, "vote": 0, "content": "<p>Both looks similar, but there are some key points:</p>\n<ol>\n<li><p><code>save()</code> will trigger any overridden <code>Model.save()</code> method, but <code>update()</code> will not trigger this and make a direct update on the database level. So if you have some models with overridden save methods, you must either avoid using update or find another way to do whatever you are doing on that overridden <code>save()</code> methods.</p></li>\n<li><p><code>obj.save()</code> may have some side effects if you are not careful. You retrieve the object with <code>get(...)</code>  and all model field values are passed to your obj. When you call <code>obj.save()</code>, django will save the current object state to record. So if some changes happens between <code>get()</code> and <code>save()</code> by some other process, then those changes will be lost. use <code>save(update_fields=[.....])</code> for avoiding such problems.</p></li>\n<li><p>Before Django version 1.5, Django was executing a <code>SELECT</code> before <code>INSERT</code>/<code>UPDATE</code>, so it costs 2 query execution. With version 1.5, that method is deprecated.</p></li>\n</ol>\n<p><a href=\"https://docs.djangoproject.com/en/1.8/ref/models/instances/#what-happens-when-you-save\" rel=\"noreferrer\">In here</a>, there is a good guide or <code>save()</code> and <code>update()</code> methods and how they are executed.</p>\n", "abstract": "Both looks similar, but there are some key points: save() will trigger any overridden Model.save() method, but update() will not trigger this and make a direct update on the database level. So if you have some models with overridden save methods, you must either avoid using update or find another way to do whatever you are doing on that overridden save() methods. obj.save() may have some side effects if you are not careful. You retrieve the object with get(...)  and all model field values are passed to your obj. When you call obj.save(), django will save the current object state to record. So if some changes happens between get() and save() by some other process, then those changes will be lost. use save(update_fields=[.....]) for avoiding such problems. Before Django version 1.5, Django was executing a SELECT before INSERT/UPDATE, so it costs 2 query execution. With version 1.5, that method is deprecated. In here, there is a good guide or save() and update() methods and how they are executed."}, {"id": 30450180, "score": 13, "vote": 0, "content": "<p>save() method can be used to insert new record and update existing record and generally used for saving instance of single record(row in mysql) in database.</p>\n<p>update() is not used to insert records and can be used to update multiple records(rows in mysql) in database.</p>\n", "abstract": "save() method can be used to insert new record and update existing record and generally used for saving instance of single record(row in mysql) in database. update() is not used to insert records and can be used to update multiple records(rows in mysql) in database."}, {"id": 63285510, "score": 9, "vote": 0, "content": "<p>Using update directly is more <strong>efficient</strong> and could also prevent <strong>integrity</strong> problems.</p>\n<p><strong>From the official documentation</strong> <a href=\"https://docs.djangoproject.com/en/3.0/ref/models/querysets/#django.db.models.query.QuerySet.update\" rel=\"noreferrer\">https://docs.djangoproject.com/en/3.0/ref/models/querysets/#django.db.models.query.QuerySet.update</a></p>\n<blockquote>\n<p>If you\u2019re just updating a record and don\u2019t need to do anything with\nthe model object, the most efficient approach is to call update(),\nrather than loading the model object into memory. For example, instead\nof doing this:</p>\n<pre><code class=\"python\">e = Entry.objects.get(id=10)\ne.comments_on = False\ne.save()\n</code></pre>\n<p>\u2026do this:</p>\n<pre><code class=\"python\">Entry.objects.filter(id=10).update(comments_on=False)\n</code></pre>\n<p>Using update() also prevents a race condition wherein something might\nchange in your database in the short period of time between loading\nthe object and calling save().</p>\n</blockquote>\n", "abstract": "Using update directly is more efficient and could also prevent integrity problems. From the official documentation https://docs.djangoproject.com/en/3.0/ref/models/querysets/#django.db.models.query.QuerySet.update If you\u2019re just updating a record and don\u2019t need to do anything with\nthe model object, the most efficient approach is to call update(),\nrather than loading the model object into memory. For example, instead\nof doing this: \u2026do this: Using update() also prevents a race condition wherein something might\nchange in your database in the short period of time between loading\nthe object and calling save()."}, {"id": 30451317, "score": 6, "vote": 0, "content": "<p>Update only works on updating querysets. If you want to update multiple fields at the same time, say from a dict for a single object instance you can do something like:</p>\n<pre><code class=\"python\">obj.__dict__.update(your_dict)\nobj.save()\n</code></pre>\n<p>Bear in mind that your dictionary will have to contain the correct mapping where the keys need to be your field names and the values the values you want to insert.</p>\n", "abstract": "Update only works on updating querysets. If you want to update multiple fields at the same time, say from a dict for a single object instance you can do something like: Bear in mind that your dictionary will have to contain the correct mapping where the keys need to be your field names and the values the values you want to insert."}, {"id": 30456448, "score": 4, "vote": 0, "content": "<p>Update will give you better performance with a queryset of more than one object, as it will make one database call per queryset. </p>\n<p>However save is useful, as it is easy to override the save method in your model and add extra logic there. In my own application for example, I update a dates when other fields are changed. </p>\n<pre><code class=\"python\">Class myModel(models.Model): \n    name = models.CharField()\n    date_created = models.DateField()\n\n    def save(self):\n        if not self.pk :\n           ### we have a newly created object, as the db id is not set\n           self.date_created = datetime.datetime.now()\n        super(myModel , self).save()\n</code></pre>\n", "abstract": "Update will give you better performance with a queryset of more than one object, as it will make one database call per queryset.  However save is useful, as it is easy to override the save method in your model and add extra logic there. In my own application for example, I update a dates when other fields are changed. "}, {"id": 69744707, "score": 0, "vote": 0, "content": "<p>Use _state.adding to differentiate update from create <a href=\"https://docs.djangoproject.com/en/3.2/ref/models/instances/\" rel=\"nofollow noreferrer\">https://docs.djangoproject.com/en/3.2/ref/models/instances/</a></p>\n<pre><code class=\"python\">def save(self, *args, **kwargs):\n    # Check how the current values differ from ._loaded_values. For example,\n    # prevent changing the creator_id of the model. (This example doesn't\n    # support cases where 'creator_id' is deferred).\n    if not self._state.adding and (\n            self.creator_id != self._loaded_values['creator_id']):\n        raise ValueError(\"Updating the value of creator isn't allowed\")\n    super().save(*args, **kwargs)\n</code></pre>\n", "abstract": "Use _state.adding to differentiate update from create https://docs.djangoproject.com/en/3.2/ref/models/instances/"}, {"id": 74173395, "score": 0, "vote": 0, "content": "<ul>\n<li><p><strong><code>save()</code></strong> with <strong><code>select_for_update()</code></strong> can run <strong><code>SELECT FOR UPDATE</code> query</strong>.</p>\n</li>\n<li><p><strong><code>update()</code></strong> with <strong><code>select_for_update()</code></strong> cannot run <strong><code>SELECT FOR UPDATE</code> query</strong>.</p>\n</li>\n</ul>\n<p>For example, <strong><code>select_for_update()</code></strong> is used to prevent <strong>race condition(lost update or write skew)</strong> when updating data in Django.</p>\n<p>So, I have <strong><code>person</code> table</strong> as shown below:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># \"store/models.py\"\n\nclass Person(models.Model):\n    name = models.CharField(max_length=30)\n\n    def __str__(self):\n        return self.name\n</code></pre>\n<p>And, I have <strong><code>test</code> view</strong> with <strong><code>save()</code></strong> as shown below:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># \"store/views.py\"\n\nfrom django.db import transaction\nfrom .models import Person\nfrom django.http import HttpResponse\n\n@transaction.atomic\ndef test(request):\n    person = Person.objects.select_for_update().filter(pk=1).first()\n    person.name = 'Tom'\n    person.save() # Here\n\n    return HttpResponse(\"Test\")\n</code></pre>\n<p>Then, when I run <strong><code>test</code> view</strong>, <strong><code>SELECT FOR UPDATE</code> and <code>UPDATE</code> queries</strong> are run as shown below. *I used <strong>PostgreSQL</strong> and these logs below are <strong>the query logs of PostgreSQL</strong>. You can check <a href=\"https://stackoverflow.com/questions/54780698/postgresql-database-log-transaction/73432601#73432601\"><strong>On PostgreSQL, how to log SQL queries with transaction queries such as \"BEGIN\" and \"COMMIT\"</strong></a>:</p>\n<p><a href=\"https://i.stack.imgur.com/zbXqw.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/zbXqw.png\"/></a></p>\n<p>Now, I remove <strong><code>first()</code></strong> to use <strong><code>update()</code></strong> as shown below:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># \"store/views.py\"\n\nfrom django.db import transaction\nfrom .models import Person\nfrom django.http import HttpResponse\n\n@transaction.atomic\ndef test(request):\n    person = Person.objects.select_for_update().filter(pk=1) # Here\n    person.update(name=\"Tom\") # Here\n    \n    # person = Person.objects.select_for_update().filter(pk=1).first()\n    # person.name = 'Tom'\n    # person.save()\n\n    return HttpResponse(\"Test\")\n</code></pre>\n<p>Then, when I run <strong><code>test</code> view</strong>, <strong><code>SELECT FOR UPDATE</code> query</strong> is not run and only <strong><code>UPDATE</code> query</strong> is run as shown below:</p>\n<p><a href=\"https://i.stack.imgur.com/oirUl.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/oirUl.png\"/></a></p>\n<p>So, <strong><code>save()</code></strong> with <strong><code>select_for_update()</code></strong> can run <strong><code>SELECT FOR UPDATE</code> query</strong> while <strong><code>update()</code></strong> with <strong><code>select_for_update()</code></strong> cannot.</p>\n", "abstract": "save() with select_for_update() can run SELECT FOR UPDATE query. update() with select_for_update() cannot run SELECT FOR UPDATE query. For example, select_for_update() is used to prevent race condition(lost update or write skew) when updating data in Django. So, I have person table as shown below: And, I have test view with save() as shown below: Then, when I run test view, SELECT FOR UPDATE and UPDATE queries are run as shown below. *I used PostgreSQL and these logs below are the query logs of PostgreSQL. You can check On PostgreSQL, how to log SQL queries with transaction queries such as \"BEGIN\" and \"COMMIT\":  Now, I remove first() to use update() as shown below: Then, when I run test view, SELECT FOR UPDATE query is not run and only UPDATE query is run as shown below:  So, save() with select_for_update() can run SELECT FOR UPDATE query while update() with select_for_update() cannot."}]}, {"link": "https://stackoverflow.com/questions/6461989/populating-django-field-with-pre-save", "question": {"id": "6461989", "title": "Populating django field with pre_save()?", "content": "<pre><code class=\"python\">class TodoList(models.Model):\n    title = models.CharField(maxlength=100)\n    slug = models.SlugField(maxlength=100)\n    def save(self):\n        self.slug = title\n        super(TodoList, self).save()\n</code></pre>\n<p>I'm assuming the above is how to create and store a slug when a title is inserted into the table TodoList, if not, please correct me!</p>\n<p>Anyhow, I've been looking into pre_save() as another way to do this, but can't figure out how it works.  How do you do it with pre_save()?</p>\n<p>is it like</p>\n<pre><code class=\"python\">def pre_save(self):\n     self.slug = title\n</code></pre>\n<p>I'm guessing no.  What is the code to do this?</p>\n<p>Thanks!</p>\n", "abstract": "I'm assuming the above is how to create and store a slug when a title is inserted into the table TodoList, if not, please correct me! Anyhow, I've been looking into pre_save() as another way to do this, but can't figure out how it works.  How do you do it with pre_save()? is it like I'm guessing no.  What is the code to do this? Thanks!"}, "answers": [{"id": 6462188, "score": 108, "vote": 0, "content": "<p>Most likely you are referring to <a href=\"https://docs.djangoproject.com/en/dev/ref/signals/#pre-save\">django's <code>pre_save</code> signal</a>. You could setup something like this:</p>\n<pre><code class=\"python\">from django.db.models.signals import pre_save\nfrom django.dispatch import receiver\nfrom django.template.defaultfilters import slugify\n\n@receiver(pre_save)\ndef my_callback(sender, instance, *args, **kwargs):\n    instance.slug = slugify(instance.title)\n</code></pre>\n<p>If you dont include the sender argument in the decorator, like <code>@receiver(pre_save, sender=MyModel)</code>, the callback will be called for all models.</p>\n<p>You can put the code in any file that is parsed during the execution of your app, <code>models.py</code> is a good place for that.</p>\n", "abstract": "Most likely you are referring to django's pre_save signal. You could setup something like this: If you dont include the sender argument in the decorator, like @receiver(pre_save, sender=MyModel), the callback will be called for all models. You can put the code in any file that is parsed during the execution of your app, models.py is a good place for that."}, {"id": 19394170, "score": 30, "vote": 0, "content": "<pre><code class=\"python\">@receiver(pre_save, sender=TodoList)\ndef my_callback(sender, instance, *args, **kwargs):\n    instance.slug = slugify(instance.title)\n</code></pre>\n", "abstract": ""}, {"id": 27601427, "score": 19, "vote": 0, "content": "<p>you can use django signals.pre_save:</p>\n<pre><code class=\"python\">from django.db.models.signals import post_save, post_delete, pre_save\n\nclass TodoList(models.Model):\n    @staticmethod\n    def pre_save(sender, instance, **kwargs):\n        #do anything you want\n\npre_save.connect(TodoList.pre_save, TodoList, dispatch_uid=\"sightera.yourpackage.models.TodoList\") \n</code></pre>\n", "abstract": "you can use django signals.pre_save:"}, {"id": 56219673, "score": 16, "vote": 0, "content": "<p>The <a href=\"https://docs.djangoproject.com/en/stable/ref/signals/#django.db.models.signals.pre_save\" rel=\"noreferrer\"><code>pre_save()</code> signal hook</a> is indeed a great place to handle slugification for a large number of models. The trick is to know what models need slugs generated, what field should be the basis for the slug value.</p>\n<p>I use a class decorator for this, one that lets me mark models for auto-slug-generation, and what field to base it on:</p>\n<pre><code class=\"python\">from django.db import models\nfrom django.dispatch import receiver\nfrom django.utils.text import slugify\n\ndef autoslug(fieldname):\n    def decorator(model):\n        # some sanity checks first\n        assert hasattr(model, fieldname), f\"Model has no field {fieldname!r}\"\n        assert hasattr(model, \"slug\"), \"Model is missing a slug field\"\n\n        @receiver(models.signals.pre_save, sender=model, weak=False)\n        def generate_slug(sender, instance, *args, raw=False, **kwargs):\n            if not raw and not instance.slug:\n                source = getattr(instance, fieldname)\n                slug = slugify(source)\n                if slug:  # not all strings result in a slug value\n                    instance.slug = slug\n        return model\n    return decorator\n</code></pre>\n<p>This registers a signal handler for specific models only, and lets you vary the source field with each model decorated:</p>\n<pre><code class=\"python\">@autoslug(\"name\")\nclass NamedModel(models.Model):\n    name = models.CharField(max_length=100)\n    slug = models.SlugField()\n\n@autoslug(\"title\")\nclass TitledModel(models.Model):\n    title = models.CharField(max_length=255)\n    slug = models.SlugField()\n</code></pre>\n<p>Note that no attempt is made to generate a <em>unique</em> slug value. That would require checking for integrity exceptions in a transaction or using a randomised value in the slug from a large enough pool as to make collisions unlikely. Integrity exception checking can only be done in the <code>save()</code> method, not in signal hooks.</p>\n", "abstract": "The pre_save() signal hook is indeed a great place to handle slugification for a large number of models. The trick is to know what models need slugs generated, what field should be the basis for the slug value. I use a class decorator for this, one that lets me mark models for auto-slug-generation, and what field to base it on: This registers a signal handler for specific models only, and lets you vary the source field with each model decorated: Note that no attempt is made to generate a unique slug value. That would require checking for integrity exceptions in a transaction or using a randomised value in the slug from a large enough pool as to make collisions unlikely. Integrity exception checking can only be done in the save() method, not in signal hooks."}, {"id": 53919097, "score": -3, "vote": 0, "content": "<p>Receiver functions must be like this:</p>\n<pre><code class=\"python\">def my_callback(sender, **kwargs):\n    print(\"Request finished!\")\n</code></pre>\n<p>Notice that the function takes a <strong>sender</strong> argument, along with wildcard keyword arguments <strong>(**kwargs)</strong>; all signal handlers must take these arguments.</p>\n<p>All signals send <strong>keyword arguments</strong>, and may change those keyword arguments at any time.</p>\n<p>Reference <a href=\"https://docs.djangoproject.com/en/1.11/topics/signals/#receiver-functions\" rel=\"nofollow noreferrer\">here</a>.</p>\n", "abstract": "Receiver functions must be like this: Notice that the function takes a sender argument, along with wildcard keyword arguments (**kwargs); all signal handlers must take these arguments. All signals send keyword arguments, and may change those keyword arguments at any time. Reference here."}]}, {"link": "https://stackoverflow.com/questions/2376846/which-key-value-store-is-the-most-promising-stable", "question": {"id": "2376846", "title": "Which key/value store is the most promising/stable?", "content": "<p>I'm looking to start using a key/value store for some side projects (mostly as a learning experience), but so many have popped up in the recent past that I've got no idea where to begin.  Just listing from memory, I can think of:</p>\n<ol>\n<li>CouchDB</li>\n<li>MongoDB</li>\n<li>Riak</li>\n<li>Redis</li>\n<li>Tokyo Cabinet</li>\n<li>Berkeley DB</li>\n<li>Cassandra</li>\n<li>MemcacheDB</li>\n</ol>\n<p>And I'm sure that there are more out there that have slipped through my search efforts.  With all the information out there, it's hard to find solid comparisons between all of the competitors.  My criteria and questions are:</p>\n<ol>\n<li><strong>(Most Important) Which do you recommend, and <em>why</em>?</strong></li>\n<li>Which one is the fastest?</li>\n<li>Which one is the most stable?</li>\n<li>Which one is the easiest to set up and install?</li>\n<li>Which ones have bindings for Python and/or Ruby?</li>\n</ol>\n<p><strong>Edit:</strong><br/>\nSo far it looks like Redis is the best solution, but that's only because I've gotten one solid response (from ardsrk).  I'm looking for more answers like his, because they point me in the direction of useful, quantitative information.  Which Key-Value store do <strong>you</strong> use, and <strong>why</strong>?</p>\n<p><strong>Edit 2:</strong><br/>\nIf anyone has experience with CouchDB, Riak, or MongoDB, I'd love to hear your experiences with them (and even more so if you can offer a comparative analysis of several of them)</p>\n", "abstract": "I'm looking to start using a key/value store for some side projects (mostly as a learning experience), but so many have popped up in the recent past that I've got no idea where to begin.  Just listing from memory, I can think of: And I'm sure that there are more out there that have slipped through my search efforts.  With all the information out there, it's hard to find solid comparisons between all of the competitors.  My criteria and questions are: Edit:\nSo far it looks like Redis is the best solution, but that's only because I've gotten one solid response (from ardsrk).  I'm looking for more answers like his, because they point me in the direction of useful, quantitative information.  Which Key-Value store do you use, and why? Edit 2:\nIf anyone has experience with CouchDB, Riak, or MongoDB, I'd love to hear your experiences with them (and even more so if you can offer a comparative analysis of several of them)"}, "answers": [{"id": 2377117, "score": 26, "vote": 0, "content": "<blockquote>\n<p>Which do you recommend, and why?</p>\n</blockquote>\n<p>I recommend Redis. Why? Continue reading!!</p>\n<blockquote>\n<p>Which one is the fastest?</p>\n</blockquote>\n<p>I can't say whether it's the fastest. But Redis is <a href=\"http://redis.io/topics/benchmarks\" rel=\"nofollow noreferrer\">fast</a>. It's fast because\nit holds all the data in RAM. Recently, virtual memory feature was added but still all the keys stay in main memory with only rarely used values being swapped to disk.</p>\n<blockquote>\n<p>Which one is the most stable?</p>\n</blockquote>\n<p>Again, since I have no direct experience with the other key-value stores I can't compare. However, Redis is being used in production by many web applications like <a href=\"http://github.com/blog/530-how-we-made-github-fast\" rel=\"nofollow noreferrer\">GitHub</a> and <a href=\"http://instagram-engineering.tumblr.com/post/12202313862/storing-hundreds-of-millions-of-simple-key-value-pairs\" rel=\"nofollow noreferrer\">Instagram</a>, among many others.</p>\n<blockquote>\n<p>Which one is the easiest to set up and install?</p>\n</blockquote>\n<p>Redis is fairly easy to setup. Grab the <a href=\"http://redis.io/download\" rel=\"nofollow noreferrer\">source</a> and on a Linux box run <code>make install</code>. This yields <code>redis-server</code> binary that you could put it on your path and start it. </p>\n<p><code>redis-server</code> binds to port 6379 by default. Have a look at <code>redis.conf</code> that comes with the source for more configuration and setup options.</p>\n<blockquote>\n<p>Which ones have bindings for Python and/or Ruby?</p>\n</blockquote>\n<p>Redis has excellent <a href=\"http://github.com/ezmobius/redis-rb\" rel=\"nofollow noreferrer\">Ruby</a> and <a href=\"http://github.com/andymccurdy/redis-py/\" rel=\"nofollow noreferrer\">Python</a> support.</p>\n<p>In response to <a href=\"https://stackoverflow.com/questions/2376846/which-key-value-store-is-the-most-promising-stable#comment2353662_2377117\">Xorlev's comment</a> below: Memcached is just a simple key-value store. Redis supports complex <a href=\"http://redis.io/topics/data-types\" rel=\"nofollow noreferrer\">data types</a> like lists, sets and sorted sets and at the same time provides a <a href=\"http://redis.io/commands\" rel=\"nofollow noreferrer\">simple interface</a> to these data types. </p>\n<p>There is also <code>make 32bit</code> that makes all pointers only 32-bits in size even on 64 bit machines. This saves considerable memory on machines with less than 4GB of RAM.</p>\n", "abstract": "Which do you recommend, and why? I recommend Redis. Why? Continue reading!! Which one is the fastest? I can't say whether it's the fastest. But Redis is fast. It's fast because\nit holds all the data in RAM. Recently, virtual memory feature was added but still all the keys stay in main memory with only rarely used values being swapped to disk. Which one is the most stable? Again, since I have no direct experience with the other key-value stores I can't compare. However, Redis is being used in production by many web applications like GitHub and Instagram, among many others. Which one is the easiest to set up and install? Redis is fairly easy to setup. Grab the source and on a Linux box run make install. This yields redis-server binary that you could put it on your path and start it.  redis-server binds to port 6379 by default. Have a look at redis.conf that comes with the source for more configuration and setup options. Which ones have bindings for Python and/or Ruby? Redis has excellent Ruby and Python support. In response to Xorlev's comment below: Memcached is just a simple key-value store. Redis supports complex data types like lists, sets and sorted sets and at the same time provides a simple interface to these data types.  There is also make 32bit that makes all pointers only 32-bits in size even on 64 bit machines. This saves considerable memory on machines with less than 4GB of RAM."}, {"id": 2616225, "score": 24, "vote": 0, "content": "<p>You need to understand what modern NoSQL phenomenon is about. <br/>\nIt is not about key-value storages. They've been available for decades (BerkeleyDB for example). Why all the fuss now ?</p>\n<p>It is not about fancy document or object oriented schemas and overcoming \"impedance mismatch\". Proponents of these features have been touting them for years and they got nowhere.</p>\n<p>It is simply about adressing 3 technical problems: automatic (for maintainers) and transparent (for application developers) failover, sharding and replication.\nThus you should ignore any trendy products that do not deliver on this front. These include Redis, MongoDB, CouchDB etc. And concentrate on truly distributed solutions like cassandra, riak etc.</p>\n<p>Otherwise you'll loose all the good stuff sql gives you (adhoc queries, Crystal Reports for your boss, third party tools and libraries) and get nothing in return.</p>\n", "abstract": "You need to understand what modern NoSQL phenomenon is about. \nIt is not about key-value storages. They've been available for decades (BerkeleyDB for example). Why all the fuss now ? It is not about fancy document or object oriented schemas and overcoming \"impedance mismatch\". Proponents of these features have been touting them for years and they got nowhere. It is simply about adressing 3 technical problems: automatic (for maintainers) and transparent (for application developers) failover, sharding and replication.\nThus you should ignore any trendy products that do not deliver on this front. These include Redis, MongoDB, CouchDB etc. And concentrate on truly distributed solutions like cassandra, riak etc. Otherwise you'll loose all the good stuff sql gives you (adhoc queries, Crystal Reports for your boss, third party tools and libraries) and get nothing in return."}, {"id": 2380691, "score": 8, "vote": 0, "content": "<p>At this year's PyCon, Jeremy Edberg of Reddit gave a talk:</p>\n<p><a href=\"http://pycon.blip.tv/file/3257303/\" rel=\"noreferrer\">http://pycon.blip.tv/file/3257303/</a></p>\n<p>He said that Reddit uses PostGres as a key-value store, presumably with a simple 2-column table; according to his talk it had benchmarked faster than any other key-value store they had tried. And, of course, it's very mature.</p>\n<p>Ultimately, OverClocked is right; your use case determines the best store. But RDMBSs have long been (ab)used as key-value stores, and they can be very fast, too.</p>\n", "abstract": "At this year's PyCon, Jeremy Edberg of Reddit gave a talk: http://pycon.blip.tv/file/3257303/ He said that Reddit uses PostGres as a key-value store, presumably with a simple 2-column table; according to his talk it had benchmarked faster than any other key-value store they had tried. And, of course, it's very mature. Ultimately, OverClocked is right; your use case determines the best store. But RDMBSs have long been (ab)used as key-value stores, and they can be very fast, too."}, {"id": 2376875, "score": 7, "vote": 0, "content": "<p>They all have different features. And don't forget <a href=\"http://project-voldemort.com/\" rel=\"nofollow noreferrer\">Project Voldemort</a> which is actually used/tested by LinkedIn in their production before each release.</p>\n<p>It's hard to compare. You have to ask yourself what you need: e.g. do you want partitioning? if so then some of them, like CouchDB, won't support it. Do you want erasure coding? Then most of them don't have that. Etc.</p>\n<p>Berkeley DB is a very basic, low level storage engine, that perhaps can be excused from this discussion. Several key-value systems are built on top of it, to provide additional features like replication, versioning, coding, etc.</p>\n<p>Also, what does your application need? Several of the solutions contain complexity that may not be necessary. E.g. if you just store static data that won't change, you can store them under data's SHA-1 content hash (i.e. use the content-hash as key). In this case, you don't have to worry about freshness, synchronization, versioning, and lots of complexities can be removed.</p>\n", "abstract": "They all have different features. And don't forget Project Voldemort which is actually used/tested by LinkedIn in their production before each release. It's hard to compare. You have to ask yourself what you need: e.g. do you want partitioning? if so then some of them, like CouchDB, won't support it. Do you want erasure coding? Then most of them don't have that. Etc. Berkeley DB is a very basic, low level storage engine, that perhaps can be excused from this discussion. Several key-value systems are built on top of it, to provide additional features like replication, versioning, coding, etc. Also, what does your application need? Several of the solutions contain complexity that may not be necessary. E.g. if you just store static data that won't change, you can store them under data's SHA-1 content hash (i.e. use the content-hash as key). In this case, you don't have to worry about freshness, synchronization, versioning, and lots of complexities can be removed."}, {"id": 2380871, "score": 7, "vote": 0, "content": "<p>I've been playing with MongoDB and it has one thing that makes it perfect for my application, the ability to store complex Maps/Lists in the database directly. I have a large Map where each value is a list and I don't have to do anything special just to write and retrieve that without knowing all the different keys and list values. I don't know much about the other options but the speed and that ability make Mongo perfect for my application. Plus the Java driver is very simple to use.</p>\n", "abstract": "I've been playing with MongoDB and it has one thing that makes it perfect for my application, the ability to store complex Maps/Lists in the database directly. I have a large Map where each value is a list and I don't have to do anything special just to write and retrieve that without knowing all the different keys and list values. I don't know much about the other options but the speed and that ability make Mongo perfect for my application. Plus the Java driver is very simple to use."}, {"id": 2381057, "score": 6, "vote": 0, "content": "<p>One distinction you have to make is what will you use the DB for?\nDon't jump on board just because it's trendy. Do you need a key value store? or do you need a document based store? What is your memory footprint requirement? running it on a small VM or a separate one? </p>\n<p>I recommend listing your requirements first and then seeing which ones overlap with your requirements.</p>\n<p>With that said, I have used CouchDB/MongoDB and prefer to use MongoDB for its ease of setup and best transition from mysql style queries. I chose mongodb over sql because of dynamic schemas(no migration files!) and better data modeling(arrays, hashes). I did not evaluate based on scalability.</p>\n<p>MongoMapper is a great MongoDB orm mapper for Ruby and there's already a working Rails 3 fork.</p>\n<p>I listed some more details about why I prefered mongodb in my scribd slides \n<a href=\"http://tommy.chheng.com/index.php/2010/02/mongodb-for-natural-development/\" rel=\"noreferrer\">http://tommy.chheng.com/index.php/2010/02/mongodb-for-natural-development/</a></p>\n", "abstract": "One distinction you have to make is what will you use the DB for?\nDon't jump on board just because it's trendy. Do you need a key value store? or do you need a document based store? What is your memory footprint requirement? running it on a small VM or a separate one?  I recommend listing your requirements first and then seeing which ones overlap with your requirements. With that said, I have used CouchDB/MongoDB and prefer to use MongoDB for its ease of setup and best transition from mysql style queries. I chose mongodb over sql because of dynamic schemas(no migration files!) and better data modeling(arrays, hashes). I did not evaluate based on scalability. MongoMapper is a great MongoDB orm mapper for Ruby and there's already a working Rails 3 fork. I listed some more details about why I prefered mongodb in my scribd slides \nhttp://tommy.chheng.com/index.php/2010/02/mongodb-for-natural-development/"}, {"id": 2384388, "score": 6, "vote": 0, "content": "<p>I notice how everyone is confusing memcached with memcachedb. They are two different systems. The op asked about memcachedb.</p>\n<p>memcached is memory storage. memcachedb uses Berkeley DB as its datastore.</p>\n", "abstract": "I notice how everyone is confusing memcached with memcachedb. They are two different systems. The op asked about memcachedb. memcached is memory storage. memcachedb uses Berkeley DB as its datastore."}, {"id": 2438183, "score": 5, "vote": 0, "content": "<p>I only have experience with Berkeley DB, so I'll mention what I like about it.</p>\n<ul>\n<li>It is fast</li>\n<li>It is very mature and stable</li>\n<li>It has outstanding documentation</li>\n<li>It has C,C++,Java &amp; C# bindings out of the box. Other language bindings are available. I believe Python comes with bindings as part of its \"batteries\".</li>\n</ul>\n<p>The only downside I've run into is that the C# bindings are new and don't seem to support every feature.</p>\n", "abstract": "I only have experience with Berkeley DB, so I'll mention what I like about it. The only downside I've run into is that the C# bindings are new and don't seem to support every feature."}, {"id": 2380915, "score": 4, "vote": 0, "content": "<p>There is also zodb.</p>\n", "abstract": "There is also zodb."}, {"id": 6257091, "score": 4, "vote": 0, "content": "<blockquote>\n<p>Which key value store is the most promising/stable?</p>\n</blockquote>\n<p><a href=\"http://forum.gwan.com/index.php?p=/discussion/comment/2253/#Comment_2253\" rel=\"nofollow\">G-WAN KV store</a> looks rather <strong>promising</strong>:</p>\n<pre><code class=\"python\">DB engine            Traversal\n-----------          ----------------------------\nSQLite               0.261 ms  (b-tree)\nTokyo-Cabinet (TC)   4.188 ms  (hash table)\nTC-FIXED             0.103 ms  (fixed-size array)\nG-WAN KV             0.010 ms  (unamed)\n</code></pre>\n<p>Also, it is used internally by G-WAN webserver, known for its high concurrency performances (that's for the <strong>stability</strong> question).</p>\n", "abstract": "Which key value store is the most promising/stable? G-WAN KV store looks rather promising: Also, it is used internally by G-WAN webserver, known for its high concurrency performances (that's for the stability question)."}, {"id": 2377161, "score": 3, "vote": 0, "content": "<p>I really like <strong>memcached</strong> personally.</p>\n<p>I use it on a couple of my sites and it's simple, fast, and easy. It really was just incredibly simple to use, the API is easy to use. It doesn't store anything on disk, thus the name memcached, so it's out if you're looking for a persistent storage engine.</p>\n<p>Python has <strong>python-memcached</strong>.</p>\n<p>I haven't used the Ruby client, but a quick Google search reveals <strong>RMemCache</strong></p>\n<p>If you just need a caching engine, memcached is the way to go. It's developed, it's stable, and it's bleedin' fast. There's a reason LiveJournal made it and Facebook develops it. It's in use at some of the largest sites out there to great effect. It scales extremely well.</p>\n", "abstract": "I really like memcached personally. I use it on a couple of my sites and it's simple, fast, and easy. It really was just incredibly simple to use, the API is easy to use. It doesn't store anything on disk, thus the name memcached, so it's out if you're looking for a persistent storage engine. Python has python-memcached. I haven't used the Ruby client, but a quick Google search reveals RMemCache If you just need a caching engine, memcached is the way to go. It's developed, it's stable, and it's bleedin' fast. There's a reason LiveJournal made it and Facebook develops it. It's in use at some of the largest sites out there to great effect. It scales extremely well."}, {"id": 2616380, "score": 2, "vote": 0, "content": "<p><a href=\"http://cassandra.apache.org/\" rel=\"nofollow noreferrer\">Cassandra</a> seems to be popular.</p>\n<blockquote>\n<p>Cassandra is in use at Digg, Facebook, Twitter, Reddit, Rackspace, Cloudkick, Cisco, SimpleGeo, Ooyala, OpenX, and more companies that have large, active data sets. The largest production cluster has over 100 TB of data in over 150 machines.</p>\n</blockquote>\n", "abstract": "Cassandra seems to be popular. Cassandra is in use at Digg, Facebook, Twitter, Reddit, Rackspace, Cloudkick, Cisco, SimpleGeo, Ooyala, OpenX, and more companies that have large, active data sets. The largest production cluster has over 100 TB of data in over 150 machines."}, {"id": 2380715, "score": 1, "vote": 0, "content": "<p>Just to make the list complete: there's Dreamcache, too. It's compatible with Memcached (in terms of protocol, so you can use any client library written for Memcached), it's just faster.</p>\n", "abstract": "Just to make the list complete: there's Dreamcache, too. It's compatible with Memcached (in terms of protocol, so you can use any client library written for Memcached), it's just faster."}, {"id": 14586720, "score": 1, "vote": 0, "content": "<p>As the others said, it depends always on your needs. I for example prefer whatever suits my applications best.</p>\n<p>I first used memcached to have fast read/write access. As Java API I\u00b4ve used SpyMemcached, what comes with an very easy interface you can use for writing and reading data. Due to memory leaks (no more RAM) I was required to look for another solution, also I was not able scale right, just increase the memory for a single process seemed to be not an good achievement. </p>\n<p>After some reviewing I saw couchbase, it comes with replication, clustering, auto-failover, and a community edition (MS Windows, MacOs, Linux). And the best thing for me was, the Java client of it implements also SpyMemcached, so I had almost nothing else to do as setup the server and use couchbase instead of memcached as datastore. Advantage? Sure, my data is now persistent, replicated, and indexed. It comes with a webconsole to write map reduce functions for document views in erlang. </p>\n<p>It has Support for Python, Ruby, .Net and more, easy configuration through the webconsole and client-tools. It runs stable. With some tests I was able to write about 10k per second for 200 - 400 byte long records. Reading Performance was way higher though (both tested locally). Have a lot of fun making your decision.</p>\n", "abstract": "As the others said, it depends always on your needs. I for example prefer whatever suits my applications best. I first used memcached to have fast read/write access. As Java API I\u00b4ve used SpyMemcached, what comes with an very easy interface you can use for writing and reading data. Due to memory leaks (no more RAM) I was required to look for another solution, also I was not able scale right, just increase the memory for a single process seemed to be not an good achievement.  After some reviewing I saw couchbase, it comes with replication, clustering, auto-failover, and a community edition (MS Windows, MacOs, Linux). And the best thing for me was, the Java client of it implements also SpyMemcached, so I had almost nothing else to do as setup the server and use couchbase instead of memcached as datastore. Advantage? Sure, my data is now persistent, replicated, and indexed. It comes with a webconsole to write map reduce functions for document views in erlang.  It has Support for Python, Ruby, .Net and more, easy configuration through the webconsole and client-tools. It runs stable. With some tests I was able to write about 10k per second for 200 - 400 byte long records. Reading Performance was way higher though (both tested locally). Have a lot of fun making your decision."}, {"id": 16867563, "score": 1, "vote": 0, "content": "<p>Only have experience with mongoDB, memchache and redis. Here's a <a href=\"http://db-engines.com/en/system/CouchDB;Memcached;MongoDB;Redis\" rel=\"nofollow noreferrer\">comparison</a> between them and couchDB.</p>\n<p>Seems mongoDB is most popular. It support sharding and replication, eventually consistent, has good support in ruby (mongoid). It also have a richer feature set than the other two. All of mongo, redis and memchache can store the key-value in memory, but redis seems to be much faster, according to <a href=\"https://stackoverflow.com/questions/5252577/how-much-faster-is-redis-than-mongodb\">this post</a>, redis is 2x write, 3x read faster than mongo. It has better designed data structures and more 'light-weight'.</p>\n<p>I would say they have different usages, mongoDB is probably good for large dataset and document storage while memchache and redis are better to store caches or logs.</p>\n", "abstract": "Only have experience with mongoDB, memchache and redis. Here's a comparison between them and couchDB. Seems mongoDB is most popular. It support sharding and replication, eventually consistent, has good support in ruby (mongoid). It also have a richer feature set than the other two. All of mongo, redis and memchache can store the key-value in memory, but redis seems to be much faster, according to this post, redis is 2x write, 3x read faster than mongo. It has better designed data structures and more 'light-weight'. I would say they have different usages, mongoDB is probably good for large dataset and document storage while memchache and redis are better to store caches or logs."}]}, {"link": "https://stackoverflow.com/questions/27264574/import-psycopg2-library-not-loaded-libssl-1-0-0-dylib", "question": {"id": "27264574", "title": "Import psycopg2 Library not loaded: libssl.1.0.0.dylib", "content": "<p>When I try to run the command:</p>\n<pre><code class=\"python\">import psycopg2\n</code></pre>\n<p>I get the error:</p>\n<pre><code class=\"python\">ImportError: dlopen(/Users/gwulfs/anaconda/lib/python2.7/site-packages/psycopg2/_psycopg.so, 2): Library not loaded: libssl.1.0.0.dylib\n  Referenced from: /Users/gwulfs/anaconda/lib/python2.7/site-packages/psycopg2/_psycopg.so\n  Reason: image not found\n</code></pre>\n<p>So far I have tried <code>brew install openssl</code> and have referenced (with no luck):</p>\n<p><a href=\"https://stackoverflow.com/questions/11365619/psycopg2-installation-error-library-not-loaded-libssl-dylib\">psycopg2 installation error - Library not loaded: libssl.dylib</a></p>\n<p><a href=\"http://joshuakehn.com/2013/10/13/Postgresapp-and-psycopg2-on-OS-X.html\" rel=\"noreferrer\">http://joshuakehn.com/2013/10/13/Postgresapp-and-psycopg2-on-OS-X.html</a></p>\n<p><a href=\"https://stackoverflow.com/questions/16407995/psycopg2-image-not-found\">Psycopg2 image not found</a></p>\n", "abstract": "When I try to run the command: I get the error: So far I have tried brew install openssl and have referenced (with no luck): psycopg2 installation error - Library not loaded: libssl.dylib http://joshuakehn.com/2013/10/13/Postgresapp-and-psycopg2-on-OS-X.html Psycopg2 image not found"}, "answers": [{"id": 30726895, "score": 77, "vote": 0, "content": "<p>Instead of playing with symlinks in system library dirs, set the <code>$DYLD_FALLBACK_LIBRARY_PATH</code> to include the anaconda libraries. eg:</p>\n<pre><code class=\"python\">export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib/:$DYLD_FALLBACK_LIBRARY_PATH\n</code></pre>\n", "abstract": "Instead of playing with symlinks in system library dirs, set the $DYLD_FALLBACK_LIBRARY_PATH to include the anaconda libraries. eg:"}, {"id": 41801121, "score": 35, "vote": 0, "content": "<p>After Homebrew wouldn't allow me to force link <code>openssl</code> the following worked fine:</p>\n<pre><code class=\"python\">pip install --global-option=build_ext \\\n            --global-option=\"-I/usr/local/opt/openssl/include\" \\\n            --global-option=\"-L/usr/local/opt/openssl/lib\" psycopg2\n</code></pre>\n<p>(this installation succeeded in a <code>virtualenv</code> on macOS)</p>\n", "abstract": "After Homebrew wouldn't allow me to force link openssl the following worked fine: (this installation succeeded in a virtualenv on macOS)"}, {"id": 27339209, "score": 31, "vote": 0, "content": "<p><strong><em>EDIT: potentially dangerous, read comments first!</em></strong></p>\n<p>See a much safer answer below: <a href=\"https://stackoverflow.com/a/30726895/308315\">https://stackoverflow.com/a/30726895/308315</a></p>\n<hr/>\n<p>I ran into this exact issue about an hour after you posted it and just figured it out. I am using Mac OS X Yosemite, Python 2.7, and the Postgresql app.</p>\n<p>There seems to be a non-working symlink set by default (or I introduced it while troubleshooting), to fix it first remove the incorrect links:</p>\n<pre><code class=\"python\">$ sudo rm /usr/lib/libssl.1.0.0.dylib\n$ sudo rm /usr/lib/libcrypto.1.0.0.dylib\n</code></pre>\n<p>Then re-link them with (replace YOURUSERNAME with your Mac user name. I found it helpful to use tab to complete each step, to confirm the directory):</p>\n<pre><code class=\"python\">$ sudo ln -s /Users/YOURUSERNAME/anaconda/lib/libssl.1.0.0.dylib /usr/lib\n$ sudo ln -s /Users/YOURUSERNAME/anaconda/lib/libcrypto.1.0.0.dylib /usr/lib\n</code></pre>\n<p>I believe the other solutions didn't work for you because your version is in anaconda.</p>\n", "abstract": "EDIT: potentially dangerous, read comments first! See a much safer answer below: https://stackoverflow.com/a/30726895/308315 I ran into this exact issue about an hour after you posted it and just figured it out. I am using Mac OS X Yosemite, Python 2.7, and the Postgresql app. There seems to be a non-working symlink set by default (or I introduced it while troubleshooting), to fix it first remove the incorrect links: Then re-link them with (replace YOURUSERNAME with your Mac user name. I found it helpful to use tab to complete each step, to confirm the directory): I believe the other solutions didn't work for you because your version is in anaconda."}, {"id": 36872624, "score": 27, "vote": 0, "content": "<p>After bashing my head against the wall for a couple hours, these two solutions are guaranteed to work:</p>\n<p><strong>Option 1.</strong> This solves our problem without messing around with environment variables. Run this in your shell:</p>\n<pre><code class=\"python\">brew install --upgrade openssl\nbrew unlink openssl &amp;&amp; brew link openssl --force\n</code></pre>\n<p>Boom! This upgrades the symbolic links in <code>/usr/local</code> for <code>libssl</code> and <code>libcrypto</code>. Now <code>import psycopg2</code> works like a charm.</p>\n<p><strong>Option 2.</strong> If for some reason you would like to maintain the current symbolic links in <code>usr/local</code>, run this command in your shell:</p>\n<p><code>export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib/:$DYLD_FALLBACK_LIBRARY_PATH</code></p>\n<p>Just make sure to replace <code>$HOME/anaconda/lib</code> above with the actual lib path. In my case, this was <code>$HOME/miniconda2/envs/ali/lib</code>.</p>\n<p>This will only work for the shell/bash session you're currently in. To make the change persistent, add the <code>export</code> statement to your <code>~/.bash_profile</code> or <code>~/.bashrc</code> file.</p>\n<p><strong>Thoughts:</strong> IMO #1 is the proper way to deal with this problem, but I left #2 in case some people prefer working with environment variables rather than fixing symbolic links (if, for example, they have software with a dependency on the older openssl file versions).</p>\n", "abstract": "After bashing my head against the wall for a couple hours, these two solutions are guaranteed to work: Option 1. This solves our problem without messing around with environment variables. Run this in your shell: Boom! This upgrades the symbolic links in /usr/local for libssl and libcrypto. Now import psycopg2 works like a charm. Option 2. If for some reason you would like to maintain the current symbolic links in usr/local, run this command in your shell: export DYLD_FALLBACK_LIBRARY_PATH=$HOME/anaconda/lib/:$DYLD_FALLBACK_LIBRARY_PATH Just make sure to replace $HOME/anaconda/lib above with the actual lib path. In my case, this was $HOME/miniconda2/envs/ali/lib. This will only work for the shell/bash session you're currently in. To make the change persistent, add the export statement to your ~/.bash_profile or ~/.bashrc file. Thoughts: IMO #1 is the proper way to deal with this problem, but I left #2 in case some people prefer working with environment variables rather than fixing symbolic links (if, for example, they have software with a dependency on the older openssl file versions)."}, {"id": 36063405, "score": 12, "vote": 0, "content": "<p>conda install psycopg works for me. It updates the following packages\nThe following packages will be UPDATED:</p>\n<pre><code class=\"python\">conda:      3.19.1-py27_0 --&gt; 4.0.5-py27_0\nopenssl:    1.0.2f-0      --&gt; 1.0.2g-0\npip:        8.0.2-py27_0  --&gt; 8.1.0-py27_0\nsetuptools: 19.6.2-py27_0 --&gt; 20.2.2-py27_0\nwheel:      0.26.0-py27_1 --&gt; 0.29.0-py27_0\n</code></pre>\n", "abstract": "conda install psycopg works for me. It updates the following packages\nThe following packages will be UPDATED:"}, {"id": 56660273, "score": 9, "vote": 0, "content": "<p>I was having this issue on Mac, trying ln -s was giving me \nln: /usr/lib/libssl.1.0.0.dylib: Operation not permitted\nI didn't want to mess with my system. Instead What worked for me is to simply install psycopg2-binary :\npip install psycopg2-binary  </p>\n<p>This installed <strong>psycopg2-binary-2.8.3</strong> version</p>\n", "abstract": "I was having this issue on Mac, trying ln -s was giving me \nln: /usr/lib/libssl.1.0.0.dylib: Operation not permitted\nI didn't want to mess with my system. Instead What worked for me is to simply install psycopg2-binary :\npip install psycopg2-binary   This installed psycopg2-binary-2.8.3 version"}, {"id": 35382240, "score": 8, "vote": 0, "content": "<p>In relation to X.L.'s answer above, I didn't want to use Anaconda when I'm already using pip, so I just gave it the path to the Postgres libraries which worked for me (I'm using PostgreSQL.app on Mac OS 10.10)...</p>\n<pre><code class=\"python\">export DYLD_FALLBACK_LIBRARY_PATH=/Library/PostgreSQL/9.5/lib:$DYLD_FALLBACK_LIBRARY_PATH\n</code></pre>\n", "abstract": "In relation to X.L.'s answer above, I didn't want to use Anaconda when I'm already using pip, so I just gave it the path to the Postgres libraries which worked for me (I'm using PostgreSQL.app on Mac OS 10.10)..."}, {"id": 33373036, "score": 4, "vote": 0, "content": "<p>I had to vary Scott Brennstuhl's answer a little: \n1. Remove broken symlinks:</p>\n<pre><code class=\"python\">$ sudo rm /usr/lib/libssl.1.0.0.dylib\n$ sudo rm /usr/lib/libcrypto.1.0.0.dylib\n$ sudo rm /usr/lib/libpq.5.dylib\n</code></pre>\n<ol start=\"2\">\n<li>Relink with postgres' included drivers:</li>\n</ol>\n<pre><code class=\"python\">$ sudo ln -s   /Applications/Postgres.app/Contents/Versions/9.4/lib/libssl.1.0.0.dylib /usr/lib    \n$ sudo ln -s /Applications/Postgres.app/Contents/Versions/9.4/lib/libcrypto.1.0.0.dylib /usr/lib\n$ sudo ln -s /Applications/Postgres.app/Contents/Versions/9.4/lib/libpq.5.dylib  /usr/lib\n</code></pre>\n", "abstract": "I had to vary Scott Brennstuhl's answer a little: \n1. Remove broken symlinks:"}, {"id": 39088345, "score": 4, "vote": 0, "content": "<p>My flavor of setup was a little different than the OP: I'm using <code>Postgres.app</code> on Mac and am within a <code>virtualenv</code>; but the symptoms were similar.</p>\n<p>For me, this occurred right after updating my <code>Postgres.app</code> from 9.3 to 9.5 on my local, and the error clearly showed the <code>psycopg2</code> path for <code>libssl.1.0.0.dylib</code> was pointing to the old 9.3 data directory location (the <code>image</code> referenced in this error?). Adding weird things to my <code>ENV</code> or removing symlinks I'm not sure the impact of definitely didn't feel right to me. I solved it by uninstalling then re-installing <code>psycopg2</code> the same way I had when it was working - something that doesn't feel very dangerous at all:</p>\n<pre><code class=\"python\"> # In my virtualenv\n pip uninstall psycopg2\n pip install psycopg2\n</code></pre>\n<p>Then I was all good!</p>\n", "abstract": "My flavor of setup was a little different than the OP: I'm using Postgres.app on Mac and am within a virtualenv; but the symptoms were similar. For me, this occurred right after updating my Postgres.app from 9.3 to 9.5 on my local, and the error clearly showed the psycopg2 path for libssl.1.0.0.dylib was pointing to the old 9.3 data directory location (the image referenced in this error?). Adding weird things to my ENV or removing symlinks I'm not sure the impact of definitely didn't feel right to me. I solved it by uninstalling then re-installing psycopg2 the same way I had when it was working - something that doesn't feel very dangerous at all: Then I was all good!"}, {"id": 34394047, "score": 3, "vote": 0, "content": "<p>Do the following to resolve Library not loaded:libssl.1.0.0.dylib error <strong>if you have openssl in /usr/local/Cellar directory</strong> <br/></p>\n<blockquote>\n<ol>\n<li><p>sudo cp /usr/local/Cellar/openssl/&lt;&lt;<em>version</em>&gt;&gt;/lib/libssl.1.0.0.dylib /usr/lib <br/></p>\n</li>\n<li><p>After doing step 1, if you still get Library not loaded:libcrypto.1.0.0.dylib error. Do the following <br/>\n\u00a0\u00a0\u00a0\u00a0sudo cp /usr/local/Cellar/openssl/&lt;&lt;<em>version</em>&gt;&gt;/lib/libcrypto.1.0.0.dylib /usr/lib</p>\n</li>\n</ol>\n</blockquote>\n", "abstract": "Do the following to resolve Library not loaded:libssl.1.0.0.dylib error if you have openssl in /usr/local/Cellar directory  sudo cp /usr/local/Cellar/openssl/<<version>>/lib/libssl.1.0.0.dylib /usr/lib  After doing step 1, if you still get Library not loaded:libcrypto.1.0.0.dylib error. Do the following \n\u00a0\u00a0\u00a0\u00a0sudo cp /usr/local/Cellar/openssl/<<version>>/lib/libcrypto.1.0.0.dylib /usr/lib"}, {"id": 59391627, "score": 3, "vote": 0, "content": "<p><code>brew reinstall openssl postgres</code> did the trick for me</p>\n", "abstract": "brew reinstall openssl postgres did the trick for me"}, {"id": 59635908, "score": 3, "vote": 0, "content": "<p>I had the same problem when I updated <code>openssl</code> from <code>1.0.0</code> to <code>1.1.1d</code>, and this fixed my problem:</p>\n<pre><code class=\"python\">brew upgrade postgresql\n</code></pre>\n", "abstract": "I had the same problem when I updated openssl from 1.0.0 to 1.1.1d, and this fixed my problem:"}, {"id": 63995009, "score": 3, "vote": 0, "content": "<p>So first for me <code>openssl</code> re-install never worked. It was quite irritating that all of the above answers failed for me. To be sure that it's a openssl issue, first, install <code>psycopg2-binary</code> using pip</p>\n<pre><code class=\"python\">  pip install psycopg2-binary\n</code></pre>\n<p>After installing <code>psycopg2-binary</code>, if you're getting error like <code>ld: library not found for -lssl </code> then do the following</p>\n<pre><code class=\"python\">  export LDFLAGS=\"-L/usr/local/opt/openssl/lib\"\n  export CPPFLAGS=\"-I/usr/local/opt/openssl/include\"\n</code></pre>\n<p>if these didn't work then you can try to upgrade <code>psycopg2</code> and re-check that issue still there or not.</p>\n<pre><code class=\"python\">  pip install psycopg2 --upgrade\n</code></pre>\n<p>if all the above didn't worked then only try reinstalling <code>openssl</code> as mentioned in all above answers.</p>\n", "abstract": "So first for me openssl re-install never worked. It was quite irritating that all of the above answers failed for me. To be sure that it's a openssl issue, first, install psycopg2-binary using pip After installing psycopg2-binary, if you're getting error like ld: library not found for -lssl  then do the following if these didn't work then you can try to upgrade psycopg2 and re-check that issue still there or not. if all the above didn't worked then only try reinstalling openssl as mentioned in all above answers."}, {"id": 35366074, "score": 2, "vote": 0, "content": "<p>I tried pip install psycopg2 which was giving similar issues.\nThen I tried conda install psycopg2, which worked!\nAlso make sure the pip you are using belongs to anaconda (which pip)</p>\n", "abstract": "I tried pip install psycopg2 which was giving similar issues.\nThen I tried conda install psycopg2, which worked!\nAlso make sure the pip you are using belongs to anaconda (which pip)"}, {"id": 57832157, "score": 1, "vote": 0, "content": "<p>I am using Mac OS Sierra:\nand got this error: </p>\n<pre><code class=\"python\"> Library not loaded: libssl.1.1.dylib\n</code></pre>\n<p>I found this library in </p>\n<pre><code class=\"python\"> /Library/PostgreSQL/11/lib\n</code></pre>\n<p>I found this solution in internet: </p>\n<pre><code class=\"python\"> export DYLD_LIBRARY_PATH=/Library/PostgreSQL/11/lib\n</code></pre>\n<p>But this is not a permanent solution. As I have to run the above command whenever I restart my server.</p>\n<p>Add the line: \nexport DYLD_LIBRARY_PATH=/Library/PostgreSQL//lib<br/>\nto \n~/.bash_profile</p>\n", "abstract": "I am using Mac OS Sierra:\nand got this error:  I found this library in  I found this solution in internet:  But this is not a permanent solution. As I have to run the above command whenever I restart my server. Add the line: \nexport DYLD_LIBRARY_PATH=/Library/PostgreSQL//lib\nto \n~/.bash_profile"}, {"id": 58176641, "score": 1, "vote": 0, "content": "<p>The solution that worked for me (<a href=\"https://stackoverflow.com/a/30726895\">https://stackoverflow.com/a/30726895</a> did not) was installing <code>psycopg2-binary</code> with: <code>pip install psycopg2-binary==2.7.6</code></p>\n<p>This is on MacOS Mohave. </p>\n", "abstract": "The solution that worked for me (https://stackoverflow.com/a/30726895 did not) was installing psycopg2-binary with: pip install psycopg2-binary==2.7.6 This is on MacOS Mohave. "}, {"id": 60845721, "score": 1, "vote": 0, "content": "<p>Simple solution: this pip command will install a package at a specific location:</p>\n<pre><code class=\"python\">pip install psycopg2-binary -t PATH\n</code></pre>\n<p>Where PATH is a path that you specify.</p>\n<p>To test this, install the package in a folder on your desktop.\nThen put a python script in the same folder that will import psycopg2. It should work with the script being in the same location as the psycopg2 package.</p>\n<p>Comments:</p>\n<p>This reason we need psycopg2-binary, according to old documentation that I found online: </p>\n<blockquote>\n<p>\"The binary packages come with their own versions of a few C libraries, among which libpq and libssl , which will be used regardless of other libraries available on the client: upgrading the system libraries will not upgrade the libraries used by psycopg2 . Please build psycopg2 from source if you want to maintain binary upgradeability.\"</p>\n</blockquote>\n<p>Source:\n<a href=\"https://access.crunchydata.com/documentation/psycopg2/2.7.3/install.html#install-from-source\" rel=\"nofollow noreferrer\">https://access.crunchydata.com/documentation/psycopg2/2.7.3/install.html#install-from-source</a></p>\n<p>I realized the issue has been that python's site package directory needs to be referenced. The location for this on my computer is:</p>\n<pre><code class=\"python\">/Users/my_name/Library/Python/3.7/lib/python/site-packages\n</code></pre>\n<p>Now, if you want to use IDLE or say PyCharm as I have been, the installation of psycopg2-binary needs to target this site directory. Additionally, you'll find two folders that appear after installing psycopg2-binary called: psycopg2, psycopg2_binary-2.8.4.dist-info</p>\n<p>I spent a long time investigating this issue. The other methods out there were not not resolving the issue as seen above regarding lib ssl and reason image not found. </p>\n<p>Setup: macOS Catalina, Python 3.7, PyCharm/IDLE project on Desktop, openssl@1.1/1.1.1d</p>\n", "abstract": "Simple solution: this pip command will install a package at a specific location: Where PATH is a path that you specify. To test this, install the package in a folder on your desktop.\nThen put a python script in the same folder that will import psycopg2. It should work with the script being in the same location as the psycopg2 package. Comments: This reason we need psycopg2-binary, according to old documentation that I found online:  \"The binary packages come with their own versions of a few C libraries, among which libpq and libssl , which will be used regardless of other libraries available on the client: upgrading the system libraries will not upgrade the libraries used by psycopg2 . Please build psycopg2 from source if you want to maintain binary upgradeability.\" Source:\nhttps://access.crunchydata.com/documentation/psycopg2/2.7.3/install.html#install-from-source I realized the issue has been that python's site package directory needs to be referenced. The location for this on my computer is: Now, if you want to use IDLE or say PyCharm as I have been, the installation of psycopg2-binary needs to target this site directory. Additionally, you'll find two folders that appear after installing psycopg2-binary called: psycopg2, psycopg2_binary-2.8.4.dist-info I spent a long time investigating this issue. The other methods out there were not not resolving the issue as seen above regarding lib ssl and reason image not found.  Setup: macOS Catalina, Python 3.7, PyCharm/IDLE project on Desktop, openssl@1.1/1.1.1d"}, {"id": 56722662, "score": 0, "vote": 0, "content": "<p>After trying for more than a day I came to the below solution.</p>\n<ul>\n<li>brew reinstall openssl@1.0</li>\n<li>disable csrutil -&gt; google it how to disable it, so that we could copy something<br/>\nto /usr/lib</li>\n<li>copy libssl.1.0.0.dylib to /usr/lib \n     I did- sudo cp \n           /usr/local/Cellar/openssl/1.0.2s/lib/libssl.1.0.0.dylib /usr/lib</li>\n<li>copy libcrypto.1.0.0.dylib to /usr/lib\n     I did- sudo cp \n            /usr/local/Cellar/openssl/1.0.2s/lib/libcrypto.1.0.0.dylib /usr/lib</li>\n</ul>\n<p>Similarly, if you face issue for <strong>Library not loaded: libssl.1.0.0.dylib</strong>\njust change the version from 1.0 to 1.1 of openssl and copy libssl.1.1 instead libssl.1.0 and libcrypto.1.1 instead libcrypto.1.0.0</p>\n<p>Done you are all set to enjoy psycopg2 in mac.</p>\n", "abstract": "After trying for more than a day I came to the below solution. Similarly, if you face issue for Library not loaded: libssl.1.0.0.dylib\njust change the version from 1.0 to 1.1 of openssl and copy libssl.1.1 instead libssl.1.0 and libcrypto.1.1 instead libcrypto.1.0.0 Done you are all set to enjoy psycopg2 in mac."}, {"id": 59435756, "score": 0, "vote": 0, "content": "<p>Homebrew upgrades default openssl from v1.0 to v1.1. If you tried @Scott solution to upgrade openssl:</p>\n<pre><code class=\"python\">brew install --upgrade openssl\nbrew unlink openssl &amp;&amp; brew link openssl --force\n</code></pre>\n<p>you may run into ssh problem. You need to upgrade openssh as well.</p>\n<pre><code class=\"python\">brew upgrade openssh\n</code></pre>\n<p>according to this blog: <a href=\"https://blog.junjizhi.com/all/2019/12/17/git-fetch-libssl-error.html\" rel=\"nofollow noreferrer\">https://blog.junjizhi.com/all/2019/12/17/git-fetch-libssl-error.html</a></p>\n", "abstract": "Homebrew upgrades default openssl from v1.0 to v1.1. If you tried @Scott solution to upgrade openssl: you may run into ssh problem. You need to upgrade openssh as well. according to this blog: https://blog.junjizhi.com/all/2019/12/17/git-fetch-libssl-error.html"}, {"id": 61526950, "score": 0, "vote": 0, "content": "<p>I encountered this problem after running <code>brew upgrade</code>, which updated openssl from 1.0.0 to 1.1.1. That causes the error in the question because I'm using pyscopg 2.7.4, which is pinned to <a href=\"https://www.psycopg.org/articles/2018/02/08/psycopg-274-released/\" rel=\"nofollow noreferrer\">openssl@1.0</a>. More recent versions of psycopg (&gt;2.8) are pinned to <a href=\"https://www.psycopg.org/articles/2019/04/14/psycopg-281-282-released/\" rel=\"nofollow noreferrer\">openssl@1.1</a>.</p>\n<p>So your options in this situation are to upgrade your psycopg version to 2.8.1 or later (in your Pipfile/Pipfile.lock, etc.), or force brew to keep using openssl@1.0 with this command, taken from <a href=\"https://github.com/Homebrew/homebrew-core/issues/47348#issuecomment-560001981\" rel=\"nofollow noreferrer\">https://github.com/Homebrew/homebrew-core/issues/47348#issuecomment-560001981</a>:</p>\n<pre><code class=\"python\">brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/64555220bfbf4a25598523c2e4d3a232560eaad7/Formula/openssl.rb -f\n</code></pre>\n", "abstract": "I encountered this problem after running brew upgrade, which updated openssl from 1.0.0 to 1.1.1. That causes the error in the question because I'm using pyscopg 2.7.4, which is pinned to openssl@1.0. More recent versions of psycopg (>2.8) are pinned to openssl@1.1. So your options in this situation are to upgrade your psycopg version to 2.8.1 or later (in your Pipfile/Pipfile.lock, etc.), or force brew to keep using openssl@1.0 with this command, taken from https://github.com/Homebrew/homebrew-core/issues/47348#issuecomment-560001981:"}, {"id": 71633052, "score": 0, "vote": 0, "content": "<p>Install pip install -i <a href=\"https://test.pypi.org/simple/\" rel=\"nofollow noreferrer\">https://test.pypi.org/simple/</a> psycopg2==2.7.6.1.dev1 worked for me</p>\n", "abstract": "Install pip install -i https://test.pypi.org/simple/ psycopg2==2.7.6.1.dev1 worked for me"}, {"id": 72315182, "score": 0, "vote": 0, "content": "<p>I had the problem that was originally posted in this thread. I traced it back to a problem with <code>libpq</code>. For me, the solution was to run</p>\n<p><code>brew install postgresql</code>,</p>\n<p>after which <code>psycopg2</code> worked like a breeze.</p>\n", "abstract": "I had the problem that was originally posted in this thread. I traced it back to a problem with libpq. For me, the solution was to run brew install postgresql, after which psycopg2 worked like a breeze."}]}, {"link": "https://stackoverflow.com/questions/7670289/sqlite3-operationalerror-unable-to-open-database-file", "question": {"id": "7670289", "title": "sqlite3.OperationalError: unable to open database file", "content": "<p>I get this error when setting up a server in Django. It is sqlite3 which means it should create the .db file but it doesn't seem to be doing so. I've stipulated SQLite as the backend and an absolute file path for where to put it, but no luck.</p>\n<p>Is this a bug or am I doing something incorrect? (Was just thinking, is the absolute file path specified differently in Ubuntu?)</p>\n<p>Here is the beginning of my settings.py file:</p>\n<pre><code class=\"python\"># Django settings for OmniCloud project.\n\nDEBUG = True\nTEMPLATE_DEBUG = DEBUG\n\nADMINS = (\n# ('Your Name', 'your_email@example.com'),\n)\n\nMANAGERS = ADMINS\n\nDATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'oracle'.\n    'NAME': '~/Harold-Server/OmniCloud.db',                      # Or path to database file if using sqlite3.\n    'USER': '',                      # Not used with sqlite3.\n    'PASSWORD': '',                  # Not used with sqlite3.\n    'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.\n    'PORT': '',                      # Set to empty string for default. Not used with sqlite3.\n}\n}\n</code></pre>\n", "abstract": "I get this error when setting up a server in Django. It is sqlite3 which means it should create the .db file but it doesn't seem to be doing so. I've stipulated SQLite as the backend and an absolute file path for where to put it, but no luck. Is this a bug or am I doing something incorrect? (Was just thinking, is the absolute file path specified differently in Ubuntu?) Here is the beginning of my settings.py file:"}, "answers": [{"id": 7670618, "score": 86, "vote": 0, "content": "<p><a href=\"https://code.djangoproject.com/wiki/NewbieMistakes#DjangosaysUnabletoOpenDatabaseFilewhenusingSQLite3\" rel=\"noreferrer\">Django NewbieMistakes</a></p>\n<blockquote>\n<p>PROBLEM  You're using SQLite3, your DATABASE_NAME is set to the\n  database file's full path, the database file is writeable by Apache,\n  but you still get the above error.</p>\n<p>SOLUTION Make sure Apache can also write to the parent directory of\n  the database. SQLite needs to be able to write to this directory.</p>\n<p>Make sure each folder of your database file's full path does not start\n  with number, eg. /www/4myweb/db (observed on Windows 2000).</p>\n<p>If DATABASE_NAME is set to something like\n  '/Users/yourname/Sites/mydjangoproject/db/db', make sure you've\n  created the 'db' directory first.</p>\n<p>Make sure your /tmp directory is world-writable (an unlikely cause as\n  other thing on your system will also not work). ls /tmp -ald should\n  produce drwxrwxrwt ....</p>\n<p>Make sure the path to the database specified in settings.py is a full\n  path.</p>\n<p>Also make sure the file is present where you expect it to be.</p>\n</blockquote>\n", "abstract": "Django NewbieMistakes PROBLEM  You're using SQLite3, your DATABASE_NAME is set to the\n  database file's full path, the database file is writeable by Apache,\n  but you still get the above error. SOLUTION Make sure Apache can also write to the parent directory of\n  the database. SQLite needs to be able to write to this directory. Make sure each folder of your database file's full path does not start\n  with number, eg. /www/4myweb/db (observed on Windows 2000). If DATABASE_NAME is set to something like\n  '/Users/yourname/Sites/mydjangoproject/db/db', make sure you've\n  created the 'db' directory first. Make sure your /tmp directory is world-writable (an unlikely cause as\n  other thing on your system will also not work). ls /tmp -ald should\n  produce drwxrwxrwt .... Make sure the path to the database specified in settings.py is a full\n  path. Also make sure the file is present where you expect it to be."}, {"id": 11242817, "score": 21, "vote": 0, "content": "<p>I faced exactly same issue. Here is my setting which worked.</p>\n<pre><code class=\"python\">'ENGINE': 'django.db.backends.sqlite3', \n'NAME': '/home/path/to/your/db/data.sqlite3'\n</code></pre>\n<p>Other setting in case of sqlite3 will be same/default.<br/>\nAnd you need to create data.sqlite3. </p>\n", "abstract": "I faced exactly same issue. Here is my setting which worked. Other setting in case of sqlite3 will be same/default.\nAnd you need to create data.sqlite3. "}, {"id": 7671689, "score": 11, "vote": 0, "content": "<p>You haven't specified the absolute path - you've used a shortcut , <code>~</code>, which might not work in this context. Use <code>/home/yourusername/Harold-Server/OmniCloud.db</code> instead.</p>\n", "abstract": "You haven't specified the absolute path - you've used a shortcut , ~, which might not work in this context. Use /home/yourusername/Harold-Server/OmniCloud.db instead."}, {"id": 18137815, "score": 7, "vote": 0, "content": "<p>You need to use full path instead of <code>~/</code>.</p>\n<p>In your case, something like <code>/home/harold/Harold-Server/OmniCloud.db</code>.</p>\n", "abstract": "You need to use full path instead of ~/. In your case, something like /home/harold/Harold-Server/OmniCloud.db."}, {"id": 29882159, "score": 6, "vote": 0, "content": "<p>In my case the sqlite db file <code>db.sqlite3</code> was stored in the <code>DocumentRoot</code> of apache. So, even after setting the following permissions it didn't work:</p>\n<pre><code class=\"python\">sudo chown www-data:www-data /path/to/db-folder\nsudo chown www-data:www-data /path/to/db-folder/sqlite-db.db\n</code></pre>\n<p>Finally when i moved <code>db.sqlite3</code> to a newly created folder <code>dbfolder</code> under <code>DocumentRoot</code> and gave the above permissions, and it worked.</p>\n", "abstract": "In my case the sqlite db file db.sqlite3 was stored in the DocumentRoot of apache. So, even after setting the following permissions it didn't work: Finally when i moved db.sqlite3 to a newly created folder dbfolder under DocumentRoot and gave the above permissions, and it worked."}, {"id": 65453583, "score": 2, "vote": 0, "content": "<p>I had this problem serving with Apache and found that using the absolute path to the sqlite3 db in my .env <code>////</code> as opposed to using the relative path <code>///</code> fixed the problem. All of the permissions and ownership mentioned above are necessary as well.</p>\n", "abstract": "I had this problem serving with Apache and found that using the absolute path to the sqlite3 db in my .env //// as opposed to using the relative path /// fixed the problem. All of the permissions and ownership mentioned above are necessary as well."}, {"id": 16826537, "score": 1, "vote": 0, "content": "<p>use this type it works for me .\n windows 7 with python 2.7 and django 1.5</p>\n<pre><code class=\"python\">'ENGINE': 'django.db.backends.sqlite3',\n'NAME': 'C:\\\\tool\\\\mysite\\\\data.db',\n</code></pre>\n<p>hope its works...</p>\n", "abstract": "use this type it works for me .\n windows 7 with python 2.7 and django 1.5 hope its works..."}]}, {"link": "https://stackoverflow.com/questions/9575310/how-can-i-iterate-over-manytomanyfield", "question": {"id": "9575310", "title": "How can I iterate over ManyToManyField?", "content": "<p>A simple question and yet I can't find an answer for it.</p>\n<p>I have a model with a ManyToMany field:</p>\n<pre><code class=\"python\">class Stuff(models.Model):\n  things = models.ManyToManyField(Thing)\n</code></pre>\n<p>then in a different function I want to do this:</p>\n<pre><code class=\"python\">myStuff = Stuff.objects.get(id=1)\nfor t in myStuff.things.all:\n  # ...\n</code></pre>\n<p>But that is giving me:</p>\n<pre><code class=\"python\">TypeError: 'instancemethod' object is not iterable\n</code></pre>\n<p>How can I iterate over a manyToManyField ?</p>\n", "abstract": "A simple question and yet I can't find an answer for it. I have a model with a ManyToMany field: then in a different function I want to do this: But that is giving me: How can I iterate over a manyToManyField ?"}, "answers": [{"id": 9575414, "score": 112, "vote": 0, "content": "<p>Try adding the <code>()</code> after <code>all</code>: <code>myStuff.things.all()</code></p>\n", "abstract": "Try adding the () after all: myStuff.things.all()"}, {"id": 9575595, "score": 2, "vote": 0, "content": "<p>Like Abid A already answered, you are missing brackets ()</p>\n<pre><code class=\"python\">for t in myStuff.things.all():\n    print t.myStuffs.all()\n</code></pre>\n", "abstract": "Like Abid A already answered, you are missing brackets ()"}, {"id": 39425582, "score": 0, "vote": 0, "content": "<p>ManyToManyField seems to have a different kind of Manager than your basic Django class.  Looking at the source here, <a href=\"https://github.com/django/django/blob/master/django/db/models/fields/related_descriptors.py#L821\" rel=\"nofollow\">https://github.com/django/django/blob/master/django/db/models/fields/related_descriptors.py#L821</a>, it seems you are looking for the related_val field which appears to contain the tuple of related objects references.</p>\n", "abstract": "ManyToManyField seems to have a different kind of Manager than your basic Django class.  Looking at the source here, https://github.com/django/django/blob/master/django/db/models/fields/related_descriptors.py#L821, it seems you are looking for the related_val field which appears to contain the tuple of related objects references."}]}, {"link": "https://stackoverflow.com/questions/12113050/how-to-automatically-destroy-django-test-database", "question": {"id": "12113050", "title": "How to automatically destroy django test database", "content": "<p>I'm currently trying to automate Django tests using Hudson CI, and am struggling to find an option that will automatically destroy the test database if it already exists (typically it will ask for confirmation to destroy it, which the automatic testing obviously cannot provide for).</p>\n<p>Any suggestions would be much appreciated!</p>\n<p>Cheers,\nR</p>\n", "abstract": "I'm currently trying to automate Django tests using Hudson CI, and am struggling to find an option that will automatically destroy the test database if it already exists (typically it will ask for confirmation to destroy it, which the automatic testing obviously cannot provide for). Any suggestions would be much appreciated! Cheers,\nR"}, "answers": [{"id": 12113633, "score": 102, "vote": 0, "content": "<p>Use --help to see the docs of the test command:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; ./manage.py test --help   \nUsage: ./manage.py test [options] [appname ...]\n\nRuns the test suite for the specified applications, or the entire site if no apps are specified.\n\n[...]\n\n--noinput             Tells Django to NOT prompt the user for input of any\n                      kind.\n</code></pre>\n<p>And use --noinput which defaults to destroying the test db;)</p>\n", "abstract": "Use --help to see the docs of the test command: And use --noinput which defaults to destroying the test db;)"}]}, {"link": "https://stackoverflow.com/questions/1303654/threaded-django-task-doesnt-automatically-handle-transactions-or-db-connections", "question": {"id": "1303654", "title": "Threaded Django task doesn&#39;t automatically handle transactions or db connections?", "content": "<p>I've got Django set up to run some recurring tasks in their own threads, and I noticed that they were always leaving behind unfinished database connection processes (pgsql \"Idle In Transaction\").</p>\n<p>I looked through the Postgres logs and found that the transactions weren't being completed (no ROLLBACK). I tried using the various transaction decorators on my functions, no luck.</p>\n<p>I switched to manual transaction management and did the rollback manually, that worked, but still left the processes as \"Idle\".</p>\n<p>So then I called connection.close(), and all is well.</p>\n<p>But I'm left wondering, why doesn't Django's typical transaction and connection management work for these threaded tasks that are being spawned from the main Django thread?</p>\n", "abstract": "I've got Django set up to run some recurring tasks in their own threads, and I noticed that they were always leaving behind unfinished database connection processes (pgsql \"Idle In Transaction\"). I looked through the Postgres logs and found that the transactions weren't being completed (no ROLLBACK). I tried using the various transaction decorators on my functions, no luck. I switched to manual transaction management and did the rollback manually, that worked, but still left the processes as \"Idle\". So then I called connection.close(), and all is well. But I'm left wondering, why doesn't Django's typical transaction and connection management work for these threaded tasks that are being spawned from the main Django thread?"}, "answers": [{"id": 1346401, "score": 111, "vote": 0, "content": "<p>After weeks of testing and reading the Django source code, I've found the answer to my own question:</p>\n<p><strong>Transactions</strong></p>\n<p>Django's default autocommit behavior still holds true for my threaded function. However, it states in the Django docs:</p>\n<blockquote>\n<p>As soon as you perform an action that needs to write to the database, Django produces the INSERT/UPDATE/DELETE statements and then does the COMMIT. There\u2019s no implicit ROLLBACK.</p>\n</blockquote>\n<p>That last sentence is very literal. It DOES NOT issue a ROLLBACK command unless something in Django has set the dirty flag. Since my function was only doing SELECT statements it never set the dirty flag and didn't trigger a COMMIT.</p>\n<p>This goes against the fact that PostgreSQL thinks the transaction requires a ROLLBACK because Django issued a SET command for the timezone. In reviewing the logs, I threw myself off because I kept seeing these ROLLBACK statements and assumed Django's transaction management was the source. Turns out it's not, and that's OK.</p>\n<p><strong>Connections</strong></p>\n<p>The connection management is where things do get tricky. It turns out Django uses <code>signals.request_finished.connect(close_connection)</code> to close the database connection it normally uses. Since nothing normally happens in Django that doesn't involve a request, you take this behavior for granted.</p>\n<p>In my case, though, there was no request because the job was scheduled. No request means no signal. No signal means the database connection was never closed.</p>\n<p>Going back to transactions, it turns out that simply issuing a call to <code>connection.close()</code> in the absence of any changes to the transaction management issues the ROLLBACK statement in the PostgreSQL log that I'd been looking for.</p>\n<p><strong>Solution</strong></p>\n<p>The solution is to allow the normal Django transaction management to proceed as normal and to simply close the connection one of three ways:</p>\n<ol>\n<li>Write a decorator that closes the connection and wrap the necessary functions in it.</li>\n<li>Hook into the existing request signals to have Django close the connection.</li>\n<li>Close the connection manually at the end of the function.</li>\n</ol>\n<p>Any of those three will (and do) work.</p>\n<p>This has driven me crazy for weeks. I hope this helps someone else in the future!</p>\n", "abstract": "After weeks of testing and reading the Django source code, I've found the answer to my own question: Transactions Django's default autocommit behavior still holds true for my threaded function. However, it states in the Django docs: As soon as you perform an action that needs to write to the database, Django produces the INSERT/UPDATE/DELETE statements and then does the COMMIT. There\u2019s no implicit ROLLBACK. That last sentence is very literal. It DOES NOT issue a ROLLBACK command unless something in Django has set the dirty flag. Since my function was only doing SELECT statements it never set the dirty flag and didn't trigger a COMMIT. This goes against the fact that PostgreSQL thinks the transaction requires a ROLLBACK because Django issued a SET command for the timezone. In reviewing the logs, I threw myself off because I kept seeing these ROLLBACK statements and assumed Django's transaction management was the source. Turns out it's not, and that's OK. Connections The connection management is where things do get tricky. It turns out Django uses signals.request_finished.connect(close_connection) to close the database connection it normally uses. Since nothing normally happens in Django that doesn't involve a request, you take this behavior for granted. In my case, though, there was no request because the job was scheduled. No request means no signal. No signal means the database connection was never closed. Going back to transactions, it turns out that simply issuing a call to connection.close() in the absence of any changes to the transaction management issues the ROLLBACK statement in the PostgreSQL log that I'd been looking for. Solution The solution is to allow the normal Django transaction management to proceed as normal and to simply close the connection one of three ways: Any of those three will (and do) work. This has driven me crazy for weeks. I hope this helps someone else in the future!"}]}, {"link": "https://stackoverflow.com/questions/1125504/django-persistent-database-connection", "question": {"id": "1125504", "title": "Django persistent database connection", "content": "<p>I'm using django with apache and mod_wsgi and PostgreSQL (all on same host), and I need to handle a lot of simple dynamic page requests (hundreds per second). I faced with problem that the bottleneck is that a django don't have persistent database connection and reconnects on each requests (that takes near 5ms).\nWhile doing a benchmark I got that with persistent connection I can handle near 500 r/s while without I get only 50 r/s.</p>\n<p>Anyone have any advice? How can I modify Django to use a persistent connection or speed up the connection from Python to DB?</p>\n", "abstract": "I'm using django with apache and mod_wsgi and PostgreSQL (all on same host), and I need to handle a lot of simple dynamic page requests (hundreds per second). I faced with problem that the bottleneck is that a django don't have persistent database connection and reconnects on each requests (that takes near 5ms).\nWhile doing a benchmark I got that with persistent connection I can handle near 500 r/s while without I get only 50 r/s. Anyone have any advice? How can I modify Django to use a persistent connection or speed up the connection from Python to DB?"}, "answers": [{"id": 19438241, "score": 35, "vote": 0, "content": "<p>Django <strong>1.6</strong> has added <a href=\"https://docs.djangoproject.com/en/stable/ref/databases/#persistent-connections\" rel=\"nofollow noreferrer\">persistent connections support (link to doc for latest stable Django )</a>:</p>\n<blockquote>\n<p>Persistent connections avoid the overhead of re-establishing a\nconnection to the database in each request. They\u2019re controlled by the\nCONN_MAX_AGE parameter which defines the maximum lifetime of a\nconnection. It can be set independently for each database.</p>\n</blockquote>\n", "abstract": "Django 1.6 has added persistent connections support (link to doc for latest stable Django ): Persistent connections avoid the overhead of re-establishing a\nconnection to the database in each request. They\u2019re controlled by the\nCONN_MAX_AGE parameter which defines the maximum lifetime of a\nconnection. It can be set independently for each database."}, {"id": 1698102, "score": 27, "vote": 0, "content": "<p>Try <a href=\"http://wiki.postgresql.org/wiki/PgBouncer\" rel=\"noreferrer\">PgBouncer</a> - a lightweight connection pooler for PostgreSQL.\nFeatures:</p>\n<ul>\n<li>Several levels of brutality when rotating connections:\n\n<ul>\n<li>Session pooling</li>\n<li>Transaction pooling</li>\n<li>Statement pooling</li>\n</ul></li>\n<li>Low memory requirements (2k per connection by default).</li>\n</ul>\n", "abstract": "Try PgBouncer - a lightweight connection pooler for PostgreSQL.\nFeatures:"}, {"id": 1175140, "score": 20, "vote": 0, "content": "<p>In Django trunk, edit <code>django/db/__init__.py</code> and comment out the line:</p>\n<pre><code class=\"python\">signals.request_finished.connect(close_connection)\n</code></pre>\n<p>This signal handler causes it to disconnect from the database after every request.  I don't know what all of the side-effects of doing this will be, but it doesn't make any sense to start a new connection after every request; it destroys performance, as you've noticed.</p>\n<p>I'm using this now, but I havn't done a full set of tests to see if anything breaks.</p>\n<p>I don't know why everyone thinks this needs a new backend or a special connection pooler or other complex solutions.  This seems very simple, though I don't doubt there are some obscure gotchas that made them do this in the first place--which should be dealt with more sensibly; 5ms overhead for every request is quite a lot for a high-performance service, as you've noticed.  (It takes me <em>150ms</em>--I havn't figured out why yet.)</p>\n<p>Edit: another necessary change is in django/middleware/transaction.py; remove the two transaction.is_dirty() tests and always call commit() or rollback().  Otherwise, it won't commit a transaction if it only read from the database, which will leave locks open that should be closed.</p>\n", "abstract": "In Django trunk, edit django/db/__init__.py and comment out the line: This signal handler causes it to disconnect from the database after every request.  I don't know what all of the side-effects of doing this will be, but it doesn't make any sense to start a new connection after every request; it destroys performance, as you've noticed. I'm using this now, but I havn't done a full set of tests to see if anything breaks. I don't know why everyone thinks this needs a new backend or a special connection pooler or other complex solutions.  This seems very simple, though I don't doubt there are some obscure gotchas that made them do this in the first place--which should be dealt with more sensibly; 5ms overhead for every request is quite a lot for a high-performance service, as you've noticed.  (It takes me 150ms--I havn't figured out why yet.) Edit: another necessary change is in django/middleware/transaction.py; remove the two transaction.is_dirty() tests and always call commit() or rollback().  Otherwise, it won't commit a transaction if it only read from the database, which will leave locks open that should be closed."}, {"id": 6712190, "score": 16, "vote": 0, "content": "<p>I created a small <a href=\"http://dumpz.org/67550/\" rel=\"noreferrer\">Django patch</a> that implements connection pooling of MySQL and PostgreSQL via sqlalchemy pooling.</p>\n<p>This works perfectly on production of <a href=\"http://grandcapital.net/\" rel=\"noreferrer\">http://grandcapital.net/</a> for a long period of time.</p>\n<p>The patch was written after googling the topic a bit.</p>\n", "abstract": "I created a small Django patch that implements connection pooling of MySQL and PostgreSQL via sqlalchemy pooling. This works perfectly on production of http://grandcapital.net/ for a long period of time. The patch was written after googling the topic a bit."}, {"id": 1125613, "score": 4, "vote": 0, "content": "<p>Disclaimer: I have not tried this.</p>\n<p>I believe you need to implement a custom database back end. There are a few examples on the web that shows how to implement a database back end with connection pooling.</p>\n<p>Using a connection pool would probably be a good solution for you case, as the network connections are kept open when connections are returned to the pool.</p>\n<ul>\n<li><a href=\"http://jasonrubenstein.blogspot.com/2008/02/quick-and-dirty-database-pooling-in.html\" rel=\"nofollow noreferrer\">This post</a> accomplishes this by patching Django (one of the comments points out that it is better to implement a custom back end outside of the core django code)</li>\n<li><a href=\"http://node.to/wordpress/2008/09/30/another-database-connection-pool-solution-for-django-mysql/\" rel=\"nofollow noreferrer\">This post</a> is an implementation of a custom db back end</li>\n</ul>\n<p>Both posts use MySQL - perhaps you are able to use similar techniques with Postgresql.</p>\n<p><strong>Edit:</strong></p>\n<ul>\n<li>The Django Book mentions Postgresql connection pooling, using <a href=\"http://pgpool.projects.postgresql.org/\" rel=\"nofollow noreferrer\">pgpool</a> (<a href=\"http://pgpool.projects.postgresql.org/pgpool-II/doc/tutorial-en.html\" rel=\"nofollow noreferrer\">tutorial</a>).</li>\n<li>Someone posted <a href=\"http://www.nabble.com/-PATCH--Proposal:-connection-pooling-with-psycopg2-td20532406.html\" rel=\"nofollow noreferrer\">a patch</a> for the psycopg2 backend that implements connection pooling. I suggest creating a copy of the existing back end in your own project and patching that one.</li>\n</ul>\n", "abstract": "Disclaimer: I have not tried this. I believe you need to implement a custom database back end. There are a few examples on the web that shows how to implement a database back end with connection pooling. Using a connection pool would probably be a good solution for you case, as the network connections are kept open when connections are returned to the pool. Both posts use MySQL - perhaps you are able to use similar techniques with Postgresql. Edit:"}, {"id": 67152292, "score": 1, "vote": 0, "content": "<p>This is a package for django connection pool:\n<a href=\"https://pypi.org/project/django-db-connection-pool/\" rel=\"nofollow noreferrer\">django-db-connection-pool</a></p>\n<pre><code class=\"python\">pip install django-db-connection-pool\n</code></pre>\n<p>You can provide additional options to pass to SQLAlchemy's pool creation, key's name is POOL_OPTIONS:</p>\n<pre><code class=\"python\">DATABASES = {\n    'default': {\n        ...\n        'POOL_OPTIONS' : {\n            'POOL_SIZE': 10,\n            'MAX_OVERFLOW': 10\n        }\n        ...\n     }\n }\n</code></pre>\n", "abstract": "This is a package for django connection pool:\ndjango-db-connection-pool You can provide additional options to pass to SQLAlchemy's pool creation, key's name is POOL_OPTIONS:"}, {"id": 1351213, "score": 0, "vote": 0, "content": "<p>I made some small custom psycopg2 backend that implements persistent connection using global variable.\nWith this I was able to improve the amout of requests per second from 350 to 1600 (on very simple page with few selects)\nJust save it in the file  called <code>base.py</code> in any directory (e.g. postgresql_psycopg2_persistent) and set in settings</p>\n<p>DATABASE_ENGINE to projectname.postgresql_psycopg2_persistent</p>\n<p><strong>NOTE!!! the code is not threadsafe - you can't use it with python threads because of unexpectable results, in case of mod_wsgi please use prefork daemon mode with threads=1</strong></p>\n<hr/>\n<pre><code class=\"python\"># Custom DB backend postgresql_psycopg2 based\n# implements persistent database connection using global variable\n\nfrom django.db.backends.postgresql_psycopg2.base import DatabaseError, DatabaseWrapper as BaseDatabaseWrapper, \\\n    IntegrityError\nfrom psycopg2 import OperationalError\n\nconnection = None\n\nclass DatabaseWrapper(BaseDatabaseWrapper):\n    def _cursor(self, *args, **kwargs):\n        global connection\n        if connection is not None and self.connection is None:\n            try: # Check if connection is alive\n                connection.cursor().execute('SELECT 1')\n            except OperationalError: # The connection is not working, need reconnect\n                connection = None\n            else:\n                self.connection = connection\n        cursor = super(DatabaseWrapper, self)._cursor(*args, **kwargs)\n        if connection is None and self.connection is not None:\n            connection = self.connection\n        return cursor\n\n    def close(self):\n        if self.connection is not None:\n            self.connection.commit()\n            self.connection = None\n</code></pre>\n<hr/>\n<p>Or here is a thread safe one, but python threads don't use multiple cores, so you won't get such performance boost as with previous one. You can use this one with multi process one too.</p>\n<pre><code class=\"python\"># Custom DB backend postgresql_psycopg2 based\n# implements persistent database connection using thread local storage\nfrom threading import local\n\nfrom django.db.backends.postgresql_psycopg2.base import DatabaseError, \\\n    DatabaseWrapper as BaseDatabaseWrapper, IntegrityError\nfrom psycopg2 import OperationalError\n\nthreadlocal = local()\n\nclass DatabaseWrapper(BaseDatabaseWrapper):\n    def _cursor(self, *args, **kwargs):\n        if hasattr(threadlocal, 'connection') and threadlocal.connection is \\\n            not None and self.connection is None:\n            try: # Check if connection is alive\n                threadlocal.connection.cursor().execute('SELECT 1')\n            except OperationalError: # The connection is not working, need reconnect\n                threadlocal.connection = None\n            else:\n                self.connection = threadlocal.connection\n        cursor = super(DatabaseWrapper, self)._cursor(*args, **kwargs)\n        if (not hasattr(threadlocal, 'connection') or threadlocal.connection \\\n             is None) and self.connection is not None:\n            threadlocal.connection = self.connection\n        return cursor\n\n    def close(self):\n        if self.connection is not None:\n            self.connection.commit()\n            self.connection = None\n</code></pre>\n", "abstract": "I made some small custom psycopg2 backend that implements persistent connection using global variable.\nWith this I was able to improve the amout of requests per second from 350 to 1600 (on very simple page with few selects)\nJust save it in the file  called base.py in any directory (e.g. postgresql_psycopg2_persistent) and set in settings DATABASE_ENGINE to projectname.postgresql_psycopg2_persistent NOTE!!! the code is not threadsafe - you can't use it with python threads because of unexpectable results, in case of mod_wsgi please use prefork daemon mode with threads=1 Or here is a thread safe one, but python threads don't use multiple cores, so you won't get such performance boost as with previous one. You can use this one with multi process one too."}]}, {"link": "https://stackoverflow.com/questions/7465796/django-set-datetimefield-to-database-servers-current-time", "question": {"id": "7465796", "title": "django set DateTimeField to database server&#39;s current time", "content": "<p>How do I do the equivalent of this SQL in django?</p>\n<pre><code class=\"python\">UPDATE table SET timestamp=NOW() WHERE ...\n</code></pre>\n<p>Particularly I want to set the datetime field using server's builtin function to get the system time from the server that the database was running on and not the time on the client machine. </p>\n<p>I know you can execute the raw sql directly but I'm looking for a more portable solution since databases have different functions for getting the current datetime. </p>\n<p>Edit: few people mentioned auto_now param. This updates the datetime on every modification while I want to update datetime only on certain occasions. </p>\n", "abstract": "How do I do the equivalent of this SQL in django? Particularly I want to set the datetime field using server's builtin function to get the system time from the server that the database was running on and not the time on the client machine.  I know you can execute the raw sql directly but I'm looking for a more portable solution since databases have different functions for getting the current datetime.  Edit: few people mentioned auto_now param. This updates the datetime on every modification while I want to update datetime only on certain occasions. "}, "answers": [{"id": 7466038, "score": 69, "vote": 0, "content": "<p>As j0ker said, if you want automatic update of the timestamp, use the <code>auto_now</code> option. E.g. <code>date_modified = models.DateTimeField(auto_now=True)</code>.</p>\n<p>If you want to set the field to now only when the object is first created you should use:</p>\n<pre><code class=\"python\">date_modified = models.DateTimeField(auto_now_add=True)\n</code></pre>\n<p>Or if you want to do it manually, isn't it a simple assignment with python <code>datetime.now()</code>?</p>\n<pre><code class=\"python\">from datetime import datetime\n\nobj.date_modified = datetime.now()\n</code></pre>\n", "abstract": "As j0ker said, if you want automatic update of the timestamp, use the auto_now option. E.g. date_modified = models.DateTimeField(auto_now=True). If you want to set the field to now only when the object is first created you should use: Or if you want to do it manually, isn't it a simple assignment with python datetime.now()?"}, {"id": 41988720, "score": 49, "vote": 0, "content": "<p>The accepted answer is outdated. Here's the current and most simple way of doing so:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from django.utils import timezone\n&gt;&gt;&gt; timezone.now()\ndatetime.datetime(2018, 12, 3, 14, 57, 11, 703055, tzinfo=&lt;UTC&gt;)\n</code></pre>\n", "abstract": "The accepted answer is outdated. Here's the current and most simple way of doing so:"}, {"id": 48496745, "score": 29, "vote": 0, "content": "<p>You can use database function <a href=\"https://docs.djangoproject.com/en/dev/ref/models/database-functions/#now\" rel=\"noreferrer\">Now</a> starting Django 1.9:</p>\n<pre><code class=\"python\">from django.db.models.functions import Now\nModel.objects.filter(...).update(timestamp=Now())\n</code></pre>\n", "abstract": "You can use database function Now starting Django 1.9:"}, {"id": 18157447, "score": 8, "vote": 0, "content": "<p>Here is how I solved this issue. Hope it saves someone time:</p>\n<pre><code class=\"python\">from django.db import models\n\nclass DBNow(object):\n    def __str__(self):\n        return 'DATABASE NOW()'\n    def as_sql(self, qn, val):\n        return 'NOW()', {}\n    @classmethod\n    def patch(cls, field):\n        orig_prep_db = field.get_db_prep_value\n        orig_prep_lookup = field.get_prep_lookup\n        orig_db_prep_lookup = field.get_db_prep_lookup\n\n        def prep_db_value(self, value, connection, prepared=False):\n            return value if isinstance(value, cls) else orig_prep_db(self, value, connection, prepared)\n\n        def prep_lookup(self, lookup_type, value):\n            return value if isinstance(value, cls) else orig_prep_lookup(self, lookup_type, value)\n\n        def prep_db_lookup(self, lookup_type, value, connection, prepared=True):\n            return value if isinstance(value, cls) else orig_db_prep_lookup(self, lookup_type, value, connection=connection, prepared=True)\n\n        field.get_db_prep_value = prep_db_value\n        field.get_prep_lookup = prep_lookup\n        field.get_db_prep_lookup = prep_db_lookup\n\n# DBNow Activator\nDBNow.patch(models.DateTimeField)\n</code></pre>\n<p>And then just using the DBNow() as a value where updating and filtering is needed:</p>\n<pre><code class=\"python\">books = Book.objects.filter(created_on__gt=DBNow())\n\n    or:\n\nbook.created_on = DBNow()\nbook.save()\n</code></pre>\n", "abstract": "Here is how I solved this issue. Hope it saves someone time: And then just using the DBNow() as a value where updating and filtering is needed:"}, {"id": 10219860, "score": 5, "vote": 0, "content": "<p>You can use something like this to create a custom value to represent the use of the current time on the database:</p>\n<pre><code class=\"python\">class DatabaseDependentValue(object):\n    def setEngine(self, engine):\n        self.engine = engine\n\n    @staticmethod\n    def Converter(value, *args, **kwargs):\n        return str(value)\n\nclass DatabaseNow(DatabaseDependentValue):\n    def __str__(self):\n        if self.engine == 'django.db.backends.mysql':\n            return 'NOW()'\n        elif self.engine == 'django.db.backends.postgresql':\n            return 'current_timestamp'\n        else:\n            raise Exception('Unimplemented for engine ' + self.engine)\n\ndjango_conversions.update({DatabaseNow: DatabaseDependentValue.Converter})\n\ndef databaseDependentPatch(cls):\n    originalGetDbPrepValue = cls.get_db_prep_value\n    def patchedGetDbPrepValue(self, value, connection, prepared=False):\n        if isinstance(value, DatabaseDependentValue):\n            value.setEngine(connection.settings_dict['ENGINE'])\n            return value\n        return originalGetDbPrepValue(self, value, connection, prepared)\n    cls.get_db_prep_value = patchedGetDbPrepValue\n</code></pre>\n<p>And then to be able to use DatabaseNow on a DateTimeField:</p>\n<pre><code class=\"python\">databaseDependentPatch(models.DateTimeField)\n</code></pre>\n<p>Which then in turn finally allows you do a nice and clean:</p>\n<pre><code class=\"python\">class Operation(models.Model):\n    dateTimeCompleted = models.DateTimeField(null=True)\n    # ...\n\noperation = # Some previous operation\noperation.dateTimeCompleted = DatabaseNow()\noperation.save()\n</code></pre>\n", "abstract": "You can use something like this to create a custom value to represent the use of the current time on the database: And then to be able to use DatabaseNow on a DateTimeField: Which then in turn finally allows you do a nice and clean:"}, {"id": 27772324, "score": 4, "vote": 0, "content": "<p>My tweaked code works with sqlite, mysql and postgresql and is a bit cleaner than the proposed solutions.</p>\n<pre><code class=\"python\">class DBCurrentTimestamp:\n    def __str__(self):\n        return 'DATABASE CURRENT_TIMESTAMP()'\n\n    def as_sql(self, qn, connection):\n        return 'CURRENT_TIMESTAMP', {}\n\n    @classmethod\n    def patch(cls, *args):\n        def create_tweaked_get_db_prep_value(orig_get_db_prep_value):\n            def get_db_prep_value(self, value, connection, prepared=False):\n                return value if isinstance(value, cls) else orig_get_db_prep_value(self, value, connection, prepared)\n\n            return get_db_prep_value\n\n        for field_class in args:\n            field_class.get_db_prep_value = create_tweaked_get_db_prep_value(field_class.get_db_prep_value)\n</code></pre>\n<p>I activate it @ the end of my models.py file like this:</p>\n<pre><code class=\"python\">DBCurrentTimestamp.patch(models.DateField, models.TimeField, models.DateTimeField)\n</code></pre>\n<p>and use it like this:</p>\n<pre><code class=\"python\">self.last_pageview = DBCurrentTimestamp()\n</code></pre>\n", "abstract": "My tweaked code works with sqlite, mysql and postgresql and is a bit cleaner than the proposed solutions. I activate it @ the end of my models.py file like this: and use it like this:"}, {"id": 25754684, "score": 3, "vote": 0, "content": "<p>I've created a Python Django plugin module which allows you to control the use of <code>CURRENT_TIMESTAMP</code> on <code>DateTimeField</code> objects, both in specific cases (see <code>usage</code> below) as well as automatically for <code>auto_now</code> and <code>auto_now_add</code> columns.</p>\n<p><strong>django-pg-current-timestamp</strong></p>\n<p>GitHub: <a href=\"https://github.com/jaytaylor/django-pg-current-timestamp\" rel=\"nofollow\">https://github.com/jaytaylor/django-pg-current-timestamp</a></p>\n<p>PyPi: <a href=\"https://pypi.python.org/pypi/django-pg-current-timestamp\" rel=\"nofollow\">https://pypi.python.org/pypi/django-pg-current-timestamp</a></p>\n<p>Example usage:</p>\n<pre><code class=\"python\">from django_pg_current_timestamp import CurrentTimestamp\n\nmm = MyModel.objects.get(id=1)\nmm.last_seen_date = CurrentTimestamp()\nmm.save()\n## Resulting SQL:\n##     UPDATE \"my_model\" SET \"last_seen_date\" = CURRENT_TIMESTAMP;\n\nprint MyModel.objects.filter(last_seen_date__lt=CURRENT_TIME).count()\n\nMyModel.objects.filter(id__in=[1, 2, 3]).update(last_seen_date=CURRENT_TIME)\n</code></pre>\n", "abstract": "I've created a Python Django plugin module which allows you to control the use of CURRENT_TIMESTAMP on DateTimeField objects, both in specific cases (see usage below) as well as automatically for auto_now and auto_now_add columns. django-pg-current-timestamp GitHub: https://github.com/jaytaylor/django-pg-current-timestamp PyPi: https://pypi.python.org/pypi/django-pg-current-timestamp Example usage:"}, {"id": 7467173, "score": 2, "vote": 0, "content": "<p>If you want the datetime from a foreign server (i.e., not the one hosting the Django application), you're going to have to peg it manually for a datatime to use. You could use a SQL command like <code>select now();</code> or something over SSH, like <code>ssh user@host \"date +%s\"</code>.</p>\n", "abstract": "If you want the datetime from a foreign server (i.e., not the one hosting the Django application), you're going to have to peg it manually for a datatime to use. You could use a SQL command like select now(); or something over SSH, like ssh user@host \"date +%s\"."}, {"id": 7466017, "score": 1, "vote": 0, "content": "<p>Maybe you should take a look into the documentation:<br/>\n<a href=\"https://docs.djangoproject.com/en/dev/ref/models/fields/#datefield\" rel=\"nofollow\">Modelfields: DateField</a></p>\n<p>The option 'auto_now' could be just what you are searching for. You can also use it with the DateTimeField. It updates the DateTime each time you're saving the model. So with that option set for your DateTimeField it should be sufficent to retrieve a data-record and save it again to set the time right.</p>\n", "abstract": "Maybe you should take a look into the documentation:\nModelfields: DateField The option 'auto_now' could be just what you are searching for. You can also use it with the DateTimeField. It updates the DateTime each time you're saving the model. So with that option set for your DateTimeField it should be sufficent to retrieve a data-record and save it again to set the time right."}, {"id": 66186337, "score": 0, "vote": 0, "content": "<p>When creating the table, the field you want to make date now in field after add data add   this code to field</p>\n<pre><code class=\"python\">class MyModel(models.Model):\n  created_at = models.DateTimeField(auto_now_add=True)\n  updated_at = models.DateTimeField(auto_now=True)\n</code></pre>\n", "abstract": "When creating the table, the field you want to make date now in field after add data add   this code to field"}]}, {"link": "https://stackoverflow.com/questions/31394998/using-sqlalchemy-to-load-csv-file-into-a-database", "question": {"id": "31394998", "title": "using sqlalchemy to load csv file into a database", "content": "<p>I would like to load csv files into a database</p>\n", "abstract": "I would like to load csv files into a database"}, "answers": [{"id": 31397990, "score": 56, "vote": 0, "content": "<p>Because of the power of SQLAlchemy, I'm also using it on a project.  It's power comes from the object-oriented way of \"talking\" to a database instead of hardcoding SQL statements that can be a pain to manage.  Not to mention, it's also a lot faster.</p>\n<p>To answer your question bluntly, yes!  Storing data from a CSV into a database using SQLAlchemy is a piece of cake.  Here's a full working example (I used SQLAlchemy 1.0.6 and Python 2.7.6):</p>\n<pre><code class=\"python\">from numpy import genfromtxt\nfrom time import time\nfrom datetime import datetime\nfrom sqlalchemy import Column, Integer, Float, Date\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\ndef Load_Data(file_name):\n    data = genfromtxt(file_name, delimiter=',', skip_header=1, converters={0: lambda s: str(s)})\n    return data.tolist()\n\nBase = declarative_base()\n\nclass Price_History(Base):\n    #Tell SQLAlchemy what the table name is and if there's any table-specific arguments it should know about\n    __tablename__ = 'Price_History'\n    __table_args__ = {'sqlite_autoincrement': True}\n    #tell SQLAlchemy the name of column and its attributes:\n    id = Column(Integer, primary_key=True, nullable=False) \n    date = Column(Date)\n    opn = Column(Float)\n    hi = Column(Float)\n    lo = Column(Float)\n    close = Column(Float)\n    vol = Column(Float)\n\nif __name__ == \"__main__\":\n    t = time()\n\n    #Create the database\n    engine = create_engine('sqlite:///csv_test.db')\n    Base.metadata.create_all(engine)\n\n    #Create the session\n    session = sessionmaker()\n    session.configure(bind=engine)\n    s = session()\n\n    try:\n        file_name = \"t.csv\" #sample CSV file used:  http://www.google.com/finance/historical?q=NYSE%3AT&amp;ei=W4ikVam8LYWjmAGjhoHACw&amp;output=csv\n        data = Load_Data(file_name) \n\n        for i in data:\n            record = Price_History(**{\n                'date' : datetime.strptime(i[0], '%d-%b-%y').date(),\n                'opn' : i[1],\n                'hi' : i[2],\n                'lo' : i[3],\n                'close' : i[4],\n                'vol' : i[5]\n            })\n            s.add(record) #Add all the records\n\n        s.commit() #Attempt to commit all the records\n    except:\n        s.rollback() #Rollback the changes on error\n    finally:\n        s.close() #Close the connection\n    print \"Time elapsed: \" + str(time() - t) + \" s.\" #0.091s\n</code></pre>\n<p>(Note:  this is not necessarily the \"best\" way to do this, but I think this format is very readable for a beginner; it's also very fast:  0.091s for 251 records inserted!)</p>\n<p>I think if you go through it line by line, you'll see what a breeze it is to use.  Notice the lack of SQL statements -- hooray!  I also took the liberty of using numpy to load the CSV contents in two lines, but it can be done without it if you like.</p>\n<p>If you wanted to compare against the traditional way of doing it, here's a full-working example for reference:</p>\n<pre><code class=\"python\">import sqlite3\nimport time\nfrom numpy import genfromtxt\n\ndef dict_factory(cursor, row):\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\n\ndef Create_DB(db):      \n    #Create DB and format it as needed\n    with sqlite3.connect(db) as conn:\n        conn.row_factory = dict_factory\n        conn.text_factory = str\n\n        cursor = conn.cursor()\n\n        cursor.execute(\"CREATE TABLE [Price_History] ([id] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL UNIQUE, [date] DATE, [opn] FLOAT, [hi] FLOAT, [lo] FLOAT, [close] FLOAT, [vol] INTEGER);\")\n\n\ndef Add_Record(db, data):\n    #Insert record into table\n    with sqlite3.connect(db) as conn:\n        conn.row_factory = dict_factory\n        conn.text_factory = str\n\n        cursor = conn.cursor()\n\n        cursor.execute(\"INSERT INTO Price_History({cols}) VALUES({vals});\".format(cols = str(data.keys()).strip('[]'), \n                    vals=str([data[i] for i in data]).strip('[]')\n                    ))\n\n\ndef Load_Data(file_name):\n    data = genfromtxt(file_name, delimiter=',', skiprows=1, converters={0: lambda s: str(s)})\n    return data.tolist()\n\n\nif __name__ == \"__main__\":\n    t = time.time() \n\n    db = 'csv_test_sql.db' #Database filename \n    file_name = \"t.csv\" #sample CSV file used:  http://www.google.com/finance/historical?q=NYSE%3AT&amp;ei=W4ikVam8LYWjmAGjhoHACw&amp;output=csv\n\n    data = Load_Data(file_name) #Get data from CSV\n\n    Create_DB(db) #Create DB\n\n    #For every record, format and insert to table\n    for i in data:\n        record = {\n                'date' : i[0],\n                'opn' : i[1],\n                'hi' : i[2],\n                'lo' : i[3],\n                'close' : i[4],\n                'vol' : i[5]\n            }\n        Add_Record(db, record)\n\n    print \"Time elapsed: \" + str(time.time() - t) + \" s.\" #3.604s\n</code></pre>\n<p>(Note:  even in the \"old\" way, this is by no means the best way to do this, but it's very readable and a \"1-to-1\" translation from the SQLAlchemy way vs. the \"old\" way.)</p>\n<p>Notice the the SQL statements:  one to create the table, the other to insert records.  Also, notice that it's a bit more cumbersome to maintain long SQL strings vs. a simple class attribute addition.  Liking SQLAlchemy so far?</p>\n<p>As for your foreign key inquiry, of course.  SQLAlchemy has the power to do this too.  Here's an example of how a class attribute would look like with a foreign key assignment (assuming the <code>ForeignKey</code> class has also been imported from the <code>sqlalchemy</code> module):</p>\n<pre><code class=\"python\">class Asset_Analysis(Base):\n    #Tell SQLAlchemy what the table name is and if there's any table-specific arguments it should know about\n    __tablename__ = 'Asset_Analysis'\n    __table_args__ = {'sqlite_autoincrement': True}\n    #tell SQLAlchemy the name of column and its attributes:\n    id = Column(Integer, primary_key=True, nullable=False) \n    fid = Column(Integer, ForeignKey('Price_History.id'))\n</code></pre>\n<p>which points the \"fid\" column as a foreign key to Price_History's id column.</p>\n<p>Hope that helps!</p>\n", "abstract": "Because of the power of SQLAlchemy, I'm also using it on a project.  It's power comes from the object-oriented way of \"talking\" to a database instead of hardcoding SQL statements that can be a pain to manage.  Not to mention, it's also a lot faster. To answer your question bluntly, yes!  Storing data from a CSV into a database using SQLAlchemy is a piece of cake.  Here's a full working example (I used SQLAlchemy 1.0.6 and Python 2.7.6): (Note:  this is not necessarily the \"best\" way to do this, but I think this format is very readable for a beginner; it's also very fast:  0.091s for 251 records inserted!) I think if you go through it line by line, you'll see what a breeze it is to use.  Notice the lack of SQL statements -- hooray!  I also took the liberty of using numpy to load the CSV contents in two lines, but it can be done without it if you like. If you wanted to compare against the traditional way of doing it, here's a full-working example for reference: (Note:  even in the \"old\" way, this is by no means the best way to do this, but it's very readable and a \"1-to-1\" translation from the SQLAlchemy way vs. the \"old\" way.) Notice the the SQL statements:  one to create the table, the other to insert records.  Also, notice that it's a bit more cumbersome to maintain long SQL strings vs. a simple class attribute addition.  Liking SQLAlchemy so far? As for your foreign key inquiry, of course.  SQLAlchemy has the power to do this too.  Here's an example of how a class attribute would look like with a foreign key assignment (assuming the ForeignKey class has also been imported from the sqlalchemy module): which points the \"fid\" column as a foreign key to Price_History's id column. Hope that helps!"}, {"id": 34523707, "score": 55, "vote": 0, "content": "<p>In case your CSV is quite large, using INSERTS is very ineffective. You should use a bulk loading mechanisms, which differ from base to base. E.g. in PostgreSQL you should use \"COPY FROM\" method:</p>\n<pre><code class=\"python\">with open(csv_file_path, 'r') as f:    \n    conn = create_engine('postgresql+psycopg2://...').raw_connection()\n    cursor = conn.cursor()\n    cmd = 'COPY tbl_name(col1, col2, col3) FROM STDIN WITH (FORMAT CSV, HEADER FALSE)'\n    cursor.copy_expert(cmd, f)\n    conn.commit()\n</code></pre>\n", "abstract": "In case your CSV is quite large, using INSERTS is very ineffective. You should use a bulk loading mechanisms, which differ from base to base. E.g. in PostgreSQL you should use \"COPY FROM\" method:"}, {"id": 53472277, "score": 20, "vote": 0, "content": "<p>I have had the exact same problem, and I found it paradoxically easier to use a 2-step process with pandas:</p>\n<pre><code class=\"python\">import pandas as pd\nwith open(csv_file_path, 'r') as file:\n    data_df = pd.read_csv(file)\ndata_df.to_sql('tbl_name', con=engine, index=True, index_label='id', if_exists='replace')\n</code></pre>\n<p>Note that my approach is similar to <a href=\"https://stackoverflow.com/questions/43453420/import-csv-to-database-using-sqlalchemy\">this one</a>, but somehow Google sent me to this thread instead, so I thought I would share. </p>\n", "abstract": "I have had the exact same problem, and I found it paradoxically easier to use a 2-step process with pandas: Note that my approach is similar to this one, but somehow Google sent me to this thread instead, so I thought I would share. "}, {"id": 57529830, "score": 7, "vote": 0, "content": "<p>To import a relatively small CSV file into database using sqlalchemy, you can use <code>engine.execute(my_table.insert(), list_of_row_dicts)</code>, as described in detail in the <a href=\"https://docs.sqlalchemy.org/en/13/core/tutorial.html#executing-multiple-statements\" rel=\"nofollow noreferrer\">\"Executing Multiple Statements\" section of the sqlalchemy tutorial</a>.</p>\n<p>This is sometimes referred to as <em>\"executemany\" style of invocation</em>, because it results in an <a href=\"https://www.python.org/dev/peps/pep-0249/#id18\" rel=\"nofollow noreferrer\"><code>executemany</code> DBAPI call</a>. The DB driver might execute a single multi-value <code>INSERT .. VALUES (..), (..), (..)</code> statement, which results in fewer round-trips to the DB and faster execution:</p>\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor-executemany.html\" rel=\"nofollow noreferrer\">the MySQL connector does that by default</a></li>\n<li>Postgres' psycopg2 <a href=\"https://stackoverflow.com/questions/8134602/psycopg2-insert-multiple-rows-with-one-query\">does</a> <a href=\"https://github.com/psycopg/psycopg2/issues/491#issuecomment-276693067\" rel=\"nofollow noreferrer\">not</a>, unless you initialize it with <a href=\"https://docs.sqlalchemy.org/en/13/dialects/postgresql.html#psycopg2-executemany-mode\" rel=\"nofollow noreferrer\">create_engine(..., executemany_mode='values')</a>)</li>\n<li>pyodbc's <a href=\"https://github.com/mkleehammer/pyodbc/wiki/Features-beyond-the-DB-API#fast_executemany\" rel=\"nofollow noreferrer\">fast_executemany flag</a> when used with MS SQL Server's ODBC drivers. (But <a href=\"https://github.com/pymssql/pymssql/issues/332\" rel=\"nofollow noreferrer\">not pymssql</a>!)</li>\n</ul>\n<p>According to the <a href=\"https://docs.sqlalchemy.org/en/13/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow\" rel=\"nofollow noreferrer\">sqlalchemy's FAQ</a>, this is the fastest you can get without using DB-specific bulk loading methods, such as <a href=\"https://stackoverflow.com/a/34523707/1026\">COPY FROM</a> in Postgres, <a href=\"https://stackoverflow.com/q/12890098/1026\">LOAD DATA LOCAL INFILE</a>  in MySQL, etc. In particular it's faster than using plain ORM (as in the answer by @Manuel J. Diaz here), <code>bulk_save_objects</code>, or <code>bulk_insert_mappings</code>.</p>\n<pre><code class=\"python\">import csv\nfrom sqlalchemy import create_engine, Table, Column, Integer, MetaData\n\nengine = create_engine('sqlite:///sqlalchemy.db', echo=True)\n\nmetadata = MetaData()\n# Define the table with sqlalchemy:\nmy_table = Table('MyTable', metadata,\n    Column('foo', Integer),\n    Column('bar', Integer),\n)\nmetadata.create_all(engine)\ninsert_query = my_table.insert()\n\n# Or read the definition from the DB:\n# metadata.reflect(engine, only=['MyTable'])\n# my_table = Table('MyTable', metadata, autoload=True, autoload_with=engine)\n# insert_query = my_table.insert()\n\n# Or hardcode the SQL query:\n# insert_query = \"INSERT INTO MyTable (foo, bar) VALUES (:foo, :bar)\"\n\nwith open('test.csv', 'r', encoding=\"utf-8\") as csvfile:\n    csv_reader = csv.reader(csvfile, delimiter=',')\n    engine.execute(\n        insert_query,\n        [{\"foo\": row[0], \"bar\": row[1]} \n            for row in csv_reader]\n    )\n</code></pre>\n", "abstract": "To import a relatively small CSV file into database using sqlalchemy, you can use engine.execute(my_table.insert(), list_of_row_dicts), as described in detail in the \"Executing Multiple Statements\" section of the sqlalchemy tutorial. This is sometimes referred to as \"executemany\" style of invocation, because it results in an executemany DBAPI call. The DB driver might execute a single multi-value INSERT .. VALUES (..), (..), (..) statement, which results in fewer round-trips to the DB and faster execution: According to the sqlalchemy's FAQ, this is the fastest you can get without using DB-specific bulk loading methods, such as COPY FROM in Postgres, LOAD DATA LOCAL INFILE  in MySQL, etc. In particular it's faster than using plain ORM (as in the answer by @Manuel J. Diaz here), bulk_save_objects, or bulk_insert_mappings."}, {"id": 66168167, "score": 3, "vote": 0, "content": "<p>CSV file with commas and header names to PostrgeSQL</p>\n<ol>\n<li>I'm using csv Python reader. CSV data divided by commas (,)</li>\n<li>Then convert it to Pandas DataFrame. Names of the columns the same as in your csv file.</li>\n<li>End the last, DataFrame to sql with engine as connection to DB. if_exists='replace/append'</li>\n</ol>\n<pre><code class=\"python\">import csv\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# Create engine to connect with DB\ntry:\n    engine = create_engine(\n        'postgresql://username:password@localhost:5432/name_of_base')\nexcept:\n    print(\"Can't create 'engine\")\n\n# Get data from CSV file to DataFrame(Pandas)\nwith open('test.csv', newline='') as csvfile:\n    reader = csv.DictReader(csvfile)\n    columns = [\n        'moment',\n        'isin',\n        'name'\n    ]\n    df = pd.DataFrame(data=reader, columns=columns)\n\n# Standart method of Pandas to deliver data from DataFrame to PastgresQL\ntry:\n    with engine.begin() as connection:\n        df.to_sql('name_of_table', con=connection, index_label='id', if_exists='replace')\n        print('Done, ok!')\nexcept:\n    print('Something went wrong!')\n</code></pre>\n", "abstract": "CSV file with commas and header names to PostrgeSQL"}, {"id": 74112210, "score": 0, "vote": 0, "content": "<p>This is the only way I could get it to work. The other answers do not explicitly commit the cursor's connection. This also implies you're using modern python, sqlalchemy, and obviously postgres since the syntax uses <code>COPY ... FROM</code>.</p>\n<p>There's no error handling, it's probably not secure, and it uses all columns in the ORM mapper definition that aren't primary keys, but for simple tasks it'll probably do fine.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import pathlib\n\nimport sqlalchemy\n\nBase: sqlalchemy.orm.DeclarativeMeta = db.orm.declarative_base()\n\ndef upload_to_model_table(\n        Model: Base,\n        csv_file_path: pathlib.Path,\n        engine: sqlalchemy.engine,\n        header=True,\n        delimiter=';'\n):\n    \"\"\" It's assumed you're using postgres, otherwise this won't work. \"\"\"\n    fieldnames = ', '.join([\n        f'\"{col.name}\"' for col in Model.__mapper__.columns if not col.primary_key\n    ])\n\n    sql = \"\"\"\n    COPY {0} ({1}) FROM stdin WITH (format CSV, header {2}, delimiter '{3}')\n    \"\"\".format(Model.__tablename__, fieldnames, header, delimiter)\n\n    with engine.connect() as connection:\n        cursor = connection.connection.cursor()\n        with open(csv_file_path, 'rt', encoding='utf-8') as csv_file:\n            cursor.copy_expert(sql, csv_file)\n        cursor.connection.commit()\n        cursor.close()\n</code></pre>\n", "abstract": "This is the only way I could get it to work. The other answers do not explicitly commit the cursor's connection. This also implies you're using modern python, sqlalchemy, and obviously postgres since the syntax uses COPY ... FROM. There's no error handling, it's probably not secure, and it uses all columns in the ORM mapper definition that aren't primary keys, but for simple tasks it'll probably do fine."}]}, {"link": "https://stackoverflow.com/questions/21310549/list-database-tables-with-sqlalchemy", "question": {"id": "21310549", "title": "List database tables with SQLAlchemy", "content": "<p>I want to implement a function that gives information about all the tables (and their column names) that are present in a database (not only those created with SQLAlchemy). While reading the documentation it seems to me that this is done via reflection but I didn't manage to get something working. Any suggestions or examples on how to do this?</p>\n", "abstract": "I want to implement a function that gives information about all the tables (and their column names) that are present in a database (not only those created with SQLAlchemy). While reading the documentation it seems to me that this is done via reflection but I didn't manage to get something working. Any suggestions or examples on how to do this?"}, "answers": [{"id": 21346185, "score": 108, "vote": 0, "content": "<p>start with an engine:</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine\nengine = create_engine(\"postgresql://u:p@host/database\")\n</code></pre>\n<p>quick path to all table /column names, use an inspector:</p>\n<pre><code class=\"python\">from sqlalchemy import inspect\ninspector = inspect(engine)\n\nfor table_name in inspector.get_table_names():\n   for column in inspector.get_columns(table_name):\n       print(\"Column: %s\" % column['name'])\n</code></pre>\n<p>docs: <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html?highlight=inspector#fine-grained-reflection-with-inspector\" rel=\"noreferrer\">http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html?highlight=inspector#fine-grained-reflection-with-inspector</a></p>\n<p>alternatively, use MetaData / Tables:</p>\n<pre><code class=\"python\">from sqlalchemy import MetaData\nm = MetaData()\nm.reflect(engine)\nfor table in m.tables.values():\n    print(table.name)\n    for column in table.c:\n        print(column.name)\n</code></pre>\n<p>docs: <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html#reflecting-all-tables-at-once\" rel=\"noreferrer\">http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html#reflecting-all-tables-at-once</a></p>\n", "abstract": "start with an engine: quick path to all table /column names, use an inspector: docs: http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html?highlight=inspector#fine-grained-reflection-with-inspector alternatively, use MetaData / Tables: docs: http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html#reflecting-all-tables-at-once"}, {"id": 56040532, "score": 4, "vote": 0, "content": "<p>First set up the sqlalchemy engine.</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine, inspect, text\nfrom sqlalchemy.engine import url\n\nconnect_url = url.URL(\n    'oracle',\n    username='db_username',\n    password='db_password',\n    host='db_host',\n    port='db_port',\n    query=dict(service_name='db_service_name'))\n\nengine = create_engine(connect_url)\n\ntry:\n    engine.connect()\nexcept Exception as error:\n    print(error)\n    return\n</code></pre>\n<p>Like others have mentioned, you can use the inspect method to get the table names.</p>\n<p>But in my case, the list of tables returned by the inspect method was incomplete.</p>\n<p>So, I found out another way to find table names by using pure SQL queries in sqlalchemy.</p>\n<pre><code class=\"python\">query = text(\"SELECT table_name FROM all_tables where owner = '%s'\"%str('db_username'))\n\ntable_name_data = self.session.execute(query).fetchall()\n</code></pre>\n<p>Just for sake of completeness of answer, here's the code to fetch table names by inspect method (if it works good in your case).</p>\n<pre><code class=\"python\">inspector = inspect(engine)\ntable_names = inspector.get_table_names()\n</code></pre>\n", "abstract": "First set up the sqlalchemy engine. Like others have mentioned, you can use the inspect method to get the table names. But in my case, the list of tables returned by the inspect method was incomplete. So, I found out another way to find table names by using pure SQL queries in sqlalchemy. Just for sake of completeness of answer, here's the code to fetch table names by inspect method (if it works good in your case)."}, {"id": 21310645, "score": 3, "vote": 0, "content": "<p>Hey I created a small module that helps easily reflecting all tables in a database you connect to with SQLAlchemy, give it a look: <a href=\"https://github.com/mathiasbc/EZAlchemy\" rel=\"nofollow noreferrer\">EZAlchemy</a></p>\n<pre><code class=\"python\">from EZAlchemy.ezalchemy import EZAlchemy\n\nDB = EZAlchemy(\n    db_user='username',\n    db_password='pezzword',\n    db_hostname='127.0.0.1',\n    db_database='mydatabase',\n    d_n_d='mysql'   # stands for dialect+driver\n)\n\n# this function loads all tables in the database to the class instance DB\nDB.connect()\n\n# List all associations to DB, you will see all the tables in that database\ndir(DB)\n</code></pre>\n", "abstract": "Hey I created a small module that helps easily reflecting all tables in a database you connect to with SQLAlchemy, give it a look: EZAlchemy"}, {"id": 65402581, "score": 2, "vote": 0, "content": "<p>I'm proposing another solution as I was not satisfied by any of the previous in the case of <strong>postgres</strong> which uses <em>schemas</em>. I hacked this solution together by looking into the <a href=\"https://github.com/pandas-dev/pandas\" rel=\"nofollow noreferrer\">pandas source code</a>.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from sqlalchemy import MetaData, create_engine\nfrom typing import List\n\ndef list_tables(pg_uri: str, schema: str) -&gt; List[str]:\n    with create_engine(pg_uri).connect() as conn:\n        meta = MetaData(conn, schema=schema)\n        meta.reflect(views=True)\n        return list(meta.tables.keys())\n</code></pre>\n<p>In order to get a list of all tables in your schema, you need to form your postgres database uri <code>pg_uri</code> (e.g. <code>\"postgresql://u:p@host/database\"</code> as in the zzzeek's answer) as well as the schema's name <code>schema</code>. So if we use the example uri as well as the typical schema <code>public</code> we would get all the tables and views with:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">list_tables(\"postgresql://u:p@host/database\", \"public\")\n</code></pre>\n", "abstract": "I'm proposing another solution as I was not satisfied by any of the previous in the case of postgres which uses schemas. I hacked this solution together by looking into the pandas source code. In order to get a list of all tables in your schema, you need to form your postgres database uri pg_uri (e.g. \"postgresql://u:p@host/database\" as in the zzzeek's answer) as well as the schema's name schema. So if we use the example uri as well as the typical schema public we would get all the tables and views with:"}, {"id": 53711090, "score": 0, "vote": 0, "content": "<p>While <a href=\"https://docs.sqlalchemy.org/en/latest/core/reflection.html\" rel=\"nofollow noreferrer\">reflection/inspection</a> is useful, I had trouble getting the data out of the database. I found <a href=\"https://sqlsoup.readthedocs.io/en/latest/tutorial.html\" rel=\"nofollow noreferrer\">sqlsoup</a> to be much more user-friendly. You create the engine using sqlalchemy and pass that engine to sqlsoup.SQlSoup. ie:</p>\n<pre><code class=\"python\">import sqlsoup\n\ndef create_engine():\n    from sqlalchemy import create_engine\n    return create_engine(f\"mysql+mysqlconnector://{database_username}:{database_pw}@{database_host}/{database_name}\")\n\ndef test_sqlsoup():\n    engine = create_engine()\n    db = sqlsoup.SQLSoup(engine)\n    # Note: database must have a table called 'users' for this example\n    users = db.users.all()\n    print(users)\n\nif __name__ == \"__main__\":\n    test_sqlsoup()\n</code></pre>\n<p>If you're familiar with sqlalchemy then you're familiar with sqlsoup. I've used this to extract data from a wordpress database. </p>\n", "abstract": "While reflection/inspection is useful, I had trouble getting the data out of the database. I found sqlsoup to be much more user-friendly. You create the engine using sqlalchemy and pass that engine to sqlsoup.SQlSoup. ie: If you're familiar with sqlalchemy then you're familiar with sqlsoup. I've used this to extract data from a wordpress database. "}]}, {"link": "https://stackoverflow.com/questions/3247183/variable-table-name-in-sqlite", "question": {"id": "3247183", "title": "Variable table name in sqlite", "content": "<p>Question: Is it possible to use a variable as your table name without having to use string constructors to do so?</p>\n<hr/>\n<p>Info: </p>\n<p>I'm working on a project right now that catalogs data from a star simulation of mine. To do so I'm loading all the data into a sqlite database. It's working pretty well, but I've decided to add a lot more flexibility, efficiency, and usability to my db. I plan on later adding planetoids to the simulation, and wanted to have a table for each star. This way I wouldn't have to query a table of 20m some planetoids for the 1-4k in each solar system. </p>\n<p>I've been told using string constructors is bad because it leaves me vulnerable to a SQL injection attack. While that isn't a big deal here as I'm the only person with access to these dbs, I would like to follow best practices. And also this way if I do a project with a similar situation where it is open to the public, I know what to do.</p>\n<p>Currently I'm doing this:</p>\n<pre><code class=\"python\">cursor.execute(\"CREATE TABLE StarFrame\"+self.name+\" (etc etc)\")\n</code></pre>\n<p>This works, but I would like to do something more like:</p>\n<pre><code class=\"python\">cursor.execute(\"CREATE TABLE StarFrame(?) (etc etc)\",self.name)\n</code></pre>\n<p>though I understand that this would probably be impossible. though I would settle for something like</p>\n<pre><code class=\"python\">cursor.execute(\"CREATE TABLE (?) (etc etc)\",self.name)\n</code></pre>\n<p>If this is not at all possible, I'll accept that answer, but if anyone knows a way to do this, do tell. :)</p>\n<p>I'm coding in python.</p>\n", "abstract": "Question: Is it possible to use a variable as your table name without having to use string constructors to do so? Info:  I'm working on a project right now that catalogs data from a star simulation of mine. To do so I'm loading all the data into a sqlite database. It's working pretty well, but I've decided to add a lot more flexibility, efficiency, and usability to my db. I plan on later adding planetoids to the simulation, and wanted to have a table for each star. This way I wouldn't have to query a table of 20m some planetoids for the 1-4k in each solar system.  I've been told using string constructors is bad because it leaves me vulnerable to a SQL injection attack. While that isn't a big deal here as I'm the only person with access to these dbs, I would like to follow best practices. And also this way if I do a project with a similar situation where it is open to the public, I know what to do. Currently I'm doing this: This works, but I would like to do something more like: though I understand that this would probably be impossible. though I would settle for something like If this is not at all possible, I'll accept that answer, but if anyone knows a way to do this, do tell. :) I'm coding in python."}, "answers": [{"id": 3247553, "score": 55, "vote": 0, "content": "<p>Unfortunately, tables can't be the target of parameter substitution (I didn't find any definitive source, but I have seen it on a few web forums).</p>\n<p>If you are worried about injection (you probably should be), you can write a function that cleans the string before passing it. Since you are looking for just a table name, you should be safe just accepting alphanumerics, stripping out all punctuation, such as <code>)(][;,</code> and whitespace. Basically, just keep <code>A-Z a-z 0-9</code>.</p>\n<pre><code class=\"python\">def scrub(table_name):\n    return ''.join( chr for chr in table_name if chr.isalnum() )\n\nscrub('); drop tables --')  # returns 'droptables'\n</code></pre>\n", "abstract": "Unfortunately, tables can't be the target of parameter substitution (I didn't find any definitive source, but I have seen it on a few web forums). If you are worried about injection (you probably should be), you can write a function that cleans the string before passing it. Since you are looking for just a table name, you should be safe just accepting alphanumerics, stripping out all punctuation, such as )(][;, and whitespace. Basically, just keep A-Z a-z 0-9."}, {"id": 50945437, "score": 12, "vote": 0, "content": "<p>For people searching for a way to make the table as a variable, I got this from another reply to same question <a href=\"https://stackoverflow.com/questions/39196462/how-to-use-variable-for-sqlite-table-name\">here</a>: </p>\n<p>It said the following and it works. It's all quoted from <strong>mhawke</strong>: </p>\n<p>You can't use parameter substitution for the table name. You need to add the table name to the query string yourself. Something like this:</p>\n<pre><code class=\"python\">query = 'SELECT * FROM {}'.format(table)\nc.execute(query)\n</code></pre>\n<p>One thing to be mindful of is the source of the value for the table name. If that comes from an untrusted source, e.g. a user, then you need to validate the table name to avoid potential SQL injection attacks. One way might be to construct a parameterised query that looks up the table name from the DB catalogue:</p>\n<pre><code class=\"python\">import sqlite3\n\ndef exists_table(db, name):\n    query = \"SELECT 1 FROM sqlite_master WHERE type='table' and name = ?\"\n    return db.execute(query, (name,)).fetchone() is not None\n</code></pre>\n", "abstract": "For people searching for a way to make the table as a variable, I got this from another reply to same question here:  It said the following and it works. It's all quoted from mhawke:  You can't use parameter substitution for the table name. You need to add the table name to the query string yourself. Something like this: One thing to be mindful of is the source of the value for the table name. If that comes from an untrusted source, e.g. a user, then you need to validate the table name to avoid potential SQL injection attacks. One way might be to construct a parameterised query that looks up the table name from the DB catalogue:"}, {"id": 3247558, "score": 9, "vote": 0, "content": "<p>I wouldn't separate the data into more than one table.  If you create an index on the star column, you won't have any problem efficiently accessing the data.</p>\n", "abstract": "I wouldn't separate the data into more than one table.  If you create an index on the star column, you won't have any problem efficiently accessing the data."}, {"id": 48241626, "score": 1, "vote": 0, "content": "<p>Try with string formatting:</p>\n<pre><code class=\"python\">sql_cmd = '''CREATE TABLE {}(id, column1, column2, column2)'''.format(\n            'table_name')\ndb.execute(sql_cmd)\n</code></pre>\n<p>Replace <code>'table_name'</code> with your desire.</p>\n", "abstract": "Try with string formatting: Replace 'table_name' with your desire."}, {"id": 64699307, "score": 0, "vote": 0, "content": "<p>To avoid hard-coding table names, I've used:</p>\n<pre><code class=\"python\">table = \"sometable\"\nc = conn.cursor()\n\nc.execute('''CREATE TABLE IF NOT EXISTS {} (\n                importantdate DATE,\n                somename VARCHAR,\n                )'''.format(table))\n\nc.execute('''INSERT INTO {} VALUES (?, ?)'''.format(table),\n                  (datetime.strftime(datetime.today(), \"%Y-%m-%d\"),\n                   myname))\n</code></pre>\n", "abstract": "To avoid hard-coding table names, I've used:"}, {"id": 33184570, "score": -1, "vote": 0, "content": "<p>As has been said in the other answers, \"tables can't be the target of parameter substitution\" but if you find yourself in a bind where you have no option, here is a method of testing if the table name supplied is valid.<br/>\nNote: I have made the table name a real pig in an attempt to cover all of the bases.</p>\n<pre><code class=\"python\">import sys\nimport sqlite3\ndef delim(s):\n  delims=\"\\\"'`\"\n  use_delim = []\n  for d in delims:\n   if d not in s:\n    use_delim.append(d)\n  return use_delim\n\ndb_name = \"some.db\"\ndb = sqlite3.connect(db_name)\nmycursor = db.cursor()\ntable = 'so\"\"m ][ `etable'\ndelimiters = delim(table)\nif len(delimiters) &lt; 1:\n    print \"The name of the database will not allow this!\"\n    sys.exit()\nuse_delimiter = delimiters[0]\nprint \"Using delimiter \", use_delimiter\nmycursor.execute('SELECT name FROM sqlite_master where (name = ?)', [table])\nrow = mycursor.fetchall()\nvalid_table = False\nif row:\n    print (table,\"table name verified\")\n    valid_table = True\nelse:\n    print (table,\"Table name not in database\", db_name)\n\nif valid_table:\n    try:\n        mycursor.execute('insert into ' +use_delimiter+ table +use_delimiter+ ' (my_data,my_column_name) values (?,?) ',(1,\"Name\"));\n        db.commit()\n    except Exception as e:\n        print \"Error:\", str(e)\n    try:\n        mycursor.execute('UPDATE ' +use_delimiter+ table +use_delimiter+ ' set my_column_name = ? where my_data = ?', [\"ReNamed\",1])\n        db.commit()\n    except Exception as e:\n        print \"Error:\", str(e)\ndb.close()\n</code></pre>\n", "abstract": "As has been said in the other answers, \"tables can't be the target of parameter substitution\" but if you find yourself in a bind where you have no option, here is a method of testing if the table name supplied is valid.\nNote: I have made the table name a real pig in an attempt to cover all of the bases."}, {"id": 55589242, "score": -1, "vote": 0, "content": "<p>you can use something like this\n   <code>conn = sqlite3.connect()\n    createTable = '''CREATE TABLE  %s (#      );''' %dateNow)\n    conn.execute(createTable)</code></p>\n<p>basically, if we want to separate the data into several tables according to the date right now, for example, you want to monitor a system based on the date. </p>\n<p>createTable = '''CREATE TABLE  %s (#      );''' %dateNow) means that you create a table with variable dateNow which according to your coding language, you can define dateNow as a variable to retrieve the current date from your coding language.</p>\n", "abstract": "you can use something like this\n   conn = sqlite3.connect()\n    createTable = '''CREATE TABLE  %s (#      );''' %dateNow)\n    conn.execute(createTable) basically, if we want to separate the data into several tables according to the date right now, for example, you want to monitor a system based on the date.  createTable = '''CREATE TABLE  %s (#      );''' %dateNow) means that you create a table with variable dateNow which according to your coding language, you can define dateNow as a variable to retrieve the current date from your coding language."}, {"id": 66768913, "score": -1, "vote": 0, "content": "<p>You can save your query in a .sql or txt file and use the open().replace() method to use variables in any part of your query. Long time reader but first time poster so I apologize if anything is off here.</p>\n<p><div class=\"snippet\" data-babel=\"false\" data-console=\"true\" data-hide=\"false\" data-lang=\"js\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code class=\"python\">```SQL in yoursql.sql```\nSel *\nFrom yourdbschema.tablenm\n\n\n```SQL to run```\ntablenm = 'yourtablename'\n\ncur = connect.cursor() \n\nquery = cur.execute(open(file = yoursql.sql).read().replace('tablenm',tablenm))</code></pre>\n</div>\n</div>\n</p>\n", "abstract": "You can save your query in a .sql or txt file and use the open().replace() method to use variables in any part of your query. Long time reader but first time poster so I apologize if anything is off here. \n\n```SQL in yoursql.sql```\nSel *\nFrom yourdbschema.tablenm\n\n\n```SQL to run```\ntablenm = 'yourtablename'\n\ncur = connect.cursor() \n\nquery = cur.execute(open(file = yoursql.sql).read().replace('tablenm',tablenm))\n\n\n"}, {"id": 38155069, "score": -3, "vote": 0, "content": "<p>You can pass a string as the SQL command:</p>\n<pre><code class=\"python\">import sqlite3\nconn = sqlite3.connect('db.db')\nc = conn.cursor()\ntablename, field_data = 'some_table','some_data'\nquery = 'SELECT * FROM '+tablename+' WHERE column1=\\\"'+field_data+\"\\\"\"\nc.execute(query)\n</code></pre>\n", "abstract": "You can pass a string as the SQL command:"}]}, {"link": "https://stackoverflow.com/questions/594/cx-oracle-how-do-i-iterate-over-a-result-set", "question": {"id": "594", "title": "cx_Oracle: How do I iterate over a result set?", "content": "<p>There are several ways to iterate over a result set. What are the tradeoff of each?</p>\n", "abstract": "There are several ways to iterate over a result set. What are the tradeoff of each?"}, "answers": [{"id": 595, "score": 55, "vote": 0, "content": "<p>The canonical way is to use the built-in cursor iterator.</p>\n<pre><code class=\"python\">curs.execute('select * from people')\nfor row in curs:\n    print row\n</code></pre>\n<hr/>\n<p>You can use <code>fetchall()</code> to get all rows at once.</p>\n<pre><code class=\"python\">for row in curs.fetchall():\n    print row\n</code></pre>\n<p>It can be convenient to use this to create a Python list containing the values returned:</p>\n<pre><code class=\"python\">curs.execute('select first_name from people')\nnames = [row[0] for row in curs.fetchall()]\n</code></pre>\n<p>This can be useful for smaller result sets, but can have bad side effects if the result set is large.</p>\n<ul>\n<li><p>You have to wait for the entire result set to be returned to\nyour client process.</p></li>\n<li><p>You may eat up a lot of memory in your client to hold\nthe built-up list.</p></li>\n<li><p>It may take a while for Python to construct and deconstruct the\nlist which you are going to immediately discard anyways.</p></li>\n</ul>\n<hr/>\n<p>If you know there's a single row being returned in the result set you can call <code>fetchone()</code> to get the single row.</p>\n<pre><code class=\"python\">curs.execute('select max(x) from t')\nmaxValue = curs.fetchone()[0]\n</code></pre>\n<hr/>\n<p>Finally, you can loop over the result set fetching one row at a time.  In general, there's no particular advantage in doing this over using the iterator.</p>\n<pre><code class=\"python\">row = curs.fetchone()\nwhile row:\n    print row\n    row = curs.fetchone()\n</code></pre>\n", "abstract": "The canonical way is to use the built-in cursor iterator. You can use fetchall() to get all rows at once. It can be convenient to use this to create a Python list containing the values returned: This can be useful for smaller result sets, but can have bad side effects if the result set is large. You have to wait for the entire result set to be returned to\nyour client process. You may eat up a lot of memory in your client to hold\nthe built-up list. It may take a while for Python to construct and deconstruct the\nlist which you are going to immediately discard anyways. If you know there's a single row being returned in the result set you can call fetchone() to get the single row. Finally, you can loop over the result set fetching one row at a time.  In general, there's no particular advantage in doing this over using the iterator."}, {"id": 125140, "score": 27, "vote": 0, "content": "<p>My preferred way is the cursor iterator, but setting first the arraysize property of the cursor. </p>\n<pre><code class=\"python\">curs.execute('select * from people')\ncurs.arraysize = 256\nfor row in curs:\n    print row\n</code></pre>\n<p>In this example, cx_Oracle will fetch rows from Oracle 256 rows at a time, reducing the number of network round trips that need to be performed</p>\n", "abstract": "My preferred way is the cursor iterator, but setting first the arraysize property of the cursor.  In this example, cx_Oracle will fetch rows from Oracle 256 rows at a time, reducing the number of network round trips that need to be performed"}, {"id": 25213, "score": 6, "vote": 0, "content": "<p>There's also the way <code>psyco-pg</code> seems to do it... From what I gather, it seems to create dictionary-like row-proxies to map key lookup into the memory block returned by the query. In that case, fetching the whole answer and working with a similar proxy-factory over the rows seems like useful idea. Come to think of it though, it feels more like Lua than Python.</p>\n<p>Also, this should be applicable to all <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP-249 DBAPI2.0</a> interfaces, not just Oracle, or did you mean just <em>fastest</em> using <em>Oracle</em>?</p>\n", "abstract": "There's also the way psyco-pg seems to do it... From what I gather, it seems to create dictionary-like row-proxies to map key lookup into the memory block returned by the query. In that case, fetching the whole answer and working with a similar proxy-factory over the rows seems like useful idea. Come to think of it though, it feels more like Lua than Python. Also, this should be applicable to all PEP-249 DBAPI2.0 interfaces, not just Oracle, or did you mean just fastest using Oracle?"}]}, {"link": "https://stackoverflow.com/questions/1281875/making-sure-that-psycopg2-database-connection-alive", "question": {"id": "1281875", "title": "Making sure that psycopg2 database connection alive", "content": "<p>I have a python application that opens a database connection that can hang online for an hours, but sometimes the database server reboots and while python still have the connection it won't work with <code>OperationalError</code> exception.</p>\n<p>So I'm looking for any reliable method to \"ping\" the database and know that connection is alive. I've checked a psycopg2 documentation but can't find anything like that. Sure I can issue some simple SQL statement like <code>SELECT 1</code> and catch the exception, but I hope there is a native method, something like PHP <a href=\"http://us2.php.net/manual/en/function.pg-connection-status.php\" rel=\"noreferrer\">pg_connection_status</a></p>\n<p>Thanks.</p>\n", "abstract": "I have a python application that opens a database connection that can hang online for an hours, but sometimes the database server reboots and while python still have the connection it won't work with OperationalError exception. So I'm looking for any reliable method to \"ping\" the database and know that connection is alive. I've checked a psycopg2 documentation but can't find anything like that. Sure I can issue some simple SQL statement like SELECT 1 and catch the exception, but I hope there is a native method, something like PHP pg_connection_status Thanks."}, "answers": [{"id": 18708605, "score": 75, "vote": 0, "content": "<p>This question is really old, but still pops up on Google searches so I think it's valuable to know that the <code>psycopg2.connection</code> instance now has a <a href=\"http://initd.org/psycopg/docs/connection.html#connection.closed\" rel=\"noreferrer\"><code>closed</code> attribute</a> that will be <code>0</code> when the connection is open, and greater than zero when the connection is closed. The following example should demonstrate:</p>\n<pre><code class=\"python\">import psycopg2\nimport subprocess\n\nconnection = psycopg2.connect(\n    dbname=database,\n    user=username,\n    password=password,\n    host=host,\n    port=port\n)\n\nprint connection.closed # 0\n\n# restart the db externally\nsubprocess.check_call(\"sudo /etc/init.d/postgresql restart\", shell=True)\n\n# this query will fail because the db is no longer connected\ntry:\n    cur = connection.cursor()\n    cur.execute('SELECT 1')\nexcept psycopg2.OperationalError:\n    pass\n\nprint connection.closed # 2\n</code></pre>\n", "abstract": "This question is really old, but still pops up on Google searches so I think it's valuable to know that the psycopg2.connection instance now has a closed attribute that will be 0 when the connection is open, and greater than zero when the connection is closed. The following example should demonstrate:"}, {"id": 1282019, "score": 27, "vote": 0, "content": "<p><code>pg_connection_status</code> is implemented using PQstatus. psycopg doesn't expose that API, so the check is not available. The only two places psycopg calls PQstatus itself is when a new connection is made, and at the beginning of execute. So yes, you will need to issue a simple SQL statement to find out whether the connection is still there.</p>\n", "abstract": "pg_connection_status is implemented using PQstatus. psycopg doesn't expose that API, so the check is not available. The only two places psycopg calls PQstatus itself is when a new connection is made, and at the beginning of execute. So yes, you will need to issue a simple SQL statement to find out whether the connection is still there."}, {"id": 20090982, "score": 19, "vote": 0, "content": "<p><code>connection.closed</code> does not reflect a connection closed/severed by the server. It only indicates a connection closed by the client using <code>connection.close()</code></p>\n<p>In order to make sure a connection is still valid, read the property <code>connection.isolation_level</code>. This will raise an OperationalError with pgcode == \"57P01\" in case the connection is dead. </p>\n<p>This adds a bit of latency for a roundtrip to the database but should be preferable to a <code>SELECT 1</code> or similar.</p>\n<pre><code class=\"python\">import psycopg2\ndsn = \"dbname=postgres\"\nconn = psycopg2.connect(dsn)\n\n# ... some time elapses, e.g. connection within a connection pool\n\ntry:\n    connection.isolation_level\nexcept OperationalError as oe:\n    conn = psycopg2.connect(dsn)\n\nc = conn.cursor()\nc.execute(\"SELECT 1\")\n</code></pre>\n", "abstract": "connection.closed does not reflect a connection closed/severed by the server. It only indicates a connection closed by the client using connection.close() In order to make sure a connection is still valid, read the property connection.isolation_level. This will raise an OperationalError with pgcode == \"57P01\" in case the connection is dead.  This adds a bit of latency for a roundtrip to the database but should be preferable to a SELECT 1 or similar."}, {"id": 61185075, "score": 3, "vote": 0, "content": "<p>howto check if connection closed:  </p>\n<ul>\n<li><p>conn.closed is 1 if closed else 0  </p></li>\n<li><p>if closed it raises <code>except psycopg2.InterfaceError as exc:</code> not only on query but in context manager: <code>with conn:</code> is sufficient for raise.  </p></li>\n<li><p>you then need to reestablish the connection. eg read out the pw and put into <code>.connect(..)</code></p></li>\n</ul>\n", "abstract": "howto check if connection closed:   conn.closed is 1 if closed else 0   if closed it raises except psycopg2.InterfaceError as exc: not only on query but in context manager: with conn: is sufficient for raise.   you then need to reestablish the connection. eg read out the pw and put into .connect(..)"}]}, {"link": "https://stackoverflow.com/questions/764710/sqlite-performance-benchmark-why-is-memory-so-slow-only-1-5x-as-fast-as-d", "question": {"id": "764710", "title": "SQLite Performance Benchmark -- why is :memory: so slow...only 1.5X as fast as disk?", "content": "<h2>Why is :memory: in sqlite so slow?</h2>\n<p>I've been trying to see if there are any performance improvements gained by using in-memory sqlite vs. disk based sqlite. Basically I'd like to trade startup time and memory to get extremely rapid queries which do <em>not</em> hit disk during the course of the application. </p>\n<p>However, the following benchmark gives me only a factor of 1.5X in improved speed. Here, I'm generating 1M rows of random data and loading it into both a disk and memory based version of the same table. I then run random queries on both dbs, returning sets of size approx 300k. I expected the memory based version to be considerably faster, but as mentioned I'm only getting 1.5X speedups. </p>\n<p>I experimented with several other sizes of dbs and query sets; the advantage of :memory: <em>does</em> seem to go up as the number of rows in the db increases. I'm not sure why the advantage is so small, though I had a few hypotheses: </p>\n<ul>\n<li>the table used isn't big enough (in rows) to make :memory: a huge winner</li>\n<li>more joins/tables would make the :memory: advantage more apparent</li>\n<li>there is some kind of caching going on at the connection or OS level such that the previous results are accessible somehow, corrupting the benchmark</li>\n<li>there is some kind of hidden disk access going on that I'm not seeing (I haven't tried lsof yet, but I did turn off the PRAGMAs for journaling)</li>\n</ul>\n<p>Am I doing something wrong here? Any thoughts on why :memory: isn't producing nearly instant lookups? Here's the benchmark: </p>\n<pre><code class=\"python\">==&gt; sqlite_memory_vs_disk_benchmark.py &lt;==\n\n#!/usr/bin/env python\n\"\"\"Attempt to see whether :memory: offers significant performance benefits.\n\n\"\"\"\nimport os\nimport time\nimport sqlite3\nimport numpy as np\n\ndef load_mat(conn,mat):\n    c = conn.cursor()\n\n    #Try to avoid hitting disk, trading safety for speed.\n    #http://stackoverflow.com/questions/304393\n    c.execute('PRAGMA temp_store=MEMORY;')\n    c.execute('PRAGMA journal_mode=MEMORY;')\n\n    # Make a demo table\n    c.execute('create table if not exists demo (id1 int, id2 int, val real);')\n    c.execute('create index id1_index on demo (id1);')\n    c.execute('create index id2_index on demo (id2);')\n    for row in mat:\n        c.execute('insert into demo values(?,?,?);', (row[0],row[1],row[2]))\n    conn.commit()\n\ndef querytime(conn,query):\n    start = time.time()\n    foo = conn.execute(query).fetchall()\n    diff = time.time() - start\n    return diff\n\n#1) Build some fake data with 3 columns: int, int, float\nnn   = 1000000 #numrows\ncmax = 700    #num uniques in 1st col\ngmax = 5000   #num uniques in 2nd col\n\nmat = np.zeros((nn,3),dtype='object')\nmat[:,0] = np.random.randint(0,cmax,nn)\nmat[:,1] = np.random.randint(0,gmax,nn)\nmat[:,2] = np.random.uniform(0,1,nn)\n\n#2) Load it into both dbs &amp; build indices\ntry: os.unlink('foo.sqlite')\nexcept OSError: pass\n\nconn_mem = sqlite3.connect(\":memory:\")\nconn_disk = sqlite3.connect('foo.sqlite')\nload_mat(conn_mem,mat)\nload_mat(conn_disk,mat)\ndel mat\n\n#3) Execute a series of random queries and see how long it takes each of these\nnumqs = 10\nnumqrows = 300000 #max number of ids of each kind\nresults = np.zeros((numqs,3))\nfor qq in range(numqs):\n    qsize = np.random.randint(1,numqrows,1)\n    id1a = np.sort(np.random.permutation(np.arange(cmax))[0:qsize]) #ensure uniqueness of ids queried\n    id2a = np.sort(np.random.permutation(np.arange(gmax))[0:qsize])\n    id1s = ','.join([str(xx) for xx in id1a])\n    id2s = ','.join([str(xx) for xx in id2a])\n    query = 'select * from demo where id1 in (%s) AND id2 in (%s);' % (id1s,id2s)\n\n    results[qq,0] = round(querytime(conn_disk,query),4)\n    results[qq,1] = round(querytime(conn_mem,query),4)\n    results[qq,2] = int(qsize)\n\n#4) Now look at the results\nprint \"  disk | memory | qsize\"\nprint \"-----------------------\"\nfor row in results:\n    print \"%.4f | %.4f | %d\" % (row[0],row[1],row[2])\n</code></pre>\n<p>Here's the results. Note that disk takes about 1.5X as long as memory for a fairly wide range of query sizes. </p>\n<pre><code class=\"python\">[ramanujan:~]$python -OO sqlite_memory_vs_disk_clean.py\n  disk | memory | qsize\n-----------------------\n9.0332 | 6.8100 | 12630\n9.0905 | 6.6953 | 5894\n9.0078 | 6.8384 | 17798\n9.1179 | 6.7673 | 60850\n9.0629 | 6.8355 | 94854\n8.9688 | 6.8093 | 17940\n9.0785 | 6.6993 | 58003\n9.0309 | 6.8257 | 85663\n9.1423 | 6.7411 | 66047\n9.1814 | 6.9794 | 11345\n</code></pre>\n<p>Shouldn't RAM be almost instant relative to disk? What's going wrong here? </p>\n<h2>Edit</h2>\n<p>Some good suggestions here. </p>\n<p>I guess the main takehome point for me is that **there's probably no way to make :memory: <em>absolutely faster</em>, but there is a way to make disk access <em>relatively slower.</em> ** </p>\n<p>In other words, the benchmark is adequately measuring the realistic performance of memory, but not the realistic performance of disk (e.g. because the cache_size pragma is too big or because I'm not doing writes). I'll mess around with those parameters and post my findings when I get a chance.  </p>\n<p>That said, if there is anyone who thinks I can squeeze some more speed out of the in-memory db (other than by jacking up the cache_size and default_cache_size, which I will do), I'm all ears...</p>\n", "abstract": "I've been trying to see if there are any performance improvements gained by using in-memory sqlite vs. disk based sqlite. Basically I'd like to trade startup time and memory to get extremely rapid queries which do not hit disk during the course of the application.  However, the following benchmark gives me only a factor of 1.5X in improved speed. Here, I'm generating 1M rows of random data and loading it into both a disk and memory based version of the same table. I then run random queries on both dbs, returning sets of size approx 300k. I expected the memory based version to be considerably faster, but as mentioned I'm only getting 1.5X speedups.  I experimented with several other sizes of dbs and query sets; the advantage of :memory: does seem to go up as the number of rows in the db increases. I'm not sure why the advantage is so small, though I had a few hypotheses:  Am I doing something wrong here? Any thoughts on why :memory: isn't producing nearly instant lookups? Here's the benchmark:  Here's the results. Note that disk takes about 1.5X as long as memory for a fairly wide range of query sizes.  Shouldn't RAM be almost instant relative to disk? What's going wrong here?  Some good suggestions here.  I guess the main takehome point for me is that **there's probably no way to make :memory: absolutely faster, but there is a way to make disk access relatively slower. **  In other words, the benchmark is adequately measuring the realistic performance of memory, but not the realistic performance of disk (e.g. because the cache_size pragma is too big or because I'm not doing writes). I'll mess around with those parameters and post my findings when I get a chance.   That said, if there is anyone who thinks I can squeeze some more speed out of the in-memory db (other than by jacking up the cache_size and default_cache_size, which I will do), I'm all ears..."}, "answers": [{"id": 764743, "score": 43, "vote": 0, "content": "<p>It has to do with the fact that SQLite has a page cache. According to the <a href=\"http://www.sqlite.org/compile.html\" rel=\"noreferrer\">Documentation</a>, the default page cache is 2000 1K pages or about 2Mb. Since this is about 75% to 90% of your data, it isn't surprising that the two number are very similar. My guess is that in addition to the SQLite page cache, the rest of the data is still in the OS disk cache. If you got SQLite to flush the page cache (and the disk cache) you would see some really significant differences.</p>\n", "abstract": "It has to do with the fact that SQLite has a page cache. According to the Documentation, the default page cache is 2000 1K pages or about 2Mb. Since this is about 75% to 90% of your data, it isn't surprising that the two number are very similar. My guess is that in addition to the SQLite page cache, the rest of the data is still in the OS disk cache. If you got SQLite to flush the page cache (and the disk cache) you would see some really significant differences."}, {"id": 1056148, "score": 22, "vote": 0, "content": "<p>My question to you is, What are you trying to benchmark?</p>\n<p>As already mentioned, SQLite's :memory: DB is just the same as the disk-based one, i.e. paged, and the only difference is that the pages are never written to disk. So the only difference between the two are the disk writes :memory: doesn't need to do (it also doesn't need to do any disk reads either, when a disk page had to be offloaded from the cache).</p>\n<p>But read/writes from the cache may represent only a fraction of the query processing time, depending on the query. Your query has a where clause with two large sets of ids the selected rows must be members of, which is expensive.</p>\n<p>As Cary Millsap demonstrates in his blog on optimizing Oracle (here's a representative post: <a href=\"http://carymillsap.blogspot.com/2009/06/profiling-with-my-boy.html\" rel=\"noreferrer\">http://carymillsap.blogspot.com/2009/06/profiling-with-my-boy.html</a>), you need to understand which parts of the query processing take time. Assuming the set membership tests represented 90% of the query time, and the disk-based IO 10%, going to :memory: saves only those 10%. That's an extreme example unlikely to be representative, but I hope that it illustrates that your particular query is slanting the results. Use a simpler query, and the IO parts of the query processing will increase, and thus the benefit of :memory:.</p>\n<p>As a final note, we've experimented with SQLite's virtual tables, where you are in charge of the actual storage, and by using C++ containers, which are typed unlike SQLite's way of storing cell values, we could see a significant improment in processing time over :memory:, but that's getting of topic a bit ;) --DD</p>\n<p>PS: I haven't enough Karma to comment on the most popular post of this thread,\n    so I'm commenting here :) to say that recent SQLite version don't use 1KB\n    pages by default on Windows: <a href=\"http://www.sqlite.org/changes.html#version_3_6_12\" rel=\"noreferrer\">http://www.sqlite.org/changes.html#version_3_6_12</a></p>\n", "abstract": "My question to you is, What are you trying to benchmark? As already mentioned, SQLite's :memory: DB is just the same as the disk-based one, i.e. paged, and the only difference is that the pages are never written to disk. So the only difference between the two are the disk writes :memory: doesn't need to do (it also doesn't need to do any disk reads either, when a disk page had to be offloaded from the cache). But read/writes from the cache may represent only a fraction of the query processing time, depending on the query. Your query has a where clause with two large sets of ids the selected rows must be members of, which is expensive. As Cary Millsap demonstrates in his blog on optimizing Oracle (here's a representative post: http://carymillsap.blogspot.com/2009/06/profiling-with-my-boy.html), you need to understand which parts of the query processing take time. Assuming the set membership tests represented 90% of the query time, and the disk-based IO 10%, going to :memory: saves only those 10%. That's an extreme example unlikely to be representative, but I hope that it illustrates that your particular query is slanting the results. Use a simpler query, and the IO parts of the query processing will increase, and thus the benefit of :memory:. As a final note, we've experimented with SQLite's virtual tables, where you are in charge of the actual storage, and by using C++ containers, which are typed unlike SQLite's way of storing cell values, we could see a significant improment in processing time over :memory:, but that's getting of topic a bit ;) --DD PS: I haven't enough Karma to comment on the most popular post of this thread,\n    so I'm commenting here :) to say that recent SQLite version don't use 1KB\n    pages by default on Windows: http://www.sqlite.org/changes.html#version_3_6_12"}, {"id": 765136, "score": 7, "vote": 0, "content": "<p>You're doing SELECTs, you're using memory cache. Try to interleave SELECTs with UPDATEs.</p>\n", "abstract": "You're doing SELECTs, you're using memory cache. Try to interleave SELECTs with UPDATEs."}, {"id": 790522, "score": 7, "vote": 0, "content": "<p>Memory database in SQLite is actually page cache that never touches the disk.\nSo you should forget using memory db in SQLite for performance tweaks</p>\n<p>It's possible to turn off journal, turn off sync mode, set large page cache and you will have almost the same performance on most of operations, but durability will be lost.</p>\n<p>From your code it's absolutely clear that you <strong>SHOULD REUSE the command</strong> and ONLY BIND parameters, because that took <strong>more than 90%</strong> of your test performance away.</p>\n", "abstract": "Memory database in SQLite is actually page cache that never touches the disk.\nSo you should forget using memory db in SQLite for performance tweaks It's possible to turn off journal, turn off sync mode, set large page cache and you will have almost the same performance on most of operations, but durability will be lost. From your code it's absolutely clear that you SHOULD REUSE the command and ONLY BIND parameters, because that took more than 90% of your test performance away."}, {"id": 10404068, "score": 6, "vote": 0, "content": "<p>Thank you for the code. I have tested in on 2 x XEON 2690 with 192GB RAM with 4 SCSI 15k hard drives in RAID 5 and the results are:</p>\n<pre><code class=\"python\">  disk | memory | qsize\n-----------------------\n6.3590 | 2.3280 | 15713\n6.6250 | 2.3690 | 8914\n6.0040 | 2.3260 | 225168\n6.0210 | 2.4080 | 132388\n6.1400 | 2.4050 | 264038\n</code></pre>\n<p>The speed-increase in memory is significant.</p>\n", "abstract": "Thank you for the code. I have tested in on 2 x XEON 2690 with 192GB RAM with 4 SCSI 15k hard drives in RAID 5 and the results are: The speed-increase in memory is significant."}, {"id": 764718, "score": 1, "vote": 0, "content": "<p>Could it be that sqlite3 isnt actually writing your data to disk from cache? which might explain why the numbers are similar.</p>\n<p>It could also be possible that your OS is paging due to low memory avaliable? </p>\n", "abstract": "Could it be that sqlite3 isnt actually writing your data to disk from cache? which might explain why the numbers are similar. It could also be possible that your OS is paging due to low memory avaliable? "}, {"id": 764719, "score": 1, "vote": 0, "content": "<p>I note you are focussing on queries that involve relatively large data sets to return. I wonder what effect you would see with smaller sets of data? To return a single row many times might require the disk to seek a lot - the random access time for memory might be much faster.</p>\n", "abstract": "I note you are focussing on queries that involve relatively large data sets to return. I wonder what effect you would see with smaller sets of data? To return a single row many times might require the disk to seek a lot - the random access time for memory might be much faster."}, {"id": 7851376, "score": 1, "vote": 0, "content": "<p>numpy arrays are slower than dict and tuple and other object sequences until you are dealing with 5 million or more objects in a sequence. You can significantly improve the speed of processing massive amounts of data by iterating over it and using generators to avoid creating and recreating temporary large objects.</p>\n<p>numpy has become your limiting factor as it is designed to deliver linear performance. It is not a star with small or even large amounts of data. But numpy's performance does not turn into a curve as the data set grows. It remains a straight line.</p>\n<p>Besides SQLite is just a really fast database. Faster even than most server databases. It begs the question of why anyone would use NOSQL databases when a light weight super fast fault tolerant database that uses SQL has been around and put to the test in everything from browsers to mobile phones for years.</p>\n", "abstract": "numpy arrays are slower than dict and tuple and other object sequences until you are dealing with 5 million or more objects in a sequence. You can significantly improve the speed of processing massive amounts of data by iterating over it and using generators to avoid creating and recreating temporary large objects. numpy has become your limiting factor as it is designed to deliver linear performance. It is not a star with small or even large amounts of data. But numpy's performance does not turn into a curve as the data set grows. It remains a straight line. Besides SQLite is just a really fast database. Faster even than most server databases. It begs the question of why anyone would use NOSQL databases when a light weight super fast fault tolerant database that uses SQL has been around and put to the test in everything from browsers to mobile phones for years."}]}, {"link": "https://stackoverflow.com/questions/38610723/how-to-insert-a-pandas-dataframe-to-an-already-existing-table-in-a-database", "question": {"id": "38610723", "title": "How to insert a pandas dataframe to an already existing table in a database?", "content": "<p>I'm using <code>sqlalchemy</code> in pandas to query postgres database and then insert results of a transformation to another table on the same database. But when I do \n<code>df.to_sql('db_table2', engine)</code> I get this error message:\n<code>ValueError: Table 'db_table2' already exists.</code> I noticed it want to create a new table. How to insert pandas dataframe to an already existing table ? </p>\n<pre><code class=\"python\">df = pd.read_sql_query('select * from \"db_table1\"',con=engine)\n#do transformation then save df to db_table2\ndf.to_sql('db_table2', engine)\n\nValueError: Table 'db_table2' already exists\n</code></pre>\n", "abstract": "I'm using sqlalchemy in pandas to query postgres database and then insert results of a transformation to another table on the same database. But when I do \ndf.to_sql('db_table2', engine) I get this error message:\nValueError: Table 'db_table2' already exists. I noticed it want to create a new table. How to insert pandas dataframe to an already existing table ? "}, "answers": [{"id": 38610751, "score": 76, "vote": 0, "content": "<p>make use of <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">if_exists</a> parameter:</p>\n<pre><code class=\"python\">df.to_sql('db_table2', engine, if_exists='replace')\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">df.to_sql('db_table2', engine, if_exists='append')\n</code></pre>\n<p>from docstring:</p>\n<pre><code class=\"python\">\"\"\"\nif_exists : {'fail', 'replace', 'append'}, default 'fail'\n    - fail: If table exists, do nothing.\n    - replace: If table exists, drop it, recreate it, and insert data.\n    - append: If table exists, insert data. Create if does not exist.\n\"\"\"\n</code></pre>\n", "abstract": "make use of if_exists parameter: or from docstring:"}, {"id": 59464788, "score": 25, "vote": 0, "content": "<blockquote>\n<p><strong>Zen of Python:</strong></p>\n<p><em>Explicit is better than implicit.</em></p>\n</blockquote>\n<pre><code class=\"python\">df.to_sql(\n    name,# Name of SQL table.\n    con, # sqlalchemy.engine.Engine or sqlite3.Connection\n    schema=None, # Something can't understand yet. just keep it.\n    if_exists='fail', # How to behave if the table already exists. You can use 'replace', 'append' to replace it.\n    index=True, # It means index of DataFrame will save. Set False to ignore the index of DataFrame.\n    index_label=None, # Depend on index. \n    chunksize=None, # Just means chunksize. If DataFrame is big will need this parameter.\n    dtype=None, # Set the columns type of sql table.\n    method=None, # Unstable. Ignore it.\n)\n</code></pre>\n<p>So, I recommend this example, normally:</p>\n<pre><code class=\"python\">df.to_sql(con=engine, name='table_name',if_exists='append', dtype={\n    'Column1': String(255),\n    'Column2': FLOAT,\n    'Column3': INT,\n    'createTime': DATETIME},index=False)\n</code></pre>\n<p>Set the sql table Primary Key manually(like: Id) and check increment in Navicat or MySQL Workbench.</p>\n<p><strong>The Id will increment automatically.</strong>\n<a href=\"https://i.stack.imgur.com/iJ87k.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/iJ87k.jpg\"/></a></p>\n<blockquote>\n<p><strong><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">The Docstring of df.to_sql</a>:</strong></p>\n</blockquote>\n<pre><code class=\"python\">Parameters\n----------\nname : string\n    Name of SQL table.\ncon : sqlalchemy.engine.Engine or sqlite3.Connection\n    Using SQLAlchemy makes it possible to use any DB supported by that\n    library. Legacy support is provided for sqlite3.Connection objects.\nschema : string, optional\n    Specify the schema (if database flavor supports this). If None, use\n    default schema.\nif_exists : {'fail', 'replace', 'append'}, default 'fail'\n    How to behave if the table already exists.\n\n    * fail: Raise a ValueError.\n    * replace: Drop the table before inserting new values.\n    * append: Insert new values to the existing table.\n\nindex : bool, default True\n    Write DataFrame index as a column. Uses `index_label` as the column\n    name in the table.\nindex_label : string or sequence, default None\n    Column label for index column(s). If None is given (default) and\n    `index` is True, then the index names are used.\n    A sequence should be given if the DataFrame uses MultiIndex.\nchunksize : int, optional\n    Rows will be written in batches of this size at a time. By default,\n    all rows will be written at once.\ndtype : dict, optional\n    Specifying the datatype for columns. The keys should be the column\n    names and the values should be the SQLAlchemy types or strings for\n    the sqlite3 legacy mode.\nmethod : {None, 'multi', callable}, default None\n    Controls the SQL insertion clause used:\n\n    * None : Uses standard SQL ``INSERT`` clause (one per row).\n    * 'multi': Pass multiple values in a single ``INSERT`` clause.\n    * callable with signature ``(pd_table, conn, keys, data_iter)``.\n\n    Details and a sample callable implementation can be found in the\n    section :ref:`insert method &lt;io.sql.method&gt;`.\n\n    .. versionadded:: 0.24.0\n</code></pre>\n<p>That's all. </p>\n", "abstract": "Zen of Python: Explicit is better than implicit. So, I recommend this example, normally: Set the sql table Primary Key manually(like: Id) and check increment in Navicat or MySQL Workbench. The Id will increment automatically.\n The Docstring of df.to_sql: That's all. "}]}, {"link": "https://stackoverflow.com/questions/2314307/python-logging-to-database", "question": {"id": "2314307", "title": "python logging to database", "content": "<p>I'm seeking a way to let the python logger module to log to database and falls back to file system when the db is down.</p>\n<p>So basically 2 things: How to let the logger log to database and how to make it fall to file logging when the db is down.</p>\n", "abstract": "I'm seeking a way to let the python logger module to log to database and falls back to file system when the db is down. So basically 2 things: How to let the logger log to database and how to make it fall to file logging when the db is down."}, "answers": [{"id": 43843623, "score": 36, "vote": 0, "content": "<p>I recently managed to write my own database logger in Python. Since I couldn't find any example I thought I post mine here. Works with MS SQL.</p>\n<p>Database table could look like this:</p>\n<pre><code class=\"python\">CREATE TABLE [db_name].[log](\n    [id] [bigint] IDENTITY(1,1) NOT NULL,\n    [log_level] [int] NULL,\n    [log_levelname] [char](32) NULL,\n    [log] [char](2048) NOT NULL,\n    [created_at] [datetime2](7) NOT NULL,\n    [created_by] [char](32) NOT NULL,\n) ON [PRIMARY]\n</code></pre>\n<p>The class itself:</p>\n<pre><code class=\"python\">class LogDBHandler(logging.Handler):\n    '''\n    Customized logging handler that puts logs to the database.\n    pymssql required\n    '''\n    def __init__(self, sql_conn, sql_cursor, db_tbl_log):\n        logging.Handler.__init__(self)\n        self.sql_cursor = sql_cursor\n        self.sql_conn = sql_conn\n        self.db_tbl_log = db_tbl_log\n\n    def emit(self, record):\n        # Set current time\n        tm = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(record.created))\n        # Clear the log message so it can be put to db via sql (escape quotes)\n        self.log_msg = record.msg\n        self.log_msg = self.log_msg.strip()\n        self.log_msg = self.log_msg.replace('\\'', '\\'\\'')\n        # Make the SQL insert\n        sql = 'INSERT INTO ' + self.db_tbl_log + ' (log_level, ' + \\\n            'log_levelname, log, created_at, created_by) ' + \\\n            'VALUES (' + \\\n            ''   + str(record.levelno) + ', ' + \\\n            '\\'' + str(record.levelname) + '\\', ' + \\\n            '\\'' + str(self.log_msg) + '\\', ' + \\\n            '(convert(datetime2(7), \\'' + tm + '\\')), ' + \\\n            '\\'' + str(record.name) + '\\')'\n        try:\n            self.sql_cursor.execute(sql)\n            self.sql_conn.commit()\n        # If error - print it out on screen. Since DB is not working - there's\n        # no point making a log about it to the database :)\n        except pymssql.Error as e:\n            print sql\n            print 'CRITICAL DB ERROR! Logging to database not possible!'\n</code></pre>\n<p>And usage example:</p>\n<pre><code class=\"python\">import pymssql\nimport time\nimport logging\n\ndb_server = 'servername'\ndb_user = 'db_user'\ndb_password = 'db_pass'\ndb_dbname = 'db_name'\ndb_tbl_log = 'log'\n\nlog_file_path = 'C:\\\\Users\\\\Yourname\\\\Desktop\\\\test_log.txt'\nlog_error_level     = 'DEBUG'       # LOG error level (file)\nlog_to_db = True                    # LOG to database?\n\nclass LogDBHandler(logging.Handler):\n    [...]\n\n# Main settings for the database logging use\nif (log_to_db):\n    # Make the connection to database for the logger\n    log_conn = pymssql.connect(db_server, db_user, db_password, db_dbname, 30)\n    log_cursor = log_conn.cursor()\n    logdb = LogDBHandler(log_conn, log_cursor, db_tbl_log)\n\n# Set logger\nlogging.basicConfig(filename=log_file_path)\n\n# Set db handler for root logger\nif (log_to_db):\n    logging.getLogger('').addHandler(logdb)\n# Register MY_LOGGER\nlog = logging.getLogger('MY_LOGGER')\nlog.setLevel(log_error_level)\n\n# Example variable\ntest_var = 'This is test message'\n\n# Log the variable contents as an error\nlog.error('This error occurred: %s' % test_var)\n</code></pre>\n<p>Above will log both to the database and to the file. If file is not needed - skip the 'logging.basicConfig(filename=log_file_path)' line. Everything logged using 'log' - will be logged as MY_LOGGER. If some external error appears (i.e. in the module imported or something) - error will appear as 'root', since 'root' logger is also active, and is using the database handler.</p>\n", "abstract": "I recently managed to write my own database logger in Python. Since I couldn't find any example I thought I post mine here. Works with MS SQL. Database table could look like this: The class itself: And usage example: Above will log both to the database and to the file. If file is not needed - skip the 'logging.basicConfig(filename=log_file_path)' line. Everything logged using 'log' - will be logged as MY_LOGGER. If some external error appears (i.e. in the module imported or something) - error will appear as 'root', since 'root' logger is also active, and is using the database handler."}, {"id": 2314325, "score": 18, "vote": 0, "content": "<p>Write yourself a <strong><a href=\"http://docs.python.org/library/logging.html#handlers\" rel=\"noreferrer\">handler</a></strong> that directs the logs to the database in question. When it fails, you can remove it from the handler list of the logger.  There are many ways to deal with the failure-modes.</p>\n", "abstract": "Write yourself a handler that directs the logs to the database in question. When it fails, you can remove it from the handler list of the logger.  There are many ways to deal with the failure-modes."}, {"id": 67305494, "score": 6, "vote": 0, "content": "<h2>Python logging to a database with a backup logger</h2>\n<hr/>\n<h2>Problem</h2>\n<p>I had the same problem when I ran a Django project inside the server since sometimes you need to check the logs remotely.</p>\n<hr/>\n<h2>Solution</h2>\n<p>First, there is a need for a handler for the logger to insert logs in to the database. Before that and since my SQL is not good, an ORM is needed that I choose <a href=\"https://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a>.</p>\n<p>model:</p>\n<pre><code class=\"python\"># models.py\nfrom sqlalchemy import Column, Integer, String, DateTime, Text\nfrom sqlalchemy.ext.declarative import declarative_base\nimport datetime\n\nbase = declarative_base()\n\n\nclass Log(base):\n    __tablename__ = \"log\"\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    time = Column(DateTime, nullable=False, default=datetime.datetime.now)\n    level_name = Column(String(10), nullable=True)\n    module = Column(String(200), nullable=True)\n    thread_name = Column(String(200), nullable=True)\n    file_name = Column(String(200), nullable=True)\n    func_name = Column(String(200), nullable=True)\n    line_no = Column(Integer, nullable=True)\n    process_name = Column(String(200), nullable=True)\n    message = Column(Text)\n    last_line = Column(Text)\n</code></pre>\n<p>This is the crud for insertion into the database:</p>\n<pre><code class=\"python\">#crud.py\nimport sqlalchemy\nfrom .models import base\nfrom traceback import print_exc\n\n\nclass Crud:\n    def __init__(self, connection_string=f'sqlite:///log_db.sqlite3',\n                 encoding='utf-8',\n                 pool_size=10,\n                 max_overflow=20,\n                 pool_recycle=3600):\n\n        self.connection_string = connection_string\n        self.encoding = encoding\n        self.pool_size = pool_size\n        self.max_overflow = max_overflow\n        self.pool_recycle = pool_recycle\n        self.engine = None\n        self.session = None\n\n    def initiate(self):\n        self.create_engine()\n        self.create_session()\n        self.create_tables()\n\n    def create_engine(self):\n        self.engine = sqlalchemy.create_engine(self.connection_string)\n\n    def create_session(self):\n        self.session = sqlalchemy.orm.Session(bind=self.engine)\n\n    def create_tables(self):\n        base.metadata.create_all(self.engine)\n\n    def insert(self, instances):\n        try:\n            self.session.add(instances)\n            self.session.commit()\n            self.session.flush()\n        except:\n            self.session.rollback()\n            raise\n\n    def __del__(self):\n        self.close_session()\n        self.close_all_connections()\n\n    def close_session(self):\n        try:\n            self.session.close()\n        except:\n            print_exc()\n        else:\n            self.session = None\n\n    def close_all_connections(self):\n        try:\n            self.engine.dispose()\n        except:\n            print_exc()\n        else:\n            self.engine = None\n</code></pre>\n<p>The handler:</p>\n<pre><code class=\"python\"># handler.py\nfrom logging import Handler, getLogger\nfrom traceback import print_exc\nfrom .crud import Crud\nfrom .models import Log\n\n\nmy_crud = Crud(\n    connection_string=&lt;connection string to reach your db&gt;,\n    encoding='utf-8',\n    pool_size=10,\n    max_overflow=20,\n    pool_recycle=3600)\n\nmy_crud.initiate()\n\n\nclass DBHandler(Handler):\n    backup_logger = None\n\n    def __init__(self, level=0, backup_logger_name=None):\n        super().__init__(level)\n        if backup_logger_name:\n            self.backup_logger = getLogger(backup_logger_name)\n\n    def emit(self, record):\n        try:\n            message = self.format(record)\n            try:\n                last_line = message.rsplit('\\n', 1)[-1]\n            except:\n                last_line = None\n\n            try:\n                new_log = Log(module=record.module,\n                              thread_name=record.threadName,\n                              file_name=record.filename,\n                              func_name=record.funcName,\n                              level_name=record.levelname,\n                              line_no=record.lineno,\n                              process_name=record.processName,\n                              message=message,\n                              last_line=last_line)\n                # raise\n\n                my_crud.insert(instances=new_log)\n            except:\n                if self.backup_logger:\n                    try:\n                        getattr(self.backup_logger, record.levelname.lower())(record.message)\n                    except:\n                        print_exc()\n                else:\n                    print_exc()\n\n        except:\n            print_exc()\n</code></pre>\n<p>Test to check the logger:</p>\n<pre><code class=\"python\"># test.py\nfrom logging import basicConfig, getLogger, DEBUG, FileHandler, Formatter\nfrom .handlers import DBHandler\n\nbasicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n            datefmt='%d-%b-%y %H:%M:%S',\n            level=DEBUG)\nformat = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\nbackup_logger = getLogger('backup_logger')\nfile_handler = FileHandler('file.log')\nfile_handler.setLevel(DEBUG)\nfile_handler.setFormatter(format)\nbackup_logger.addHandler(file_handler)\n\ndb_logger = getLogger('logger')\ndb_handler = DBHandler(backup_logger_name='backup_logger')\ndb_handler.setLevel(DEBUG)\ndb_handler.setFormatter(format)\ndb_logger.addHandler(db_handler)\n\nif __name__ == \"__main__\":\n    db_logger.debug('debug: hello world!')\n    db_logger.info('info: hello world!')\n    db_logger.warning('warning: hello world!')\n    db_logger.error('error: hello world!')\n    db_logger.critical('critical: hello world!!!!')\n</code></pre>\n<hr/>\n<p>You can see the handler accepts a backup logger that can use it when the database insertion fails.</p>\n<p>A good improvement can be logging into the database by threading.</p>\n", "abstract": "I had the same problem when I ran a Django project inside the server since sometimes you need to check the logs remotely. First, there is a need for a handler for the logger to insert logs in to the database. Before that and since my SQL is not good, an ORM is needed that I choose SQLAlchemy. model: This is the crud for insertion into the database: The handler: Test to check the logger: You can see the handler accepts a backup logger that can use it when the database insertion fails. A good improvement can be logging into the database by threading."}, {"id": 46617404, "score": 3, "vote": 0, "content": "<p>I am digging this out again.</p>\n<p>There is a solution with SqlAlchemy (<em>Pyramid is NOT required for this recipe</em>):</p>\n<p><a href=\"https://docs.pylonsproject.org/projects/pyramid-cookbook/en/latest/logging/sqlalchemy_logger.html\" rel=\"nofollow noreferrer\">https://docs.pylonsproject.org/projects/pyramid-cookbook/en/latest/logging/sqlalchemy_logger.html</a></p>\n<p>And you could improve logging by adding extra fields, here is a guide: <a href=\"https://stackoverflow.com/a/17558764/1115187\">https://stackoverflow.com/a/17558764/1115187</a></p>\n<h2>Fallback to FS</h2>\n<p>Not sure that this is 100% correct, but you could have 2 handlers:</p>\n<ol>\n<li>database handler (write to DB)</li>\n<li>file handler (write to file or stream)</li>\n</ol>\n<p>Just wrap the DB-commit with a <code>try-except</code>. But be aware: the file will contain ALL log entries, but not only entries for which DB saving was failed.</p>\n", "abstract": "I am digging this out again. There is a solution with SqlAlchemy (Pyramid is NOT required for this recipe): https://docs.pylonsproject.org/projects/pyramid-cookbook/en/latest/logging/sqlalchemy_logger.html And you could improve logging by adding extra fields, here is a guide: https://stackoverflow.com/a/17558764/1115187 Not sure that this is 100% correct, but you could have 2 handlers: Just wrap the DB-commit with a try-except. But be aware: the file will contain ALL log entries, but not only entries for which DB saving was failed."}, {"id": 46617613, "score": 2, "vote": 0, "content": "<p>Old question, but dropping this for others. If you want to use python logging, you can add two handlers. One for writing to file, a rotating file handler. This is robust, and can be done regardless if the dB is up or not. \nThe other one can write to another service/module, like a pymongo integration. </p>\n<p>Look up logging.config on how to setup your handlers from code or json. </p>\n", "abstract": "Old question, but dropping this for others. If you want to use python logging, you can add two handlers. One for writing to file, a rotating file handler. This is robust, and can be done regardless if the dB is up or not. \nThe other one can write to another service/module, like a pymongo integration.  Look up logging.config on how to setup your handlers from code or json. "}]}, {"link": "https://stackoverflow.com/questions/4245438/is-there-any-nosql-flat-file-database-just-as-sqlite", "question": {"id": "4245438", "title": "Is there any nosql flat file database just as sqlite?", "content": "<p>Short Question:\nIs there any nosql flat-file database available as sqlite?</p>\n<p>Explanation:\nFlat file database can be opened in different processes to read, and keep one process to write. I think its perfect for read cache if there's no strict consistent needed. Say 1-2 secs write to the file or even memory block and the readers get updated data after that.</p>\n<p>So I almost choose to use sqlite, as my python server read cache. But there's still one problem. I don't like to rewrite sqls again in another place and construct another copy of my data tables in sqlite just as the same as I did in PostgreSql which used as back-end database.</p>\n<p>so is there any other choice?thanks!</p>\n", "abstract": "Short Question:\nIs there any nosql flat-file database available as sqlite? Explanation:\nFlat file database can be opened in different processes to read, and keep one process to write. I think its perfect for read cache if there's no strict consistent needed. Say 1-2 secs write to the file or even memory block and the readers get updated data after that. So I almost choose to use sqlite, as my python server read cache. But there's still one problem. I don't like to rewrite sqls again in another place and construct another copy of my data tables in sqlite just as the same as I did in PostgreSql which used as back-end database. so is there any other choice?thanks!"}, "answers": [{"id": 4245762, "score": 19, "vote": 0, "content": "<p>Maybe <code>shelve</code>? It's basically a key-value store where you can store python objects. <a href=\"http://docs.python.org/library/shelve.html\" rel=\"noreferrer\">http://docs.python.org/library/shelve.html</a></p>\n<p>Or maybe you could just use the filesystem?</p>\n", "abstract": "Maybe shelve? It's basically a key-value store where you can store python objects. http://docs.python.org/library/shelve.html Or maybe you could just use the filesystem?"}, {"id": 4245898, "score": 8, "vote": 0, "content": "<p>BerkeleyDB is a widely-used embedded database that's been around forever (it originally derived from the database library included in BSD, hence the name) and has excellent performance characteristics for many use cases (and caching is a frequently-used one), but it does have some significant limitations.</p>\n<p>If you want to use it with Python, you'll probably want the <a href=\"http://www.jcea.es/programacion/pybsddb.htm\" rel=\"noreferrer\">externally-maintained pybsddb/<code>bsddb3</code> library</a>, not the <a href=\"http://docs.python.org/library/bsddb.html\" rel=\"noreferrer\">deprecated <code>bsddb</code> library</a> included in Python 2.x (and no longer in 3.x).</p>\n<p>It's currently owned by Oracle, but available under an open-source license. Do take careful note of licensing terms -- current versions are GPLish (and GPL-compatible), so make sure that's compatible with what you plan to do.</p>\n<p>More information:</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/BerkeleyDB\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/BerkeleyDB</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Sleepycat_License\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Sleepycat_License</a></li>\n<li><a href=\"http://www.oracle.com/us/products/database/berkeley-db/index.html\" rel=\"noreferrer\">http://www.oracle.com/us/products/database/berkeley-db/index.html</a></li>\n</ul>\n", "abstract": "BerkeleyDB is a widely-used embedded database that's been around forever (it originally derived from the database library included in BSD, hence the name) and has excellent performance characteristics for many use cases (and caching is a frequently-used one), but it does have some significant limitations. If you want to use it with Python, you'll probably want the externally-maintained pybsddb/bsddb3 library, not the deprecated bsddb library included in Python 2.x (and no longer in 3.x). It's currently owned by Oracle, but available under an open-source license. Do take careful note of licensing terms -- current versions are GPLish (and GPL-compatible), so make sure that's compatible with what you plan to do. More information:"}, {"id": 15588028, "score": 0, "vote": 0, "content": "<p>Something trivial but workable, if you are looking storage backed up key value data structure use pickled dictionary. Use cPickle for better performance if needed. </p>\n", "abstract": "Something trivial but workable, if you are looking storage backed up key value data structure use pickled dictionary. Use cPickle for better performance if needed. "}]}, {"link": "https://stackoverflow.com/questions/43757977/replacing-values-greater-than-a-number-in-pandas-dataframe", "question": {"id": "43757977", "title": "Replacing values greater than a number in pandas dataframe", "content": "<p>I have a large dataframe which looks as:</p>\n<pre><code class=\"python\">df1['A'].ix[1:3]\n2017-01-01 02:00:00    [33, 34, 39]\n2017-01-01 03:00:00    [3, 43, 9]\n</code></pre>\n<p>I want to replace each element greater than 9 with 11.</p>\n<p>So, the desired output for above example is:</p>\n<pre><code class=\"python\">df1['A'].ix[1:3]\n2017-01-01 02:00:00    [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n</code></pre>\n<p>Edit:</p>\n<p>My actual dataframe has about 20,000 rows and each row has list of size 2000.</p>\n<p>Is there a way to use <code>numpy.minimum</code> function for each row? I assume that it will be faster than <code>list comprehension</code> method? </p>\n", "abstract": "I have a large dataframe which looks as: I want to replace each element greater than 9 with 11. So, the desired output for above example is: Edit: My actual dataframe has about 20,000 rows and each row has list of size 2000. Is there a way to use numpy.minimum function for each row? I assume that it will be faster than list comprehension method? "}, "answers": [{"id": 52605328, "score": 45, "vote": 0, "content": "<p>Very simply : <code>df[df &gt; 9] = 11</code></p>\n", "abstract": "Very simply : df[df > 9] = 11"}, {"id": 43758045, "score": 41, "vote": 0, "content": "<p>You can use <code>apply</code> with <code>list comprehension</code>:</p>\n<pre><code class=\"python\">df1['A'] = df1['A'].apply(lambda x: [y if y &lt;= 9 else 11 for y in x])\nprint (df1)\n                                A\n2017-01-01 02:00:00  [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n</code></pre>\n<p>Faster solution is first convert to <code>numpy array</code> and then use <a href=\"http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.where.html\" rel=\"noreferrer\"><code>numpy.where</code></a>:</p>\n<pre><code class=\"python\">a = np.array(df1['A'].values.tolist())\nprint (a)\n[[33 34 39]\n [ 3 43  9]]\n\ndf1['A'] = np.where(a &gt; 9, 11, a).tolist()\nprint (df1)\n                                A\n2017-01-01 02:00:00  [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n</code></pre>\n", "abstract": "You can use apply with list comprehension: Faster solution is first convert to numpy array and then use numpy.where:"}, {"id": 54426197, "score": 26, "vote": 0, "content": "<p>You can use numpy indexing, accessed through the <code>.values</code> function.</p>\n<p><code>df['col'].values[df['col'].values &gt; x] = y</code></p>\n<p>where you are replacing any value greater than x with the value of y. </p>\n<p>So for the example in the question:</p>\n<p><code>df1['A'].values[df1['A'] &gt; 9] = 11</code></p>\n", "abstract": "You can use numpy indexing, accessed through the .values function. df['col'].values[df['col'].values > x] = y where you are replacing any value greater than x with the value of y.  So for the example in the question: df1['A'].values[df1['A'] > 9] = 11"}, {"id": 66837090, "score": 25, "vote": 0, "content": "<p>I know this is an old post, but pandas now supports <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html\" rel=\"noreferrer\"><code>DataFrame.where</code></a> directly. In your example:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">df.where(df &lt;= 9, 11, inplace=True)\n</code></pre>\n<p>Please note that pandas' <code>where</code> is different than <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.where.html\" rel=\"noreferrer\"><code>numpy.where</code></a>. In pandas, when the <code>condition == True</code>, the current value in the dataframe is used. When <code>condition == False</code>, the other value is taken.</p>\n<p>EDIT:</p>\n<p>You can achieve the same for just a column with <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.where.html\" rel=\"noreferrer\"><code>Series.where</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">df['A'].where(df['A'] &lt;= 9, 11, inplace=True)\n</code></pre>\n", "abstract": "I know this is an old post, but pandas now supports DataFrame.where directly. In your example: Please note that pandas' where is different than numpy.where. In pandas, when the condition == True, the current value in the dataframe is used. When condition == False, the other value is taken. EDIT: You can achieve the same for just a column with Series.where:"}, {"id": 57987931, "score": 6, "vote": 0, "content": "<p>I came for a solution to replacing each element larger than h by 1 else 0, which has the simple solution:</p>\n<pre><code class=\"python\">df = (df &gt; h) * 1\n</code></pre>\n<p>(This does not solve the OP's question as all df &lt;= h are replaced by 0.)</p>\n", "abstract": "I came for a solution to replacing each element larger than h by 1 else 0, which has the simple solution: (This does not solve the OP's question as all df <= h are replaced by 0.)"}]}, {"link": "https://stackoverflow.com/questions/11066411/mysql-error-error-1018-hy000-cant-read-dir-of-errno-13", "question": {"id": "11066411", "title": "mysql error : ERROR 1018 (HY000): Can&#39;t read dir of &#39;.&#39; (errno: 13)", "content": "<p>when i try to view the databases in mysql i get this error:</p>\n<pre><code class=\"python\">ERROR 1018 (HY000): Can't read dir of '.' (errno: 13)\n</code></pre>\n<p>And that stops my app from displaying...</p>\n<p>My django debugger says:</p>\n<pre><code class=\"python\">(2002, \"Can't connect to local MySQL server through socket '/var/lib/mysql/my_database' (13)\")\n</code></pre>\n<p>Here is my settings file :</p>\n<pre><code class=\"python\">DATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.\n    'NAME': 'my_database',                      # Or path to database file if using sqlite3.\n    'USER': 'root',                      # Not used with sqlite3.\n    'PASSWORD': '****',                  # Not used with sqlite3.\n    'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.\n    'PORT': '3306',                      # Set to empty string for default. Not used with sqlite3.\n</code></pre>\n<p>What can cause the problem?</p>\n<p>Thanks in advance</p>\n", "abstract": "when i try to view the databases in mysql i get this error: And that stops my app from displaying... My django debugger says: Here is my settings file : What can cause the problem? Thanks in advance"}, "answers": [{"id": 11066467, "score": 83, "vote": 0, "content": "<p>You need to set ownership and permissions for directory:</p>\n<pre><code class=\"python\">chown -R mysql:mysql /var/lib/mysql/ #your mysql user may have different name\nchmod -R 755 /var/lib/mysql/\n</code></pre>\n<p>Note: <code>-R</code> makes commands recursive - you may omit it, if there is no subdirs in <code>/var/lib/mysql/</code>.</p>\n", "abstract": "You need to set ownership and permissions for directory: Note: -R makes commands recursive - you may omit it, if there is no subdirs in /var/lib/mysql/."}, {"id": 34185550, "score": 37, "vote": 0, "content": "<p>This should work for Mac users:</p>\n<pre><code class=\"python\">sudo chown -R mysql:mysql /usr/local/mysql/\nsudo chmod -R 755 /usr/local/mysql/\n</code></pre>\n<p>If this doesn't work, try running <code>which mysql</code> to see where your mysql installation is located, and then replace <code>/usr/local/mysql/</code> in the command above with whatever is before the 'bin' directory.</p>\n<p>For example, on my system <code>which mysql</code> produces the following output:</p>\n<pre><code class=\"python\">/usr/local/mysql/bin/mysql\n</code></pre>\n<p>so my path is <code>/usr/local/mysql/</code></p>\n", "abstract": "This should work for Mac users: If this doesn't work, try running which mysql to see where your mysql installation is located, and then replace /usr/local/mysql/ in the command above with whatever is before the 'bin' directory. For example, on my system which mysql produces the following output: so my path is /usr/local/mysql/"}, {"id": 49298907, "score": 3, "vote": 0, "content": "<p>On CentOS/RedHat, you should do the same thing on a different path: </p>\n<pre><code class=\"python\">chown -R mysql:mysql /data/mysql/\nchmod -R 755 /data/mysql/\n</code></pre>\n", "abstract": "On CentOS/RedHat, you should do the same thing on a different path: "}, {"id": 21190704, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">chown -R mysql:mysql /var/lib/mysql/\n\nchmod -R 755 /var/lib/mysql/\n</code></pre>\n<p>I can confirm that these two chmod statements worked for me (Webmin didn't see the databases nor did show tables) but I'm not sure why I had to do this after setting up perhaps two dozen servers (Centos) with MySQL in that past few years.</p>\n", "abstract": "I can confirm that these two chmod statements worked for me (Webmin didn't see the databases nor did show tables) but I'm not sure why I had to do this after setting up perhaps two dozen servers (Centos) with MySQL in that past few years."}, {"id": 47368732, "score": 0, "vote": 0, "content": "<p>osx high sierra use the following command solves the issue:</p>\n<pre><code class=\"python\">chown -R mysql:mysql /usr/local/mysql\n</code></pre>\n", "abstract": "osx high sierra use the following command solves the issue:"}, {"id": 59613672, "score": 0, "vote": 0, "content": "<p>if you installed mariadb using homebrew you can run the following the command for OS X</p>\n<pre><code class=\"python\">sudo chown -R mysql:mysql /var/lib/var/mysql/\nsudo chmod -R 777 /usr/local/var/mysql/\n</code></pre>\n", "abstract": "if you installed mariadb using homebrew you can run the following the command for OS X"}]}, {"link": "https://stackoverflow.com/questions/9540154/which-database-engine-to-choose-for-django-app", "question": {"id": "9540154", "title": "Which database engine to choose for Django app?", "content": "<p>I'm new to Django and have only been using sqlite3 as a database engine in Django. Now one of the applications I'm working on is getting pretty big, both in terms of models' complexity and requests/second.</p>\n<p>How do database engines supported by Django compare in terms of performance? Any pitfalls in using any of them? And the last but not least, how easy is it to switch to another engine once you've used one for a while?</p>\n", "abstract": "I'm new to Django and have only been using sqlite3 as a database engine in Django. Now one of the applications I'm working on is getting pretty big, both in terms of models' complexity and requests/second. How do database engines supported by Django compare in terms of performance? Any pitfalls in using any of them? And the last but not least, how easy is it to switch to another engine once you've used one for a while?"}, "answers": [{"id": 9540312, "score": 53, "vote": 0, "content": "<p>If you are going to use a relational database, the most popular in the Django community seems to be PostgreSQL.  It's my personal favorite.  But, MongoDB seems to be getting pretty popular in the Python/Django community as well (I have never done a project with it, though).  There are a lot of successful projects out there on MySQL as well.  But, I personally prefer PostgreSQL 9.0 or 9.1.  Hope this helps.</p>\n<p>EDIT: I didn't do that great of a job with this post.  Just want to add a couple of more considerations.</p>\n<p>For the vast majority of websites, either MySQL or PostgreSQL will work fine.  Both have their strengths and weaknesses.  I suggest you google \"MySQL vs. PostgreSQL\"  There are a lot of hits for this search (at the time writing this, I get over 3,000,000).  Here are a few tips in doing your evaluation.</p>\n<ol>\n<li>Give strong preference to more recent articles.  Try to make sure you are comparing MySQL 5.5 to PostgreSQL 9.0 or 9.1.</li>\n<li>MySQL let's you choose your storage engine.  IMO, the closes Apple to Apples comparison is InnoDB to Postgres.  </li>\n<li>Keep in mind that you may not need all of the features of InnoDB or Postgres.  You should also look at some of the other Storage engines.</li>\n</ol>\n<p>Also, if you plan on using any triggers in your system, there a couple of really nasty bugs with MySQL and InnoDB related to them and ACID compliance.  Here's <a href=\"http://bugs.mysql.com/bug.php?id=61555\" rel=\"noreferrer\">the first one</a> and here is <a href=\"http://bugs.mysql.com/bug.php?id=11472\" rel=\"noreferrer\">another one</a>.  You may not need this functionality, just be aware of it.</p>\n<p>One last thing that might make a difference to you is that with PostgreSQL you can write db functions with Python.  Here is a <a href=\"http://www.postgresql.org/docs/9.1/static/plpython.html\" rel=\"noreferrer\">link to the docs</a> for this.</p>\n", "abstract": "If you are going to use a relational database, the most popular in the Django community seems to be PostgreSQL.  It's my personal favorite.  But, MongoDB seems to be getting pretty popular in the Python/Django community as well (I have never done a project with it, though).  There are a lot of successful projects out there on MySQL as well.  But, I personally prefer PostgreSQL 9.0 or 9.1.  Hope this helps. EDIT: I didn't do that great of a job with this post.  Just want to add a couple of more considerations. For the vast majority of websites, either MySQL or PostgreSQL will work fine.  Both have their strengths and weaknesses.  I suggest you google \"MySQL vs. PostgreSQL\"  There are a lot of hits for this search (at the time writing this, I get over 3,000,000).  Here are a few tips in doing your evaluation. Also, if you plan on using any triggers in your system, there a couple of really nasty bugs with MySQL and InnoDB related to them and ACID compliance.  Here's the first one and here is another one.  You may not need this functionality, just be aware of it. One last thing that might make a difference to you is that with PostgreSQL you can write db functions with Python.  Here is a link to the docs for this."}, {"id": 9540695, "score": 16, "vote": 0, "content": "<p>MySQL and Postgres are the two most common DB backends used in the Django community and have comparable performance. I would agree that Postgres is more popular in the Django community though I don't have any hard numbers to back that up. I certainly don't mean to pick on MySQL but I would say there are some common pitfalls when using MySQL with Django (or MySQL in general):</p>\n<ol>\n<li>No transaction support with MyISAM (no longer the default in 5.5.5)</li>\n<li>No millisecond support for datetimes</li>\n<li>No timezone support for datetimes</li>\n<li>Unique character fields must be less than 255 characters</li>\n<li>Default collation is case-insensitive</li>\n</ol>\n<p>There are some docs on the various features of Django which aren't supported on various DB backends: <a href=\"https://docs.djangoproject.com/en/1.3/ref/databases/\">https://docs.djangoproject.com/en/1.3/ref/databases/</a>.</p>\n", "abstract": "MySQL and Postgres are the two most common DB backends used in the Django community and have comparable performance. I would agree that Postgres is more popular in the Django community though I don't have any hard numbers to back that up. I certainly don't mean to pick on MySQL but I would say there are some common pitfalls when using MySQL with Django (or MySQL in general): There are some docs on the various features of Django which aren't supported on various DB backends: https://docs.djangoproject.com/en/1.3/ref/databases/."}, {"id": 42179889, "score": 7, "vote": 0, "content": "<p><strong>I am not sure which DB to use either but if anyone is planning to use MongoDB then, be aware that it only works for py2 and not for py3.</strong></p>\n<p>Reference Links:</p>\n<p><a href=\"https://stackoverflow.com/questions/39555701/project-setup-with-django-1-10-mongodb-and-python-3-4-3\">Project Setup with Django 1.10, mongodb and Python 3.4.3</a></p>\n<p><a href=\"https://stackoverflow.com/questions/34256041/error-while-setting-up-mongodb-with-django-using-django-mongodb-engine-on-window\">Error while setting up MongoDB with django using django mongodb engine on windows</a></p>\n<p><a href=\"https://stackoverflow.com/questions/29874628/setting-up-mongodb-django\">Setting up MongoDB + Django</a></p>\n", "abstract": "I am not sure which DB to use either but if anyone is planning to use MongoDB then, be aware that it only works for py2 and not for py3. Reference Links: Project Setup with Django 1.10, mongodb and Python 3.4.3 Error while setting up MongoDB with django using django mongodb engine on windows Setting up MongoDB + Django"}, {"id": 9540685, "score": 6, "vote": 0, "content": "<p>MySQL and PostgreSQL work best with Django. I would highly suggest that when you choose one that you change your development settings to use it while development (opposed to using sqlite3 in dev mode and a \"real\" database in prod) as there are subtle behavioral differences that can caused lots of headaches in the future.</p>\n", "abstract": "MySQL and PostgreSQL work best with Django. I would highly suggest that when you choose one that you change your development settings to use it while development (opposed to using sqlite3 in dev mode and a \"real\" database in prod) as there are subtle behavioral differences that can caused lots of headaches in the future."}]}, {"link": "https://stackoverflow.com/questions/28126140/python-sqlite3-operationalerror-no-such-table", "question": {"id": "28126140", "title": "Python sqlite3.OperationalError: no such table:", "content": "<p>I am trying to store data about pupils at a school. I've done a few tables before, such as one for passwords and Teachers which I will later bring together in one program.  </p>\n<p>I have pretty much copied the create table function from one of these and changed the values to for the Pupil's information. It works fine on the other programs but I keep getting:</p>\n<pre><code class=\"python\">sqlite3.OperationalError: no such table: PupilPremiumTable\n</code></pre>\n<p>when I try to add a pupil to the table, it occurs on the line:</p>\n<pre><code class=\"python\">cursor.execute(\"select MAX(RecordID) from PupilPremiumTable\")\n</code></pre>\n<p>I look in the folder and there is a file called <code>PupilPremiumTable.db</code> and the table has already been created before, so I don't know why it isn't working.</p>\n<p>Here is some of my code, if you need more feel free to tell me so, as I said it worked before so I have no clue why it isn't working or even what isn't working:</p>\n<pre><code class=\"python\">with sqlite3.connect(\"PupilPremiumTable.db\") as db:\n    cursor = db.cursor()\n    cursor.execute(\"select MAX(RecordID) from PupilPremiumTable\")\n    Value = cursor.fetchone()\n    Value = str('.'.join(str(x) for x in Value))\n    if Value == \"None\":\n        Value = int(0)\n    else:\n        Value = int('.'.join(str(x) for x in Value))\n    if Value == 'None,':\n        Value = 0\n    TeacherID = Value + 1\n    print(\"This RecordID is: \",RecordID)\n</code></pre>\n", "abstract": "I am trying to store data about pupils at a school. I've done a few tables before, such as one for passwords and Teachers which I will later bring together in one program.   I have pretty much copied the create table function from one of these and changed the values to for the Pupil's information. It works fine on the other programs but I keep getting: when I try to add a pupil to the table, it occurs on the line: I look in the folder and there is a file called PupilPremiumTable.db and the table has already been created before, so I don't know why it isn't working. Here is some of my code, if you need more feel free to tell me so, as I said it worked before so I have no clue why it isn't working or even what isn't working:"}, "answers": [{"id": 28126276, "score": 82, "vote": 0, "content": "<p>You are assuming that the current working directory is the same as the directory your script lives in. It is not an assumption you can make. Your script is opening a <em>new</em> database in a different directory, one that is empty.</p>\n<p>Use an absolute path for your database file. You can base it on the absolute path of your script:</p>\n<pre><code class=\"python\">import os.path\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\ndb_path = os.path.join(BASE_DIR, \"PupilPremiumTable.db\")\nwith sqlite3.connect(db_path) as db:\n</code></pre>\n<p>You can verify what the current working directory is with <a href=\"https://docs.python.org/2/library/os.html#os.getcwd\"><code>os.getcwd()</code></a> if you want to figure out where instead you are opening the new database file; you probably want to clean up the extra file you created there.</p>\n", "abstract": "You are assuming that the current working directory is the same as the directory your script lives in. It is not an assumption you can make. Your script is opening a new database in a different directory, one that is empty. Use an absolute path for your database file. You can base it on the absolute path of your script: You can verify what the current working directory is with os.getcwd() if you want to figure out where instead you are opening the new database file; you probably want to clean up the extra file you created there."}, {"id": 65462584, "score": 3, "vote": 0, "content": "<p>I had the same problem and here's how I solved it.</p>\n<ol>\n<li>I killed the server by pressing Ctrl+C</li>\n<li>I deleted the <strong>pychache</strong> folder. You'll find this folder in your project folder.</li>\n<li>I deleted the sqlite db.</li>\n<li>I made migrations with <code>python manage.py makemigrations &lt;app_name&gt;</code> where &lt;app_name&gt; is the specific app that contains the model that's causing the error. In my case it was the mail app so I ran python manage.py makemigrations app.</li>\n<li>I migrated in the normal way.</li>\n<li>Then I started the server and it was all solved.</li>\n</ol>\n<p>I believe the issue is as Jorge Cardenas said:</p>\n<blockquote>\n<p>Maybe you are loading views or queries to database but you haven\u00b4t\ngranted enough time for Django to migrate the models to DB. That's why\nthe \"table doesn't exist\".</p>\n</blockquote>\n<p>This solution is based on this <a href=\"https://www.youtube.com/watch?v=CJX9Aw-mKGE\" rel=\"nofollow noreferrer\">youtube video</a></p>\n", "abstract": "I had the same problem and here's how I solved it. I believe the issue is as Jorge Cardenas said: Maybe you are loading views or queries to database but you haven\u00b4t\ngranted enough time for Django to migrate the models to DB. That's why\nthe \"table doesn't exist\". This solution is based on this youtube video"}, {"id": 61781543, "score": 1, "vote": 0, "content": "<p>First, you need to check if that table 100% exist in the database. You can use sqlite viewer for that: <a href=\"https://inloop.github.io/sqlite-viewer/\" rel=\"nofollow noreferrer\">https://inloop.github.io/sqlite-viewer/</a>.</p>\n<p>If the table exists, then you can write your table name in <code>''</code>, for example:</p>\n<pre><code class=\"python\">Select * from 'TableName'\n</code></pre>\n<p>Whatever your query is, I am just using <code>Select *</code> as an example.</p>\n", "abstract": "First, you need to check if that table 100% exist in the database. You can use sqlite viewer for that: https://inloop.github.io/sqlite-viewer/. If the table exists, then you can write your table name in '', for example: Whatever your query is, I am just using Select * as an example."}, {"id": 60959463, "score": 0, "vote": 0, "content": "<p>I have to face same issue and there are a couple of approaches, but the one I think is the most probable one.</p>\n<p>Maybe you are loading views or queries to database but you haven\u00b4t granted enough time for Django to migrate the models to DB. That's why the \"table doesn't exist\".</p>\n<p>Make sure you use this sort of initialization in you view's code:</p>\n<pre><code class=\"python\">form RegisterForm(forms.Form):\n\n    def __init__(self, *args, **kwargs):\n        super(RegisterForm, self).__init__(*args, **kwargs)\n</code></pre>\n<p>A second approach is you clean previous migrations, delete the database and start over the migration process.</p>\n", "abstract": "I have to face same issue and there are a couple of approaches, but the one I think is the most probable one. Maybe you are loading views or queries to database but you haven\u00b4t granted enough time for Django to migrate the models to DB. That's why the \"table doesn't exist\". Make sure you use this sort of initialization in you view's code: A second approach is you clean previous migrations, delete the database and start over the migration process."}, {"id": 69815601, "score": 0, "vote": 0, "content": "<p>I had the same issue when I was following the flask blog tutorial. I had initialized the database one time when it started giving me the sqlite3.OperationalError: then I tried to initialize again and turns out I had lots of errors in my schema and db.py file. Fixed them and initialized again and it worked.</p>\n", "abstract": "I had the same issue when I was following the flask blog tutorial. I had initialized the database one time when it started giving me the sqlite3.OperationalError: then I tried to initialize again and turns out I had lots of errors in my schema and db.py file. Fixed them and initialized again and it worked."}, {"id": 72834838, "score": 0, "vote": 0, "content": "<p>Adding this worked for me:</p>\n<pre><code class=\"python\">import os.path\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\ndb_dir = (BASE_DIR + '\\\\PupilPremiumTable.db')\n</code></pre>\n<p>Note the need for <code>\\\\</code> before <code>PupilPremiumTable.bd</code> for the code to work .</p>\n", "abstract": "Adding this worked for me: Note the need for \\\\ before PupilPremiumTable.bd for the code to work ."}]}, {"link": "https://stackoverflow.com/questions/1030270/race-conditions-in-django", "question": {"id": "1030270", "title": "Race conditions in django", "content": "<p>Here is a simple example of a django view with a potential race condition:</p>\n<pre><code class=\"python\"># myapp/views.py\nfrom django.contrib.auth.models import User\nfrom my_libs import calculate_points\n\ndef add_points(request):\n    user = request.user\n    user.points += calculate_points(user)\n    user.save()\n</code></pre>\n<p>The race condition should be fairly obvious: A user can make this request twice, and the application could potentially execute <code>user = request.user</code> simultaneously, causing one of the requests to override the other.</p>\n<p>Suppose the function <code>calculate_points</code> is relatively complicated, and makes calculations based on all kinds of weird stuff that cannot be placed in a single <code>update</code> and would be difficult to put in a stored procedure.</p>\n<p>So here is my question: What kind of locking mechanisms are available to django, to deal with situations similar to this?</p>\n", "abstract": "Here is a simple example of a django view with a potential race condition: The race condition should be fairly obvious: A user can make this request twice, and the application could potentially execute user = request.user simultaneously, causing one of the requests to override the other. Suppose the function calculate_points is relatively complicated, and makes calculations based on all kinds of weird stuff that cannot be placed in a single update and would be difficult to put in a stored procedure. So here is my question: What kind of locking mechanisms are available to django, to deal with situations similar to this?"}, "answers": [{"id": 10987224, "score": 51, "vote": 0, "content": "<p>Django 1.4+ supports <a href=\"https://docs.djangoproject.com/en/stable/ref/models/querysets/#django.db.models.query.QuerySet.select_for_update\" rel=\"nofollow noreferrer\">select_for_update</a>, in earlier versions you may execute raw SQL queries e.g. <code>select ... for update</code> which depending on underlying DB will lock the row from any updates, you can do whatever you want with that row until the end of transaction. e.g.</p>\n<pre><code class=\"python\">from django.db import transaction\n\n@transaction.commit_manually()\ndef add_points(request):\n    user = User.objects.select_for_update().get(id=request.user.id)\n    # you can go back at this point if something is not right \n    if user.points &gt; 1000:\n        # too many points\n        return\n    user.points += calculate_points(user)\n    user.save()\n    transaction.commit()\n</code></pre>\n", "abstract": "Django 1.4+ supports select_for_update, in earlier versions you may execute raw SQL queries e.g. select ... for update which depending on underlying DB will lock the row from any updates, you can do whatever you want with that row until the end of transaction. e.g."}, {"id": 1955721, "score": 21, "vote": 0, "content": "<p>As of Django 1.1 you can use the ORM's F() expressions to solve this specific problem. </p>\n<pre><code class=\"python\">from django.db.models import F\n\nuser = request.user\nuser.points  = F('points') + calculate_points(user)\nuser.save()\n</code></pre>\n<p>For more details see the documentation:</p>\n<p><a href=\"https://docs.djangoproject.com/en/1.8/ref/models/instances/#updating-attributes-based-on-existing-fields\" rel=\"noreferrer\">https://docs.djangoproject.com/en/1.8/ref/models/instances/#updating-attributes-based-on-existing-fields</a></p>\n<p><a href=\"https://docs.djangoproject.com/en/1.8/ref/models/expressions/#django.db.models.F\" rel=\"noreferrer\">https://docs.djangoproject.com/en/1.8/ref/models/expressions/#django.db.models.F</a></p>\n", "abstract": "As of Django 1.1 you can use the ORM's F() expressions to solve this specific problem.  For more details see the documentation: https://docs.djangoproject.com/en/1.8/ref/models/instances/#updating-attributes-based-on-existing-fields https://docs.djangoproject.com/en/1.8/ref/models/expressions/#django.db.models.F"}, {"id": 1030464, "score": 8, "vote": 0, "content": "<p>Database locking is the way to go here. There are plans to add \"select for update\" support to Django (<a href=\"http://code.djangoproject.com/ticket/2705\" rel=\"nofollow noreferrer\">here</a>), but for now the simplest would be to use raw SQL to UPDATE the user object before you start to calculate the score.</p>\n<hr/>\n<p>Pessimistic locking is now supported by Django 1.4's ORM when the underlying DB (such as Postgres) supports it.  See the <a href=\"https://docs.djangoproject.com/en/dev/releases/1.4-alpha-1/#select-for-update-support\" rel=\"nofollow noreferrer\">Django 1.4a1 release notes</a>.</p>\n", "abstract": "Database locking is the way to go here. There are plans to add \"select for update\" support to Django (here), but for now the simplest would be to use raw SQL to UPDATE the user object before you start to calculate the score. Pessimistic locking is now supported by Django 1.4's ORM when the underlying DB (such as Postgres) supports it.  See the Django 1.4a1 release notes."}, {"id": 1030381, "score": 7, "vote": 0, "content": "<p>You have many ways to single-thread this kind of thing.</p>\n<p>One standard approach is <strong>Update First</strong>.  You do an update which will seize an exclusive lock on the row; then do your work; and finally commit the change.  For this to work, you need to bypass the ORM's caching. </p>\n<p>Another standard approach is to have a separate, single-threaded application server that isolates the Web transactions from the complex calculation.  </p>\n<ul>\n<li><p>Your web application can create a queue of scoring requests, spawn a separate process, and then write the scoring requests to this queue.  The spawn can be put in Django's <code>urls.py</code> so it happens on web-app startup.  Or it can be put into separate <code>manage.py</code> admin script.  Or it can be done \"as needed\" when the first scoring request is attempted.</p></li>\n<li><p>You can also create a separate WSGI-flavored web server using Werkzeug which accepts WS requests via urllib2.  If you have a single port number for this server, requests are queued by TCP/IP.  If your WSGI handler has one thread, then, you've achieved serialized single-threading.  This is slightly more scalable, since the scoring engine is a WS request and can be run anywhere.</p></li>\n</ul>\n<p>Yet another approach is to have some other resource that has to be acquired and held to do the calculation.  </p>\n<ul>\n<li><p>A Singleton object in the database.  A single row in a unique table can be updated with a session ID to seize control; update with session ID of <code>None</code> to release control.  The essential update has to include a <code>WHERE SESSION_ID IS NONE</code> filter to assure that the update fails when the lock is held by someone else.  This is interesting because it's inherently race-free -- it's a single update -- not a SELECT-UPDATE sequence.</p></li>\n<li><p>A garden-variety semaphore can be used outside the database.  Queues (generally) are easier to work with than a low-level semaphore.</p></li>\n</ul>\n", "abstract": "You have many ways to single-thread this kind of thing. One standard approach is Update First.  You do an update which will seize an exclusive lock on the row; then do your work; and finally commit the change.  For this to work, you need to bypass the ORM's caching.  Another standard approach is to have a separate, single-threaded application server that isolates the Web transactions from the complex calculation.   Your web application can create a queue of scoring requests, spawn a separate process, and then write the scoring requests to this queue.  The spawn can be put in Django's urls.py so it happens on web-app startup.  Or it can be put into separate manage.py admin script.  Or it can be done \"as needed\" when the first scoring request is attempted. You can also create a separate WSGI-flavored web server using Werkzeug which accepts WS requests via urllib2.  If you have a single port number for this server, requests are queued by TCP/IP.  If your WSGI handler has one thread, then, you've achieved serialized single-threading.  This is slightly more scalable, since the scoring engine is a WS request and can be run anywhere. Yet another approach is to have some other resource that has to be acquired and held to do the calculation.   A Singleton object in the database.  A single row in a unique table can be updated with a session ID to seize control; update with session ID of None to release control.  The essential update has to include a WHERE SESSION_ID IS NONE filter to assure that the update fails when the lock is held by someone else.  This is interesting because it's inherently race-free -- it's a single update -- not a SELECT-UPDATE sequence. A garden-variety semaphore can be used outside the database.  Queues (generally) are easier to work with than a low-level semaphore."}, {"id": 1030471, "score": 1, "vote": 0, "content": "<p>This may be oversimplifying your situation, but what about just a JavaScript link replacement?  In other words when the user clicks the link or button wrap the request in a JavaScript function which immediately disables / \"greys out\" the link and replaces the text with \"Loading...\" or \"Submitting request...\" info or something similar.  Would that work for you?</p>\n", "abstract": "This may be oversimplifying your situation, but what about just a JavaScript link replacement?  In other words when the user clicks the link or button wrap the request in a JavaScript function which immediately disables / \"greys out\" the link and replaces the text with \"Loading...\" or \"Submitting request...\" info or something similar.  Would that work for you?"}, {"id": 24381313, "score": 0, "vote": 0, "content": "<p>Now, you must use:</p>\n<pre><code class=\"python\">Model.objects.select_for_update().get(foo=bar)\n</code></pre>\n", "abstract": "Now, you must use:"}]}, {"link": "https://stackoverflow.com/questions/10317114/postgresql-drop-table-doesnt-work", "question": {"id": "10317114", "title": "Postgresql DROP TABLE doesn&#39;t work", "content": "<p>I'm trying to drop a few tables with the <code>\"DROP TABLE\"</code> command but for a unknown reason, the program just \"sits\" and doesn't delete the table that I want it to in the database.</p>\n<p>I have 3 tables in the database:</p>\n<p>Product, Bill and Bill_Products which is used for referencing products in bills.</p>\n<p>I managed to delete/drop Product, but I can't do the same for bill and Bill_Products.\nI'm issuing the same <code>\"DROP TABLE Bill CASCADE;\"</code> command but the command line just stalls. I've also used the simple version without the <code>CASCADE</code> option.</p>\n<p>Do you have any idea why this is happening?</p>\n<p><b>Update:</b></p>\n<p>I've been thinking that it is possible for the databases to keep some references from products to bills and maybe that's why it won't delete the Bill table.</p>\n<p>So, for that matter i issued a simple <code>SELECT * from Bill_Products</code> and after a few (10-15) seconds (strangely, because I don't think it's normal for it to last such a long time when there's an empty table) it printed out the table and it's contents, which are none. (so <b><em>apparently</em></b> there are no references left from Products to Bill).</p>\n", "abstract": "I'm trying to drop a few tables with the \"DROP TABLE\" command but for a unknown reason, the program just \"sits\" and doesn't delete the table that I want it to in the database. I have 3 tables in the database: Product, Bill and Bill_Products which is used for referencing products in bills. I managed to delete/drop Product, but I can't do the same for bill and Bill_Products.\nI'm issuing the same \"DROP TABLE Bill CASCADE;\" command but the command line just stalls. I've also used the simple version without the CASCADE option. Do you have any idea why this is happening? Update: I've been thinking that it is possible for the databases to keep some references from products to bills and maybe that's why it won't delete the Bill table. So, for that matter i issued a simple SELECT * from Bill_Products and after a few (10-15) seconds (strangely, because I don't think it's normal for it to last such a long time when there's an empty table) it printed out the table and it's contents, which are none. (so apparently there are no references left from Products to Bill)."}, "answers": [{"id": 10317371, "score": 66, "vote": 0, "content": "<p>What is the output of</p>\n<pre><code class=\"python\">SELECT *\n  FROM pg_locks l\n  JOIN pg_class t ON l.relation = t.oid AND t.relkind = 'r'\n WHERE t.relname = 'Bill';\n</code></pre>\n<p>It might be that there're other sessions using your table in parallel and you cannot obtain <a href=\"http://www.postgresql.org/docs/7.4/static/explicit-locking.html#LOCKING-TABLES\">Access Exclusive</a> lock to drop it.</p>\n", "abstract": "What is the output of It might be that there're other sessions using your table in parallel and you cannot obtain Access Exclusive lock to drop it."}, {"id": 33415675, "score": 25, "vote": 0, "content": "<p>Just do</p>\n<pre><code class=\"python\">SELECT pid, relname\nFROM pg_locks l\nJOIN pg_class t ON l.relation = t.oid AND t.relkind = 'r'\nWHERE t.relname = 'Bill';\n</code></pre>\n<p>And then kill every pid by</p>\n<pre><code class=\"python\">kill 1234\n</code></pre>\n<p>Where 1234 is your actual pid from query results.</p>\n<p>You can pipe it all together like this (so you don't have to copy-paste every pid manually):</p>\n<pre><code class=\"python\">psql -c \"SELECT pid FROM pg_locks l \n    JOIN pg_class t ON l.relation = t.oid AND t.relkind = 'r' \n    WHERE t.relname = 'Bill';\" | tail -n +3 | head -n -2 | xargs kill\n</code></pre>\n", "abstract": "Just do And then kill every pid by Where 1234 is your actual pid from query results. You can pipe it all together like this (so you don't have to copy-paste every pid manually):"}, {"id": 20831911, "score": 10, "vote": 0, "content": "<p>So I was hitting my head against the wall for some hours trying to solve the same issue, and here is the solution that worked for me:</p>\n<p>Check if PostgreSQL has a pending prepared transaction that's never been committed or rolled back:</p>\n<pre><code class=\"python\">SELECT database, gid FROM pg_prepared_xacts;\n</code></pre>\n<p>If you get a result, then for each transaction <strong>gid</strong> you must execute a <strong>ROLLBACK</strong> from the database having the problem:</p>\n<pre><code class=\"python\">ROLLBACK PREPARED 'the_gid';\n</code></pre>\n<p>For further information, <a href=\"http://doc.nuxeo.com/display/KB/I+can%27t+delete+my+PostgreSQL+database\" rel=\"noreferrer\">click here</a>.</p>\n", "abstract": "So I was hitting my head against the wall for some hours trying to solve the same issue, and here is the solution that worked for me: Check if PostgreSQL has a pending prepared transaction that's never been committed or rolled back: If you get a result, then for each transaction gid you must execute a ROLLBACK from the database having the problem: For further information, click here."}, {"id": 62331906, "score": 9, "vote": 0, "content": "<p>If this is for AWS  postgres run  the first statement to get the PID (Process ID) and then run the second query to terminate the process (it would be very similar to do kill -9 but since this is in the cloud that's what AWS recommends)</p>\n<pre><code class=\"python\">-- gets you the PID\nSELECT pid, relname FROM pg_locks l JOIN pg_class t ON l.relation = t.oid AND t.relkind = 'r' WHERE t.relname = 'YOUR_TABLE_NAME'\n\n-- what actually kills the PID ...it is select statement but it kills the job!\nSELECT pg_terminate_backend(YOUR_PID_FROM_PREVIOUS_QUERY);\n</code></pre>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-postgresql-running-queries/\" rel=\"noreferrer\">source</a></p>\n", "abstract": "If this is for AWS  postgres run  the first statement to get the PID (Process ID) and then run the second query to terminate the process (it would be very similar to do kill -9 but since this is in the cloud that's what AWS recommends) source"}, {"id": 60367779, "score": 6, "vote": 0, "content": "<p>I ran into this today, I was issuing a:</p>\n<p><code>DROP TABLE TableNameHere</code></p>\n<p>and getting <code>ERROR:  table \"tablenamehere\" does not exist</code>. I realized that for case-sensitive tables (as was mine), you need to quote the table name:</p>\n<p><code>DROP TABLE \"TableNameHere\"</code></p>\n", "abstract": "I ran into this today, I was issuing a: DROP TABLE TableNameHere and getting ERROR:  table \"tablenamehere\" does not exist. I realized that for case-sensitive tables (as was mine), you need to quote the table name: DROP TABLE \"TableNameHere\""}, {"id": 19072541, "score": 5, "vote": 0, "content": "<p>Had the same problem.</p>\n<p>There were not any locks on the table.</p>\n<p>Reboot helped.</p>\n", "abstract": "Had the same problem. There were not any locks on the table. Reboot helped."}, {"id": 40749694, "score": 2, "vote": 0, "content": "<p>Old question but ran into a similar issue. Could not reboot the database so tested a few things until this sequence worked :</p>\n<ul>\n<li>truncate table foo;</li>\n<li>drop index concurrently foo_something; times 4-5x</li>\n<li>alter table foo drop column whatever_foreign_key; times 3x</li>\n<li>alter table foo drop column id;</li>\n<li>drop table foo;</li>\n</ul>\n", "abstract": "Old question but ran into a similar issue. Could not reboot the database so tested a few things until this sequence worked :"}, {"id": 69412889, "score": 1, "vote": 0, "content": "<p>The same thing happened for me--except that it was because I forgot the semicolon. <em>face palm</em></p>\n", "abstract": "The same thing happened for me--except that it was because I forgot the semicolon. face palm"}]}, {"link": "https://stackoverflow.com/questions/226693/python-disk-based-dictionary", "question": {"id": "226693", "title": "Python Disk-Based Dictionary", "content": "<p>I was running some dynamic programming code (trying to brute-force disprove the Collatz conjecture =P) and I was using a dict to store the lengths of the chains I had already computed. Obviously, it ran out of memory at some point. Is there any easy way to use some variant of a <code>dict</code> which will page parts of itself out to disk when it runs out of room? Obviously it will be slower than an in-memory dict, and it will probably end up eating my hard drive space, but this could apply to other problems that are not so futile.</p>\n<p>I realized that a disk-based dictionary is pretty much a database, so I manually implemented one using sqlite3, but I didn't do it in any smart way and had it look up every element in the DB one at a time... it was about 300x slower.</p>\n<p>Is the smartest way to just create my own set of dicts, keeping only one in memory at a time, and paging them out in some efficient manner?</p>\n", "abstract": "I was running some dynamic programming code (trying to brute-force disprove the Collatz conjecture =P) and I was using a dict to store the lengths of the chains I had already computed. Obviously, it ran out of memory at some point. Is there any easy way to use some variant of a dict which will page parts of itself out to disk when it runs out of room? Obviously it will be slower than an in-memory dict, and it will probably end up eating my hard drive space, but this could apply to other problems that are not so futile. I realized that a disk-based dictionary is pretty much a database, so I manually implemented one using sqlite3, but I didn't do it in any smart way and had it look up every element in the DB one at a time... it was about 300x slower. Is the smartest way to just create my own set of dicts, keeping only one in memory at a time, and paging them out in some efficient manner?"}, "answers": [{"id": 228837, "score": 58, "vote": 0, "content": "<p>The 3rd party <a href=\"http://pypi.python.org/pypi/shove\" rel=\"noreferrer\">shove</a> module is also worth taking a look at. It's very similar to shelve in that it is a simple dict-like object, however it can store to various backends (such as file, SVN, and S3), provides optional compression, and is even threadsafe. It's a very handy module</p>\n<pre><code class=\"python\">from shove import Shove\n\nmem_store = Shove()\nfile_store = Shove('file://mystore')\n\nfile_store['key'] = value\n</code></pre>\n", "abstract": "The 3rd party shove module is also worth taking a look at. It's very similar to shelve in that it is a simple dict-like object, however it can store to various backends (such as file, SVN, and S3), provides optional compression, and is even threadsafe. It's a very handy module"}, {"id": 226796, "score": 22, "vote": 0, "content": "<p>Hash-on-disk is generally addressed with Berkeley DB or something similar - several options are listed in the <a href=\"http://docs.python.org/library/persistence.html\" rel=\"noreferrer\">Python Data Persistence documentation</a>. You can front it with an in-memory cache, but I'd test against native performance first; with operating system caching in place it might come out about the same.</p>\n", "abstract": "Hash-on-disk is generally addressed with Berkeley DB or something similar - several options are listed in the Python Data Persistence documentation. You can front it with an in-memory cache, but I'd test against native performance first; with operating system caching in place it might come out about the same."}, {"id": 228333, "score": 7, "vote": 0, "content": "<p>The <a href=\"http://docs.python.org/library/shelve.html\" rel=\"noreferrer\">shelve</a> module may do it; at any rate, it should be simple to test.  Instead of:</p>\n<pre><code class=\"python\">self.lengths = {}\n</code></pre>\n<p>do:</p>\n<pre><code class=\"python\">import shelve\nself.lengths = shelve.open('lengths.shelf')\n</code></pre>\n<p>The only catch is that keys to shelves must be strings, so you'll have to replace</p>\n<pre><code class=\"python\">self.lengths[indx]\n</code></pre>\n<p>with</p>\n<pre><code class=\"python\">self.lengths[str(indx)]\n</code></pre>\n<p>(I'm assuming your keys are just integers, as per your comment to Charles Duffy's post)</p>\n<p>There's no built-in caching in memory, but your operating system may do that for you anyway.</p>\n<p>[actually, that's not quite true: you can pass the argument 'writeback=True' on creation.  The intent of this is to make sure storing lists and other mutable things in the shelf works correctly.  But a side-effect is that the whole dictionary is cached in memory.  Since this caused problems for you, it's probably not a good idea :-) ]</p>\n", "abstract": "The shelve module may do it; at any rate, it should be simple to test.  Instead of: do: The only catch is that keys to shelves must be strings, so you'll have to replace with (I'm assuming your keys are just integers, as per your comment to Charles Duffy's post) There's no built-in caching in memory, but your operating system may do that for you anyway. [actually, that's not quite true: you can pass the argument 'writeback=True' on creation.  The intent of this is to make sure storing lists and other mutable things in the shelf works correctly.  But a side-effect is that the whole dictionary is cached in memory.  Since this caused problems for you, it's probably not a good idea :-) ]"}, {"id": 226803, "score": 6, "vote": 0, "content": "<p>Last time I was facing a problem like this, I rewrote to use SQLite rather than a dict, and had a massive performance increase. That performance increase was at least partially on account of the database's indexing capabilities; depending on your algorithms, YMMV.</p>\n<p>A thin wrapper that does SQLite queries in <code>__getitem__</code> and <code>__setitem__</code> isn't much code to write.</p>\n", "abstract": "Last time I was facing a problem like this, I rewrote to use SQLite rather than a dict, and had a massive performance increase. That performance increase was at least partially on account of the database's indexing capabilities; depending on your algorithms, YMMV. A thin wrapper that does SQLite queries in __getitem__ and __setitem__ isn't much code to write."}, {"id": 226900, "score": 3, "vote": 0, "content": "<p>With a little bit of thought it seems like you could get the <a href=\"http://blog.doughellmann.com/2007/08/pymotw-shelve.html\" rel=\"nofollow noreferrer\">shelve module</a> to do what you want.</p>\n", "abstract": "With a little bit of thought it seems like you could get the shelve module to do what you want."}, {"id": 298422, "score": 1, "vote": 0, "content": "<p>I've read you think shelve is too slow and you tried to hack your own dict using sqlite.</p>\n<p>Another did this too :</p>\n<p><a href=\"http://sebsauvage.net/python/snyppets/index.html#dbdict\" rel=\"nofollow noreferrer\">http://sebsauvage.net/python/snyppets/index.html#dbdict</a></p>\n<p>It seems pretty efficient (and sebsauvage is a pretty good coder). Maybe you could give it a try ?</p>\n", "abstract": "I've read you think shelve is too slow and you tried to hack your own dict using sqlite. Another did this too : http://sebsauvage.net/python/snyppets/index.html#dbdict It seems pretty efficient (and sebsauvage is a pretty good coder). Maybe you could give it a try ?"}, {"id": 226837, "score": 0, "vote": 0, "content": "<p>You should bring more than one item at a time if there's some heuristic to know which are the most likely items to be retrieved next, and don't forget the indexes like Charles mentions.</p>\n", "abstract": "You should bring more than one item at a time if there's some heuristic to know which are the most likely items to be retrieved next, and don't forget the indexes like Charles mentions."}, {"id": 58850487, "score": 0, "vote": 0, "content": "<p>For simple use cases <a href=\"https://github.com/RaRe-Technologies/sqlitedict\" rel=\"nofollow noreferrer\">sqlitedict</a>\ncan help. However when you have much more complex databases you might one to try one of the more upvoted answers.</p>\n", "abstract": "For simple use cases sqlitedict\ncan help. However when you have much more complex databases you might one to try one of the more upvoted answers."}]}, {"link": "https://stackoverflow.com/questions/897020/a-good-way-to-escape-quotes-in-a-database-query-string", "question": {"id": "897020", "title": "A good way to escape quotes in a database query string?", "content": "<p>I've tried all manner of Python modules and they either escape too much or in the wrong way.\nWhat's the best way you've found to escape quotes (\", ') in Python?</p>\n", "abstract": "I've tried all manner of Python modules and they either escape too much or in the wrong way.\nWhat's the best way you've found to escape quotes (\", ') in Python?"}, "answers": [{"id": 897061, "score": 34, "vote": 0, "content": "<p>If it's part of a Database query you should be able to use a <a href=\"https://web.archive.org/web/20100813151305/http://python.projects.postgresql.org/docs/0.8/driver.html\" rel=\"noreferrer\">Parameterized SQL Statement</a>.</p>\n<p>As well as escaping your quotes, this will deal with all special characters and will protect you from <a href=\"https://stackoverflow.com/questions/681583/sql-injection-on-insert\">SQL injection attacks</a>.</p>\n", "abstract": "If it's part of a Database query you should be able to use a Parameterized SQL Statement. As well as escaping your quotes, this will deal with all special characters and will protect you from SQL injection attacks."}, {"id": 14930615, "score": 24, "vote": 0, "content": "<p>Use <code>json.dumps</code>.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import json\n&gt;&gt;&gt; print json.dumps('a\"bc')\n\"a\\\"bc\"\n</code></pre>\n", "abstract": "Use json.dumps."}, {"id": 13676745, "score": 12, "vote": 0, "content": "<p>The easy and standard way to escape strings, and convert other objects to programmatic form, is to use the built in <code>repr()</code> function.  It converts an object into the representation you would need to enter it with manual code.</p>\n<p>E.g.:</p>\n<pre><code class=\"python\">s = \"I'm happy I am \\\"here\\\" now\"\nprint repr(s)\n&gt;&gt;  'I\\'m happy I am \"here\" now'\n</code></pre>\n<p>No weird hacks, it's built in and it just works for most purposes.</p>\n", "abstract": "The easy and standard way to escape strings, and convert other objects to programmatic form, is to use the built in repr() function.  It converts an object into the representation you would need to enter it with manual code. E.g.: No weird hacks, it's built in and it just works for most purposes."}, {"id": 37897034, "score": 4, "vote": 0, "content": "<p>If using psycopg2, its <code>execute()</code> method has built-in escaping:</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT column FROM table WHERE column=%s AND column2=%s\", (value1, value2))\n</code></pre>\n<p>Note, that you are giving two arguments to execute method (string and tuple), instead of using Python's % operator to modify string. </p>\n<p>Answer stolen from here: <a href=\"https://stackoverflow.com/questions/3823735/psycopg2-equivalent-of-mysqldb-escape-string\">psycopg2 equivalent of mysqldb.escape_string?</a></p>\n", "abstract": "If using psycopg2, its execute() method has built-in escaping: Note, that you are giving two arguments to execute method (string and tuple), instead of using Python's % operator to modify string.  Answer stolen from here: psycopg2 equivalent of mysqldb.escape_string?"}, {"id": 48532315, "score": 4, "vote": 0, "content": "<p>Triple single quotes will conveniently encapsulate the single quotes often used in SQL queries:</p>\n<pre><code class=\"python\">c.execute('''SELECT sval FROM sdat WHERE instime &gt; NOW() - INTERVAL '1 days' ORDER BY instime ASC''')\n</code></pre>\n", "abstract": "Triple single quotes will conveniently encapsulate the single quotes often used in SQL queries:"}, {"id": 2429285, "score": 2, "vote": 0, "content": "<p>If you're using psycopg2 that has a method for escaping strings: <strong><code>psycopg2.extensions.adapt()</code></strong> See <a href=\"https://stackoverflow.com/questions/309945/how-to-quote-a-string-value-explicitly-python-db-api-psycopg2\">How to quote a string value explicitly (Python DB API/Psycopg2)</a> for the full answer</p>\n", "abstract": "If you're using psycopg2 that has a method for escaping strings: psycopg2.extensions.adapt() See How to quote a string value explicitly (Python DB API/Psycopg2) for the full answer"}, {"id": 2429209, "score": 0, "vote": 0, "content": "<p>For a solution to a more generic problem, I have a program where I needed to store <em>any</em> set of characters in a flat file, tab delimited. Obviously, having tabs in the 'set' was causing problems.</p>\n<p>Instead of output_f.write(str), I used output_f.write(repr(str)), which solved my problem.\nIt is slower to read, as I need to eval() the input when I read it, but overall, it makes the code cleaner because I don't need to check for fringe cases anymore.</p>\n", "abstract": "For a solution to a more generic problem, I have a program where I needed to store any set of characters in a flat file, tab delimited. Obviously, having tabs in the 'set' was causing problems. Instead of output_f.write(str), I used output_f.write(repr(str)), which solved my problem.\nIt is slower to read, as I need to eval() the input when I read it, but overall, it makes the code cleaner because I don't need to check for fringe cases anymore."}, {"id": 9028006, "score": 0, "vote": 0, "content": "<p>Triple-double quotes are best for escaping:</p>\n<pre>string = \"\"\"This will span across 'single quotes', \"double quotes\",\nand literal EOLs all in the same string.\"\"\"</pre>\n", "abstract": "Triple-double quotes are best for escaping:"}, {"id": 56159405, "score": -1, "vote": 0, "content": "<p>For my use case, I was saving a paragraph against the database and somewhere in the paragraph there might have been some text with a single quote <code>(example: Charlie's apple sauce was soggy)</code></p>\n<p>I found this to work best:</p>\n<pre><code class=\"python\">database_cursor.execute('''INSERT INTO books.collection (book_name, book_quoted_text) VALUES ('%s', \"%s\")''' % (book_name, page_text.strip()))\n</code></pre>\n<p>You'll notice that I use <code>\"\"</code> after wrapping the INSERT statement in <code>'''</code></p>\n", "abstract": "For my use case, I was saving a paragraph against the database and somewhere in the paragraph there might have been some text with a single quote (example: Charlie's apple sauce was soggy) I found this to work best: You'll notice that I use \"\" after wrapping the INSERT statement in '''"}]}, {"link": "https://stackoverflow.com/questions/1690605/reliable-and-efficient-key-value-database-for-linux", "question": {"id": "1690605", "title": "Reliable and efficient key--value database for Linux?", "content": "<p>I need a fast, reliable and memory-efficient key--value database for Linux. My keys are about 128 bytes, and the maximum value size can be 128K or 256K. The database subsystem shouldn't use more than about 1 MB of RAM. The total database size is 20G (!), but only a small random fraction of the data is accessed at a time. If necessary, I can move some data blobs out of the database (to regular files), so the size gets down to 2 GB maximum. The database must survive a system crash without any loss in recently unmodified data. I'll have about 100 times more reads than writes. It is a plus if it can use a block device (without a filesystem) as storage. I don't need client-server functionality, just a library. I need Python bindings (but I can implement them if they are not available).</p>\n<p>Which solutions should I consider, and which one do you recommend?</p>\n<p>Candidates I know of which could work:</p>\n<ul>\n<li><a href=\"http://fallabs.com/tokyocabinet/\" rel=\"nofollow noreferrer\">Tokyo Cabinet</a> (Python bindings are <a href=\"http://pypi.python.org/pypi/pytc\" rel=\"nofollow noreferrer\">pytc</a>, see also <a href=\"http://github.com/turian/pytc-example/blob/master/hashdb.py\" rel=\"nofollow noreferrer\">pytc example code</a>, supports hashes and B+trees, transaction log files and more, the size of the bucket array is fixed at database creation time; the writer must close the file to give others a chance; lots of small writes with reopening the file for each of them are very slow; the Tyrant server can help with the lots of small writes; <a href=\"http://michael.susens-schurter.com/tokyotalk/tokyotalk.html\" rel=\"nofollow noreferrer\">speed comparison between Tokyo Cabinet, Tokyo Tyrant and Berkeley DB</a>)</li>\n<li><a href=\"http://repetae.net/computer/vsdb/\" rel=\"nofollow noreferrer\">VSDB</a> (safe even on NFS, without locking; what about barriers?; updates are very slow, but not as slow as in cdb; last version in 2003)</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Berkeley_DB\" rel=\"nofollow noreferrer\">BerkeleyDB</a> (provides crash recovery; provides transactions; the <code>bsddb</code> Python module provides bindings)</li>\n<li><a href=\"http://tdb.samba.org/\" rel=\"nofollow noreferrer\">Samba's TDB</a> (with transactions and Python bindings, some users <a href=\"http://lists.samba.org/archive/samba/2009-January/145793.html\" rel=\"nofollow noreferrer\">experienced corruption</a>, sometimes <code>mmap()</code>s the whole file, the <code>repack</code> operation sometimes doubles the file size, produces mysterious failures if the database is larger than 2G (even on 64-bit systems), cluster implementation (<a href=\"http://ctdb.samba.org/\" rel=\"nofollow noreferrer\">CTDB</a>) also available; file grows too large after lots of modifications; file becomes too slow after lots of hash contention; no built-in way to rebuild the file; very fast parallel updates by locking individual hash buckets)</li>\n<li><a href=\"https://sourceforge.net/projects/aodbm/\" rel=\"nofollow noreferrer\">aodbm</a> (append-only so survives a system crash, with Python bindings)</li>\n<li><a href=\"http://www.hamsterdb.com/about/features\" rel=\"nofollow noreferrer\">hamsterdb</a> (with Python bindings)</li>\n<li><a href=\"http://en.wikipedia.org/wiki/C-tree\" rel=\"nofollow noreferrer\">C-tree</a> (mature, versatile commercial solution with high performance, has a free edition with reduced functionality)</li>\n<li>the old <a href=\"http://sourceforge.net/projects/tdb/\" rel=\"nofollow noreferrer\">TDB</a> (from 2001)</li>\n<li><a href=\"https://bitbucket.org/basho/bitcask\" rel=\"nofollow noreferrer\">bitcask</a> (log-structured, written in Erlang)</li>\n<li>various other DBM implementations (such as GDBM, NDBM, QDBM,, Perl's SDBM or Ruby's; probably they don't have proper crash recovery)</li>\n</ul>\n<p>I won't use these:</p>\n<ul>\n<li><a href=\"http://memcachedb.org/\" rel=\"nofollow noreferrer\">MemcacheDB</a> (client-server, uses BereleleyDB as a backend)</li>\n<li><a href=\"http://cr.yp.to/cdb.html\" rel=\"nofollow noreferrer\">cdb</a> (needs to regenerate the whole database upon each write)</li>\n<li><a href=\"http://www.wildsparx.com/apbcdb/\" rel=\"nofollow noreferrer\">http://www.wildsparx.com/apbcdb/</a> (ditto)</li>\n<li><a href=\"http://code.google.com/p/redis/\" rel=\"nofollow noreferrer\">Redis</a> (keeps the whole database in memory)</li>\n<li><a href=\"http://www.sqlite.org/\" rel=\"nofollow noreferrer\">SQLite</a> (it becomes very slow without periodic vacuuming, see autocompletion in the in the location bar in Firefox 3.0, even though versions 3.1 and later of sqlite allow <code>auto_vacuum</code>ing; beware: small writing transactions can be very slow; beware: if a busy process is doing many transactions, other processes starve, and they can never get the lock)</li>\n<li><a href=\"http://www.mongodb.org/\" rel=\"nofollow noreferrer\">MongoDB</a> (too heavy-weight, treats values as objects with internal structure)</li>\n<li><a href=\"http://www.firebirdsql.org/index.php?id=about-firebird\" rel=\"nofollow noreferrer\">Firebird</a> (SQL-based RDBMS, too heavy-weight)</li>\n</ul>\n<p>FYI, a <a href=\"http://www.linux-mag.com/cache/7579/1.html\" rel=\"nofollow noreferrer\">recent article about key--value databases</a> in the Linux magazine.</p>\n<p>FYI, an <a href=\"http://linuxfinances.info/info/dbmsisam.html\" rel=\"nofollow noreferrer\">older software list</a></p>\n<p>FYI, a <a href=\"http://timyang.net/data/mcdb-tt-redis/\" rel=\"nofollow noreferrer\">speed comparison of MemcacheDB, Redis and Tokyo Cabinet Tyrant</a></p>\n<p>Related questions on StackOverflow:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/775474/key-value-database-for-windows\">Key Value Database For Windows?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/639545/is-there-a-business-proven-cloud-store-keyvalue-database-open-source\">Is there a business proven cloud store / Key=&gt;Value Database? (Open Source)</a></li>\n</ul>\n", "abstract": "I need a fast, reliable and memory-efficient key--value database for Linux. My keys are about 128 bytes, and the maximum value size can be 128K or 256K. The database subsystem shouldn't use more than about 1 MB of RAM. The total database size is 20G (!), but only a small random fraction of the data is accessed at a time. If necessary, I can move some data blobs out of the database (to regular files), so the size gets down to 2 GB maximum. The database must survive a system crash without any loss in recently unmodified data. I'll have about 100 times more reads than writes. It is a plus if it can use a block device (without a filesystem) as storage. I don't need client-server functionality, just a library. I need Python bindings (but I can implement them if they are not available). Which solutions should I consider, and which one do you recommend? Candidates I know of which could work: I won't use these: FYI, a recent article about key--value databases in the Linux magazine. FYI, an older software list FYI, a speed comparison of MemcacheDB, Redis and Tokyo Cabinet Tyrant Related questions on StackOverflow:"}, "answers": [{"id": 25123876, "score": 11, "vote": 0, "content": "<p>LMDB is the most memory-efficient database around\n<a href=\"http://symas.com/mdb/inmem/\" rel=\"noreferrer\">http://symas.com/mdb/inmem/</a></p>\n<p>and also proven to be the most reliable - completely crash-proof.\n<a href=\"http://wisdom.cs.wisc.edu/workshops/spring-14/talks/Thanu.pdf\" rel=\"noreferrer\">http://wisdom.cs.wisc.edu/workshops/spring-14/talks/Thanu.pdf</a></p>\n<p>Of the ones you've mentioned, Tokyo Cabinet has documented corruption issues\n<a href=\"https://www.google.com/search?q=cfengine+tokyo+cabinet+corruption\" rel=\"noreferrer\">https://www.google.com/search?q=cfengine+tokyo+cabinet+corruption</a></p>\n<p>BerkeleyDB also has well-documented corruption issues, as does Bitcask. (And bitcask is an in-memory-only DB anyway, so useless for your 1MB RAM requirement.)</p>\n<p>LMDB is also well-supported in Python, with a couple different bindings available.\n<a href=\"https://github.com/dw/py-lmdb/\" rel=\"noreferrer\">https://github.com/dw/py-lmdb/</a>\n<a href=\"https://github.com/tspurway/pymdb-lightning\" rel=\"noreferrer\">https://github.com/tspurway/pymdb-lightning</a></p>\n<p>Disclaimer - I am the author of LMDB. But these are documented facts: LMDB is the smallest, most efficient, and most reliable key/value store in the world and nothing else comes anywhere close.</p>\n", "abstract": "LMDB is the most memory-efficient database around\nhttp://symas.com/mdb/inmem/ and also proven to be the most reliable - completely crash-proof.\nhttp://wisdom.cs.wisc.edu/workshops/spring-14/talks/Thanu.pdf Of the ones you've mentioned, Tokyo Cabinet has documented corruption issues\nhttps://www.google.com/search?q=cfengine+tokyo+cabinet+corruption BerkeleyDB also has well-documented corruption issues, as does Bitcask. (And bitcask is an in-memory-only DB anyway, so useless for your 1MB RAM requirement.) LMDB is also well-supported in Python, with a couple different bindings available.\nhttps://github.com/dw/py-lmdb/\nhttps://github.com/tspurway/pymdb-lightning Disclaimer - I am the author of LMDB. But these are documented facts: LMDB is the smallest, most efficient, and most reliable key/value store in the world and nothing else comes anywhere close."}, {"id": 1758210, "score": 2, "vote": 0, "content": "<p>I've had good luck with the Tokyo Cabinet/pytc solution. It's very fast (a bit faster than using the shelve module using anydbm in my implementation), both for reading and writing (though I too do far more reading). The problem for me was the spartan documentation on the python bindings, but there's enough example code around to figure out how to do what you need to do. Additionally, tokyo cabinet is quite easy to install (as are the python bindings), doesn't require a server (as you mention) <strike>and seems to be actively supported</strike> (stable but no longer under active development). You can open files in read-only mode, allowing concurrent access, or read/write mode, preventing other processes from accessing the database.</p>\n<p>I was looking at various options over the summer, and the advice I got then was this: try out the different options and see what works best for you. It'd be nice if there were simply a \"best\" option, but everyone is looking for slightly different features and are willing to make different trade-offs. You know best.</p>\n<p>(That said, it'd be useful to others if you shared what ended up working the best for you, and why you chose that solution over others!)</p>\n", "abstract": "I've had good luck with the Tokyo Cabinet/pytc solution. It's very fast (a bit faster than using the shelve module using anydbm in my implementation), both for reading and writing (though I too do far more reading). The problem for me was the spartan documentation on the python bindings, but there's enough example code around to figure out how to do what you need to do. Additionally, tokyo cabinet is quite easy to install (as are the python bindings), doesn't require a server (as you mention) and seems to be actively supported (stable but no longer under active development). You can open files in read-only mode, allowing concurrent access, or read/write mode, preventing other processes from accessing the database. I was looking at various options over the summer, and the advice I got then was this: try out the different options and see what works best for you. It'd be nice if there were simply a \"best\" option, but everyone is looking for slightly different features and are willing to make different trade-offs. You know best. (That said, it'd be useful to others if you shared what ended up working the best for you, and why you chose that solution over others!)"}, {"id": 1823412, "score": 2, "vote": 0, "content": "<p>cdb can handle any database up to 4 GB, making it too small for the 20GB matter at hand.</p>\n", "abstract": "cdb can handle any database up to 4 GB, making it too small for the 20GB matter at hand."}, {"id": 2273492, "score": 2, "vote": 0, "content": "<p>Riak runs on Linux, and allows you to dynamically add nodes</p>\n", "abstract": "Riak runs on Linux, and allows you to dynamically add nodes"}, {"id": 1693372, "score": 1, "vote": 0, "content": "<p>how about Python 3.0's dbm.ndbm ?</p>\n", "abstract": "how about Python 3.0's dbm.ndbm ?"}, {"id": 1759980, "score": 1, "vote": 0, "content": "<p>Another suggestion is <a href=\"http://tdb.samba.org/\" rel=\"nofollow noreferrer\">TDB</a> (a part of the Samba project). I've used it through the <a href=\"http://pypi.python.org/pypi/tdb/1.0\" rel=\"nofollow noreferrer\">tdb</a> module, however I can't say I've tested its reliability on crashes; the projects I used it in didn't have such requirements, and I can't find relevant documentation.</p>\n", "abstract": "Another suggestion is TDB (a part of the Samba project). I've used it through the tdb module, however I can't say I've tested its reliability on crashes; the projects I used it in didn't have such requirements, and I can't find relevant documentation."}, {"id": 1690615, "score": 0, "vote": 0, "content": "<p>how about a SQLite?</p>\n", "abstract": "how about a SQLite?"}, {"id": 1690623, "score": 0, "vote": 0, "content": "<p>I've used bsddb.hashlib() with Python, it worked pretty good.</p>\n", "abstract": "I've used bsddb.hashlib() with Python, it worked pretty good."}, {"id": 1690628, "score": 0, "vote": 0, "content": "<p>You might like <a href=\"http://cr.yp.to/\" rel=\"nofollow noreferrer\">djb</a>'s <a href=\"http://cr.yp.to/cdb.html\" rel=\"nofollow noreferrer\">cdb</a>, which has the properties you mention.</p>\n", "abstract": "You might like djb's cdb, which has the properties you mention."}, {"id": 1823351, "score": 0, "vote": 0, "content": "<p>In <a href=\"https://stackoverflow.com/questions/525065/which-embedded-database-capable-of-100-million-records-has-the-best-c-api\">my query for a cross-platform ISAM-style database</a> (similar), I also received suggestions for the embedded version of <a href=\"http://www.firebirdsql.org/\" rel=\"nofollow noreferrer\">Firebird</a> and <a href=\"http://library.gnome.org/devel/glib/stable/\" rel=\"nofollow noreferrer\">GLib</a>.</p>\n", "abstract": "In my query for a cross-platform ISAM-style database (similar), I also received suggestions for the embedded version of Firebird and GLib."}]}, {"link": "https://stackoverflow.com/questions/8935130/how-can-i-access-amazon-dynamodb-via-python", "question": {"id": "8935130", "title": "How can I access Amazon DynamoDB via Python?", "content": "<p>I'm currently using hbase with my Python apps and wanted to try out Amazon <a href=\"http://aws.amazon.com/dynamodb/\">DynamoDB</a>. Is there a way to use Python to read, write and query data?</p>\n", "abstract": "I'm currently using hbase with my Python apps and wanted to try out Amazon DynamoDB. Is there a way to use Python to read, write and query data?"}, "answers": [{"id": 8935165, "score": 29, "vote": 0, "content": "<p>You can use boto3: <a href=\"https://github.com/boto/boto3\" rel=\"nofollow noreferrer\">https://github.com/boto/boto3</a></p>\n<p>docs: <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html\" rel=\"nofollow noreferrer\">https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html</a></p>\n<p>api reference: <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html\" rel=\"nofollow noreferrer\">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html</a></p>\n", "abstract": "You can use boto3: https://github.com/boto/boto3 docs: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html api reference: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html"}, {"id": 22636410, "score": 26, "vote": 0, "content": "<p>Another alternative is <a href=\"http://pynamodb.readthedocs.org/en/latest/\">PynamoDB</a>. PynamoDB provides an ORM like interface to DynamoDB and supports <strong>both</strong> Python 2 and Python 3. The entire DynamoDB API is supported by PynamoDB - including global and local secondary indexes, batch operations, binary attributes, queries, scans, etc. </p>\n<p>Disclaimer: I wrote PynamoDB.</p>\n", "abstract": "Another alternative is PynamoDB. PynamoDB provides an ORM like interface to DynamoDB and supports both Python 2 and Python 3. The entire DynamoDB API is supported by PynamoDB - including global and local secondary indexes, batch operations, binary attributes, queries, scans, etc.  Disclaimer: I wrote PynamoDB."}, {"id": 12008089, "score": 14, "vote": 0, "content": "<p>Disclaimer: I'm the current maintainer</p>\n<p>You can use <a href=\"http://pypi.python.org/pypi/dynamodb-mapper\">Dynamodb-mapper</a> Python library. It's a simple/tiny abstraction layer that allows you to <strong>map plain Python object to DynamoDB</strong>. It also features a transaction engine.</p>\n<p>For advanced tasks such as table management it is still better to directly use <a href=\"https://github.com/boto/boto\">Boto</a> (which we rely on, anyway).</p>\n", "abstract": "Disclaimer: I'm the current maintainer You can use Dynamodb-mapper Python library. It's a simple/tiny abstraction layer that allows you to map plain Python object to DynamoDB. It also features a transaction engine. For advanced tasks such as table management it is still better to directly use Boto (which we rely on, anyway)."}, {"id": 46165374, "score": 3, "vote": 0, "content": "<p>This question has been years so I believe your problem was already resolved. Just want to mention that you could use <a href=\"http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html\" rel=\"nofollow noreferrer\">boto3</a> to access DynamoDB as well nowadays.</p>\n", "abstract": "This question has been years so I believe your problem was already resolved. Just want to mention that you could use boto3 to access DynamoDB as well nowadays."}, {"id": 67762675, "score": 2, "vote": 0, "content": "<p>I'm the author of Lucid-Dynamodb, a minimalist wrapper to AWS DynamoDB. It covers all the Dynamodb operations.</p>\n<p><strong>Reference:</strong> <a href=\"https://github.com/dineshsonachalam/Lucid-Dynamodb\" rel=\"nofollow noreferrer\">https://github.com/dineshsonachalam/Lucid-Dynamodb</a></p>\n", "abstract": "I'm the author of Lucid-Dynamodb, a minimalist wrapper to AWS DynamoDB. It covers all the Dynamodb operations. Reference: https://github.com/dineshsonachalam/Lucid-Dynamodb"}]}, {"link": "https://stackoverflow.com/questions/30601107/move-models-between-django-1-8-apps-with-required-foreignkey-references", "question": {"id": "30601107", "title": "Move models between Django (1.8) apps with required ForeignKey references", "content": "<p>This is an extension to this question: <a href=\"https://stackoverflow.com/questions/25648393/how-to-move-a-model-between-two-django-apps-django-1-7/26472482#26472482\">How to move a model between two Django apps (Django 1.7)</a></p>\n<p>I need to move a bunch of models from <code>old_app</code> to <code>new_app</code>. The best answer seems to be <a href=\"https://stackoverflow.com/questions/25648393/how-to-move-a-model-between-two-django-apps-django-1-7/26472482#26472482\">Ozan's</a>, but with required foreign key references, things are bit trickier. @halfnibble presents a solution in the comments to Ozan's answer, but I'm still having trouble with the precise order of steps (e.g. when do I copy the models over to <code>new_app</code>, when do I delete the models from <code>old_app</code>, which migrations will sit in <code>old_app.migrations</code> vs. <code>new_app.migrations</code>, etc.) </p>\n<p>Any help is much appreciated! </p>\n", "abstract": "This is an extension to this question: How to move a model between two Django apps (Django 1.7) I need to move a bunch of models from old_app to new_app. The best answer seems to be Ozan's, but with required foreign key references, things are bit trickier. @halfnibble presents a solution in the comments to Ozan's answer, but I'm still having trouble with the precise order of steps (e.g. when do I copy the models over to new_app, when do I delete the models from old_app, which migrations will sit in old_app.migrations vs. new_app.migrations, etc.)  Any help is much appreciated! "}, "answers": [{"id": 30613732, "score": 87, "vote": 0, "content": "<p><strong>Migrating a model between apps.</strong></p>\n<p>The short answer is, <em>don't do it!!</em></p>\n<p>But that answer rarely works in the real world of living projects and production databases. Therefore, I have created a <a href=\"https://github.com/halfnibble/factory\" rel=\"noreferrer\">sample GitHub repo</a> to demonstrate this rather complicated process.</p>\n<p>I am using MySQL. <em>(No, those aren't my real credentials).</em></p>\n<p><strong>The Problem</strong></p>\n<p>The example I'm using is a factory project with a <strong>cars</strong> app that initially has a <code>Car</code> model and a <code>Tires</code> model. </p>\n<pre><code class=\"python\">factory\n  |_ cars\n    |_ Car\n    |_ Tires\n</code></pre>\n<p>The <code>Car</code> model has a ForeignKey relationship with <code>Tires</code>. (As in, you specify the tires via the car model). </p>\n<p>However, we soon realize that <code>Tires</code> is going to be a large model with its own views, etc., and therefore we want it in its own app. The desired structure is therefore:</p>\n<pre><code class=\"python\">factory\n  |_ cars\n    |_ Car\n  |_ tires\n    |_ Tires\n</code></pre>\n<p>And we need to keep the ForeignKey relationship between <code>Car</code> and <code>Tires</code> because too much depends on preserving the data.</p>\n<p><strong>The Solution</strong></p>\n<p><strong>Step 1.</strong> Setup initial app with bad design.</p>\n<p>Browse through the code of <a href=\"https://github.com/halfnibble/factory/tree/step1\" rel=\"noreferrer\">step 1.</a></p>\n<p><strong>Step 2.</strong> Create an admin interface and add a bunch of data containing ForeignKey relationships. </p>\n<p>View <a href=\"https://github.com/halfnibble/factory/tree/step2\" rel=\"noreferrer\">step 2.</a></p>\n<p><strong>Step 3.</strong> Decide to move the <code>Tires</code> model to its own app. Meticulously cut and paste code into the new tires app. Make sure you update the <code>Car</code> model to point to the new <code>tires.Tires</code> model.</p>\n<p>Then run <code>./manage.py makemigrations</code> and backup the database somewhere (just in case this fails horribly).</p>\n<p>Finally, run <code>./manage.py migrate</code> and see the error message of doom,</p>\n<p><strong>django.db.utils.IntegrityError: (1217, 'Cannot delete or update a parent row: a foreign key constraint fails')</strong></p>\n<p>View code and migrations so far in <a href=\"https://github.com/halfnibble/factory/tree/step3\" rel=\"noreferrer\">step 3.</a></p>\n<p><strong>Step 4.</strong> The tricky part. The auto-generated migration fails to see that you've merely copied a model to a different app. So, we have to do some things to remedy this.</p>\n<p>You can follow along and view the final migrations with comments in <a href=\"https://github.com/halfnibble/factory/tree/step4\" rel=\"noreferrer\">step 4.</a> I did test this to verify it works. </p>\n<p>First, we are going to work on <code>cars</code>. You have to make a new, empty migration. This migration actually needs to run before the most recently created migration (the one that failed to execute). Therefore, I renumbered the migration I created and changed the dependencies to run my custom migration first and then the last auto-generated migration for the <code>cars</code> app.</p>\n<p>You can create an empty migration with:</p>\n<pre><code class=\"python\">./manage.py makemigrations --empty cars\n</code></pre>\n<p><strong>Step 4.a.</strong> Make custom <em>old_app</em> migration.</p>\n<p>In this first custom migration, I'm only going to perform a \"database_operations\" migration. Django gives you the option to split \"state\" and \"database\" operations. You can see how this is done by viewing the <a href=\"https://github.com/halfnibble/factory/blob/step4/cars/migrations/0002_auto_20150603_0642.py\" rel=\"noreferrer\">code here</a>.</p>\n<p>My goal in this first step is to rename the database tables from <code>oldapp_model</code> to <code>newapp_model</code> without messing with Django's state. You have to figure out what Django would have named your database table based on the app name and model name. </p>\n<p>Now you are ready to modify the initial <code>tires</code> migration.</p>\n<p><strong>Step 4.b.</strong> Modify <em>new_app</em> initial migration</p>\n<p>The operations are fine, but we only want to modify the \"state\" and not the database. Why? Because we are keeping the database tables from the <code>cars</code> app. Also, you need to make sure that the previously made custom migration is a dependency of this migration. See the tires <a href=\"https://github.com/halfnibble/factory/blob/step4/tires/migrations/0001_initial.py\" rel=\"noreferrer\">migration file</a>.</p>\n<p>So, now we have renamed <code>cars.Tires</code> to <code>tires.Tires</code> in the database, and changed the Django state to recognize the <code>tires.Tires</code> table. </p>\n<p><strong>Step 4.c.</strong> Modify <em>old_app</em> last auto-generated migration.</p>\n<p>Going <em>back</em> to cars, we need to modify that last auto-generated migration. It should require our first custom cars migration, and the initial tires migration (that we just modified). </p>\n<p>Here we should leave the <code>AlterField</code> operations because the <code>Car</code> model <em>is pointing</em> to a different model (even though it has the same data). However, we need to remove the lines of migration concerning <code>DeleteModel</code> because the <code>cars.Tires</code> model no longer exists. It has fully converted into <code>tires.Tires</code>. View <a href=\"https://github.com/halfnibble/factory/blob/step4/cars/migrations/0003_auto_20150603_0630.py\" rel=\"noreferrer\">this migration</a>.</p>\n<p><strong>Step 4.d.</strong> Clean up stale model in <em>old_app</em>.</p>\n<p>Last but not least, you need to make a final custom migration in the cars app. Here, we will do a \"state\" operation only to delete the <code>cars.Tires</code> model. It is state-only because the database table for <code>cars.Tires</code> has already been renamed. This <a href=\"https://github.com/halfnibble/factory/blob/step4/cars/migrations/0004_auto_20150603_0701.py\" rel=\"noreferrer\">last migration</a> cleans up the remaining Django state.</p>\n", "abstract": "Migrating a model between apps. The short answer is, don't do it!! But that answer rarely works in the real world of living projects and production databases. Therefore, I have created a sample GitHub repo to demonstrate this rather complicated process. I am using MySQL. (No, those aren't my real credentials). The Problem The example I'm using is a factory project with a cars app that initially has a Car model and a Tires model.  The Car model has a ForeignKey relationship with Tires. (As in, you specify the tires via the car model).  However, we soon realize that Tires is going to be a large model with its own views, etc., and therefore we want it in its own app. The desired structure is therefore: And we need to keep the ForeignKey relationship between Car and Tires because too much depends on preserving the data. The Solution Step 1. Setup initial app with bad design. Browse through the code of step 1. Step 2. Create an admin interface and add a bunch of data containing ForeignKey relationships.  View step 2. Step 3. Decide to move the Tires model to its own app. Meticulously cut and paste code into the new tires app. Make sure you update the Car model to point to the new tires.Tires model. Then run ./manage.py makemigrations and backup the database somewhere (just in case this fails horribly). Finally, run ./manage.py migrate and see the error message of doom, django.db.utils.IntegrityError: (1217, 'Cannot delete or update a parent row: a foreign key constraint fails') View code and migrations so far in step 3. Step 4. The tricky part. The auto-generated migration fails to see that you've merely copied a model to a different app. So, we have to do some things to remedy this. You can follow along and view the final migrations with comments in step 4. I did test this to verify it works.  First, we are going to work on cars. You have to make a new, empty migration. This migration actually needs to run before the most recently created migration (the one that failed to execute). Therefore, I renumbered the migration I created and changed the dependencies to run my custom migration first and then the last auto-generated migration for the cars app. You can create an empty migration with: Step 4.a. Make custom old_app migration. In this first custom migration, I'm only going to perform a \"database_operations\" migration. Django gives you the option to split \"state\" and \"database\" operations. You can see how this is done by viewing the code here. My goal in this first step is to rename the database tables from oldapp_model to newapp_model without messing with Django's state. You have to figure out what Django would have named your database table based on the app name and model name.  Now you are ready to modify the initial tires migration. Step 4.b. Modify new_app initial migration The operations are fine, but we only want to modify the \"state\" and not the database. Why? Because we are keeping the database tables from the cars app. Also, you need to make sure that the previously made custom migration is a dependency of this migration. See the tires migration file. So, now we have renamed cars.Tires to tires.Tires in the database, and changed the Django state to recognize the tires.Tires table.  Step 4.c. Modify old_app last auto-generated migration. Going back to cars, we need to modify that last auto-generated migration. It should require our first custom cars migration, and the initial tires migration (that we just modified).  Here we should leave the AlterField operations because the Car model is pointing to a different model (even though it has the same data). However, we need to remove the lines of migration concerning DeleteModel because the cars.Tires model no longer exists. It has fully converted into tires.Tires. View this migration. Step 4.d. Clean up stale model in old_app. Last but not least, you need to make a final custom migration in the cars app. Here, we will do a \"state\" operation only to delete the cars.Tires model. It is state-only because the database table for cars.Tires has already been renamed. This last migration cleans up the remaining Django state."}, {"id": 44508527, "score": 4, "vote": 0, "content": "<p>Just now moved two models from <code>old_app</code> to <code>new_app</code>, but the FK references were in some models from <code>app_x</code> and <code>app_y</code>, instead of models from <code>old_app</code>.</p>\n<p>In this case, follow the steps provided by Nostalg.io like this:</p>\n<ul>\n<li>Move the models from <code>old_app</code> to <code>new_app</code>, then update the <code>import</code> statements across the code base.</li>\n<li><code>makemigrations</code>.</li>\n<li>Follow Step 4.a. But use <code>AlterModelTable</code> for all moved models. Two for me.</li>\n<li>Follow Step 4.b. as is.</li>\n<li>Follow Step 4.c. But also, for each app that has a newly generated migration file, manually edit them, so you migrate the <code>state_operations</code> instead.</li>\n<li>Follow Step 4.d But use <code>DeleteModel</code> for all moved models.</li>\n</ul>\n<p>Notes:</p>\n<ul>\n<li>All the edited auto-generated migration files from other apps have a dependency on the custom migration file from <code>old_app</code> where <code>AlterModelTable</code> is used to rename the table(s). (created in Step 4.a.)</li>\n<li>In my case, I had to remove the auto-generated migration file from <code>old_app</code> because I didn't have any <code>AlterField</code> operations, only <code>DeleteModel</code> and <code>RemoveField</code> operations. Or keep it with empty <code>operations = []</code></li>\n<li><p>To avoid migration exceptions when creating the test DB from scratch, make sure the custom migration from <code>old_app</code> created at Step 4.a. has all previous migration dependencies from other apps.</p>\n<pre><code class=\"python\">old_app\n  0020_auto_others\n  0021_custom_rename_models.py\n    dependencies:\n      ('old_app', '0020_auto_others'),\n      ('app_x', '0002_auto_20170608_1452'),\n      ('app_y', '0005_auto_20170608_1452'),\n      ('new_app', '0001_initial'),\n  0022_auto_maybe_empty_operations.py\n    dependencies:\n      ('old_app', '0021_custom_rename_models'),\n  0023_custom_clean_models.py\n    dependencies:\n      ('old_app', '0022_auto_maybe_empty_operations'),\napp_x\n  0001_initial.py\n  0002_auto_20170608_1452.py\n  0003_update_fk_state_operations.py\n    dependencies\n      ('app_x', '0002_auto_20170608_1452'),\n      ('old_app', '0021_custom_rename_models'),\napp_y\n  0004_auto_others_that_could_use_old_refs.py\n  0005_auto_20170608_1452.py\n  0006_update_fk_state_operations.py\n    dependencies\n      ('app_y', '0005_auto_20170608_1452'),\n      ('old_app', '0021_custom_rename_models'),\n</code></pre></li>\n</ul>\n<p>BTW: There is an open ticket about this: <a href=\"https://code.djangoproject.com/ticket/24686\" rel=\"nofollow noreferrer\">https://code.djangoproject.com/ticket/24686</a></p>\n", "abstract": "Just now moved two models from old_app to new_app, but the FK references were in some models from app_x and app_y, instead of models from old_app. In this case, follow the steps provided by Nostalg.io like this: Notes: To avoid migration exceptions when creating the test DB from scratch, make sure the custom migration from old_app created at Step 4.a. has all previous migration dependencies from other apps. BTW: There is an open ticket about this: https://code.djangoproject.com/ticket/24686"}, {"id": 47392970, "score": 2, "vote": 0, "content": "<p>In case you need to move the model and you don't have access to the app anymore (or you don't want the access), you can create a new Operation and consider to create a new model only if the migrated model does not exist.</p>\n<p>In this example I am passing 'MyModel' from old_app to myapp. </p>\n<pre><code class=\"python\">class MigrateOrCreateTable(migrations.CreateModel):\n    def __init__(self, source_table, dst_table, *args, **kwargs):\n        super(MigrateOrCreateTable, self).__init__(*args, **kwargs)\n        self.source_table = source_table\n        self.dst_table = dst_table\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        table_exists = self.source_table in schema_editor.connection.introspection.table_names()\n        if table_exists:\n            with schema_editor.connection.cursor() as cursor:\n                cursor.execute(\"RENAME TABLE {} TO {};\".format(self.source_table, self.dst_table))\n        else:\n            return super(MigrateOrCreateTable, self).database_forwards(app_label, schema_editor, from_state, to_state)\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('myapp', '0002_some_migration'),\n    ]\n\n    operations = [\n        MigrateOrCreateTable(\n            source_table='old_app_mymodel',\n            dst_table='myapp_mymodel',\n            name='MyModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=18))\n            ],\n        ),\n    ]\n</code></pre>\n", "abstract": "In case you need to move the model and you don't have access to the app anymore (or you don't want the access), you can create a new Operation and consider to create a new model only if the migrated model does not exist. In this example I am passing 'MyModel' from old_app to myapp. "}, {"id": 47537780, "score": 1, "vote": 0, "content": "<p>After work was done I tried to make new migration. But I facing with following error:\n<code>\nValueError: Unhandled pending operations for models:\n  oldapp.modelname (referred to by fields: oldapp.HistoricalProductModelName.model_ref_obj)\n</code></p>\n<p>If your Django model using <code>HistoricalRecords</code> field don't forget add additinal models/tables while following @Nostalg.io answer.</p>\n<p>Add following item to <code>database_operations</code> at the first step (4.a):</p>\n<pre><code class=\"python\">    migrations.AlterModelTable('historicalmodelname', 'newapp_historicalmodelname'),\n</code></pre>\n<p>and add additional Delete into <code>state_operations</code> at the last step (4.d):</p>\n<pre><code class=\"python\">    migrations.DeleteModel(name='HistoricalModleName'),\n</code></pre>\n", "abstract": "After work was done I tried to make new migration. But I facing with following error:\n\nValueError: Unhandled pending operations for models:\n  oldapp.modelname (referred to by fields: oldapp.HistoricalProductModelName.model_ref_obj)\n If your Django model using HistoricalRecords field don't forget add additinal models/tables while following @Nostalg.io answer. Add following item to database_operations at the first step (4.a): and add additional Delete into state_operations at the last step (4.d):"}, {"id": 48443912, "score": 1, "vote": 0, "content": "<p>Nostalg.io's way worked in forwards (auto-generating all other apps FKs referencing it). But i needed also backwards. For this, the backward AlterTable has to happen before any FKs are backwarded (in original it would happen after that).  So for this, i split the AlterTable in to 2 separate AlterTableF and AlterTableR, each working only in one direction, then using forward one instead of the original in first custom migration, and reverse one in the last cars migration (both happen in cars app). Something like this:</p>\n<pre><code class=\"python\">#cars/migrations/0002...py :\n\nclass AlterModelTableF( migrations.AlterModelTable):\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        print( 'nothing back on', app_label, self.name, self.table)\n\nclass Migration(migrations.Migration):                                                         \n    dependencies = [\n        ('cars', '0001_initial'),\n    ]\n\n    database_operations= [\n        AlterModelTableF( 'tires', 'tires_tires' ),\n        ]\n    operations = [\n        migrations.SeparateDatabaseAndState( database_operations= database_operations)         \n    ]           \n\n\n#cars/migrations/0004...py :\n\nclass AlterModelTableR( migrations.AlterModelTable):\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        print( 'nothing forw on', app_label, self.name, self.table)\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        super().database_forwards( app_label, schema_editor, from_state, to_state)\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        ('cars', '0003_auto_20150603_0630'),\n    ]\n\n    # This needs to be a state-only operation because the database model was renamed, and no longer exists according to Django.\n    state_operations = [\n        migrations.DeleteModel(\n            name='Tires',\n        ),\n    ]\n\n    database_operations= [\n        AlterModelTableR( 'tires', 'tires_tires' ),\n        ]\n    operations = [\n        # After this state operation, the Django DB state should match the actual database structure.\n       migrations.SeparateDatabaseAndState( state_operations=state_operations,\n         database_operations=database_operations)\n    ]   \n</code></pre>\n", "abstract": "Nostalg.io's way worked in forwards (auto-generating all other apps FKs referencing it). But i needed also backwards. For this, the backward AlterTable has to happen before any FKs are backwarded (in original it would happen after that).  So for this, i split the AlterTable in to 2 separate AlterTableF and AlterTableR, each working only in one direction, then using forward one instead of the original in first custom migration, and reverse one in the last cars migration (both happen in cars app). Something like this:"}, {"id": 48447926, "score": 1, "vote": 0, "content": "<p>I've built a management command to do just that - move a model from one Django app to another - based on nostalgic.io's suggestions at <a href=\"https://stackoverflow.com/a/30613732/1639699\">https://stackoverflow.com/a/30613732/1639699</a></p>\n<p>You can find it on GitHub at <a href=\"https://github.com/alexei/django-move-model\" rel=\"nofollow noreferrer\">alexei/django-move-model</a></p>\n", "abstract": "I've built a management command to do just that - move a model from one Django app to another - based on nostalgic.io's suggestions at https://stackoverflow.com/a/30613732/1639699 You can find it on GitHub at alexei/django-move-model"}, {"id": 57997935, "score": 1, "vote": 0, "content": "<p>You can do this relatively straightforwardly, but you need to follow these steps, which are summarized from a question in the <a href=\"https://groups.google.com/forum/#!searchin/django-users/Matthew$20Pava%7Csort:date/django-users/M1WYXDNtNEs/OkAvqCybBQAJ\" rel=\"nofollow noreferrer\">Django Users' Group</a>.</p>\n<ol>\n<li><p>Before moving your model to the new app, which we will call <code>new</code>, add the <code>db_table</code> option to the current model's <code>Meta</code> class. We will call the model that you want to move <code>M</code>. But you can do multiple models at once if you want to.</p>\n<pre><code class=\"python\">class M(models.Model):\n    a = models.ForeignKey(B, on_delete=models.CASCADE)\n    b = models.IntegerField()\n\n    class Meta:\n        db_table = \"new_M\"\n</code></pre></li>\n<li><p>Run <code>python manage.py makemigrations</code>. This generates a new migration file that will rename the table in the database from <code>current_M</code> to <code>new_M</code>. We will refer to this migration file as <code>x</code> later on.</p></li>\n<li><p>Now move the models to your <code>new</code> app. Remove the reference to <code>db_table</code> because Django will automatically put it in the table called <code>new_M</code>.</p></li>\n<li><p>Make the new migrations. Run <code>python manage.py makemigrations</code>. This will generate <strong><em>two</em></strong> new migrations files in our example. The first one will be in the <code>new</code> app. Verify that in the dependencies property, Django has listed <code>x</code> from the previous migrations file. The second one will be in the <code>current</code> app. Now wrap the operations list in both migrations files in a call to <code>SeparateDatabaseAndState</code> to be like so:</p>\n<pre><code class=\"python\">operations = [\n    SeparateDatabaseAndState([], [\n        migrations.CreateModel(...), ...\n    ]),\n]\n</code></pre></li>\n<li><p>Run <code>python manage.py migrate</code>. You are done. The time to do this is relatively fast because unlike some answers, you're not copying records from one table to the other. You are just renaming tables, which is a fast operation by itself.</p></li>\n</ol>\n", "abstract": "You can do this relatively straightforwardly, but you need to follow these steps, which are summarized from a question in the Django Users' Group. Before moving your model to the new app, which we will call new, add the db_table option to the current model's Meta class. We will call the model that you want to move M. But you can do multiple models at once if you want to. Run python manage.py makemigrations. This generates a new migration file that will rename the table in the database from current_M to new_M. We will refer to this migration file as x later on. Now move the models to your new app. Remove the reference to db_table because Django will automatically put it in the table called new_M. Make the new migrations. Run python manage.py makemigrations. This will generate two new migrations files in our example. The first one will be in the new app. Verify that in the dependencies property, Django has listed x from the previous migrations file. The second one will be in the current app. Now wrap the operations list in both migrations files in a call to SeparateDatabaseAndState to be like so: Run python manage.py migrate. You are done. The time to do this is relatively fast because unlike some answers, you're not copying records from one table to the other. You are just renaming tables, which is a fast operation by itself."}, {"id": 46967311, "score": 0, "vote": 0, "content": "<p>This worked for me but I'm sure I'll hear why it's a terrible idea. Add this function and an operation that calls it to your old_app migration:</p>\n<pre><code class=\"python\">def migrate_model(apps, schema_editor):\n    old_model = apps.get_model('old_app', 'MovingModel')\n    new_model = apps.get_model('new_app', 'MovingModel')\n    for mod in old_model.objects.all():\n        mod.__class__ = new_model\n        mod.save()\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('new_app', '0006_auto_20171027_0213'),\n    ]\n\n    operations = [\n        migrations.RunPython(migrate_model),\n        migrations.DeleteModel(\n            name='MovingModel',\n        ),\n    ]     \n</code></pre>\n<p>Step 1: backup your database!<br/>\nMake sure your new_app migration is run first, and/or a requirement of the old_app migration. Decline deleting the stale content type until you've completed the old_app migration.</p>\n<p>after Django 1.9 you may want to step thru a bit more carefully:<br/>\nMigration1: Create new table<br/>\nMigration2: Populate table<br/>\nMigration3: Alter fields on other tables<br/>\nMigration4: Delete old table</p>\n", "abstract": "This worked for me but I'm sure I'll hear why it's a terrible idea. Add this function and an operation that calls it to your old_app migration: Step 1: backup your database!\nMake sure your new_app migration is run first, and/or a requirement of the old_app migration. Decline deleting the stale content type until you've completed the old_app migration. after Django 1.9 you may want to step thru a bit more carefully:\nMigration1: Create new table\nMigration2: Populate table\nMigration3: Alter fields on other tables\nMigration4: Delete old table"}, {"id": 55220419, "score": 0, "vote": 0, "content": "<p>Coming back to this after a couple of months (after successfully implementing Lucianovici's approach), It seems to me that it becomes <em>much</em> simpler if you take care to point <code>db_table</code> to the old table (if you only care about the code organisation and don't mind outdated names in the database).</p>\n<ul>\n<li>You won't need AlterModelTable migrations, so there's no need for the custom first step.</li>\n<li>You still need to change the models and relations without touching the database.</li>\n</ul>\n<p>So what I did was just take the automatic migrations from Django and wrap them into migrations.SeparateDatabaseAndState.</p>\n<p>Note (again) that this only could work if you took care to point db_table to the <em>old</em> table for each model.</p>\n<p>I'm not sure if something is wrong with this that I don't see yet, but it seemed to have worked on my devel system (which I took care to backup, of course). All data looks intact. I'll take a closer look to check if any problems come up...</p>\n<p>Maybe it's also possible to later rename the database tables as well in a separate step, making this whole process less complicated.</p>\n", "abstract": "Coming back to this after a couple of months (after successfully implementing Lucianovici's approach), It seems to me that it becomes much simpler if you take care to point db_table to the old table (if you only care about the code organisation and don't mind outdated names in the database). So what I did was just take the automatic migrations from Django and wrap them into migrations.SeparateDatabaseAndState. Note (again) that this only could work if you took care to point db_table to the old table for each model. I'm not sure if something is wrong with this that I don't see yet, but it seemed to have worked on my devel system (which I took care to backup, of course). All data looks intact. I'll take a closer look to check if any problems come up... Maybe it's also possible to later rename the database tables as well in a separate step, making this whole process less complicated."}, {"id": 57435792, "score": 0, "vote": 0, "content": "<p>Coming this is one a little late but if you want the <strong>easiest path</strong> AND don't care too much about preserving your migration history. The simple solution is just to wipe migrations and refresh. </p>\n<p>I had a rather complicated app and after trying the above solutions without success for hours, I realized that I could just do.</p>\n<pre><code class=\"python\">rm cars/migrations/*\n./manage.py makemigrations\n./manage.py migrate --fake-initial\n</code></pre>\n<p>Presto! The migration history is still in Git if I need it. And since this is essentially a no-op, rolling back wasn't a concern. </p>\n", "abstract": "Coming this is one a little late but if you want the easiest path AND don't care too much about preserving your migration history. The simple solution is just to wipe migrations and refresh.  I had a rather complicated app and after trying the above solutions without success for hours, I realized that I could just do. Presto! The migration history is still in Git if I need it. And since this is essentially a no-op, rolling back wasn't a concern. "}]}, {"link": "https://stackoverflow.com/questions/4404742/how-do-i-turn-mongodb-query-into-a-json", "question": {"id": "4404742", "title": "How do I turn MongoDB query into a JSON?", "content": "<pre><code class=\"python\">for p in db.collection.find({\"test_set\":\"abc\"}):\n    posts.append(p)\nthejson = json.dumps({'results':posts})\nreturn  HttpResponse(thejson, mimetype=\"application/javascript\")\n</code></pre>\n<p>In my Django/Python code, I can't return a JSON from a mongo query because of \"ObjectID\". The error says that \"ObjectID\" is not serializable.</p>\n<p>What do I have to do?\nA hacky way would be to loop through:</p>\n<pre><code class=\"python\">for p in posts:\n    p['_id'] = \"\"\n</code></pre>\n", "abstract": "In my Django/Python code, I can't return a JSON from a mongo query because of \"ObjectID\". The error says that \"ObjectID\" is not serializable. What do I have to do?\nA hacky way would be to loop through:"}, "answers": [{"id": 4405290, "score": 31, "vote": 0, "content": "<p>The <strong>json</strong> module won't work due to things like the ObjectID. </p>\n<p>Luckily <a href=\"http://api.mongodb.com/python/current/\" rel=\"nofollow noreferrer\">PyMongo</a> provides <a href=\"http://api.mongodb.com/python/current/api/bson/json_util.html?highlight=json_util#module-bson.json_util\" rel=\"nofollow noreferrer\">json_util</a> which ...</p>\n<blockquote>\n<p>... allow[s] for specialized encoding and\n  decoding of BSON documents into Mongo\n  Extended JSON's Strict mode. This lets\n  you encode / decode BSON documents to\n  JSON even when they use special BSON\n  types.</p>\n</blockquote>\n", "abstract": "The json module won't work due to things like the ObjectID.  Luckily PyMongo provides json_util which ... ... allow[s] for specialized encoding and\n  decoding of BSON documents into Mongo\n  Extended JSON's Strict mode. This lets\n  you encode / decode BSON documents to\n  JSON even when they use special BSON\n  types."}, {"id": 11543206, "score": 27, "vote": 0, "content": "<p>Here is a simple sample, using pymongo 2.2.1</p>\n<pre><code class=\"python\">import os\nimport sys\nimport json\nimport pymongo\nfrom bson import BSON\nfrom bson import json_util\n\nif __name__ == '__main__':\n  try:\n    connection = pymongo.Connection('mongodb://localhost:27017')\n    database = connection['mongotest']\n  except:\n    print('Error: Unable to Connect')\n    connection = None\n\n  if connection is not None:\n    database[\"test\"].insert({'name': 'foo'})\n    doc = database[\"test\"].find_one({'name': 'foo'})\n    return json.dumps(doc, sort_keys=True, indent=4, default=json_util.default)\n</code></pre>\n", "abstract": "Here is a simple sample, using pymongo 2.2.1"}, {"id": 4407284, "score": 9, "vote": 0, "content": "<p>It's pretty easy to write a custom serializer which copes with the ObjectIds. Django already includes one which handles decimals and dates, so you can extend that:</p>\n<pre><code class=\"python\">from django.core.serializers.json import DjangoJSONEncoder\nfrom bson import objectid\n\nclass MongoAwareEncoder(DjangoJSONEncoder):\n    \"\"\"JSON encoder class that adds support for Mongo objectids.\"\"\"\n    def default(self, o):\n        if isinstance(o, objectid.ObjectId):\n            return str(o)\n        else:\n            return super(MongoAwareEncoder, self).default(o)\n</code></pre>\n<p>Now you can just tell <code>json</code> to use your custom serializer:</p>\n<pre><code class=\"python\">thejson = json.dumps({'results':posts}, cls=MongoAwareEncoder)\n</code></pre>\n", "abstract": "It's pretty easy to write a custom serializer which copes with the ObjectIds. Django already includes one which handles decimals and dates, so you can extend that: Now you can just tell json to use your custom serializer:"}, {"id": 41398861, "score": 8, "vote": 0, "content": "<p>Something even simpler which works for me on Python 3.6 using\nmotor==1.1\npymongo==3.4.0</p>\n<pre><code class=\"python\">from bson.json_util import dumps, loads\n\nfor mongo_doc in await cursor.to_list(length=10):\n    # mongo_doc is a &lt;class 'dict'&gt; returned from the async mongo driver, in this acse motor / pymongo.\n    # result of executing a simple find() query.\n\n    json_string = dumps(mongo_doc)\n    # serialize the &lt;class 'dict'&gt; into a &lt;class 'str'&gt; \n\n    back_to_dict = loads(json_string)\n    # to unserialize, thus return the string back to a &lt;class 'dict'&gt; with the original 'ObjectID' type.\n</code></pre>\n", "abstract": "Something even simpler which works for me on Python 3.6 using\nmotor==1.1\npymongo==3.4.0"}]}, {"link": "https://stackoverflow.com/questions/1734/any-experiences-with-protocol-buffers", "question": {"id": "1734", "title": "Any experiences with Protocol Buffers?", "content": "<p>I was just looking through some information about Google's <a href=\"http://code.google.com/apis/protocolbuffers/\" rel=\"noreferrer\">protocol buffers</a> data interchange format.  Has anyone played around with the code or even created a project around it?</p>\n<p>I'm currently using XML in a Python project for structured content created by hand in a text editor, and I was wondering what the general opinion was on Protocol Buffers as a user-facing input format.  The speed and brevity benefits definitely seem to be there, but there are so many factors when it comes to actually generating and processing the data.</p>\n", "abstract": "I was just looking through some information about Google's protocol buffers data interchange format.  Has anyone played around with the code or even created a project around it? I'm currently using XML in a Python project for structured content created by hand in a text editor, and I was wondering what the general opinion was on Protocol Buffers as a user-facing input format.  The speed and brevity benefits definitely seem to be there, but there are so many factors when it comes to actually generating and processing the data."}, "answers": [{"id": 1780, "score": 13, "vote": 0, "content": "<p>If you are looking for user facing interaction, stick with xml. It has more support, understanding, and general acceptance currently. If it's internal, I would say that protocol buffers are a great idea.</p>\n<p>Maybe in a few years as more tools come out to support protocol buffers, then start looking towards that for a public facing api. Until then... <a href=\"http://en.wikipedia.org/wiki/JSON\" rel=\"noreferrer\">JSON</a>?</p>\n", "abstract": "If you are looking for user facing interaction, stick with xml. It has more support, understanding, and general acceptance currently. If it's internal, I would say that protocol buffers are a great idea. Maybe in a few years as more tools come out to support protocol buffers, then start looking towards that for a public facing api. Until then... JSON?"}, {"id": 6161, "score": 11, "vote": 0, "content": "<p>Protocol buffers are intended to optimize communications between machines. They are really not intended for human interaction. Also, the format is binary, so it could not replace XML in that use case. </p>\n<p>I would also recommend <a href=\"http://en.wikipedia.org/wiki/JSON\" rel=\"noreferrer\">JSON</a> as being the most compact text-based format.</p>\n", "abstract": "Protocol buffers are intended to optimize communications between machines. They are really not intended for human interaction. Also, the format is binary, so it could not replace XML in that use case.  I would also recommend JSON as being the most compact text-based format."}, {"id": 1418610, "score": 4, "vote": 0, "content": "<p>Another drawback of binary format like PB is that if there is a single bit of error, the entire data file is not parsable, but with JSON or XML, as the last resort you can still manually fix the error because it is human readable and has redundancy built-in..</p>\n", "abstract": "Another drawback of binary format like PB is that if there is a single bit of error, the entire data file is not parsable, but with JSON or XML, as the last resort you can still manually fix the error because it is human readable and has redundancy built-in.."}, {"id": 123093, "score": 3, "vote": 0, "content": "<p>From your brief description, it sounds like protocol buffers is not the right fit.  The phrase \"structured content created by hand in a text editor\" pretty much screams for XML.</p>\n<p>But if you want efficient, low latency communications with data structures that are not shared outside your organization, binary serialization such as protocol buffers can offer a huge win.</p>\n", "abstract": "From your brief description, it sounds like protocol buffers is not the right fit.  The phrase \"structured content created by hand in a text editor\" pretty much screams for XML. But if you want efficient, low latency communications with data structures that are not shared outside your organization, binary serialization such as protocol buffers can offer a huge win."}]}, {"link": "https://stackoverflow.com/questions/839069/cursor-rowcount-always-1-in-sqlite3-in-python3k", "question": {"id": "839069", "title": "cursor.rowcount always -1 in sqlite3 in python3k", "content": "<p>I am trying to get the <code>rowcount</code> of a <code>sqlite3</code> <code>cursor</code> in my Python3k program, but I am puzzled, as the <code>rowcount</code> is always <code>-1</code>, despite what Python3 docs say (actually it is contradictory, it should be <code>None</code>). Even after fetching all the rows, <code>rowcount</code> stays at <code>-1</code>. Is it a <code>sqlite3</code> bug? I have already checked if there are rows in the table.</p>\n<p>I can get around this checking if a <code>fetchone()</code> returns something different than <code>None</code>, but I thought this issue would be nice to discuss.</p>\n<p>Thanks.</p>\n", "abstract": "I am trying to get the rowcount of a sqlite3 cursor in my Python3k program, but I am puzzled, as the rowcount is always -1, despite what Python3 docs say (actually it is contradictory, it should be None). Even after fetching all the rows, rowcount stays at -1. Is it a sqlite3 bug? I have already checked if there are rows in the table. I can get around this checking if a fetchone() returns something different than None, but I thought this issue would be nice to discuss. Thanks."}, "answers": [{"id": 839419, "score": 27, "vote": 0, "content": "<p>From the <a href=\"http://docs.python.org/3.0/library/sqlite3.html#sqlite3.Cursor.rowcount\" rel=\"noreferrer\">documentation</a>:</p>\n<blockquote>\n<p>As required by the Python DB API Spec,\n  the rowcount attribute \u201cis -1 in case\n  no executeXX() has been performed on\n  the cursor or the rowcount of the last\n  operation is not determinable by the\n  interface\u201d.</p>\n<p><strong>This includes <code>SELECT</code> statements\n  because we cannot determine the number\n  of rows a query produced until all\n  rows were fetched.</strong></p>\n</blockquote>\n<p>That means <strong>all</strong> <code>SELECT</code> statements <strong>won't have a <code>rowcount</code></strong>. The behaviour you're observing is documented. </p>\n<p><strong>EDIT:</strong> Documentation doesn't say anywhere that <code>rowcount</code> <strong>will</strong> be updated after you do a <code>fetchall()</code> so it is just wrong to assume that.</p>\n", "abstract": "From the documentation: As required by the Python DB API Spec,\n  the rowcount attribute \u201cis -1 in case\n  no executeXX() has been performed on\n  the cursor or the rowcount of the last\n  operation is not determinable by the\n  interface\u201d. This includes SELECT statements\n  because we cannot determine the number\n  of rows a query produced until all\n  rows were fetched. That means all SELECT statements won't have a rowcount. The behaviour you're observing is documented.  EDIT: Documentation doesn't say anywhere that rowcount will be updated after you do a fetchall() so it is just wrong to assume that."}, {"id": 21838197, "score": 22, "vote": 0, "content": "<pre><code class=\"python\">cursor = newdb.execute('select * from mydb;')\nprint len(cursor.fetchall())\n</code></pre>\n<p>The fetchall() will return a list of the rows returned from the select.  Len of that list will give you the rowcount.</p>\n", "abstract": "The fetchall() will return a list of the rows returned from the select.  Len of that list will give you the rowcount."}, {"id": 845555, "score": 9, "vote": 0, "content": "<p>Instead of \"checking if a fetchone() returns something different than None\", I suggest:</p>\n<pre><code class=\"python\">cursor.execute('SELECT * FROM foobar')\nfor row in cursor:\n   ...\n</code></pre>\n<p>this is <code>sqlite</code>-only (not supported in other DB API implementations) but very handy for <code>sqlite</code>-specific Python code (<em>and</em> fully documented, see <a href=\"http://docs.python.org/library/sqlite3.html\" rel=\"noreferrer\">http://docs.python.org/library/sqlite3.html</a>).</p>\n", "abstract": "Instead of \"checking if a fetchone() returns something different than None\", I suggest: this is sqlite-only (not supported in other DB API implementations) but very handy for sqlite-specific Python code (and fully documented, see http://docs.python.org/library/sqlite3.html)."}, {"id": 37788479, "score": 9, "vote": 0, "content": "<p>May better count the rows this way:</p>\n<pre><code class=\"python\"> print cur.execute(\"SELECT COUNT(*) FROM table_name\").fetchone()[0]\n</code></pre>\n", "abstract": "May better count the rows this way:"}, {"id": 67355852, "score": 1, "vote": 0, "content": "<p>I've spent too long trying to find this, if you use this line you would want to use the <code>.rowcount</code> it should work for you. I'm using it to check if my statement will return any data.</p>\n<pre><code class=\"python\">    if (len(cursor.execute(sql).fetchall())) &lt; 1: # checks there will be data by seeing if the length of the list make when getting the data is at least 1\n        print(\"No data gathered from statement\") #\n    else:\n       #RUN CODE HERE\n</code></pre>\n", "abstract": "I've spent too long trying to find this, if you use this line you would want to use the .rowcount it should work for you. I'm using it to check if my statement will return any data."}, {"id": 41626042, "score": -6, "vote": 0, "content": "<p>Using PYCharm, in debug mode, if you put your cursor on your cursor variable, a small + will appear, when you click on it, you see different properties of your object.</p>\n<p>In my cursor of 1 element, I see:</p>\n<pre><code class=\"python\">rowcount={int}\narraysize={int}1\n</code></pre>\n<p>So, in your code, just use:</p>\n<pre><code class=\"python\">print(cursor.arraysize)\n1\n</code></pre>\n", "abstract": "Using PYCharm, in debug mode, if you put your cursor on your cursor variable, a small + will appear, when you click on it, you see different properties of your object. In my cursor of 1 element, I see: So, in your code, just use:"}]}, {"link": "https://stackoverflow.com/questions/27047630/django-batching-bulk-update-or-create", "question": {"id": "27047630", "title": "Django batching/bulk update_or_create?", "content": "<p>I have data in the database which needs updating periodically. The source of the data returns everything that's available at that point in time, so will include new data that is not already in the database.</p>\n<p>As I loop through the source data I don't want to be making 1000s of individual writes if possible.</p>\n<p>Is there anything such as <code>update_or_create</code> but works in batches?</p>\n<p>One thought was using <code>update_or_create</code> in combination with manual transactions, but I'm not sure if that just queues up the individual writes or if it would combine it all into one SQL insert?</p>\n<p>Or similarly could using <code>@commit_on_success()</code> on a function with <code>update_or_create</code> inside a the loop work?</p>\n<p>I am not doing anything with the data other than translating it and saving it to a model. Nothing is dependent on that model existing during the loop.</p>\n", "abstract": "I have data in the database which needs updating periodically. The source of the data returns everything that's available at that point in time, so will include new data that is not already in the database. As I loop through the source data I don't want to be making 1000s of individual writes if possible. Is there anything such as update_or_create but works in batches? One thought was using update_or_create in combination with manual transactions, but I'm not sure if that just queues up the individual writes or if it would combine it all into one SQL insert? Or similarly could using @commit_on_success() on a function with update_or_create inside a the loop work? I am not doing anything with the data other than translating it and saving it to a model. Nothing is dependent on that model existing during the loop."}, "answers": [{"id": 66322489, "score": 11, "vote": 0, "content": "<p>Since Django added support for bulk_update, this is now somewhat possible, though you need to do 3 database calls (a get, a bulk create, and a bulk update) per batch.  It's a bit challenging to make a good interface to a general purpose function here, as you want the function to support both efficient querying as well as the updates.  Here is a method I implemented that is designed for bulk update_or_create where you have a number of common identifying keys (which could be empty) and one identifying key that varies among the batch.</p>\n<p>This is implemented as a method on a base model, but can be used independently of that.  This also assumes that the base model has an <code>auto_now</code> timestamp on the model named <code>updated_on</code>; if this is not the case, the lines of the code that assume this have been commented for easy modification.</p>\n<p>In order to use this in batches, chunk your updates into batches before calling it.  This is also a way to get around data that can have one of a small number of values for a secondary identifier without having to change the interface.</p>\n<pre><code class=\"python\">class BaseModel(models.Model):\n    updated_on = models.DateTimeField(auto_now=True)\n    \n    @classmethod\n    def bulk_update_or_create(cls, common_keys, unique_key_name, unique_key_to_defaults):\n        \"\"\"\n        common_keys: {field_name: field_value}\n        unique_key_name: field_name\n        unique_key_to_defaults: {field_value: {field_name: field_value}}\n        \n        ex. Event.bulk_update_or_create(\n            {\"organization\": organization}, \"external_id\", {1234: {\"started\": True}}\n        )\n        \"\"\"\n        with transaction.atomic():\n            filter_kwargs = dict(common_keys)\n            filter_kwargs[f\"{unique_key_name}__in\"] = unique_key_to_defaults.keys()\n            existing_objs = {\n                getattr(obj, unique_key_name): obj\n                for obj in cls.objects.filter(**filter_kwargs).select_for_update()\n            }\n            \n            create_data = {\n                k: v for k, v in unique_key_to_defaults.items() if k not in existing_objs\n            }\n            for unique_key_value, obj in create_data.items():\n                obj[unique_key_name] = unique_key_value\n                obj.update(common_keys)\n            creates = [cls(**obj_data) for obj_data in create_data.values()]\n            if creates:\n                cls.objects.bulk_create(creates)\n\n            # This set should contain the name of the `auto_now` field of the model\n            update_fields = {\"updated_on\"}\n            updates = []\n            for key, obj in existing_objs.items():\n                obj.update(unique_key_to_defaults[key], save=False)\n                update_fields.update(unique_key_to_defaults[key].keys())\n                updates.append(obj)\n            if existing_objs:\n                cls.objects.bulk_update(updates, update_fields)\n        return len(creates), len(updates)\n\n    def update(self, update_dict=None, save=True, **kwargs):\n        \"\"\" Helper method to update objects \"\"\"\n        if not update_dict:\n            update_dict = kwargs\n        # This set should contain the name of the `auto_now` field of the model\n        update_fields = {\"updated_on\"}\n        for k, v in update_dict.items():\n            setattr(self, k, v)\n            update_fields.add(k)\n        if save:\n            self.save(update_fields=update_fields)\n</code></pre>\n<p>Example usage:</p>\n<pre><code class=\"python\">class Event(BaseModel):\n    organization = models.ForeignKey(Organization)\n    external_id = models.IntegerField(unique=True)\n    started = models.BooleanField()\n\n\norganization = Organization.objects.get(...)\nupdates_by_external_id = {\n    1234: {\"started\": True},\n    2345: {\"started\": True},\n    3456: {\"started\": False},\n}\nEvent.bulk_update_or_create(\n    {\"organization\": organization}, \"external_id\", updates_by_external_id\n)\n</code></pre>\n<h2>Possible Race Conditions</h2>\n<p>The code above leverages a transaction and select-for-update to prevent race conditions on updates.  There is, however, a possible race condition on inserts if two threads or processes are trying to create objects with the same identifiers.</p>\n<p>The easy mitigation is to ensure that the combination of your common_keys and your unique_key is a database-enforced uniqueness constraint (which is the intended use of this function).  This can be achieved with either the unique_key referencing a field with <code>unique=True</code>, or with the unique_key combined with a subset of the common_keys enforced as unique together by a <a href=\"https://docs.djangoproject.com/en/dev/ref/models/constraints/#django.db.models.UniqueConstraint\" rel=\"nofollow noreferrer\">UniqueConstraint</a>).  With database-enforced uniqueness protection, if multiple threads are trying to perform conflicting creates, all but one will fail with an <code>IntegrityError</code>.  Due to the enclosing transaction, threads that fail will perform no changes and can be safely retried or ignored (a conflicting create that failed could just be treated as a create that happened first and then was immediately overwritten).</p>\n<p>If leveraging uniqueness constraints is not possible, then you will either need to implement your own concurrency control or <a href=\"https://stackoverflow.com/q/19686204/2800876\">lock the entire table</a>.</p>\n", "abstract": "Since Django added support for bulk_update, this is now somewhat possible, though you need to do 3 database calls (a get, a bulk create, and a bulk update) per batch.  It's a bit challenging to make a good interface to a general purpose function here, as you want the function to support both efficient querying as well as the updates.  Here is a method I implemented that is designed for bulk update_or_create where you have a number of common identifying keys (which could be empty) and one identifying key that varies among the batch. This is implemented as a method on a base model, but can be used independently of that.  This also assumes that the base model has an auto_now timestamp on the model named updated_on; if this is not the case, the lines of the code that assume this have been commented for easy modification. In order to use this in batches, chunk your updates into batches before calling it.  This is also a way to get around data that can have one of a small number of values for a secondary identifier without having to change the interface. Example usage: The code above leverages a transaction and select-for-update to prevent race conditions on updates.  There is, however, a possible race condition on inserts if two threads or processes are trying to create objects with the same identifiers. The easy mitigation is to ensure that the combination of your common_keys and your unique_key is a database-enforced uniqueness constraint (which is the intended use of this function).  This can be achieved with either the unique_key referencing a field with unique=True, or with the unique_key combined with a subset of the common_keys enforced as unique together by a UniqueConstraint).  With database-enforced uniqueness protection, if multiple threads are trying to perform conflicting creates, all but one will fail with an IntegrityError.  Due to the enclosing transaction, threads that fail will perform no changes and can be safely retried or ignored (a conflicting create that failed could just be treated as a create that happened first and then was immediately overwritten). If leveraging uniqueness constraints is not possible, then you will either need to implement your own concurrency control or lock the entire table."}, {"id": 36779677, "score": 3, "vote": 0, "content": "<p>Batching your updates is going to be an upsert command and like @imposeren said, Postgres 9.5 gives you that ability. I think Mysql 5.7 does as well (see <a href=\"http://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html\" rel=\"nofollow\">http://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html</a>) depending on your exact needs. That said it's probably easiest to just use a db cursor. Nothing wrong with that, it's there for when the ORM just isn't enough.</p>\n<p>Something along these lines should work. It's psuedo-ish code, so don't just cut-n-paste this but the concept is there for ya.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">class GroupByChunk(object):\n    def __init__(self, size):\n        self.count = 0\n        self.size = size\n        self.toggle = False\n\n    def __call__(self, *args, **kwargs):\n        if self.count &gt;= self.size:  # Allows for size 0\n            self.toggle = not self.toggle\n            self.count = 0\n        self.count += 1\n        return self.toggle\n\ndef batch_update(db_results, upsert_sql):\n    with transaction.atomic():\n        cursor = connection.cursor()   \n        for chunk in itertools.groupby(db_results, GroupByChunk(size=1000)):\n            cursor.execute_many(upsert_sql, chunk)\n</code></pre>\n<p>Assumptions here are:</p>\n<ul>\n<li><code>db_results</code> is some kind of results iterator, either in a list or dictionary</li>\n<li>A result from <code>db_results</code> can be fed directly into a raw sql exec statement</li>\n<li>If any of the batch updates fail, you'll be rolling back ALL of them. If you want to move that to for each chunk, just push the <code>with</code> block down a bit</li>\n</ul>\n", "abstract": "Batching your updates is going to be an upsert command and like @imposeren said, Postgres 9.5 gives you that ability. I think Mysql 5.7 does as well (see http://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html) depending on your exact needs. That said it's probably easiest to just use a db cursor. Nothing wrong with that, it's there for when the ORM just isn't enough. Something along these lines should work. It's psuedo-ish code, so don't just cut-n-paste this but the concept is there for ya. Assumptions here are:"}, {"id": 71050396, "score": 1, "vote": 0, "content": "<p>There is <a href=\"https://pypi.org/project/django-bulk-update-or-create/\" rel=\"nofollow noreferrer\">django-bulk-update-or-create</a> library for Django that can do that.</p>\n", "abstract": "There is django-bulk-update-or-create library for Django that can do that."}, {"id": 73621304, "score": 0, "vote": 0, "content": "<p>I have been using the @Zags answer and I think it's the best solution. But I'd want to advice about a little issue in his code.</p>\n<pre><code class=\"python\">        update_fields = {\"updated_on\"}\n        updates = []\n        for key, obj in existing_objs.items():\n            obj.update(unique_key_to_defaults[key], save=False)\n            update_fields.update(unique_key_to_defaults[key].keys())\n            updates.append(obj)\n        if existing_objs:\n            cls.objects.bulk_update(updates, update_fields)\n</code></pre>\n<p>If you are using auto_now=True fields they are not going to be updated if you use .update() or bulk_update() this is because the fields \"auto_now\" triggers with a .save() as you can read in the <a href=\"https://docs.djangoproject.com/en/4.1/ref/models/fields/#django.db.models.DateField.auto_now\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n<p>In case you have an auto_now field F.e: updated_on, it will be better to add it explicitly in the unique_key_to_defaults dict.</p>\n<pre><code class=\"python\">\"unique_value\" : {\n        \"field1..\" : value...,\n        \"updated_on\" : timezone.now()\n    }...\n</code></pre>\n", "abstract": "I have been using the @Zags answer and I think it's the best solution. But I'd want to advice about a little issue in his code. If you are using auto_now=True fields they are not going to be updated if you use .update() or bulk_update() this is because the fields \"auto_now\" triggers with a .save() as you can read in the documentation. In case you have an auto_now field F.e: updated_on, it will be better to add it explicitly in the unique_key_to_defaults dict."}, {"id": 74189912, "score": 0, "vote": 0, "content": "<p>As of Django 4.1, the <strong><a href=\"https://docs.djangoproject.com/en/4.1/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\" rel=\"nofollow noreferrer\"><code>bulk_create</code></a></strong> method supports <em>upserts</em> via <strong><code>update_conflicts</code></strong>, which is the single query, batch equivalent of <strong><code>update_or_create</code></strong>:</p>\n<pre><code class=\"python\">class Foo(models.Model):\n    a = models.IntegerField(unique=True)\n    b = models.IntegerField()\n\nqueryset = [Foo(1, 1), Foo(1, 2)]\n\nFoo.objects.bulk_create(\n    queryset, \n    update_conflicts=True,\n    unique_fields=['a'],\n    update_fields=['b'],\n)\n</code></pre>\n", "abstract": "As of Django 4.1, the bulk_create method supports upserts via update_conflicts, which is the single query, batch equivalent of update_or_create:"}]}, {"link": "https://stackoverflow.com/questions/3506678/in-django-how-do-i-select-100-random-records-from-the-database", "question": {"id": "3506678", "title": "In Django, how do I select 100 random records from the database?", "content": "<pre><code class=\"python\">myqueryset = Content.objects.filter(random 100)\n</code></pre>\n", "abstract": ""}, "answers": [{"id": 3506692, "score": 72, "vote": 0, "content": "<pre><code class=\"python\">Content.objects.all().order_by('?')[:100]\n</code></pre>\n<p>See the <a href=\"http://docs.djangoproject.com/en/dev/ref/models/querysets/#order-by-fields\" rel=\"noreferrer\">order_by docs</a>. Also be aware this approach does not scale well (in fact, it scales really, really badly). See <a href=\"https://stackoverflow.com/questions/1731346/how-to-get-two-random-records-with-django/6405601#6405601\">this SO answer</a> for a better way to handle random selection when you have large amounts of data.</p>\n", "abstract": "See the order_by docs. Also be aware this approach does not scale well (in fact, it scales really, really badly). See this SO answer for a better way to handle random selection when you have large amounts of data."}, {"id": 3506779, "score": 10, "vote": 0, "content": "<p>If you're going to do this more than once, you need to design this into your database.</p>\n<p>If you're doing it once, you can afford to pay the hefty penalty.  This gets you exactly 100 with really good random properties.  However, it uses a lot of memory.</p>\n<pre><code class=\"python\">pool= list( Content.objects.all() )\nrandom.shuffle( pool )\nobject_list = pool[:100]\n</code></pre>\n<p>Here's another algorithm that's also kind of slow since it may search the entire table.  It doesn't use very much memory at all and it may not get exactly 100.</p>\n<pre><code class=\"python\">total_count= Content.objects.count()\nfraction = 100./total_count\nobject_list = [ c for c in Content.objects.all() if random.random() &lt; fraction ]\n</code></pre>\n<p>If you want to do this more than once, you need to add an attribute to Content to allow effective filtering for \"random\" values.  For example, you might do this.</p>\n<pre><code class=\"python\">class Content( models.Model ):\n    ... etc. ...\n    def subset( self ):\n        return self.id % 32768\n</code></pre>\n<p>This will partition your data into 32768 distinct subsets.  Each subset is 1/32768'th of your data.  To get 100 random items, you need 100*32768/total_count subsets of your data.</p>\n<pre><code class=\"python\">total_count = Content.objects.count()\nno_of_subsets= 100*32768/total_count\nobject_list = Content.objects.filter( subset__lte=no_of_subsets )\n</code></pre>\n<p>This is <strong>fast</strong> and it's reproducible.  The subsets are \"arbitrary\" not technically \"random\".</p>\n", "abstract": "If you're going to do this more than once, you need to design this into your database. If you're doing it once, you can afford to pay the hefty penalty.  This gets you exactly 100 with really good random properties.  However, it uses a lot of memory. Here's another algorithm that's also kind of slow since it may search the entire table.  It doesn't use very much memory at all and it may not get exactly 100. If you want to do this more than once, you need to add an attribute to Content to allow effective filtering for \"random\" values.  For example, you might do this. This will partition your data into 32768 distinct subsets.  Each subset is 1/32768'th of your data.  To get 100 random items, you need 100*32768/total_count subsets of your data. This is fast and it's reproducible.  The subsets are \"arbitrary\" not technically \"random\"."}, {"id": 10897176, "score": 1, "vote": 0, "content": "<p>I do:</p>\n<pre><code class=\"python\">import random    \nobject_list = list(Content.objects.filter(foo=bar).values()[:100])\nrandom.shuffle(object_list)\n</code></pre>\n<p>Runs only single-simple MySQL query and is good on performance.</p>\n", "abstract": "I do: Runs only single-simple MySQL query and is good on performance."}]}, {"link": "https://stackoverflow.com/questions/3047412/in-memory-database-in-python", "question": {"id": "3047412", "title": "in-memory database in Python", "content": "<p>I'm doing some queries in Python on a large database to get some stats out of the database. I want these stats to be in-memory so other programs can use them without going to a database. </p>\n<p>I was thinking of how to structure them, and after trying to set up some complicated nested dictionaries, I realized that a good representation would be an SQL table. I don't want to store the data back into the persistent database, though. Are there any in-memory implementations of an SQL database that supports querying the data with SQL syntax?</p>\n", "abstract": "I'm doing some queries in Python on a large database to get some stats out of the database. I want these stats to be in-memory so other programs can use them without going to a database.  I was thinking of how to structure them, and after trying to set up some complicated nested dictionaries, I realized that a good representation would be an SQL table. I don't want to store the data back into the persistent database, though. Are there any in-memory implementations of an SQL database that supports querying the data with SQL syntax?"}, "answers": [{"id": 3047435, "score": 55, "vote": 0, "content": "<p>SQLite3 might work. The Python interface <a href=\"http://docs.python.org/library/sqlite3.html\" rel=\"noreferrer\">does support</a> the in-memory implementation that the SQLite3 C API offers. </p>\n<p>From the spec:</p>\n<blockquote>\n<p>You can also supply the special name\n  :memory: to create a database in RAM.</p>\n</blockquote>\n<p>It's also relatively cheap with transactions, depending on what you are doing. To get going, just:</p>\n<pre><code class=\"python\">import sqlite3\nconn = sqlite3.connect(':memory:')\n</code></pre>\n<p>You can then proceed like you were using a regular database.</p>\n<p>Depending on your data - if you can get by with key/value (strings, hashes, lists, sets, sorted sets, etc) - <a href=\"http://redis.io\" rel=\"noreferrer\">Redis</a> might be another option to explore (as you mentioned that you wanted to share with other programs). </p>\n", "abstract": "SQLite3 might work. The Python interface does support the in-memory implementation that the SQLite3 C API offers.  From the spec: You can also supply the special name\n  :memory: to create a database in RAM. It's also relatively cheap with transactions, depending on what you are doing. To get going, just: You can then proceed like you were using a regular database. Depending on your data - if you can get by with key/value (strings, hashes, lists, sets, sorted sets, etc) - Redis might be another option to explore (as you mentioned that you wanted to share with other programs). "}, {"id": 61931555, "score": 7, "vote": 0, "content": "<p>It may not seem obvious, but <a href=\"https://pandas.pydata.org/\" rel=\"noreferrer\">pandas</a> has a lot of relational capabilities.  See <a href=\"https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#compare-with-sql-join\" rel=\"noreferrer\">comparison with SQL</a></p>\n", "abstract": "It may not seem obvious, but pandas has a lot of relational capabilities.  See comparison with SQL"}, {"id": 3047432, "score": 1, "vote": 0, "content": "<p>I guess, SQLite3 will be the best option then.</p>\n<p>If possible, take a look at <a href=\"http://memcached.org/\" rel=\"nofollow noreferrer\">memcached</a>. (for key-value pair, lighting fast!)</p>\n<p><strong>UPDATE 1:</strong></p>\n<p><a href=\"http://hsqldb.org/\" rel=\"nofollow noreferrer\">HSQLDB</a> for SQL Like tables. (no python support)</p>\n", "abstract": "I guess, SQLite3 will be the best option then. If possible, take a look at memcached. (for key-value pair, lighting fast!) UPDATE 1: HSQLDB for SQL Like tables. (no python support)"}, {"id": 60002811, "score": 1, "vote": 0, "content": "<p>Extremely late to the party, but pyfilesystem2 (with which I am not affiliated) seems to be a perfect fit:</p>\n<p><a href=\"https://pyfilesystem2.readthedocs.io\" rel=\"nofollow noreferrer\">https://pyfilesystem2.readthedocs.io</a></p>\n<pre><code class=\"python\">pip install fs\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from fs import open_fs\nmem_fs = open_fs(u'mem://')\n...\n</code></pre>\n", "abstract": "Extremely late to the party, but pyfilesystem2 (with which I am not affiliated) seems to be a perfect fit: https://pyfilesystem2.readthedocs.io"}, {"id": 65153849, "score": 1, "vote": 0, "content": "<p>In-memory databases usually do not support memory paging option (for the whole database or certain tables), i,e, total size of the database should be smaller than the available physical memory or maximum shared memory size.</p>\n<p>Depending on your application, data-access pattern, size of database and available system memory for database, you have a few choices:</p>\n<p>a. <strong>Pickled Python Data in File System</strong><br/>\nIt stores structured Python data structure (such as list of dictionaries/lists/tuples/sets, dictionary of lists/pandas dataframes/numpy series, etc.) in pickled format so that they could be used immediately and convienently upon unpickled. AFAIK, Python does not use file system as backing store for Python objects in memory implicitly but host operating system may swap out Python processes for higher priority processes. This is suitable for static data, having smaller memory size compared to available system memory. These pickled data could be copied to other computers, read by multiple dependent or independent processes in the same computer. The actual database file or memory size has higher overhead than size of the data. It is the fastest way to access the data as the data is in the same memory of the Python process, and without a query parsing step.</p>\n<p>b. <strong>In-memory Database</strong><br/>\nIt stores dynamic or static data in the memory. Possible in-memory libraries that with Python API binding are Redis, sqlite3, Berkeley Database, rqlite, etc.  Different in-memory databases offer different features</p>\n<ul>\n<li>Database may be locked in the physical memory so that it is not swapped to memory backing store by the host operating system. However the actual implementation for the same libray may vary across different operating systems.</li>\n<li>The database may be served by a database server process.</li>\n<li>The in-memory may be accessed by multiple dependent or independent processes.</li>\n<li>Support full, partial or no ACID model.</li>\n<li>In-memory database could be persistent to physical files so that it is available when the host operating is restarted.</li>\n<li>Support snapshots or/and different database copies for backup or database management.</li>\n<li>Support distributed database using master-slave, cluster models.</li>\n<li>Support from simple key-value lookup to advanced query, filter, group functions (such as SQL, NoSQL)</li>\n</ul>\n<p>c. <strong>Memory-map Database/Data Structure</strong><br/>\nIt stores static or dynamic data which could be larger than physical memory of the host operating system. Python developers could use API such as <code>mmap.mmap()</code> <code>numpy.memmap()</code> to map certain files into process memory space.  The files could be arranged into index and data so that data could be lookup/accessed via index lookup.  This is actually the mechanism used by various database libraries.  Python developers could implement custom techniques to access/update data efficiency.</p>\n", "abstract": "In-memory databases usually do not support memory paging option (for the whole database or certain tables), i,e, total size of the database should be smaller than the available physical memory or maximum shared memory size. Depending on your application, data-access pattern, size of database and available system memory for database, you have a few choices: a. Pickled Python Data in File System\nIt stores structured Python data structure (such as list of dictionaries/lists/tuples/sets, dictionary of lists/pandas dataframes/numpy series, etc.) in pickled format so that they could be used immediately and convienently upon unpickled. AFAIK, Python does not use file system as backing store for Python objects in memory implicitly but host operating system may swap out Python processes for higher priority processes. This is suitable for static data, having smaller memory size compared to available system memory. These pickled data could be copied to other computers, read by multiple dependent or independent processes in the same computer. The actual database file or memory size has higher overhead than size of the data. It is the fastest way to access the data as the data is in the same memory of the Python process, and without a query parsing step. b. In-memory Database\nIt stores dynamic or static data in the memory. Possible in-memory libraries that with Python API binding are Redis, sqlite3, Berkeley Database, rqlite, etc.  Different in-memory databases offer different features c. Memory-map Database/Data Structure\nIt stores static or dynamic data which could be larger than physical memory of the host operating system. Python developers could use API such as mmap.mmap() numpy.memmap() to map certain files into process memory space.  The files could be arranged into index and data so that data could be lookup/accessed via index lookup.  This is actually the mechanism used by various database libraries.  Python developers could implement custom techniques to access/update data efficiency."}, {"id": 3047444, "score": 0, "vote": 0, "content": "<p>You could possibly use a database like <a href=\"http://www.sqlite.org/\" rel=\"nofollow noreferrer\">SQLite</a>.  It's not strictly speaking in memory, but it is fairly light and would be completely separate from your main database.</p>\n", "abstract": "You could possibly use a database like SQLite.  It's not strictly speaking in memory, but it is fairly light and would be completely separate from your main database."}]}, {"link": "https://stackoverflow.com/questions/13092268/how-do-you-join-two-tables-on-a-foreign-key-field-using-django-orm", "question": {"id": "13092268", "title": "How do you join two tables on a foreign key field using django ORM?", "content": "<p>Let's assume I have the following models:</p>\n<pre><code class=\"python\">class Position(models.Model):\n    name = models.CharField()\n\nclass PositionStats(models.Model):\n    position = models.ForeignKey(Position)\n    averageYards = models.CharField()\n    averageCatches = models.CharField()\n\nclass PlayerStats(models.Model):\n    player = models.ForeignKey(Player)\n    averageYards = models.CharField()\n    averageCatches = models.CharField()\n\nclass Player(models.Model):\n    name = models.CharField()\n    position = models.ForeignKey(Position)\n</code></pre>\n<p>I want to perform the equivalent SQL query using django's ORM:</p>\n<pre><code class=\"python\">SELECT *\n\nFROM PlayerStats\n\nJOIN Player ON player\n\nJOIN PositionStats ON PositionStats.position = Player.position\n</code></pre>\n<p>How would I do that with django's ORM? The query isn't exactly correct, but the idea is that I want a single query, using django's ORM, that gives me <code>PlayerStats</code> joined with <code>PositionStats</code> based on the player's position.</p>\n", "abstract": "Let's assume I have the following models: I want to perform the equivalent SQL query using django's ORM: How would I do that with django's ORM? The query isn't exactly correct, but the idea is that I want a single query, using django's ORM, that gives me PlayerStats joined with PositionStats based on the player's position."}, "answers": [{"id": 41191600, "score": 36, "vote": 0, "content": "<p>I've been working with django for a while now and I have had a pretty rough time figuring out the table joins, but I think I finally understand and I would like to pass this on to others so they may avoid the frustration that I had with it.</p>\n<p>Consider the following model.py:</p>\n<pre><code class=\"python\">class EventsMeetinglocation(models.Model):\n    id = models.IntegerField(primary_key=True)\n    name = models.CharField(max_length=100)\n    address = models.CharField(max_length=200)\n\n    class Meta:\n        managed = True\n        db_table = 'events_meetinglocation'\n\nclass EventsBoardmeeting(models.Model):\n    id = models.IntegerField(primary_key=True)\n    date = models.DateTimeField()\n    agenda_id = models.IntegerField(blank=True, null=True)\n    location_id = models.ForeignKey(EventsMeetinglocation)\n    minutes_id = models.IntegerField(blank=True, null=True)\n\n    class Meta:\n       managed = True\n       db_table = 'events_boardmeeting'\n</code></pre>\n<p>Here we can see that <code>location_id</code> in <code>EventsBoardmeeting</code> is a foreign key for the id in <code>EventsMeetinglocation</code>. This means that we should be able to query the information in <code>EventsMeetinglocation</code> by going through <code>EventsBoardmeeting</code>. </p>\n<p>Now consider the following views.py:</p>\n<pre><code class=\"python\">def meetings(request):\n    meetingData = EventsBoardmeeting.objects.all()\n    return render(request, 'board/meetings.html', {'data': meetingData })\n</code></pre>\n<p>As stated many times before in may other posts, django takes care of joins automatically. When we query everything in <code>EventsBoardmeeting</code> we also get any related information by foreign key as well, But the way that we access this in html is a little different. We have to go through the variable used as the foreign key to access the information associated with that join. For example:</p>\n<pre><code class=\"python\">{% for x in data %}\n   {{ x.location_id.name }}\n{% endfor %}\n</code></pre>\n<p>The above references ALL of the names in the table that were the result of the join on foreign key. <code>x</code> is essentially the <code>EventsBoardmeeting</code> table, so when we access <code>x.location_id</code> we are accessing the foreign key which gives us access to the information in <code>EventsMeetinglocation</code>. </p>\n", "abstract": "I've been working with django for a while now and I have had a pretty rough time figuring out the table joins, but I think I finally understand and I would like to pass this on to others so they may avoid the frustration that I had with it. Consider the following model.py: Here we can see that location_id in EventsBoardmeeting is a foreign key for the id in EventsMeetinglocation. This means that we should be able to query the information in EventsMeetinglocation by going through EventsBoardmeeting.  Now consider the following views.py: As stated many times before in may other posts, django takes care of joins automatically. When we query everything in EventsBoardmeeting we also get any related information by foreign key as well, But the way that we access this in html is a little different. We have to go through the variable used as the foreign key to access the information associated with that join. For example: The above references ALL of the names in the table that were the result of the join on foreign key. x is essentially the EventsBoardmeeting table, so when we access x.location_id we are accessing the foreign key which gives us access to the information in EventsMeetinglocation. "}, {"id": 49357908, "score": 16, "vote": 0, "content": "<p><code>select_related()</code> and <code>prefetch_related()</code> is your solution. They work almost same way but has some difference.</p>\n<p><code>select_related()</code> works by creating an SQL join and including the fields of the related object in the SELECT statement. For this reason, <code>select_related</code> gets the related objects in the same database query. But it only works for one-to-one or one-to-many relation. Example is below-</p>\n<pre><code class=\"python\">entry = Entry.objects.select_related('blog').get(id=5)\nor\nentries = Entry.objects.filter(foo='bar').select_related('blog')\n</code></pre>\n<p><code>prefetch_related()</code>, on the other hand, does a separate lookup for each relationship and does the \u2018joining\u2019 in Python. This allows it to prefetch many-to-many and many-to-one objects, which cannot be done using <code>select_related</code>. So <code>prefetch_related</code> will execute only one query for each relation. Example is given below-</p>\n<pre><code class=\"python\">Pizza.objects.all().prefetch_related('toppings')\n</code></pre>\n", "abstract": "select_related() and prefetch_related() is your solution. They work almost same way but has some difference. select_related() works by creating an SQL join and including the fields of the related object in the SELECT statement. For this reason, select_related gets the related objects in the same database query. But it only works for one-to-one or one-to-many relation. Example is below- prefetch_related(), on the other hand, does a separate lookup for each relationship and does the \u2018joining\u2019 in Python. This allows it to prefetch many-to-many and many-to-one objects, which cannot be done using select_related. So prefetch_related will execute only one query for each relation. Example is given below-"}, {"id": 13096423, "score": 13, "vote": 0, "content": "<p>It isn't one query, but it's pretty efficient. This does one query for each table involved, and joins them in Python.  More on <code>prefetch_related</code> here: <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#prefetch-related\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#prefetch-related</a></p>\n<pre><code class=\"python\">Player.objects.filter(name=\"Bob\").prefetch_related(\n        'position__positionstats_set', 'playerstats_set')\n</code></pre>\n", "abstract": "It isn't one query, but it's pretty efficient. This does one query for each table involved, and joins them in Python.  More on prefetch_related here: https://docs.djangoproject.com/en/dev/ref/models/querysets/#prefetch-related"}, {"id": 67787020, "score": 3, "vote": 0, "content": "<p>In Django 3.2, the framework automatically follows relationships when using method QuerySet.filter()</p>\n<pre><code class=\"python\"># The API automatically follows relationships as far as you need.\n# Use double underscores to separate relationships.\n# This works as many levels deep as you want; there's no limit.\n# Find all Choices for any question whose pub_date is in this year\n# (reusing the 'current_year' variable we created above).\n&gt;&gt;&gt; Choice.objects.filter(question__pub_date__year=current_year)\n</code></pre>\n<p>This compiles to the following SQL query:</p>\n<pre><code class=\"python\">SELECT\n    \"polls_choice\".\"id\",\n    \"polls_choice\".\"question_id\",\n    \"polls_choice\".\"choice_text\",\n    \"polls_choice\".\"votes\"\nFROM\n    \"polls_choice\"\nINNER JOIN \"polls_question\" ON\n    (\"polls_choice\".\"question_id\" = \"polls_question\".\"id\")\nWHERE\n    \"polls_question\".\"pub_date\" BETWEEN 2020-12-31 23:00:00 AND 2021-12-31 22:59:59.999999\n</code></pre>\n<p>See tutorial here: <a href=\"https://docs.djangoproject.com/en/3.2/intro/tutorial02/\" rel=\"nofollow noreferrer\">https://docs.djangoproject.com/en/3.2/intro/tutorial02/</a></p>\n", "abstract": "In Django 3.2, the framework automatically follows relationships when using method QuerySet.filter() This compiles to the following SQL query: See tutorial here: https://docs.djangoproject.com/en/3.2/intro/tutorial02/"}, {"id": 19029831, "score": -4, "vote": 0, "content": "<p>From <code>django.db</code> import connection In your view include the below statement:</p>\n<pre><code class=\"python\">cursor = connection.cursor()\ncursor.execute(\"select * From Postion ON Position.name = Player.position JOIN\nPlayerStats ON Player.name = \nPlayerStats.player JOIN PositionStats ON Position.name = PositionStats.player\")\nsolution = cursor.fetchall()\n</code></pre>\n", "abstract": "From django.db import connection In your view include the below statement:"}]}, {"link": "https://stackoverflow.com/questions/10214042/can-sqlalchemy-be-configured-to-be-non-blocking", "question": {"id": "10214042", "title": "Can SQLAlchemy be configured to be non-blocking?", "content": "<p>I'm under the impression that database calls through SQLAlchemy will block and aren't suitable for use in anything other than synchronous code. Am I correct (I hope I'm not!) or is there a way to configure it to be non-blocking?</p>\n", "abstract": "I'm under the impression that database calls through SQLAlchemy will block and aren't suitable for use in anything other than synchronous code. Am I correct (I hope I'm not!) or is there a way to configure it to be non-blocking?"}, "answers": [{"id": 10216120, "score": 29, "vote": 0, "content": "<p>You can use SQLA in a non-blocking style using <a href=\"http://www.gevent.org/\">gevent</a>.  Here's an example using psycopg2, using psycopg2's <a href=\"http://initd.org/psycopg/docs/advanced.html#support-to-coroutine-libraries\">coroutine support</a>:</p>\n<p><a href=\"https://bitbucket.org/zzzeek/green_sqla/\">https://bitbucket.org/zzzeek/green_sqla/</a></p>\n<p>I've also heard folks use the same idea with <a href=\"https://github.com/petehunt/PyMySQL/\">pymysql</a>.  As pymysql is in pure Python and uses the sockets library, gevent patches the socket library to be asynchronous.</p>\n", "abstract": "You can use SQLA in a non-blocking style using gevent.  Here's an example using psycopg2, using psycopg2's coroutine support: https://bitbucket.org/zzzeek/green_sqla/ I've also heard folks use the same idea with pymysql.  As pymysql is in pure Python and uses the sockets library, gevent patches the socket library to be asynchronous."}, {"id": 10214684, "score": 6, "vote": 0, "content": "<p>Have a look at <a href=\"http://www.tornadoweb.org/\" rel=\"noreferrer\">Tornado</a> as they've got some neat non-blocking libraries, particularly tornado.gen.</p>\n<p>We use that along with <a href=\"https://github.com/FSX/momoko\" rel=\"noreferrer\">Momoko</a>, a non-blocking psycopg wrapper lib for Tornado. It's been great so far. Perhaps the only drawback is you lose all the model object stuff that SQLAlchemy gives you. Performance is unreal though.</p>\n", "abstract": "Have a look at Tornado as they've got some neat non-blocking libraries, particularly tornado.gen. We use that along with Momoko, a non-blocking psycopg wrapper lib for Tornado. It's been great so far. Perhaps the only drawback is you lose all the model object stuff that SQLAlchemy gives you. Performance is unreal though."}, {"id": 45646354, "score": 2, "vote": 0, "content": "<p>Without the help of greenlet, the answer is no, in the context of asyncio.</p>\n<p>However it is possible to use only a part of SQLAlchemy in asyncio. Please find example in the <a href=\"https://github.com/fantix/gino\" rel=\"nofollow noreferrer\">GINO project</a>, where we used only SQLAlchemy core without engine and full execution context to make a simple ORM in asyncio.</p>\n", "abstract": "Without the help of greenlet, the answer is no, in the context of asyncio. However it is possible to use only a part of SQLAlchemy in asyncio. Please find example in the GINO project, where we used only SQLAlchemy core without engine and full execution context to make a simple ORM in asyncio."}]}, {"link": "https://stackoverflow.com/questions/15300422/difference-between-manytoonerel-and-foreignkey", "question": {"id": "15300422", "title": "Difference between ManyToOneRel and ForeignKey?", "content": "<p>In django, what's the difference between a ManyToOneRel and a ForeignKey field?</p>\n", "abstract": "In django, what's the difference between a ManyToOneRel and a ForeignKey field?"}, "answers": [{"id": 17047519, "score": 49, "vote": 0, "content": "<p>Django relations model exposes (and documents) only <em>OneToOneField</em>, <em>ForeignKey</em> and <em>ManyToManyField</em>, which corresponds to the inner</p>\n<ul>\n<li><strong>OneToOneField</strong> -&gt; <strong>OneToOneRel</strong></li>\n<li><strong>ForeignKey</strong> -&gt; <strong>ManyToOneRel</strong></li>\n<li><strong>ManyToManyField</strong> -&gt; <strong>ManyToManyRel</strong></li>\n</ul>\n<p>See source of <em>django.db.models.fields.related</em> for further details.</p>\n", "abstract": "Django relations model exposes (and documents) only OneToOneField, ForeignKey and ManyToManyField, which corresponds to the inner See source of django.db.models.fields.related for further details."}, {"id": 15300763, "score": 42, "vote": 0, "content": "<p><code>ManyToOneRel</code> is not a <code>django.db.models.fields.Field</code>, it is a class that is used inside Django but not in the user code.</p>\n", "abstract": "ManyToOneRel is not a django.db.models.fields.Field, it is a class that is used inside Django but not in the user code."}]}, {"link": "https://stackoverflow.com/questions/27498991/sqlalchemy-primary-key-without-auto-increment", "question": {"id": "27498991", "title": "sqlalchemy primary key without auto-increment", "content": "<p>I'm trying to establish a table with a layout approximately like the following:</p>\n<pre><code class=\"python\">class Widget(db.Model):\n    __tablename__ = 'widgets'\n    ext_id = db.Column(db.Integer, primary_key=True)\n    w_code = db.Column(db.String(34), unique=True)\n    # other traits follow...\n</code></pre>\n<p>All field values are provided through an external system, and new Widgets are discovered and some of the omitted trait values may change over time (very gradually) but the ext_id and w_code are guaranteed to be unique.  Given the nature of the values for ext_id it behaves ideally as a primary key.</p>\n<p>However when I create a new record, specifying the ext_id value, the value is not used in storage. Instead the values in ext_id follow an auto-increment behavior.  </p>\n<pre><code class=\"python\">&gt;&gt;&gt; # from a clean database\n&gt;&gt;&gt; skill = Widget(ext_id=7723, w_code=u'IGF35ac9')\n&gt;&gt;&gt; session.add(skill)\n&gt;&gt;&gt; session.commit()\n&gt;&gt;&gt; Skill.query.first().ext_id\n1\n&gt;&gt;&gt; \n</code></pre>\n<p>How can I specify to SQLAlchemy that the ext_id field should be used as the primary key field without auto-increment? </p>\n<p>Note: \nI could add an extra synthetic id column as the primary key and make ext_id be a unique column instead but this both complicates my code and adds a (minimal) extra bloat to the database and all I/O to it. I'm hoping to avoid that.</p>\n<p>Issue originated from a larger project but I was able to create a smaller repro.</p>\n<p>Testing with sqlite</p>\n", "abstract": "I'm trying to establish a table with a layout approximately like the following: All field values are provided through an external system, and new Widgets are discovered and some of the omitted trait values may change over time (very gradually) but the ext_id and w_code are guaranteed to be unique.  Given the nature of the values for ext_id it behaves ideally as a primary key. However when I create a new record, specifying the ext_id value, the value is not used in storage. Instead the values in ext_id follow an auto-increment behavior.   How can I specify to SQLAlchemy that the ext_id field should be used as the primary key field without auto-increment?  Note: \nI could add an extra synthetic id column as the primary key and make ext_id be a unique column instead but this both complicates my code and adds a (minimal) extra bloat to the database and all I/O to it. I'm hoping to avoid that. Issue originated from a larger project but I was able to create a smaller repro. Testing with sqlite"}, "answers": [{"id": 27506856, "score": 52, "vote": 0, "content": "<p>Set <a href=\"http://docs.sqlalchemy.org/en/rel_1_1/core/metadata.html#sqlalchemy.schema.Column.params.autoincrement\" rel=\"noreferrer\"><code>autoincrement=False</code></a> to disable creating a sequence or serial for the primary key.</p>\n<pre><code class=\"python\">ext_id = db.Column(db.Integer, primary_key=True, autoincrement=False)\n</code></pre>\n", "abstract": "Set autoincrement=False to disable creating a sequence or serial for the primary key."}]}, {"link": "https://stackoverflow.com/questions/5642323/tool-for-automatically-creating-data-for-django-model", "question": {"id": "5642323", "title": "Tool for automatically creating data for django model", "content": "<p>I want to test models which are explained in <a href=\"https://docs.djangoproject.com/en/3.1/topics/db/aggregation/\" rel=\"nofollow noreferrer\">the Django tutorial</a>. Is there an automatic way to fill them with sample data? It's one of them:</p>\n<pre><code class=\"python\">class Book(models.Model):\n    name = models.CharField(max_length=300)\n    pages = models.IntegerField()\n    price = models.DecimalField(max_digits=10, decimal_places=2)\n    rating = models.FloatField()\n    authors = models.ManyToManyField(Author)\n    publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE)\n    pubdate = models.DateField()\n</code></pre>\n<p>Any suggestion?</p>\n", "abstract": "I want to test models which are explained in the Django tutorial. Is there an automatic way to fill them with sample data? It's one of them: Any suggestion?"}, "answers": [{"id": 5642396, "score": 25, "vote": 0, "content": "<p>I haven't used it myself, but <a href=\"http://pypi.python.org/pypi/django-autofixture\" rel=\"noreferrer\">django-autofixture</a> looks pretty much like what you are after.</p>\n<p>Other similar apps are listed in this grid: <a href=\"https://www.djangopackages.com/grids/g/fixtures/\" rel=\"noreferrer\">https://www.djangopackages.com/grids/g/fixtures/</a></p>\n", "abstract": "I haven't used it myself, but django-autofixture looks pretty much like what you are after. Other similar apps are listed in this grid: https://www.djangopackages.com/grids/g/fixtures/"}, {"id": 5642411, "score": 8, "vote": 0, "content": "<p><a href=\"http://www.generatedata.com/\" rel=\"noreferrer\">http://www.generatedata.com/</a></p>\n<p>This has some pretty nice generic field types that aren't Django-specific</p>\n", "abstract": "http://www.generatedata.com/ This has some pretty nice generic field types that aren't Django-specific"}, {"id": 5642561, "score": 8, "vote": 0, "content": "<p><a href=\"https://github.com/aerosol/django-dilla\" rel=\"nofollow\">django-dilla</a> was built specifically to populate your django models with 'spam' data. The below is taken directly from the site example after defining some settings. It will even let you define your own 'spammers' that will generate data in a particular format.</p>\n<pre><code class=\"python\">$ ./manage.py run_dilla --cycles=100\nDilla is going to spam your database. Do you wish to proceed? (Y/N)Y\nDilla finished!\n    2 app(s) spammed 900 row(s) affected, 2498 field(s) filled, \\\n    502 field(s) ommited.\n</code></pre>\n", "abstract": "django-dilla was built specifically to populate your django models with 'spam' data. The below is taken directly from the site example after defining some settings. It will even let you define your own 'spammers' that will generate data in a particular format."}, {"id": 5994289, "score": 2, "vote": 0, "content": "<p>Checkout django-mockups: <a href=\"https://github.com/sorl/django-mockups\" rel=\"nofollow\">https://github.com/sorl/django-mockups</a></p>\n<p>It will automatically generate data for any model, including foreign key and many to many.  You can run it as-is out-of-the-box, giving it the maximum depth for relationships, and it will generate data fully exploiting your model.</p>\n<p>You can also write your own generators and factories to get fine-grained control over relationships and to generate data specific to your application, rather than just random data.  I just got through using it on a project, and it saved me literally days of work setting up test data.</p>\n", "abstract": "Checkout django-mockups: https://github.com/sorl/django-mockups It will automatically generate data for any model, including foreign key and many to many.  You can run it as-is out-of-the-box, giving it the maximum depth for relationships, and it will generate data fully exploiting your model. You can also write your own generators and factories to get fine-grained control over relationships and to generate data specific to your application, rather than just random data.  I just got through using it on a project, and it saved me literally days of work setting up test data."}, {"id": 52577969, "score": 1, "vote": 0, "content": "<p>If the question is still actual you may try package <a href=\"https://github.com/lk-geimfari/mimesis\" rel=\"nofollow noreferrer\">django-mimesis</a>.\nIt offers to fill database with dummy data in different languages according to types of fields. Also you may download some pictures automatically using specified topics. Not always works great, sometime pictures' topics are mismatched. But for me it is enough.</p>\n", "abstract": "If the question is still actual you may try package django-mimesis.\nIt offers to fill database with dummy data in different languages according to types of fields. Also you may download some pictures automatically using specified topics. Not always works great, sometime pictures' topics are mismatched. But for me it is enough."}, {"id": 23159846, "score": 0, "vote": 0, "content": "<p><a href=\"http://django-eadred.readthedocs.org/en/latest/about.html\" rel=\"nofollow noreferrer\">Django-eadred</a> was designed for \"generating sample data.\"</p>\n<p>As its docs state</p>\n<blockquote>\n<p>eadred allows you to programmatically generate the data using model makers, factories, fixtures, random seeds\u2014whatever your needs are.</p>\n<p>Additionally, eadred provides library functions to make generating data easier.</p>\n</blockquote>\n", "abstract": "Django-eadred was designed for \"generating sample data.\" As its docs state eadred allows you to programmatically generate the data using model makers, factories, fixtures, random seeds\u2014whatever your needs are. Additionally, eadred provides library functions to make generating data easier."}]}, {"link": "https://stackoverflow.com/questions/4791555/how-do-i-do-greater-than-less-than-using-mongodb", "question": {"id": "4791555", "title": "How do I do greater than/less than using MongoDB?", "content": "<p>I'm using the pymongo driver.</p>\n<p>Can someone take a look at pymongo and tell me how to do greater than? I\"m used to doing : for everything.</p>\n", "abstract": "I'm using the pymongo driver. Can someone take a look at pymongo and tell me how to do greater than? I\"m used to doing : for everything."}, "answers": [{"id": 4791583, "score": 50, "vote": 0, "content": "<p>Have you seen <a href=\"https://pymongo.readthedocs.io/en/stable/tutorial.html#range-queries\" rel=\"noreferrer\">the doc</a> ? Take from the manual :</p>\n<pre><code class=\"python\">&gt;&gt;&gt; d = datetime.datetime(2009, 11, 12, 12)\n&gt;&gt;&gt; for post in posts.find({\"date\": {\"$lt\": d}}).sort(\"author\"):\n...   post\n...\n{u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n</code></pre>\n", "abstract": "Have you seen the doc ? Take from the manual :"}, {"id": 4791611, "score": 26, "vote": 0, "content": "<p>If you wanna query and find docs which have a field greater than something you can</p>\n<pre><code class=\"python\">users.find({\"age\": {\"$gt\": 20}})\n</code></pre>\n<p>Check out the advanced query section of Mongodb for more reference.</p>\n<p>--Sai</p>\n", "abstract": "If you wanna query and find docs which have a field greater than something you can Check out the advanced query section of Mongodb for more reference. --Sai"}, {"id": 4791597, "score": 6, "vote": 0, "content": "<p><code>gt</code> &amp; <code>lt</code> are the operators. See <a href=\"http://www.mongodb.org/display/DOCS/SQL+to+Mongo+Mapping+Chart\" rel=\"noreferrer\">http://www.mongodb.org/display/DOCS/SQL+to+Mongo+Mapping+Chart</a></p>\n", "abstract": "gt & lt are the operators. See http://www.mongodb.org/display/DOCS/SQL+to+Mongo+Mapping+Chart"}]}, {"link": "https://stackoverflow.com/questions/3985812/how-to-implement-autoincrement-on-google-appengine", "question": {"id": "3985812", "title": "How to implement &quot;autoincrement&quot; on Google AppEngine", "content": "<p>I have to label something in a \"strong monotone increasing\" fashion. Be it Invoice Numbers, shipping label numbers or the like.</p>\n<ol>\n<li>A number MUST NOT BE used twice</li>\n<li>Every number SHOULD BE used when exactly all smaller numbers have been used (no holes).</li>\n</ol>\n<p>Fancy way of saying: I need to count 1,2,3,4 ...\nThe number Space I have available are typically 100.000 numbers and I need perhaps 1000 a day.</p>\n<p>I know this is a hard Problem in distributed systems and often we are much better of with GUIDs. But in this case for legal reasons I need \"traditional numbering\".</p>\n<p>Can this be implemented on Google AppEngine (preferably in Python)?</p>\n", "abstract": "I have to label something in a \"strong monotone increasing\" fashion. Be it Invoice Numbers, shipping label numbers or the like. Fancy way of saying: I need to count 1,2,3,4 ...\nThe number Space I have available are typically 100.000 numbers and I need perhaps 1000 a day. I know this is a hard Problem in distributed systems and often we are much better of with GUIDs. But in this case for legal reasons I need \"traditional numbering\". Can this be implemented on Google AppEngine (preferably in Python)?"}, "answers": [{"id": 3986265, "score": 25, "vote": 0, "content": "<p>If you absolutely have to have sequentially increasing numbers with no gaps, you'll need to use a single entity, which you update in a transaction  to 'consume' each new number. You'll be limited, in practice, to about 1-5 numbers generated per second - which sounds like it'll be fine for your requirements.</p>\n", "abstract": "If you absolutely have to have sequentially increasing numbers with no gaps, you'll need to use a single entity, which you update in a transaction  to 'consume' each new number. You'll be limited, in practice, to about 1-5 numbers generated per second - which sounds like it'll be fine for your requirements."}, {"id": 4056817, "score": 7, "vote": 0, "content": "<p>If you drop the requirement that IDs must be strictly sequential, you can use a hierarchical allocation scheme. The basic idea/limitation is that transactions must not affect multiple storage groups.</p>\n<p>For example, assuming you have the notion of \"users\", you can allocate a storage group for each user (creating some global object per user). Each user has a list of reserved IDs. When allocating an ID for a user, pick a reserved one (in a transaction). If no IDs are left, make a new transaction allocating 100 IDs (say) from the global pool, then make a new transaction to add them to the user and simultaneously withdraw one. Assuming each user interacts with the application only sequentially, there will be no concurrency on the user objects.</p>\n", "abstract": "If you drop the requirement that IDs must be strictly sequential, you can use a hierarchical allocation scheme. The basic idea/limitation is that transactions must not affect multiple storage groups. For example, assuming you have the notion of \"users\", you can allocate a storage group for each user (creating some global object per user). Each user has a list of reserved IDs. When allocating an ID for a user, pick a reserved one (in a transaction). If no IDs are left, make a new transaction allocating 100 IDs (say) from the global pool, then make a new transaction to add them to the user and simultaneously withdraw one. Assuming each user interacts with the application only sequentially, there will be no concurrency on the user objects."}, {"id": 4177134, "score": 7, "vote": 0, "content": "<p>The <a href=\"https://github.com/mdornseif/appengine-toolkit\" rel=\"noreferrer\">gaetk - Google AppEngine Toolkit</a> now comes with a simple library function to get a number in a sequence. It is based on Nick Johnson's transactional approach and can be used quite easily as a foundation for Martin von L\u00f6wis' sharding approach:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from gaeth.sequences import * \n&gt;&gt;&gt; init_sequence('invoce_number', start=1, end=0xffffffff)\n&gt;&gt;&gt; get_numbers('invoce_number', 2)\n[1, 2]\n</code></pre>\n<p>The functionality is basically implemented like this:</p>\n<pre><code class=\"python\">def _get_numbers_helper(keys, needed):\n  results = []\n\n  for key in keys:\n    seq = db.get(key)\n    start = seq.current or seq.start\n    end = seq.end\n    avail = end - start\n    consumed = needed\n    if avail &lt;= needed:\n      seq.active = False\n      consumed = avail\n    seq.current = start + consumed\n    seq.put()\n    results += range(start, start + consumed)\n    needed -= consumed\n    if needed == 0:\n      return results\n  raise RuntimeError('Not enough sequence space to allocate %d numbers.' % needed)\n\ndef get_numbers(needed):\n  query = gaetkSequence.all(keys_only=True).filter('active = ', True)\n  return db.run_in_transaction(_get_numbers_helper, query.fetch(5), needed)\n</code></pre>\n", "abstract": "The gaetk - Google AppEngine Toolkit now comes with a simple library function to get a number in a sequence. It is based on Nick Johnson's transactional approach and can be used quite easily as a foundation for Martin von L\u00f6wis' sharding approach: The functionality is basically implemented like this:"}, {"id": 15724101, "score": 6, "vote": 0, "content": "<p>If you aren't too strict on the sequential, you can \"shard\" your incrementer.  This could be thought of as an \"eventually sequential\" counter.</p>\n<p>Basically, you have one entity that is the \"master\" count.  Then you have a number of entities (based on the load you need to handle) that have their own counters.  These shards reserve chunks of ids from the master and serve out from their range until they run out of values.</p>\n<p>Quick algorithm:</p>\n<ol>\n<li>You need to get an ID.</li>\n<li>Pick a shard at random.</li>\n<li>If the shard's start is less than its end, take it's start and increment it.</li>\n<li>If the shard's start is equal to (or more oh-oh) its end, go to the master, take the value and add an amount <code>n</code> to it.  Set the shards start to the retrieved value plus one and end to the retrieved plus <code>n</code>.</li>\n</ol>\n<p>This can scale quite well, however, the amount you can be out by is the number of shards multiplied by your <code>n</code> value.  If you want your records to appear to go up this will probably work, but if you want to have them represent order it won't be accurate.  It is also important to note that the latest values may have holes, so if you are using that to scan for some reason you will have to mind the gaps.</p>\n<h3>Edit</h3>\n<p>I needed this for my app (that was why I was searching the question :P ) so I have implemented my solution.  It can grab single IDs as well as efficiently grab batches. I have tested it in a controlled environment (on appengine) and it performed very well.  You can find the code <a href=\"https://github.com/kevincox/appengine-sequential-ids\" rel=\"nofollow\">on github</a>.</p>\n", "abstract": "If you aren't too strict on the sequential, you can \"shard\" your incrementer.  This could be thought of as an \"eventually sequential\" counter. Basically, you have one entity that is the \"master\" count.  Then you have a number of entities (based on the load you need to handle) that have their own counters.  These shards reserve chunks of ids from the master and serve out from their range until they run out of values. Quick algorithm: This can scale quite well, however, the amount you can be out by is the number of shards multiplied by your n value.  If you want your records to appear to go up this will probably work, but if you want to have them represent order it won't be accurate.  It is also important to note that the latest values may have holes, so if you are using that to scan for some reason you will have to mind the gaps. I needed this for my app (that was why I was searching the question :P ) so I have implemented my solution.  It can grab single IDs as well as efficiently grab batches. I have tested it in a controlled environment (on appengine) and it performed very well.  You can find the code on github."}, {"id": 3985846, "score": 4, "vote": 0, "content": "<p>Take a look at how the <a href=\"http://code.google.com/appengine/articles/sharding_counters.html\" rel=\"nofollow\">sharded counters</a> are made. It may help you. Also do you really need them to be numeric. If unique is satisfying just use the entity keys.</p>\n", "abstract": "Take a look at how the sharded counters are made. It may help you. Also do you really need them to be numeric. If unique is satisfying just use the entity keys."}, {"id": 24977138, "score": 1, "vote": 0, "content": "<p>Alternatively, you could use allocate_ids(), as people have suggested, then creating these entities up front (i.e. with placeholder property values).</p>\n<pre><code class=\"python\">first, last = MyModel.allocate_ids(1000000)\nkeys = [Key(MyModel, id) for id in range(first, last+1)]\n</code></pre>\n<p>Then, when creating a new invoice, your code could run through these entries to find the one with the lowest ID such that the placeholder properties have not yet been overwritten with real data.</p>\n<p>I haven't put that into practice, but seems like it should work in theory, most likely with the same limitations people have already mentioned.</p>\n", "abstract": "Alternatively, you could use allocate_ids(), as people have suggested, then creating these entities up front (i.e. with placeholder property values). Then, when creating a new invoice, your code could run through these entries to find the one with the lowest ID such that the placeholder properties have not yet been overwritten with real data. I haven't put that into practice, but seems like it should work in theory, most likely with the same limitations people have already mentioned."}, {"id": 15731054, "score": 0, "vote": 0, "content": "<p>Remember: Sharding increases the probability that you will get a unique, auto-increment value, but does not guarantee it. Please take Nick's advice if you MUST have a unique auto-incrment.</p>\n", "abstract": "Remember: Sharding increases the probability that you will get a unique, auto-increment value, but does not guarantee it. Please take Nick's advice if you MUST have a unique auto-incrment."}, {"id": 24977092, "score": 0, "vote": 0, "content": "<p>I implemented something very simplistic for my blog, which increments an IntegerProperty, <code>iden</code> rather than the Key ID.</p>\n<p>I define <code>max_iden()</code> to find the maximum <code>iden</code> integer currently being used. This function scans through all existing blog posts.</p>\n<pre><code class=\"python\">def max_iden():\n    max_entity = Post.gql(\"order by iden desc\").get()\n    if max_entity:\n        return max_entity.iden\n    return 1000    # If this is the very first entry, start at number 1000\n</code></pre>\n<p>Then, when creating a new blog post, I assign it an <code>iden</code> property of <code>max_iden() + 1</code></p>\n<pre><code class=\"python\">new_iden = max_iden() + 1\np = Post(parent=blog_key(), header=header, body=body, iden=new_iden)\np.put()\n</code></pre>\n<p>I wonder if you might also want to add some sort of verification function after this, i.e. to ensure the max_iden() has now incremented, before moving onto the next invoice.</p>\n<p>Altogether: fragile, inefficient code.</p>\n", "abstract": "I implemented something very simplistic for my blog, which increments an IntegerProperty, iden rather than the Key ID. I define max_iden() to find the maximum iden integer currently being used. This function scans through all existing blog posts. Then, when creating a new blog post, I assign it an iden property of max_iden() + 1 I wonder if you might also want to add some sort of verification function after this, i.e. to ensure the max_iden() has now incremented, before moving onto the next invoice. Altogether: fragile, inefficient code."}, {"id": 29419735, "score": 0, "vote": 0, "content": "<p>I'm thinking in using the following solution: use CloudSQL (MySQL) to insert the records and assign the sequential ID (maybe with a Task Queue), later (using a Cron Task) move the records from CloudSQL back to the Datastore.</p>\n<p>The entities also can have a UUID, so we can map the entities from the Datastore in CloudSQL, and also have the sequential ID (for legal reasons).</p>\n", "abstract": "I'm thinking in using the following solution: use CloudSQL (MySQL) to insert the records and assign the sequential ID (maybe with a Task Queue), later (using a Cron Task) move the records from CloudSQL back to the Datastore. The entities also can have a UUID, so we can map the entities from the Datastore in CloudSQL, and also have the sequential ID (for legal reasons)."}]}, {"link": "https://stackoverflow.com/questions/4019081/how-to-copy-a-sqlite-table-from-a-disk-database-to-a-memory-database-in-python", "question": {"id": "4019081", "title": "How to copy a sqlite table from a disk database to a memory database in python?", "content": "<p>How to copy a disk based sqlite table to a memory database in python? I know the schema of the table.</p>\n", "abstract": "How to copy a disk based sqlite table to a memory database in python? I know the schema of the table."}, "answers": [{"id": 4019182, "score": 37, "vote": 0, "content": "<p>this code is more general but maybe it can help you:</p>\n<pre><code class=\"python\">import sqlite3\n\nnew_db = sqlite3.connect(':memory:') # create a memory database\n\nold_db = sqlite3.connect('test.db')\n\nquery = \"\".join(line for line in old_db.iterdump())\n\n# Dump old database in the new one. \nnew_db.executescript(query)\n</code></pre>\n<p><strong>EDIT :</strong> for getting your specify table you can just change in the for loop like this:</p>\n<pre><code class=\"python\">name_table = \"test_table\"  # name of the table that you want to get.\n\nfor line in old_db.iterdump():\n    if name_table in line:\n        query = line\n        break\n</code></pre>\n", "abstract": "this code is more general but maybe it can help you: EDIT : for getting your specify table you can just change in the for loop like this:"}, {"id": 4020717, "score": 3, "vote": 0, "content": "<p>Check out the <a href=\"http://www.sqlite.org/backup.html\" rel=\"nofollow\">SQLite Backup API</a>. The example is in C, but this should show you how it's done efficiently.</p>\n", "abstract": "Check out the SQLite Backup API. The example is in C, but this should show you how it's done efficiently."}, {"id": 15534010, "score": 1, "vote": 0, "content": "<p>An alternative, for those using python and sqlalchemy:</p>\n<p><a href=\"http://www.tylerlesmann.com/2009/apr/27/copying-databases-across-platforms-sqlalchemy/\" rel=\"nofollow\">http://www.tylerlesmann.com/2009/apr/27/copying-databases-across-platforms-sqlalchemy/</a></p>\n<p>The idea behind is to replicate the metadata from the source database to the target database and then transfer column after column.</p>\n", "abstract": "An alternative, for those using python and sqlalchemy: http://www.tylerlesmann.com/2009/apr/27/copying-databases-across-platforms-sqlalchemy/ The idea behind is to replicate the metadata from the source database to the target database and then transfer column after column."}]}, {"link": "https://stackoverflow.com/questions/36367555/confusion-between-prepared-statement-and-parameterized-query-in-python", "question": {"id": "36367555", "title": "Confusion between prepared statement and parameterized query in Python", "content": "<p>As far as I understand, prepared statements are (mainly) a database feature that allows you to separate parameters from the code that uses such parameters. Example:</p>\n<pre><code class=\"python\">PREPARE fooplan (int, text, bool, numeric) AS\n    INSERT INTO foo VALUES($1, $2, $3, $4);\nEXECUTE fooplan(1, 'Hunter Valley', 't', 200.00);\n</code></pre>\n<p>A parameterized query substitutes the manual string interpolation, so instead of doing</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT FROM tablename WHERE fieldname = %s\" % value)\n</code></pre>\n<p>we can do</p>\n<pre><code class=\"python\">cursor.execute(\"SELECT FROM tablename WHERE fieldname = %s\", [value])\n</code></pre>\n<p>Now, it seems that prepared statements are, for the most part, used in the database language and parameterized queries are mainly used in the programming language connecting to the database, although I have seen exceptions to this rule.</p>\n<p>The problem is that asking about the difference between prepared statement and parameterized query brings a lot of confusion. Their purpose is admittedly the same, but their methodology seems distinct. Yet, there are <a href=\"http://www.programmerinterview.com/index.php/database-sql/parameterized-queries-vs-prepared-statements/\" rel=\"noreferrer\">sources</a> indicating that both are the same. MySQLdb and Psycopg2 seem to support parameterized queries but don\u2019t support prepared statements (e.g. <a href=\"https://stackoverflow.com/a/17237567/460147\">here</a> for MySQLdb and in the <a href=\"https://wiki.postgresql.org/wiki/Python_PostgreSQL_Driver_TODO\" rel=\"noreferrer\">TODO list for postgres drivers</a> or <a href=\"https://groups.google.com/d/msg/sqlalchemy/LN0evK1hyOM/78nvIwtT7GgJ\" rel=\"noreferrer\">this answer</a> in the sqlalchemy group). Actually, there is a <a href=\"https://gist.github.com/dvarrazzo/3797445\" rel=\"noreferrer\">gist</a> implementing a psycopg2 cursor supporting prepared statements and a minimal <a href=\"https://stackoverflow.com/questions/9866350/prepared-statements-using-psycopg\">explanation</a> about it. There is also a <a href=\"http://initd.org/psycopg/articles/2012/10/01/prepared-statements-psycopg/\" rel=\"noreferrer\">suggestion</a> of subclassing the cursor object in psycopg2 to provide the prepared statement manually.</p>\n<p>I would like to get an authoritative answer to the following questions:</p>\n<ul>\n<li><p>Is there a meaningful difference between prepared statement and parameterized query? Does this matter in practice? If you use parameterized queries, do you need to worry about prepared statements?</p></li>\n<li><p>If there is a difference, what is the current status of prepared statements in the Python ecosystem? Which database adapters support prepared statements?</p></li>\n</ul>\n", "abstract": "As far as I understand, prepared statements are (mainly) a database feature that allows you to separate parameters from the code that uses such parameters. Example: A parameterized query substitutes the manual string interpolation, so instead of doing we can do Now, it seems that prepared statements are, for the most part, used in the database language and parameterized queries are mainly used in the programming language connecting to the database, although I have seen exceptions to this rule. The problem is that asking about the difference between prepared statement and parameterized query brings a lot of confusion. Their purpose is admittedly the same, but their methodology seems distinct. Yet, there are sources indicating that both are the same. MySQLdb and Psycopg2 seem to support parameterized queries but don\u2019t support prepared statements (e.g. here for MySQLdb and in the TODO list for postgres drivers or this answer in the sqlalchemy group). Actually, there is a gist implementing a psycopg2 cursor supporting prepared statements and a minimal explanation about it. There is also a suggestion of subclassing the cursor object in psycopg2 to provide the prepared statement manually. I would like to get an authoritative answer to the following questions: Is there a meaningful difference between prepared statement and parameterized query? Does this matter in practice? If you use parameterized queries, do you need to worry about prepared statements? If there is a difference, what is the current status of prepared statements in the Python ecosystem? Which database adapters support prepared statements?"}, "answers": [{"id": 36516415, "score": 31, "vote": 0, "content": "<ul>\n<li><p>Prepared statement: A reference to a pre-interpreted query routine on the database, ready to accept parameters</p>\n</li>\n<li><p>Parametrized query: A query made by your code in such a way that you are passing values in <em>alongside</em> some SQL that has placeholder values, usually <code>?</code> or <code>%s</code> or something of that flavor.</p>\n</li>\n</ul>\n<p>The confusion here seems to stem from the (apparent) lack of distinction between the ability to directly get a prepared statement object and the ability to pass values into a 'parametrized query' method that acts very much like one... because it is one, or at least makes one for you.</p>\n<p>For example: the C interface of the SQLite3 library has a lot of tools for working with <a href=\"https://www.sqlite.org/c3ref/stmt.html\" rel=\"noreferrer\">prepared statement objects</a>, but the <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">Python api</a> makes almost no mention of them. You can't prepare a statement and use it multiple times whenever you want. Instead, you can use <code>sqlite3.executemany(sql, params)</code> which takes the SQL code, creates a prepared statement <em>internally</em>, then uses that statement in a loop to process each of your parameter tuples in the iterable you gave.</p>\n<p>Many other SQL libraries in Python behave the same way. Working with prepared statement objects can be a real pain, and can lead to ambiguity, and in a language like Python which has such a lean towards clarity and ease over raw execution speed they aren't really the greatest option. Essentially, if you find yourself having to make hundreds of thousands or millions of calls to a complex SQL query that gets re-interpreted every time, you should probably be doing things differently. Regardless, sometimes people wish they could have direct access to these objects because if you keep the same prepared statement around the database server won't have to keep interpreting the same SQL code over and over; most of the time this will be approaching the problem from the wrong direction and you will get much greater savings elsewhere or by restructuring your code.*</p>\n<p>Perhaps more importantly in general is the way that prepared statements and parametrized queries keep your data sanitary and separate from your SQL code. <strong>This is vastly preferable to string formatting!</strong> You should think of parametrized queries and prepared statements, in one form or another, as <strong>the only way to pass variable data from your application into the database</strong>. If you try to build the SQL statement otherwise, it will not only run significantly slower but you will be vulnerable to <a href=\"https://xkcd.com/327/\" rel=\"noreferrer\">other problems</a>.</p>\n<p><em>*e.g., by producing the data that is to be fed into the DB in a generator function then using <code>executemany()</code> to insert it all at once from the generator, rather than calling <code>execute()</code> each time you loop.</em></p>\n<h2>tl;dr</h2>\n<p><strong>A parametrized query is a single operation which generates a prepared statement internally, then passes in your parameters and executes.</strong></p>\n<p><em>edit:</em> A lot of people see this answer! I want to also clarify that many database engines also have concepts of a prepared statement that can be constructed explicitly with plaintext query syntax, then reused over the lifetime of a client's session (in <a href=\"https://www.postgresql.org/docs/current/sql-prepare.html\" rel=\"noreferrer\">postgres</a> for example). Sometimes you have control over whether the query plan is cached to save even more time. Some frameworks use these automatically (I've seen rails' ORM do it aggressively), sometimes usefully and sometimes to their detriment when there are permutations of form for the queries being prepared.</p>\n<p>Also if you want to nit pick, parametrized queries do not <em>always</em> use a prepared statement under the hood; they should do so if possible, but sometimes it's just formatting in the parameter values. The real difference between 'prepared statement' and 'parametrized query' here is really just the shape of the API you use.</p>\n", "abstract": "Prepared statement: A reference to a pre-interpreted query routine on the database, ready to accept parameters Parametrized query: A query made by your code in such a way that you are passing values in alongside some SQL that has placeholder values, usually ? or %s or something of that flavor. The confusion here seems to stem from the (apparent) lack of distinction between the ability to directly get a prepared statement object and the ability to pass values into a 'parametrized query' method that acts very much like one... because it is one, or at least makes one for you. For example: the C interface of the SQLite3 library has a lot of tools for working with prepared statement objects, but the Python api makes almost no mention of them. You can't prepare a statement and use it multiple times whenever you want. Instead, you can use sqlite3.executemany(sql, params) which takes the SQL code, creates a prepared statement internally, then uses that statement in a loop to process each of your parameter tuples in the iterable you gave. Many other SQL libraries in Python behave the same way. Working with prepared statement objects can be a real pain, and can lead to ambiguity, and in a language like Python which has such a lean towards clarity and ease over raw execution speed they aren't really the greatest option. Essentially, if you find yourself having to make hundreds of thousands or millions of calls to a complex SQL query that gets re-interpreted every time, you should probably be doing things differently. Regardless, sometimes people wish they could have direct access to these objects because if you keep the same prepared statement around the database server won't have to keep interpreting the same SQL code over and over; most of the time this will be approaching the problem from the wrong direction and you will get much greater savings elsewhere or by restructuring your code.* Perhaps more importantly in general is the way that prepared statements and parametrized queries keep your data sanitary and separate from your SQL code. This is vastly preferable to string formatting! You should think of parametrized queries and prepared statements, in one form or another, as the only way to pass variable data from your application into the database. If you try to build the SQL statement otherwise, it will not only run significantly slower but you will be vulnerable to other problems. *e.g., by producing the data that is to be fed into the DB in a generator function then using executemany() to insert it all at once from the generator, rather than calling execute() each time you loop. A parametrized query is a single operation which generates a prepared statement internally, then passes in your parameters and executes. edit: A lot of people see this answer! I want to also clarify that many database engines also have concepts of a prepared statement that can be constructed explicitly with plaintext query syntax, then reused over the lifetime of a client's session (in postgres for example). Sometimes you have control over whether the query plan is cached to save even more time. Some frameworks use these automatically (I've seen rails' ORM do it aggressively), sometimes usefully and sometimes to their detriment when there are permutations of form for the queries being prepared. Also if you want to nit pick, parametrized queries do not always use a prepared statement under the hood; they should do so if possible, but sometimes it's just formatting in the parameter values. The real difference between 'prepared statement' and 'parametrized query' here is really just the shape of the API you use."}, {"id": 36505148, "score": 6, "vote": 0, "content": "<p>First, your questions shows very good preparation - well done.</p>\n<p>I am not sure, if I am the person to provide authoritative answer, but I will try to explain my\nunderstanding of the situation.</p>\n<p><strong>Prepared statement</strong> is an object, created on side of database server as a result of <code>PREPARE</code>\nstatement, turning provided SQL statement into sort of temporary procedure with parameters. Prepared\nstatement has lifetime of current database session and are discarded after the session is over.\nSQL statement <code>DEALOCATE</code> allows destroying the prepared statement explicitly.</p>\n<p>Database clients can use SQL statement <code>EXECUTE</code> to execute the prepared statement by calling it's\nname and parameters.</p>\n<p><strong>Parametrized statement</strong> is alias for prepared statement as usually, the prepared statement has\nsome parameters.</p>\n<p><strong>Parametrized query</strong> seems to be less often used alias for the same (24 mil Google hits for\nparametrized statement, 14 mil hits for parametrized query). It is possible, that some people use\nthis term for another purpose.</p>\n<p>Advantages of prepared statements are:</p>\n<ul>\n<li>faster execution of actual prepared statement call (not counting the time for <code>PREPARE</code>)</li>\n<li>resistency to SQL injection attack</li>\n</ul>\n<h2>Players in executing SQL query</h2>\n<p>Real application will probably have following participants:</p>\n<ul>\n<li>application code</li>\n<li>ORM package (e.g. sqlalchemy)</li>\n<li>database driver</li>\n<li>database server</li>\n</ul>\n<p>From application point of view it is not easy to know, if the code will really use prepared\nstatement on database server or not as <strong>any of participants may lack support of prepared\nstatements</strong>.</p>\n<h2>Conclusions</h2>\n<p><strong>In application code</strong> prevent direct shaping of SQL query as it is prone to SQL injection attack. For\nthis reason it is recommended using whatever the ORM provides to parametrized query even if it does\nnot result on using prepared statements on database server side as the ORM code can be optimized to\nprevent this sort of attack.</p>\n<p><strong>Decide, if prepared statement is worth for performance reasons</strong>. If you have simple SQL query,\nwhich is executed only few times, it will not help, sometime it will even slow down the execution a\nbit.</p>\n<p>For complex query being executed many times and having relatively short execution time will be\nthe effect the biggest. In such a case, you may follow these steps:</p>\n<ul>\n<li>check, that the database you are going to use supports the <code>PREPARE</code> statement. In most cases it\nwill be present.</li>\n<li>check, that the drive you use is supporting prepared statements and if not, try to find another\none supporting it.</li>\n<li>Check support of this feature on ORM package level. Sometime it vary driver by driver (e.g.\nsqlalchemy states some limitations on prepared statements with MySQL due to how MySQL manages\nthat). </li>\n</ul>\n<p>If you are in search for real authoritative answer, I would head to authors of sqlalchemy.</p>\n", "abstract": "First, your questions shows very good preparation - well done. I am not sure, if I am the person to provide authoritative answer, but I will try to explain my\nunderstanding of the situation. Prepared statement is an object, created on side of database server as a result of PREPARE\nstatement, turning provided SQL statement into sort of temporary procedure with parameters. Prepared\nstatement has lifetime of current database session and are discarded after the session is over.\nSQL statement DEALOCATE allows destroying the prepared statement explicitly. Database clients can use SQL statement EXECUTE to execute the prepared statement by calling it's\nname and parameters. Parametrized statement is alias for prepared statement as usually, the prepared statement has\nsome parameters. Parametrized query seems to be less often used alias for the same (24 mil Google hits for\nparametrized statement, 14 mil hits for parametrized query). It is possible, that some people use\nthis term for another purpose. Advantages of prepared statements are: Real application will probably have following participants: From application point of view it is not easy to know, if the code will really use prepared\nstatement on database server or not as any of participants may lack support of prepared\nstatements. In application code prevent direct shaping of SQL query as it is prone to SQL injection attack. For\nthis reason it is recommended using whatever the ORM provides to parametrized query even if it does\nnot result on using prepared statements on database server side as the ORM code can be optimized to\nprevent this sort of attack. Decide, if prepared statement is worth for performance reasons. If you have simple SQL query,\nwhich is executed only few times, it will not help, sometime it will even slow down the execution a\nbit. For complex query being executed many times and having relatively short execution time will be\nthe effect the biggest. In such a case, you may follow these steps: If you are in search for real authoritative answer, I would head to authors of sqlalchemy."}, {"id": 36367693, "score": 0, "vote": 0, "content": "<p>An sql statement can't be execute immediately: the DBMS must interpret them before the execution.</p>\n<p>Prepared statements are statement already interpreted, the DBMS change parameters and the query starts immediately. This is a feature of certain DBMS and you can achieve fast response (comparable with stored procedures).</p>\n<p>Parametrized statement are just a way you compose the query string in your programming languages. Since it doesn't matter how sql string are formed, you have slower response by DBMS.</p>\n<p>If you measure time executing 3-4 time the same query (select with different conditions) you will see the same time with parametrized queries, the time is smaller from the second execution of prepared statement (the first time the DBMS has to interpret the script anyway).</p>\n", "abstract": "An sql statement can't be execute immediately: the DBMS must interpret them before the execution. Prepared statements are statement already interpreted, the DBMS change parameters and the query starts immediately. This is a feature of certain DBMS and you can achieve fast response (comparable with stored procedures). Parametrized statement are just a way you compose the query string in your programming languages. Since it doesn't matter how sql string are formed, you have slower response by DBMS. If you measure time executing 3-4 time the same query (select with different conditions) you will see the same time with parametrized queries, the time is smaller from the second execution of prepared statement (the first time the DBMS has to interpret the script anyway)."}, {"id": 64599065, "score": 0, "vote": 0, "content": "<p>I think the comment about using executemany fails to deal with the case where an application stores data into the database OVER TIME and wants each insert statement to be as efficient as possible. Thus the desire to prepare the insert statement once and re-use the prepared statement.\nAlternatively, one could put the desired statement(s) in a stored proc and re-use it.</p>\n", "abstract": "I think the comment about using executemany fails to deal with the case where an application stores data into the database OVER TIME and wants each insert statement to be as efficient as possible. Thus the desire to prepare the insert statement once and re-use the prepared statement.\nAlternatively, one could put the desired statement(s) in a stored proc and re-use it."}]}, {"link": "https://stackoverflow.com/questions/55523299/best-practices-for-persistent-database-connections-in-python-when-using-flask", "question": {"id": "55523299", "title": "Best practices for persistent database connections in Python when using Flask", "content": "<p>My question is about the recommended approach for handling database connections when using Flask in a production environment or other environment where performance is a concern. In Flask, the g object is available for storing things, and open database connections can be placed there to allow the application to reuse them in subsequent database queries during the same request. However, the g object doesn't persist across requests, so it seems that each new request would require a new database connection (and the performance hit that entails).</p>\n<p>The most related question I found on this matter is this one: <a href=\"https://stackoverflow.com/questions/6688413/how-to-preserve-database-connection-in-a-python-web-server\">How to preserve database connection in a python web server</a> but the answers only raise the abstract idea of connection pooling (without tying it to how one might use it within Flask and how it would survive across requests) or propose a solution that is only relevant to one particular type of database or particular stack.</p>\n<p>So my question is about the general approach that should be taken when productionising apps built on Flask that connect to <em>any</em> kind of database. It seems like something involving connection pooling is in the right direction, especially since that would work for a traditional Python application. But I'm wondering what the recommended approach is when using Flask because of the aforementioned issues of persistence across connections, and also the fact that Flask apps in production are run from WSGI servers, which potentially add further complications.</p>\n<p>EDIT: Based on the comments recommending flask sqlalchemy. Assuming flask sqlalchemy solves the problem, does it also work for Neo4J or indeed any arbitrary database that the Flask app uses? Many of the existing database connectors already support pooling natively, so why introduce an additional dependency whose primary purpose is to provide ORM capabilities rather than connection management? Also, how does sqlalchemy get around the fundamental problem of persistence across requests?</p>\n", "abstract": "My question is about the recommended approach for handling database connections when using Flask in a production environment or other environment where performance is a concern. In Flask, the g object is available for storing things, and open database connections can be placed there to allow the application to reuse them in subsequent database queries during the same request. However, the g object doesn't persist across requests, so it seems that each new request would require a new database connection (and the performance hit that entails). The most related question I found on this matter is this one: How to preserve database connection in a python web server but the answers only raise the abstract idea of connection pooling (without tying it to how one might use it within Flask and how it would survive across requests) or propose a solution that is only relevant to one particular type of database or particular stack. So my question is about the general approach that should be taken when productionising apps built on Flask that connect to any kind of database. It seems like something involving connection pooling is in the right direction, especially since that would work for a traditional Python application. But I'm wondering what the recommended approach is when using Flask because of the aforementioned issues of persistence across connections, and also the fact that Flask apps in production are run from WSGI servers, which potentially add further complications. EDIT: Based on the comments recommending flask sqlalchemy. Assuming flask sqlalchemy solves the problem, does it also work for Neo4J or indeed any arbitrary database that the Flask app uses? Many of the existing database connectors already support pooling natively, so why introduce an additional dependency whose primary purpose is to provide ORM capabilities rather than connection management? Also, how does sqlalchemy get around the fundamental problem of persistence across requests?"}, "answers": [{"id": 55537278, "score": 31, "vote": 0, "content": "<p>Turns out there is a straightforward way to achieve what I was after. But as the commenters suggested, if it is at all possible to go the flask sqlalchemy route, then you might want to go that way. My approach to solving the problem is to save the connection object in a module level variable that is then imported as necessary. That way it will be available for use from within Flask and by other modules. Here is a simplified version of what I did:</p>\n<p><em>app.py</em></p>\n<pre><code class=\"python\">from flask import Flask\nfrom extensions import neo4j\n\napp = Flask(__name__)\nneo4j.init_app(app)\n</code></pre>\n<p><em>extensions.py</em></p>\n<pre><code class=\"python\">from neo4j_db import Neo4j\n\nneo4j = Neo4j()\n</code></pre>\n<p><em>neo4j_db.py</em></p>\n<pre><code class=\"python\">from neo4j import GraphDatabase\n\nclass Neo4j:\n    def __init__(self):\n        self.app = None\n        self.driver = None\n\n    def init_app(self, app):\n        self.app = app\n        self.connect()\n\n    def connect(self):\n        self.driver = GraphDatabase.driver('bolt://xxx')\n        return self.driver\n\n    def get_db(self):\n        if not self.driver:\n            return self.connect()\n        return self.driver\n</code></pre>\n<p><em>example.py</em></p>\n<pre><code class=\"python\">from extensions import neo4j\n\ndriver = neo4j.get_db()\n</code></pre>\n<p>And from here <em>driver</em> will contain the database driver that will persist across Flask requests.</p>\n<p>Hope that helps anyone that has the same issue.</p>\n", "abstract": "Turns out there is a straightforward way to achieve what I was after. But as the commenters suggested, if it is at all possible to go the flask sqlalchemy route, then you might want to go that way. My approach to solving the problem is to save the connection object in a module level variable that is then imported as necessary. That way it will be available for use from within Flask and by other modules. Here is a simplified version of what I did: app.py extensions.py neo4j_db.py example.py And from here driver will contain the database driver that will persist across Flask requests. Hope that helps anyone that has the same issue."}]}, {"link": "https://stackoverflow.com/questions/3017101/twisted-sqlalchemy-and-the-best-way-to-do-it", "question": {"id": "3017101", "title": "Twisted + SQLAlchemy and the best way to do it", "content": "<p>So I'm writing yet another Twisted based daemon.  It'll have an xmlrpc interface as usual so I can easily communicate with it and have other processes interchange data with it as needed.</p>\n<p>This daemon needs to access a database.  We've been using SQL Alchemy in place of hard coding SQL strings for our latest projects - those mostly done for web apps in Pylons.</p>\n<p>We'd like to do the same for this app and re-use library code that makes use of SQL Alchemy.  So what to do?  Well of course since that library was written for use in a Pylons app it's all the straight-forward blocking style code that everyone is accustomed to and all of the non-blocking is magically handled by Pylons via threading, thread locals, scoped sessions and so on.</p>\n<p>So now for Twisted I guess I'm a bit stuck.  I could:</p>\n<ol>\n<li>Just write the sql I need directly if it's minimal and use the dbapi pool in twisted to do runInteractions etc when I need to hit the db.</li>\n<li>Use the objects and inherently blocking methods in our library and block now and then in my Twisted daemon. Bah.</li>\n<li>Use sAsync which was last updated in 2008 and kind of reuse the models we have defined already but not really and this doesn't address that the library code needs to work in Pylons too. Does that even work with the latest version SQL Alchemy?  Who knows. That project looked great though - why was it apparently abandoned?  </li>\n<li>Spawn a separate subprocess and have it deal with the library code and all it's blocking, the results being returned back to my daemon when ready as objects marshalled via YAML over xmlrpc.</li>\n<li>Use deferToThread and then expunge the objects returned having made sure to do eager loads so that I have all my stuff that I might need.  Seems kind of ugha to me.</li>\n</ol>\n<p>I'm also stuck using Python 2.5.4 atm so no 2.6 yet and I don't think I can just do an import from future to get access to the cool new multiprocessing module stuff in there. That's OK though I guess as we've got dealing with interprocess communication down pretty well.</p>\n<p>So I'm leaning towards option 4 mostly as that would avoid the mortal sin of logic  duplication with option 1 while also staying the heck away from threads.  </p>\n<p>My first attempt though will be option 2 to just get the thing going and then separate out the calls to the library code perhaps into a separate process if it looks like there's a good chance that something might take a bit too long to block on.  Sad.  Maybe a combination of Stackless Python and Twisted would be interesting here.</p>\n<p>Any better ideas?</p>\n", "abstract": "So I'm writing yet another Twisted based daemon.  It'll have an xmlrpc interface as usual so I can easily communicate with it and have other processes interchange data with it as needed. This daemon needs to access a database.  We've been using SQL Alchemy in place of hard coding SQL strings for our latest projects - those mostly done for web apps in Pylons. We'd like to do the same for this app and re-use library code that makes use of SQL Alchemy.  So what to do?  Well of course since that library was written for use in a Pylons app it's all the straight-forward blocking style code that everyone is accustomed to and all of the non-blocking is magically handled by Pylons via threading, thread locals, scoped sessions and so on. So now for Twisted I guess I'm a bit stuck.  I could: I'm also stuck using Python 2.5.4 atm so no 2.6 yet and I don't think I can just do an import from future to get access to the cool new multiprocessing module stuff in there. That's OK though I guess as we've got dealing with interprocess communication down pretty well. So I'm leaning towards option 4 mostly as that would avoid the mortal sin of logic  duplication with option 1 while also staying the heck away from threads.   My first attempt though will be option 2 to just get the thing going and then separate out the calls to the library code perhaps into a separate process if it looks like there's a good chance that something might take a bit too long to block on.  Sad.  Maybe a combination of Stackless Python and Twisted would be interesting here. Any better ideas?"}, "answers": [{"id": 24638240, "score": 10, "vote": 0, "content": "<p>In the intervening couple of years, Alex Gaynor created <a href=\"https://github.com/alex/alchimia\" rel=\"noreferrer\">https://github.com/alex/alchimia</a> which may be a better central repository for doing integration with SQLAlchemy and Twisted.</p>\n", "abstract": "In the intervening couple of years, Alex Gaynor created https://github.com/alex/alchimia which may be a better central repository for doing integration with SQLAlchemy and Twisted."}, {"id": 3045427, "score": 6, "vote": 0, "content": "<p>Firstly, I can unfortunately only second your opinion that twisted and\nSQLAlchemy don't play along very well. I have worked some with both\nand would be somewhat afraid of the complexity that would arise from\nputting them together.</p>\n<p>All the database integration layers that I know of to date use\ntwisteds threading integration layer, and if you want to avoid that at\nall costs you are pretty much stuck with point 4 in your list.</p>\n<p>On the other hand, I have seen examples of database connecting code\nusing deferToThread() and friends that worked very well.</p>\n<p>Anyway, some pointers if you'd be ready to consider other frameworks\nthan SQLAlchemy:</p>\n<p>The DivMod guys have been doing some tentative work on twisted -\ndatabase integration based on the Storm ORM (google for \"storm orm\").</p>\n<p>See this link for an example:</p>\n<p><a href=\"http://divmod.readthedocs.org/en/latest/products/nevow/storm-approach.html\" rel=\"nofollow noreferrer\">http://divmod.readthedocs.org/en/latest/products/nevow/storm-approach.html</a></p>\n<p>Also, head over to DivMod's site and have a look at the sources of\ntheir Axiom db layer (probably not of any use to you directly since\nit's Sqlite only, but it's principles might be useful).</p>\n", "abstract": "Firstly, I can unfortunately only second your opinion that twisted and\nSQLAlchemy don't play along very well. I have worked some with both\nand would be somewhat afraid of the complexity that would arise from\nputting them together. All the database integration layers that I know of to date use\ntwisteds threading integration layer, and if you want to avoid that at\nall costs you are pretty much stuck with point 4 in your list. On the other hand, I have seen examples of database connecting code\nusing deferToThread() and friends that worked very well. Anyway, some pointers if you'd be ready to consider other frameworks\nthan SQLAlchemy: The DivMod guys have been doing some tentative work on twisted -\ndatabase integration based on the Storm ORM (google for \"storm orm\"). See this link for an example: http://divmod.readthedocs.org/en/latest/products/nevow/storm-approach.html Also, head over to DivMod's site and have a look at the sources of\ntheir Axiom db layer (probably not of any use to you directly since\nit's Sqlite only, but it's principles might be useful)."}, {"id": 3098958, "score": 6, "vote": 0, "content": "<p>There's a storm branch that you can use with twisted directly (internally it does the defer to thread stuff) on launchpad <a href=\"https://code.launchpad.net/~therve/storm/twisted-integration\" rel=\"noreferrer\">https://code.launchpad.net/~therve/storm/twisted-integration</a>. I've used it nicely.</p>\n<p>Sadly sqlalchemy is significantly more complex in implementation to audit for async usage. If you really want to use it, i'd recommend an out of process approach with a storage rpc layer.</p>\n<p>alternatively if your feeling adventurous and using postgresql, the latest pyscopg2 supports true async usage (<a href=\"https://launchpad.net/txpostgres\" rel=\"noreferrer\">https://launchpad.net/txpostgres</a>), and the storm source is pretty simple to hack on ;-)</p>\n<p>incidentally the storm you tried last year may not have had the C-extension on by default (it is now in the latest releases.) which might account for your speed issues.</p>\n", "abstract": "There's a storm branch that you can use with twisted directly (internally it does the defer to thread stuff) on launchpad https://code.launchpad.net/~therve/storm/twisted-integration. I've used it nicely. Sadly sqlalchemy is significantly more complex in implementation to audit for async usage. If you really want to use it, i'd recommend an out of process approach with a storage rpc layer. alternatively if your feeling adventurous and using postgresql, the latest pyscopg2 supports true async usage (https://launchpad.net/txpostgres), and the storm source is pretty simple to hack on ;-) incidentally the storm you tried last year may not have had the C-extension on by default (it is now in the latest releases.) which might account for your speed issues."}, {"id": 11236210, "score": 3, "vote": 0, "content": "<p>Perhaps twistar is what you're looking for. It's a native active record (aka ORM) implementation for twisted, working on top of <code>twisted.enterprise.adbapi</code>.</p>\n<p><a href=\"http://findingscience.com/twistar/\" rel=\"nofollow\">http://findingscience.com/twistar/</a></p>\n", "abstract": "Perhaps twistar is what you're looking for. It's a native active record (aka ORM) implementation for twisted, working on top of twisted.enterprise.adbapi. http://findingscience.com/twistar/"}]}, {"link": "https://stackoverflow.com/questions/11821322/elegant-way-to-store-dictionary-permanently-with-python", "question": {"id": "11821322", "title": "Elegant way to store dictionary permanently with Python?", "content": "<p>Currently expensively parsing a file, which generates a dictionary of ~400 key, value pairs, which is seldomly updated. Previously had a function which parsed the file, wrote it to a text file in dictionary syntax (ie. <code>dict = {'Adam': 'Room 430', 'Bob': 'Room 404'}</code>) etc, and copied and pasted it into another function whose sole purpose was to return that parsed dictionary. </p>\n<p>Hence, in every file where I would use that dictionary, I would import that function, and assign it to a variable, which is now that dictionary. Wondering if there's a more elegant way to do this, which does not involve explicitly copying and pasting code around? Using a database kind of seems unnecessary, and the text file gave me the benefit of seeing whether the parsing was done correctly before adding it to the function. But I'm open to suggestions.</p>\n", "abstract": "Currently expensively parsing a file, which generates a dictionary of ~400 key, value pairs, which is seldomly updated. Previously had a function which parsed the file, wrote it to a text file in dictionary syntax (ie. dict = {'Adam': 'Room 430', 'Bob': 'Room 404'}) etc, and copied and pasted it into another function whose sole purpose was to return that parsed dictionary.  Hence, in every file where I would use that dictionary, I would import that function, and assign it to a variable, which is now that dictionary. Wondering if there's a more elegant way to do this, which does not involve explicitly copying and pasting code around? Using a database kind of seems unnecessary, and the text file gave me the benefit of seeing whether the parsing was done correctly before adding it to the function. But I'm open to suggestions."}, "answers": [{"id": 11821343, "score": 64, "vote": 0, "content": "<p>Why not dump it to a JSON file, and then load it from there where you need it?</p>\n<pre><code class=\"python\">import json\n\nwith open('my_dict.json', 'w') as f:\n    json.dump(my_dict, f)\n\n# elsewhere...\n\nwith open('my_dict.json') as f:\n    my_dict = json.load(f)\n</code></pre>\n<p>Loading from JSON is fairly efficient.</p>\n<p>Another option would be to use <a href=\"http://docs.python.org/library/pickle\" rel=\"noreferrer\"><code>pickle</code></a>, but unlike JSON, the files it generates aren't human-readable so you lose out on the visual verification you liked from your old method.</p>\n", "abstract": "Why not dump it to a JSON file, and then load it from there where you need it? Loading from JSON is fairly efficient. Another option would be to use pickle, but unlike JSON, the files it generates aren't human-readable so you lose out on the visual verification you liked from your old method."}, {"id": 11823256, "score": 23, "vote": 0, "content": "<p>Why mess with all these serialization methods? It's already written to a file as a Python dict (although with the unfortunate name 'dict').  Change your program to write out the data with a better variable name - maybe 'data', or 'catalog', and save the file as a Python file, say data.py.  Then you can just import the data directly at runtime without any clumsy copy/pasting or JSON/shelve/etc. parsing:</p>\n<pre><code class=\"python\">from data import catalog\n</code></pre>\n", "abstract": "Why mess with all these serialization methods? It's already written to a file as a Python dict (although with the unfortunate name 'dict').  Change your program to write out the data with a better variable name - maybe 'data', or 'catalog', and save the file as a Python file, say data.py.  Then you can just import the data directly at runtime without any clumsy copy/pasting or JSON/shelve/etc. parsing:"}, {"id": 11821487, "score": 6, "vote": 0, "content": "<p>JSON is probably the right way to go in many cases; but there might be an alternative. It looks like your keys and your values are always strings, is that right? You might consider using <a href=\"http://docs.python.org/library/dbm.html\" rel=\"noreferrer\"><code>dbm</code></a>/<a href=\"http://docs.python.org/library/anydbm.html#module-anydbm\" rel=\"noreferrer\"><code>anydbm</code></a>. These are \"databases\" but they act almost exactly like dictionaries. They're great for cheap data persistence.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import anydbm\n&gt;&gt;&gt; dict_of_strings = anydbm.open('data', 'c')\n&gt;&gt;&gt; dict_of_strings['foo'] = 'bar'\n&gt;&gt;&gt; dict_of_strings.close()\n&gt;&gt;&gt; dict_of_strings = anydbm.open('data')\n&gt;&gt;&gt; dict_of_strings['foo']\n'bar'\n</code></pre>\n", "abstract": "JSON is probably the right way to go in many cases; but there might be an alternative. It looks like your keys and your values are always strings, is that right? You might consider using dbm/anydbm. These are \"databases\" but they act almost exactly like dictionaries. They're great for cheap data persistence."}, {"id": 11821591, "score": 5, "vote": 0, "content": "<p>If the keys are all strings, you can use the <a href=\"http://docs.python.org/library/shelve\" rel=\"noreferrer\">shelve</a> module</p>\n<blockquote>\n<p>A <a href=\"http://docs.python.org/library/shelve\" rel=\"noreferrer\">shelf</a> is a persistent, dictionary-like object. The difference with\n  \u201cdbm\u201d databases is that the values (not the keys!) in a shelf can be\n  essentially arbitrary Python objects \u2014 anything that the pickle module\n  can handle. This includes most class instances, recursive data types,\n  and objects containing lots of shared sub-objects. The keys are\n  ordinary strings.</p>\n</blockquote>\n<p><code>json</code> would be a good choice if you need to use the data from other languages</p>\n", "abstract": "If the keys are all strings, you can use the shelve module A shelf is a persistent, dictionary-like object. The difference with\n  \u201cdbm\u201d databases is that the values (not the keys!) in a shelf can be\n  essentially arbitrary Python objects \u2014 anything that the pickle module\n  can handle. This includes most class instances, recursive data types,\n  and objects containing lots of shared sub-objects. The keys are\n  ordinary strings. json would be a good choice if you need to use the data from other languages"}, {"id": 11821401, "score": 3, "vote": 0, "content": "<p>If storage efficiency matters, use Pickle or CPickle(for execution performance gain). As Amber pointed out, you can also dump/load via Json. It will be human-readable, but takes more disk.</p>\n", "abstract": "If storage efficiency matters, use Pickle or CPickle(for execution performance gain). As Amber pointed out, you can also dump/load via Json. It will be human-readable, but takes more disk."}, {"id": 11821679, "score": 3, "vote": 0, "content": "<p>I suggest you consider using the <code>shelve</code> module since your data-structure is a mapping.\nThat was my <a href=\"https://stackoverflow.com/questions/4529914/python-how-to-get-a-object-database/4530124#4530124\">answer</a> to a similar question titled <a href=\"https://stackoverflow.com/questions/5823236/if-i-want-to-build-a-custom-database-how-could-i\"><em>If I want to build a custom database, how could I?</em></a> There's also a bit of sample code in another <a href=\"https://stackoverflow.com/a/4530124/355230\">answer</a> of mine promoting its use for the question <a href=\"https://stackoverflow.com/questions/4529914/how-to-get-a-object-database\"><em>How to get a object database?</em></a></p>\n<p>ActiveState has a highly rated <a href=\"http://code.activestate.com/recipes/576642\" rel=\"nofollow noreferrer\">PersistentDict</a> recipe which supports csv, json, and pickle output file formats. It's pretty fast since all three of those formats are implement in C (although the recipe itself is pure Python), so the fact that it reads the whole file into memory when it's opened might be acceptable.</p>\n", "abstract": "I suggest you consider using the shelve module since your data-structure is a mapping.\nThat was my answer to a similar question titled If I want to build a custom database, how could I? There's also a bit of sample code in another answer of mine promoting its use for the question How to get a object database? ActiveState has a highly rated PersistentDict recipe which supports csv, json, and pickle output file formats. It's pretty fast since all three of those formats are implement in C (although the recipe itself is pure Python), so the fact that it reads the whole file into memory when it's opened might be acceptable."}, {"id": 11821761, "score": 0, "vote": 0, "content": "<p>on the JSON direction there is also something called simpleJSON.  My first time using json in python the json library didnt work for me/ i couldnt figure it out.  simpleJSON was...easier to use</p>\n", "abstract": "on the JSON direction there is also something called simpleJSON.  My first time using json in python the json library didnt work for me/ i couldnt figure it out.  simpleJSON was...easier to use"}, {"id": 11822009, "score": 0, "vote": 0, "content": "<p>JSON (or YAML, or whatever) serialisation is probably better, but if you're already writing the dictionary to a text file in python syntax, complete with a variable name binding, you could just write that to a .py file instead. Then that python file would be importable and usable as is. There's no need for the \"function which returns a dictionary\"  approach, since you can directly use it as a global in that file. e.g.</p>\n<pre><code class=\"python\"># generated.py\nplease_dont_use_dict_as_a_variable_name = {'Adam': 'Room 430', 'Bob': 'Room 404'}\n</code></pre>\n<p>rather than:</p>\n<pre><code class=\"python\"># manually_copied.py\ndef get_dict():\n    return {'Adam': 'Room 430', 'Bob': 'Room 404'}\n</code></pre>\n<p>The only difference is that <code>manually_copied.get_dict</code> gives you a fresh copy of the dictionary every time, whereas <code>generated.please_dont_use_dict_as_a_variable_name</code>[1] is a single shared object. This may matter if you're modifying the dictionary in your program after retrieving it, but you can always use <code>copy.copy</code> or <code>copy.deepcopy</code> to create a new copy if you need to modify one independently of the others.</p>\n<hr/>\n<p>[1] <code>dict</code>, <code>list</code>, <code>str</code>, <code>int</code>, <code>map</code>, etc are generally viewed as bad variable names. The reason is that these are already defined as built-ins, and are used very commonly. So if you give something a name like that, at the least it's going to cause cognitive-dissonance for people reading your code (including you after you've been away for a while) as they have to keep in mind that \"<code>dict</code> doesn't mean what it normally does here\". It's also quite likely that at some point you'll get an infuriating-to-solve bug reporting that <code>dict</code> objects aren't callable (or something), because some piece of code is trying to use the <em>type</em> <code>dict</code>, but is getting the dictionary object you bound to the name <code>dict</code> instead.</p>\n", "abstract": "JSON (or YAML, or whatever) serialisation is probably better, but if you're already writing the dictionary to a text file in python syntax, complete with a variable name binding, you could just write that to a .py file instead. Then that python file would be importable and usable as is. There's no need for the \"function which returns a dictionary\"  approach, since you can directly use it as a global in that file. e.g. rather than: The only difference is that manually_copied.get_dict gives you a fresh copy of the dictionary every time, whereas generated.please_dont_use_dict_as_a_variable_name[1] is a single shared object. This may matter if you're modifying the dictionary in your program after retrieving it, but you can always use copy.copy or copy.deepcopy to create a new copy if you need to modify one independently of the others. [1] dict, list, str, int, map, etc are generally viewed as bad variable names. The reason is that these are already defined as built-ins, and are used very commonly. So if you give something a name like that, at the least it's going to cause cognitive-dissonance for people reading your code (including you after you've been away for a while) as they have to keep in mind that \"dict doesn't mean what it normally does here\". It's also quite likely that at some point you'll get an infuriating-to-solve bug reporting that dict objects aren't callable (or something), because some piece of code is trying to use the type dict, but is getting the dictionary object you bound to the name dict instead."}]}, {"link": "https://stackoverflow.com/questions/9561243/how-to-check-if-something-exists-in-a-postgresql-database-using-django", "question": {"id": "9561243", "title": "How to check if something exists in a postgresql database using django?", "content": "<p>I want to check to see if row in the database already contains a particular input.  If it does already exist, prevent it from being added again, if not then add it like normal. </p>\n<p>How can I ask the database if something exists without pulling all of the contents out of the database in order to check?</p>\n", "abstract": "I want to check to see if row in the database already contains a particular input.  If it does already exist, prevent it from being added again, if not then add it like normal.  How can I ask the database if something exists without pulling all of the contents out of the database in order to check?"}, "answers": [{"id": 9562494, "score": 68, "vote": 0, "content": "<p>You can use</p>\n<pre><code class=\"python\">Entry.objects.filter(name='name', title='title').exists()\n</code></pre>\n<p>This will return to you true/false values. When you use count the orm generates query which will be executed much longer than in exists method. The get method will raise an exception when object does not exists.</p>\n<p>request.POST is a dictionary so to check db with it you use, i.e.:</p>\n<pre><code class=\"python\">Entry.objects.filter(name=request.POST['name'], title=request.POST['title']).exists()\n</code></pre>\n", "abstract": "You can use This will return to you true/false values. When you use count the orm generates query which will be executed much longer than in exists method. The get method will raise an exception when object does not exists. request.POST is a dictionary so to check db with it you use, i.e.:"}, {"id": 9561356, "score": 3, "vote": 0, "content": "<p>The answer to your question is \"Yes\". However, I think you should also investigate an alternative to querying the database; create a unique key on the set of fields you don't want duplicates to exist for.</p>\n<p>Now, to answer your question. Check out the Django docs for making a query:</p>\n<p><a href=\"https://docs.djangoproject.com/en/dev/topics/db/queries/\" rel=\"nofollow\">https://docs.djangoproject.com/en/dev/topics/db/queries/</a></p>\n<p>In short, if you have a data model for a Thing, Thing.objects is the interface for accessing queries. from the docs (using Blog Entry, which has a string field \"headline\" as an example):</p>\n<pre><code class=\"python\">Entry.objects.get(headline__exact=\"Man bites dog\")\n</code></pre>\n<p>The full capabilities of the interface are what you would expect from a database (there is a rich set of comparisons to data other than exact matches). I'd suggest looking further into the documentation for your specific problem.</p>\n", "abstract": "The answer to your question is \"Yes\". However, I think you should also investigate an alternative to querying the database; create a unique key on the set of fields you don't want duplicates to exist for. Now, to answer your question. Check out the Django docs for making a query: https://docs.djangoproject.com/en/dev/topics/db/queries/ In short, if you have a data model for a Thing, Thing.objects is the interface for accessing queries. from the docs (using Blog Entry, which has a string field \"headline\" as an example): The full capabilities of the interface are what you would expect from a database (there is a rich set of comparisons to data other than exact matches). I'd suggest looking further into the documentation for your specific problem."}]}, {"link": "https://stackoverflow.com/questions/18547468/multiple-databases-and-multiple-models-in-django", "question": {"id": "18547468", "title": "multiple databases and multiple models in django", "content": "<p>I have two databases and two models:the Admin and the user.</p>\n<p>I want to sync my models to the two databases;\nadmin model to database A and user model to database B;</p>\n<p>If I am setting the model path to <code>INSTALLED_APPS</code> and <code>syncdb</code>, the two models will sync to the default database.</p>\n<p>if I set the database in the <code>syncdb</code> command such as <code>sync --database=\"B\"</code>, and the two models will sync to database B.</p>\n<p>So my problem is, how do I sync the two models to two databases?</p>\n", "abstract": "I have two databases and two models:the Admin and the user. I want to sync my models to the two databases;\nadmin model to database A and user model to database B; If I am setting the model path to INSTALLED_APPS and syncdb, the two models will sync to the default database. if I set the database in the syncdb command such as sync --database=\"B\", and the two models will sync to database B. So my problem is, how do I sync the two models to two databases?"}, "answers": [{"id": 18548287, "score": 31, "vote": 0, "content": "<p>I fully agree with @alecxe on using the database router.  I am currently using a single admin interface to manage multiple databases.  Note that authentication for all databases are stored in the default database, so when you do the <code>syncdb</code> (with no arguments).</p>\n<p><strong>Generic Database Router</strong> </p>\n<p>I found <a href=\"http://diegobz.net/2011/02/10/django-database-router-using-settings/\">this</a> implementation to be extremely flexible and useful.</p>\n<p><strong>Settings.py</strong> </p>\n<pre><code class=\"python\"># Define the database manager to setup the various projects\nDATABASE_ROUTERS = ['manager.router.DatabaseAppsRouter']\nDATABASE_APPS_MAPPING = {'mux_data': 't29_db', \n                         'T50_VATC':'t50_db'}\n\nDATABASES = {\n    'default': {\n            'ENGINE': 'django.db.backends.postgresql_psycopg2', \n            'NAME': 'fail_over',                    \n            'USER': 'SomeUser',                      \n            'PASSWORD': 'SomePassword',                  \n            'HOST': '127.0.0.1',                     \n            'PORT': '',                      \n    },\n\n    't29_db': {\n            'ENGINE': 'django.db.backends.postgresql_psycopg2', \n            'NAME': 'mux_stage',                    \n            'USER': 'SomeUser',                      \n            'PASSWORD': 'SomePassword',                  \n            'HOST': '127.0.0.1',                      \n            'PORT': '',                      \n    },\n\n    't50_db': {\n            'ENGINE': 'django.db.backends.postgresql_psycopg2', \n            'NAME': 't50_vatc',                    \n            'USER': 'SomeUser',                      \n            'PASSWORD': 'SomePassword',                 \n            'HOST': '127.0.0.1',                     \n            'PORT': '',                      \n    },\n}\n</code></pre>\n<p><strong>Sample Models</strong> </p>\n<pre><code class=\"python\"># Create your models here.\nclass Card_Test(models.Model):\n    name = models.TextField(max_length=100)\n    description = models.TextField(max_length=200)\n    units = models.TextField(max_length=500)\n    result_tags = models.TextField(max_length=500)\n\n    class Meta:\n        app_label = 'mux_data'\n\n    def __unicode__(self):\n        return self.name\n\nclass Status_Type(models.Model):\n    status = models.CharField(max_length=25)\n\n    class Meta:\n        app_label = 'mux_data'\n\n    def __unicode__(self):\n        return self.status\n</code></pre>\n", "abstract": "I fully agree with @alecxe on using the database router.  I am currently using a single admin interface to manage multiple databases.  Note that authentication for all databases are stored in the default database, so when you do the syncdb (with no arguments). Generic Database Router  I found this implementation to be extremely flexible and useful. Settings.py  Sample Models "}, {"id": 18547515, "score": 22, "vote": 0, "content": "<p>In order to define specific databases used for specific models, you need to define a <a href=\"https://docs.djangoproject.com/en/dev/topics/db/multi-db/#database-routers\" rel=\"noreferrer\">database router</a>:</p>\n<blockquote>\n<p>The easiest way to use multiple databases is to set up a database\n  routing scheme. The default routing scheme ensures that objects remain\n  \u2018sticky\u2019 to their original database (i.e., an object retrieved from\n  the foo database will be saved on the same database). The default\n  routing scheme ensures that if a database isn\u2019t specified, all queries\n  fall back to the default database.</p>\n</blockquote>\n<p>See this snippet as an example: <a href=\"http://djangosnippets.org/snippets/2687/\" rel=\"noreferrer\">http://djangosnippets.org/snippets/2687/</a></p>\n<p>Also see:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/8054195/django-multi-database-routing\">Django multi-database routing</a></li>\n<li><a href=\"http://nxvl.blogspot.ru/2012/10/multiple-databases-in-django.html\" rel=\"noreferrer\">Multiple Databases in Django</a></li>\n<li><a href=\"https://thenewcircle.com/s/post/1242/django_multiple_database_support\" rel=\"noreferrer\">Tutorial: Using Django's Multiple Database Support</a></li>\n<li><a href=\"http://justcramer.com/2010/12/30/database-routers-in-django/\" rel=\"noreferrer\">Database Routers in Django</a></li>\n</ul>\n", "abstract": "In order to define specific databases used for specific models, you need to define a database router: The easiest way to use multiple databases is to set up a database\n  routing scheme. The default routing scheme ensures that objects remain\n  \u2018sticky\u2019 to their original database (i.e., an object retrieved from\n  the foo database will be saved on the same database). The default\n  routing scheme ensures that if a database isn\u2019t specified, all queries\n  fall back to the default database. See this snippet as an example: http://djangosnippets.org/snippets/2687/ Also see:"}]}, {"link": "https://stackoverflow.com/questions/5058361/is-there-sqlalchemy-automigration-tool-like-south-for-django", "question": {"id": "5058361", "title": "Is there SQLAlchemy automigration tool like South for Django?", "content": "<p>Is there SQLAlchemy automigration tool like South for Django?</p>\n<p>I looked to <a href=\"http://code.google.com/p/sqlalchemy-migrate/\" rel=\"noreferrer\">sqlalchemy-migrate</a> but it doesn't seem to generate sql update scripts automatically or upgrade downgrade DB</p>\n<p>Looks like with sqlalchemy-migrate you need to \na) manually copy your old model to a new file\nb) crate new model in application and copy it to a new file\nc) write manually create/drop/alter tables in python sqlalchemy extended dialect \nd) generate sql alter script\ne) run command to execute alter sql script</p>\n<p>As for me it doesn't solve the problem and only adds overhead, as I can simply do d) manually and it will be much faster then do a), b), c) manually just to d) that you can do in one step.</p>\n<p>Is there any auto migration libraries for SQLAlchemy like South for Django, or many RoR-like migration tools?</p>\n<p>What I need is to change SQLAlchemy model in python app, run tool and it will compare current DB schema to new DB schema that new model should use, and create Alter scripts that I can adjust manually and execute. </p>\n<p>Is there any solution like this in Python?</p>\n", "abstract": "Is there SQLAlchemy automigration tool like South for Django? I looked to sqlalchemy-migrate but it doesn't seem to generate sql update scripts automatically or upgrade downgrade DB Looks like with sqlalchemy-migrate you need to \na) manually copy your old model to a new file\nb) crate new model in application and copy it to a new file\nc) write manually create/drop/alter tables in python sqlalchemy extended dialect \nd) generate sql alter script\ne) run command to execute alter sql script As for me it doesn't solve the problem and only adds overhead, as I can simply do d) manually and it will be much faster then do a), b), c) manually just to d) that you can do in one step. Is there any auto migration libraries for SQLAlchemy like South for Django, or many RoR-like migration tools? What I need is to change SQLAlchemy model in python app, run tool and it will compare current DB schema to new DB schema that new model should use, and create Alter scripts that I can adjust manually and execute.  Is there any solution like this in Python?"}, "answers": [{"id": 27820088, "score": 15, "vote": 0, "content": "<p>You can perform automatic migrations with <a href=\"https://pypi.org/project/alembic/\" rel=\"noreferrer\">Alembic</a>. I use it in two large-scale projects I am currently working on. The automatic migration generator can recognize:</p>\n<ul>\n<li>Table additions and removals</li>\n<li>Column additions and removals</li>\n<li>Change of nullable status on columns</li>\n<li>Basic changes in indexes, explicitly-named unique constraints, and foreign keys</li>\n</ul>\n<p>(see also: <a href=\"https://alembic.sqlalchemy.org/en/latest/autogenerate.html\" rel=\"noreferrer\">https://alembic.sqlalchemy.org/en/latest/autogenerate.html</a>)</p>\n<h2>Install alembic</h2>\n<pre><code class=\"python\">pip install alembic\n</code></pre>\n<p>or (depending on the version of Python you are using):</p>\n<pre><code class=\"python\">pip3 install alembic\n</code></pre>\n<h2>Configure alembic</h2>\n<p>Execute the following command in your project:</p>\n<pre><code class=\"python\">alembic init alembic\n</code></pre>\n<p>This will set up alembic for your project, in a folder called <code>alembic</code>.</p>\n<p>You will then need to edit the generated <code>alembic.ini</code> configuration file.</p>\n<p>In the file <code>env.py</code>, tell Alembic where to find SQLAlchemy's <code>metadata</code> object in your project.</p>\n<p>(see also: <a href=\"https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file\" rel=\"noreferrer\">https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file</a>)</p>\n<h2>Generate the migration</h2>\n<p>Simply execute the following command line:</p>\n<pre><code class=\"python\">alembic revision --autogenerate -m \"Message for this migration\"\n</code></pre>\n<p>Or even (not recommended):</p>\n<pre><code class=\"python\">alembic revision --autogenerate\n</code></pre>\n<h2>Upgrade the database</h2>\n<p>After this, I upgrade the database with this simple command from the folder containing the  <code>alembic.ini</code> configuration file:</p>\n<pre><code class=\"python\">alembic upgrade head\n</code></pre>\n<p>(see also: <a href=\"http://rodic.fr/blog/automatic-migrations-sqlalchemy-alembic/\" rel=\"noreferrer\">http://rodic.fr/blog/automatic-migrations-sqlalchemy-alembic/</a>)</p>\n", "abstract": "You can perform automatic migrations with Alembic. I use it in two large-scale projects I am currently working on. The automatic migration generator can recognize: (see also: https://alembic.sqlalchemy.org/en/latest/autogenerate.html) or (depending on the version of Python you are using): Execute the following command in your project: This will set up alembic for your project, in a folder called alembic. You will then need to edit the generated alembic.ini configuration file. In the file env.py, tell Alembic where to find SQLAlchemy's metadata object in your project. (see also: https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file) Simply execute the following command line: Or even (not recommended): After this, I upgrade the database with this simple command from the folder containing the  alembic.ini configuration file: (see also: http://rodic.fr/blog/automatic-migrations-sqlalchemy-alembic/)"}, {"id": 10190793, "score": 8, "vote": 0, "content": "<p>There is <a href=\"http://alembic.readthedocs.org/en/latest/index.html\" rel=\"noreferrer\">Alembic</a> which looks very promising, but the problem is (for now) that the support for SQlite databases is very limited.</p>\n", "abstract": "There is Alembic which looks very promising, but the problem is (for now) that the support for SQlite databases is very limited."}, {"id": 18214026, "score": -1, "vote": 0, "content": "<p>No there is none at this moment. Alembic require you to write code regarding adding/deleting/altering table structure or creating/dropping table. So nothing like django south exists for sqlalchemy.</p>\n", "abstract": "No there is none at this moment. Alembic require you to write code regarding adding/deleting/altering table structure or creating/dropping table. So nothing like django south exists for sqlalchemy."}, {"id": 5087099, "score": -4, "vote": 0, "content": "<p>Have you looked into using <code>sqlalchemy-migrate</code>?\n<a href=\"http://shane.caraveo.com/2010/09/13/database-migrations-for-sqlalchemy-part-duex/\" rel=\"nofollow\">http://shane.caraveo.com/2010/09/13/database-migrations-for-sqlalchemy-part-duex/</a></p>\n", "abstract": "Have you looked into using sqlalchemy-migrate?\nhttp://shane.caraveo.com/2010/09/13/database-migrations-for-sqlalchemy-part-duex/"}]}, {"link": "https://stackoverflow.com/questions/811548/sqlite-and-python-return-a-dictionary-using-fetchone", "question": {"id": "811548", "title": "Sqlite and Python -- return a dictionary using fetchone()?", "content": "<p>I'm using sqlite3 in python 2.5.  I've created a table that looks like this:</p>\n<pre><code class=\"python\">   create table votes (\n      bill text,\n      senator_id text,\n      vote text)\n</code></pre>\n<p>I'm accessing it with something like this:</p>\n<pre><code class=\"python\">v_cur.execute(\"select * from votes\")\nrow = v_cur.fetchone()\nbill = row[0]\nsenator_id = row[1]\nvote = row[2]\n</code></pre>\n<p>What I'd like to be able to do is have fetchone (or some other method) return a dictionary, rather than a list, so that I can refer to the field by name rather than position.  For example:</p>\n<pre><code class=\"python\">bill = row['bill'] \nsenator_id = row['senator_id']\nvote = row['vote']\n</code></pre>\n<p>I know you can do this with MySQL, but does anyone know how to do it with SQLite?</p>\n<p>Thanks!!!</p>\n", "abstract": "I'm using sqlite3 in python 2.5.  I've created a table that looks like this: I'm accessing it with something like this: What I'd like to be able to do is have fetchone (or some other method) return a dictionary, rather than a list, so that I can refer to the field by name rather than position.  For example: I know you can do this with MySQL, but does anyone know how to do it with SQLite? Thanks!!!"}, "answers": [{"id": 2526294, "score": 77, "vote": 0, "content": "<p>There is actually an option for this in sqlite3.  Change the <code>row_factory</code> member of the connection object to <code>sqlite3.Row</code>:</p>\n<pre><code class=\"python\">conn = sqlite3.connect('db', row_factory=sqlite3.Row)\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">conn.row_factory = sqlite3.Row\n</code></pre>\n<p>This will allow you to access row elements by name--dictionary-style--or by index.  This is much more efficient than creating your own work-around.</p>\n", "abstract": "There is actually an option for this in sqlite3.  Change the row_factory member of the connection object to sqlite3.Row: or This will allow you to access row elements by name--dictionary-style--or by index.  This is much more efficient than creating your own work-around."}, {"id": 811637, "score": 20, "vote": 0, "content": "<p>The way I've done this in the past:</p>\n<pre><code class=\"python\">def dict_factory(cursor, row):\n    d = {}\n    for idx,col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n</code></pre>\n<p>Then you set it up in your connection:</p>\n<pre><code class=\"python\">from pysqlite2 import dbapi2 as sqlite\nconn = sqlite.connect(...)\nconn.row_factory = dict_factory\n</code></pre>\n<p>This works under pysqlite-2.4.1 and python 2.5.4.</p>\n", "abstract": "The way I've done this in the past: Then you set it up in your connection: This works under pysqlite-2.4.1 and python 2.5.4."}, {"id": 14574700, "score": 11, "vote": 0, "content": "<p>I was recently trying to do something similar while using sqlite3.Row(). While sqlite3.Row() is great for providing a dictionary-like interface or a tuple like interface, it didn't work when I piped in the row using **kwargs. So, needed a quick way of converting it to a dictionary. I realised that the Row() object can be converted to a dictionary simply by using itertools.</p>\n<p>Python 2:</p>\n<pre><code class=\"python\">db.row_factory = sqlite3.Row\ndbCursor = db.cursor()\ndbCursor.execute(\"SELECT * FROM table\")\nrow = dbCursor.fetchone()\n\nrowDict = dict(itertools.izip(row.keys(), row))\n</code></pre>\n<p>Or in Python 3, more simply:</p>\n<pre><code class=\"python\">dbCursor = db.cursor()\ndbCursor.execute(\"SELECT * FROM table\")\nrow = dbCursor.fetchone()\nrowDict = dict(zip([c[0] for c in dbCursor.description], row))\n</code></pre>\n<p>Similarly, you can use the dbCursor.fetchall() command and convert the entire set of rows to a list of dictionaries in a for loop.</p>\n", "abstract": "I was recently trying to do something similar while using sqlite3.Row(). While sqlite3.Row() is great for providing a dictionary-like interface or a tuple like interface, it didn't work when I piped in the row using **kwargs. So, needed a quick way of converting it to a dictionary. I realised that the Row() object can be converted to a dictionary simply by using itertools. Python 2: Or in Python 3, more simply: Similarly, you can use the dbCursor.fetchall() command and convert the entire set of rows to a list of dictionaries in a for loop."}, {"id": 811623, "score": 5, "vote": 0, "content": "<p>Sure, make yourself a DictConnection and DictCursor as explained and shown at <a href=\"http://trac.edgewall.org/pysqlite.org-mirror/wiki/PysqliteFactories\" rel=\"nofollow noreferrer\">http://trac.edgewall.org/pysqlite.org-mirror/wiki/PysqliteFactories</a> for example.</p>\n", "abstract": "Sure, make yourself a DictConnection and DictCursor as explained and shown at http://trac.edgewall.org/pysqlite.org-mirror/wiki/PysqliteFactories for example."}, {"id": 819454, "score": 2, "vote": 0, "content": "<p>I know you're not asking this, but why not just use sqlalchemy to build an orm for the database?  then you can do things like,</p>\n<pre><code class=\"python\">\nentry = model.Session.query(model.Votes).first()\nprint entry.bill, entry.senator_id, entry.vote\n</code></pre>\n<p>as an added bonus your code will be easily portable to an alternative database, and connections and whatnot will be managed for free.</p>\n", "abstract": "I know you're not asking this, but why not just use sqlalchemy to build an orm for the database?  then you can do things like, as an added bonus your code will be easily portable to an alternative database, and connections and whatnot will be managed for free."}, {"id": 18948238, "score": 1, "vote": 0, "content": "<p>I've used this:</p>\n<pre><code class=\"python\">def get_dict(sql):\n    return dict(c.execute(sql,()).fetchall())\n</code></pre>\n<p>Then you can do this:</p>\n<pre><code class=\"python\">c = conn.cursor()\nd = get_dict(\"select user,city from vals where user like 'a%'\");\n</code></pre>\n<p>Now <code>d</code> is a dictionary where the keys are <code>user</code> and the values are <code>city</code>. This also works for <code>group by</code></p>\n", "abstract": "I've used this: Then you can do this: Now d is a dictionary where the keys are user and the values are city. This also works for group by"}, {"id": 57095094, "score": 1, "vote": 0, "content": "<p>Simple solution, initialize a cursor object:</p>\n<pre><code class=\"python\">cursor = conn.cursor(buffered = True, dictionary = True)\n</code></pre>\n<p>Another option:</p>\n<pre><code class=\"python\">cursor = conn.cursor(MySQLdb.cursors.DictCursor)\n</code></pre>\n<p>Rest of Code:</p>\n<pre><code class=\"python\">query = \"SELECT * FROM table\"\ncursor.execute(query)\nrow = cursor.fetchone()\n</code></pre>\n<p>Sources:  <a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor.html\" rel=\"nofollow noreferrer\">mysql.connector.cursor</a> , <a href=\"http://mysql-python.sourceforge.net/MySQLdb-1.2.2/public/MySQLdb.cursors.DictCursor-class.html\" rel=\"nofollow noreferrer\">MySQLdb.cursors.DictCursor</a></p>\n", "abstract": "Simple solution, initialize a cursor object: Another option: Rest of Code: Sources:  mysql.connector.cursor , MySQLdb.cursors.DictCursor"}, {"id": 52037422, "score": 0, "vote": 0, "content": "<p>I use something like this:</p>\n<pre><code class=\"python\">class SqliteRow(object):\n    def __init__(self):\n        self.fields = []\n\n    def add_field(self, name, value):\n        self.fields.append(name)\n        setattr(self, name, value)\n\n    def to_tuple(self):\n        return tuple([getattr(self, x) for x in self.fields])\n</code></pre>\n<p>with this:</p>\n<pre><code class=\"python\">def myobject_factory(cursor, row):\n    myobject= MyObject()\n    for idx, col in enumerate(cursor.description):\n        name, value = (col[0], row[idx])\n\n        myobject.add_field(name, value)\n    return myobject\n</code></pre>\n<p><code>MyObject()</code> is a class that inherits from <code>SqliteRow</code>.\nSqliteRow class is a base class for every object that I want to have returned by a query.\nEvery column becomes an attribute and is logged into the <code>fields</code> list.\nFunction <code>to_tuple</code> is used to change the whole object to a form suitable for queries (simply pass the whole object and forget). </p>\n<p>To get different class types of that function. You would need to make a factory object, that will generate objects based on the list of fields (for example: dict with { some_unique_value_made_of_fields: class}  )</p>\n<p>This way I get a simple ORM.</p>\n", "abstract": "I use something like this: with this: MyObject() is a class that inherits from SqliteRow.\nSqliteRow class is a base class for every object that I want to have returned by a query.\nEvery column becomes an attribute and is logged into the fields list.\nFunction to_tuple is used to change the whole object to a form suitable for queries (simply pass the whole object and forget).  To get different class types of that function. You would need to make a factory object, that will generate objects based on the list of fields (for example: dict with { some_unique_value_made_of_fields: class}  ) This way I get a simple ORM."}]}, {"link": "https://stackoverflow.com/questions/447117/django-increment-blog-entry-view-count-by-one-is-this-efficient", "question": {"id": "447117", "title": "Django: Increment blog entry view count by one. Is this efficient?", "content": "<p>I have the following code in my index view.</p>\n<pre><code class=\"python\">latest_entry_list = Entry.objects.filter(is_published=True).order_by('-date_published')[:10]\nfor entry in latest_entry_list:\n    entry.views = entry.views + 1\n    entry.save()\n</code></pre>\n<p>If there are ten (the limit) rows returned from the initial query, will the save issue 10 seperate updated calls to the database, or is Django \"smart\" enough to issue just one update call?</p>\n<p>Is there a more efficient method to achieve this result?</p>\n", "abstract": "I have the following code in my index view. If there are ten (the limit) rows returned from the initial query, will the save issue 10 seperate updated calls to the database, or is Django \"smart\" enough to issue just one update call? Is there a more efficient method to achieve this result?"}, "answers": [{"id": 889463, "score": 69, "vote": 0, "content": "<p>You can use <code>F()</code> objects for this. </p>\n<p>Here is how you import <code>F</code>: <code>from django.db.models import F</code></p>\n<p><strong><a href=\"http://docs.djangoproject.com/en/dev/topics/db/queries/#updating-multiple-objects-at-once\" rel=\"noreferrer\">New in Django 1.1</a>.</strong><br/>\nCalls to update can also use F() objects to update one field based on the value of another field in the model. This is especially useful for incrementing counters based upon their current value.</p>\n<pre><code class=\"python\">Entry.objects.filter(is_published=True).update(views=F('views')+1)\n</code></pre>\n<p>Although you can't do an update on a sliced query set... <strong>edit: actually you can...</strong></p>\n<p>This can be done completely in django ORM. You need two SQL queries:</p>\n<ol>\n<li>Do your filter and collect a list of primary keys</li>\n<li>Do an update on a non-sliced query set of items matching any of those primary keys.</li>\n</ol>\n<p>Getting the non-sliced query set is the hard bit. I wondered about using <a href=\"http://docs.djangoproject.com/en/dev/ref/models/querysets/#in-bulk-id-list\" rel=\"noreferrer\"><code>in_bulk</code></a> but that returns a dictionary, not a query set. One would usually use <a href=\"http://docs.djangoproject.com/en/dev/topics/db/queries/#complex-lookups-with-q-objects\" rel=\"noreferrer\"><code>Q objects</code></a> to do complex OR type queries and that will work, but <a href=\"http://docs.djangoproject.com/en/dev/ref/models/querysets/#in\" rel=\"noreferrer\"><code>pk__in</code></a> does the job much more simply.</p>\n<pre><code class=\"python\">latest_entry_ids = Entry.objects.filter(is_published=True)\\\n                                      .order_by('-date_published')\n                                      .values_list('id', flat=True)[:10]  \nnon_sliced_query_set = Entry.objects.filter(pk__in=latest_entry_ids)  \nn = non_sliced_query_set.update(views=F('views')+1)  \nprint n or 0, 'items updated'\n</code></pre>\n<p>Due to the way that django executes queries lazily, this results in just 2 database hits, no matter how many items are updated. </p>\n", "abstract": "You can use F() objects for this.  Here is how you import F: from django.db.models import F New in Django 1.1.\nCalls to update can also use F() objects to update one field based on the value of another field in the model. This is especially useful for incrementing counters based upon their current value. Although you can't do an update on a sliced query set... edit: actually you can... This can be done completely in django ORM. You need two SQL queries: Getting the non-sliced query set is the hard bit. I wondered about using in_bulk but that returns a dictionary, not a query set. One would usually use Q objects to do complex OR type queries and that will work, but pk__in does the job much more simply. Due to the way that django executes queries lazily, this results in just 2 database hits, no matter how many items are updated. "}, {"id": 447433, "score": 13, "vote": 0, "content": "<p>You could handle the updates in a single transaction, which could improve performance significantly.  Use a separate function, decorated with @transaction.commit_manually.</p>\n<pre><code class=\"python\">@transaction.commit_manually\ndef update_latest_entries(latest_entry_list):\n    for entry in latest_entry_list:\n        entry.views += 1\n        entry.save()\n    transaction.commit()\n</code></pre>\n", "abstract": "You could handle the updates in a single transaction, which could improve performance significantly.  Use a separate function, decorated with @transaction.commit_manually."}, {"id": 448196, "score": 3, "vote": 0, "content": "<p>If you really need the efficiency, at the moment you'd have to drop down into SQL and run the update yourself.  That's not worth the added complexity in this case, though.</p>\n<p>By Django 1.1 you'll be able to do this in a single SQL call via the ORM using <a href=\"http://groups.google.com/group/django-developers/browse_thread/thread/c8cff7e5e16c692a/64f348822f2d43e5?lnk=gst&amp;q=F()#64f348822f2d43e5\" rel=\"nofollow noreferrer\">F() objects</a> to reference fields in the update value.</p>\n", "abstract": "If you really need the efficiency, at the moment you'd have to drop down into SQL and run the update yourself.  That's not worth the added complexity in this case, though. By Django 1.1 you'll be able to do this in a single SQL call via the ORM using F() objects to reference fields in the update value."}, {"id": 447133, "score": 2, "vote": 0, "content": "<p><strong>Revised</strong></p>\n<p>You're updating 10 separate, individual, distinct objects.</p>\n<p>The 10 separate, individual, distinct updates can't easily be collapsed into one magical update that somehow touches 10 objects.</p>\n", "abstract": "Revised You're updating 10 separate, individual, distinct objects. The 10 separate, individual, distinct updates can't easily be collapsed into one magical update that somehow touches 10 objects."}, {"id": 2044825, "score": 2, "vote": 0, "content": "<p>A performance improvement to the previous entry. This results in one database hit, with a subquery.</p>\n<pre><code class=\"python\">latest_entry_query_set = Entry.objects.filter(is_published=True)\n                                      .order_by('-date_published')[:10]  \nnon_sliced_query_set = Entry.objects.filter(pk__in=latest_entry_query_set.values('id'))  \nn = non_sliced_query_set.update(views=F('views')+1)  \nprint n or 0, 'items updated'\n</code></pre>\n", "abstract": "A performance improvement to the previous entry. This results in one database hit, with a subquery."}]}, {"link": "https://stackoverflow.com/questions/1219326/how-do-i-do-database-transactions-with-psycopg2-python-db-api", "question": {"id": "1219326", "title": "How do I do database transactions with psycopg2/python db api?", "content": "<p>Im fiddling with psycopg2 , and while there's a .commit() and .rollback() there's no .begin() or similar to start a transaction , or so it seems ?\nI'd expect to be able to do </p>\n<pre><code class=\"python\">db.begin() # possible even set the isolation level here\ncurs = db.cursor()\ncursor.execute('select etc... for update')\n...\ncursor.execute('update ... etc.')\ndb.commit();\n</code></pre>\n<p>So, how do transactions work with psycopg2 ? \nHow would I set/change the isolation level ?</p>\n", "abstract": "Im fiddling with psycopg2 , and while there's a .commit() and .rollback() there's no .begin() or similar to start a transaction , or so it seems ?\nI'd expect to be able to do  So, how do transactions work with psycopg2 ? \nHow would I set/change the isolation level ?"}, "answers": [{"id": 1219376, "score": 34, "vote": 0, "content": "<p>Use <code>db.set_isolation_level(n)</code>, assuming <code>db</code> is your connection object. As Federico wrote <a href=\"https://web.archive.org/web/20100828225638/http://lists.initd.org/pipermail/psycopg/2004-February/002577.html\" rel=\"noreferrer\">here</a>, the meaning of <code>n</code> is:</p>\n<pre><code class=\"python\">0 -&gt; autocommit\n1 -&gt; read committed\n2 -&gt; serialized (but not officially supported by pg)\n3 -&gt; serialized\n</code></pre>\n<p>As documented <a href=\"https://www.psycopg.org/docs/extensions.html#isolation-level-constants\" rel=\"noreferrer\">here</a>, <code>psycopg2.extensions</code> gives you symbolic constants for the purpose:</p>\n<pre><code class=\"python\">Setting transaction isolation levels\n====================================\n\npsycopg2 connection objects hold informations about the PostgreSQL `transaction\nisolation level`_.  The current transaction level can be read from the\n`.isolation_level` attribute.  The default isolation level is ``READ\nCOMMITTED``.  A different isolation level con be set through the\n`.set_isolation_level()` method.  The level can be set to one of the following\nconstants, defined in `psycopg2.extensions`:\n\n`ISOLATION_LEVEL_AUTOCOMMIT`\n    No transaction is started when command are issued and no\n    `.commit()`/`.rollback()` is required.  Some PostgreSQL command such as\n    ``CREATE DATABASE`` can't run into a transaction: to run such command use\n    `.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)`.\n\n`ISOLATION_LEVEL_READ_COMMITTED`\n    This is the default value.  A new transaction is started at the first\n    `.execute()` command on a cursor and at each new `.execute()` after a\n    `.commit()` or a `.rollback()`.  The transaction runs in the PostgreSQL\n    ``READ COMMITTED`` isolation level.\n\n`ISOLATION_LEVEL_SERIALIZABLE`\n    Transactions are run at a ``SERIALIZABLE`` isolation level.\n\n\n.. _transaction isolation level: \n   http://www.postgresql.org/docs/8.1/static/transaction-iso.html\n</code></pre>\n", "abstract": "Use db.set_isolation_level(n), assuming db is your connection object. As Federico wrote here, the meaning of n is: As documented here, psycopg2.extensions gives you symbolic constants for the purpose:"}, {"id": 1265499, "score": 18, "vote": 0, "content": "<p>The <code>BEGIN</code> with python standard DB API is always implicit. When you start working with the database the driver issues a <code>BEGIN</code> and after any <code>COMMIT</code> or <code>ROLLBACK</code> another <code>BEGIN</code> is issued. A python DB API compliant with the specification should always work this way (not only the postgresql).</p>\n<p>You can change this setting the isolation level to autocommit with <code>db.set_isolation_level(n)</code> as pointed by Alex Martelli.</p>\n<p>As Tebas said the begin is implicit but not executed until an SQL is executed, so if you don't execute any SQL, the session is not in a transaction.</p>\n", "abstract": "The BEGIN with python standard DB API is always implicit. When you start working with the database the driver issues a BEGIN and after any COMMIT or ROLLBACK another BEGIN is issued. A python DB API compliant with the specification should always work this way (not only the postgresql). You can change this setting the isolation level to autocommit with db.set_isolation_level(n) as pointed by Alex Martelli. As Tebas said the begin is implicit but not executed until an SQL is executed, so if you don't execute any SQL, the session is not in a transaction."}, {"id": 1226713, "score": 9, "vote": 0, "content": "<p>I prefer to explicitly see where my transactions are : </p>\n<ul>\n<li>cursor.execute(\"BEGIN\")</li>\n<li>cursor.execute(\"COMMIT\")</li>\n</ul>\n", "abstract": "I prefer to explicitly see where my transactions are : "}]}, {"link": "https://stackoverflow.com/questions/426378/what-is-your-favorite-solution-for-managing-database-migrations-in-django", "question": {"id": "426378", "title": "What is your favorite solution for managing database migrations in django?", "content": "<p>I quite like Rails' database migration management system.  It is not 100% perfect, but it does the trick.  Django does not ship with such a database migration system (yet?) but there are a number of open source projects to do just that, such as django-evolution and south for example.</p>\n<p>So I am wondering, what database migration management solution for django do you prefer? (one option per answer please)</p>\n", "abstract": "I quite like Rails' database migration management system.  It is not 100% perfect, but it does the trick.  Django does not ship with such a database migration system (yet?) but there are a number of open source projects to do just that, such as django-evolution and south for example. So I am wondering, what database migration management solution for django do you prefer? (one option per answer please)"}, "answers": [{"id": 432679, "score": 22, "vote": 0, "content": "<p>I've been using <a href=\"http://south.aeracode.org/\" rel=\"noreferrer\">South</a>, but <a href=\"http://bitbucket.org/DeadWisdom/migratory/\" rel=\"noreferrer\">Migratory</a> looks promising as well.</p>\n", "abstract": "I've been using South, but Migratory looks promising as well."}, {"id": 426411, "score": 9, "vote": 0, "content": "<p><a href=\"http://bitbucket.org/DeadWisdom/migratory/\" rel=\"nofollow noreferrer\">Migratory</a> looks nice and simple.</p>\n", "abstract": "Migratory looks nice and simple."}, {"id": 35093161, "score": 7, "vote": 0, "content": "<p>If you are using <a href=\"https://pypi.python.org/pypi/SQLAlchemy\" rel=\"noreferrer\">SQLAlchemy</a> as your ORM then the de facto standard is <a href=\"https://pypi.python.org/pypi/alembic\" rel=\"noreferrer\">Alembic</a>.</p>\n<p>Another alternative that haven't been mentioned is <a href=\"https://pypi.python.org/pypi/yoyo-migrations\" rel=\"noreferrer\">yoyo-migrations</a>.</p>\n", "abstract": "If you are using SQLAlchemy as your ORM then the de facto standard is Alembic. Another alternative that haven't been mentioned is yoyo-migrations."}, {"id": 431901, "score": 3, "vote": 0, "content": "<p>We use Django at work, and we've been using <a href=\"http://code.google.com/p/dmigrations/\" rel=\"nofollow noreferrer\">dmigrations</a>. While it has its quirks, it's been useful so far. Some features:</p>\n<ul>\n<li>It uses a table in the database to keep track of which migrations have been applied.</li>\n<li>Because it knows which ones have been applied, you can migrate up and back down.</li>\n<li>It integrates with <code>manage.py</code> as a command.</li>\n<li>The individual migration scripts are Python, but if your migration logic is pure SQL, <em>dmigrations</em> makes it easy to just can the SQL and have it executed.</li>\n</ul>\n<p>One problem is that it only currently supports MySQL. However, one of our guys make a local hack to it to support PostgreSQL, which we use. As I recall, the hack wasn't all that extensive, so it shouldn't be terribly difficult to hack it up to support other RDBMSs.</p>\n", "abstract": "We use Django at work, and we've been using dmigrations. While it has its quirks, it's been useful so far. Some features: One problem is that it only currently supports MySQL. However, one of our guys make a local hack to it to support PostgreSQL, which we use. As I recall, the hack wasn't all that extensive, so it shouldn't be terribly difficult to hack it up to support other RDBMSs."}, {"id": 426391, "score": 2, "vote": 0, "content": "<p>I like <a href=\"http://code.google.com/p/django-evolution/\" rel=\"nofollow noreferrer\">django-evolution</a>:</p>\n<p>pros:</p>\n<ul>\n<li>clean design</li>\n<li>no SQL needed</li>\n<li>flexible</li>\n<li>trivial to install</li>\n<li>easy to use</li>\n</ul>\n<p>cons:</p>\n<ul>\n<li>migrations are not fixed in the codebase</li>\n<li>a risk exists of accidently running a migration twice</li>\n</ul>\n", "abstract": "I like django-evolution: pros: cons:"}, {"id": 664280, "score": 1, "vote": 0, "content": "<p>Besides South, dmigrations, django-evolution, and Migratory I thought I would add <a href=\"http://github.com/ricardochimal/simplemigrations/tree/master\" rel=\"nofollow noreferrer\">simplemigrations</a> as another tool I've seen for automating Django migrations.</p>\n<p>I've used three of these in the past but do migrations by hand now.  I'm thinking about trying South again due to the latest features added.</p>\n", "abstract": "Besides South, dmigrations, django-evolution, and Migratory I thought I would add simplemigrations as another tool I've seen for automating Django migrations. I've used three of these in the past but do migrations by hand now.  I'm thinking about trying South again due to the latest features added."}, {"id": 9116190, "score": 1, "vote": 0, "content": "<p>Just to note that since 2009, pretty much every project mentioned here other than South is dead. <a href=\"http://south.aeracode.org/\" rel=\"nofollow\">South</a> is the de facto standard, for better or worse.</p>\n", "abstract": "Just to note that since 2009, pretty much every project mentioned here other than South is dead. South is the de facto standard, for better or worse."}, {"id": 9534828, "score": 0, "vote": 0, "content": "<p>I've been using <a href=\"https://github.com/guilhermechapiewski/simple-db-migrate\" rel=\"nofollow\" title=\"simple-db-migrate\">simple-db-migrate</a></p>\n<p>Pros:</p>\n<ul>\n<li>it allows me to rollback the migrations (IDK if other do this too). </li>\n<li>integrates with manage.py</li>\n<li>everyone that knows SQL can create a migration</li>\n<li>it doesn't run a migration twice: the application writes the migration information(timestamp, query, etc.) on a table </li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>if you add a migration with a lower timestamp than the latest migration installed, this migration doesn't run</li>\n<li>Only MySQL is supported</li>\n</ul>\n", "abstract": "I've been using simple-db-migrate Pros: Cons:"}]}, {"link": "https://stackoverflow.com/questions/63109987/nameerror-name-mysql-is-not-defined-after-setting-change-to-mysql", "question": {"id": "63109987", "title": "NameError: name &#39;_mysql&#39; is not defined after setting change to mysql", "content": "<p>I have a running Django blog with sqlite3 db at my local machine. What I want is to</p>\n<ol>\n<li>convert sqlite3 db to mysql db</li>\n<li>change Django settings.py file to serve MySQL db</li>\n</ol>\n<p>Before I ran into the first step, I jumped into the second first. I followed <a href=\"https://www.digitalocean.com/community/tutorials/how-to-create-a-django-app-and-connect-it-to-a-database\" rel=\"nofollow noreferrer\">this web page</a> (on MacOS). I created databases called <code>djangolocaldb</code> on root user and have those infos in <code>/etc/mysql/my.cnf</code> like this:</p>\n<pre><code class=\"python\"># /etc/mysql/my.cnf\n\n[client]\ndatabase=djangolocaldb\nuser=root\npassword=ROOTPASSWORD\ndefault-character-set=utf8\n</code></pre>\n<p>Of course I created db, but not table within it.</p>\n<pre><code class=\"python\">mysql&gt; show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| djangolocaldb      |\n| employees          |\n| information_schema |\n| mydatabase         |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n7 rows in set (0.00 sec)\n</code></pre>\n<p>I changed <code>settings.py</code> like this as the web page suggested. Here's how:</p>\n<pre><code class=\"python\"># settings.py\n\n...\n\n# Database\n# https://docs.djangoproject.com/en/3.0/ref/settings/#databases\n\nDATABASES = {\n        'default': {\n            'ENGINE': 'django.db.backends.mysql',\n            #'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n            'OPTIONS' : {\n                'read_default_file': '/etc/mysql/my.cnf',\n                }\n            }\n        }\n\n...\n</code></pre>\n<p>Now, when I run <code>python manage.py runserver</code> with my <code>venv</code> activated, I got a brutal traceback like this(I ran <code>python manage.py migrate</code> first, and the traceback looked almost the same anyway):</p>\n<pre><code class=\"python\">(.venv) \u279c  django-local-blog git:(master) \u2717 python manage.py runserver\nWatching for file changes with StatReloader\nException in thread django-main-thread:\nTraceback (most recent call last):\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 18, in &lt;module&gt;\n    from . import _mysql\nImportError: dlopen(/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libmysqlclient.21.dylib\n  Referenced from: /Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so\n  Reason: image not found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py\", line 53, in wrapper\n    fn(*args, **kwargs)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/commands/runserver.py\", line 109, in inner_run\n    autoreload.raise_last_exception()\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py\", line 76, in raise_last_exception\n    raise _exception[1]\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/core/management/__init__.py\", line 357, in execute\n    autoreload.check_errors(django.setup)()\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/utils/autoreload.py\", line 53, in wrapper\n    fn(*args, **kwargs)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/__init__.py\", line 24, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/registry.py\", line 114, in populate\n    app_config.import_models()\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/apps/config.py\", line 211, in import_models\n    self.models_module = import_module(models_module_name)\n  File \"/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 1006, in _gcd_import\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 983, in _find_and_load\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 967, in _find_and_load_unlocked\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 677, in _load_unlocked\n  File \"&lt;frozen importlib._bootstrap_external&gt;\", line 728, in exec_module\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 219, in _call_with_frames_removed\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/models.py\", line 2, in &lt;module&gt;\n    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/contrib/auth/base_user.py\", line 47, in &lt;module&gt;\n    class AbstractBaseUser(models.Model):\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py\", line 121, in __new__\n    new_class.add_to_class('_meta', Options(meta, app_label))\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/base.py\", line 325, in add_to_class\n    value.contribute_to_class(cls, name)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/models/options.py\", line 208, in contribute_to_class\n    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/__init__.py\", line 28, in __getattr__\n    return getattr(connections[DEFAULT_DB_ALIAS], item)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py\", line 207, in __getitem__\n    backend = load_backend(db['ENGINE'])\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/utils.py\", line 111, in load_backend\n    return import_module('%s.base' % backend_name)\n  File \"/Users/gwanghyeongim/.pyenv/versions/3.7.6/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/django/db/backends/mysql/base.py\", line 16, in &lt;module&gt;\n    import MySQLdb as Database\n  File \"/Users/gwanghyeongim/Documents/py/coreyMS_pj/django-local-blog/.venv/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 24, in &lt;module&gt;\n    version_info, _mysql.version_info, _mysql.__file__\nNameError: name '_mysql' is not defined\n</code></pre>\n<p>So this <code>NameError: name '_mysql' is not defined</code> is the problem. I installed <code>mysqlclient</code> before, changed <code>settings.py</code>, made db in mysql, but none of the steps made it any helpful yet.</p>\n<p>And I noticed that even I changed my <code>settings.py</code> back to sqlite3, my blog spit the same <em>_mysql not defined error</em>. So I ended up reverting my commit and now I'm back to sqlite3 (at least my blog is running with it).</p>\n<p>I'm guessing it could be that I didn't convert data first, but I'm not 100% sure of it.</p>\n<p>Any suggestion would be much appreciated. Thank you in advance!</p>\n", "abstract": "I have a running Django blog with sqlite3 db at my local machine. What I want is to Before I ran into the first step, I jumped into the second first. I followed this web page (on MacOS). I created databases called djangolocaldb on root user and have those infos in /etc/mysql/my.cnf like this: Of course I created db, but not table within it. I changed settings.py like this as the web page suggested. Here's how: Now, when I run python manage.py runserver with my venv activated, I got a brutal traceback like this(I ran python manage.py migrate first, and the traceback looked almost the same anyway): So this NameError: name '_mysql' is not defined is the problem. I installed mysqlclient before, changed settings.py, made db in mysql, but none of the steps made it any helpful yet. And I noticed that even I changed my settings.py back to sqlite3, my blog spit the same _mysql not defined error. So I ended up reverting my commit and now I'm back to sqlite3 (at least my blog is running with it). I'm guessing it could be that I didn't convert data first, but I'm not 100% sure of it. Any suggestion would be much appreciated. Thank you in advance!"}, "answers": [{"id": 66332092, "score": 19, "vote": 0, "content": "<p>This did the job for me! Just install libmysqlclient-dev (<code>sudo apt-get install libmysqlclient-dev</code> for Ubuntu). Sometimes, the lib files simply are missing even if you just installed mysql. :)</p>\n", "abstract": "This did the job for me! Just install libmysqlclient-dev (sudo apt-get install libmysqlclient-dev for Ubuntu). Sometimes, the lib files simply are missing even if you just installed mysql. :)"}, {"id": 63118817, "score": 16, "vote": 0, "content": "<p>So, I'm answering my own question. Since my blog has database, I gave it a shot to make another project without db, start fresh.</p>\n<p>What I noticed was there's a problem importing <code>MySQLdb</code> module(sub module of mysqlclient) with this traceback: <code>Library not loaded: @rpath/libmysqlclient.21.dylib</code>.</p>\n<p>For browsing a few hours I realised that for some reason Mac security setting keeps this from being imported properly.</p>\n<p>On <code>mysqlclient</code> library github I found <a href=\"https://github.com/PyMySQL/mysqlclient-python/issues/14#issuecomment-538783371\" rel=\"noreferrer\">one issue</a> reporting the same as mine. It suggests I run <code>cp -r /usr/local/mysql/lib/* /usr/local/lib/</code>. After this I set <code>settings.py</code> for <code>django.db.backends.mysql</code>, ran <code>python manage.py migrate</code> and it worked. So for empty database, this could be a solution. Still struggling with database one though.</p>\n<p>I use</p>\n<ul>\n<li>MacOS Catalina 10.15.6</li>\n<li>pyenv</li>\n</ul>\n", "abstract": "So, I'm answering my own question. Since my blog has database, I gave it a shot to make another project without db, start fresh. What I noticed was there's a problem importing MySQLdb module(sub module of mysqlclient) with this traceback: Library not loaded: @rpath/libmysqlclient.21.dylib. For browsing a few hours I realised that for some reason Mac security setting keeps this from being imported properly. On mysqlclient library github I found one issue reporting the same as mine. It suggests I run cp -r /usr/local/mysql/lib/* /usr/local/lib/. After this I set settings.py for django.db.backends.mysql, ran python manage.py migrate and it worked. So for empty database, this could be a solution. Still struggling with database one though. I use"}, {"id": 65869751, "score": 13, "vote": 0, "content": "<p>this worked for me:</p>\n<p><strong>add this to PATH:</strong></p>\n<pre><code class=\"python\">export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"\n</code></pre>\n", "abstract": "this worked for me: add this to PATH:"}, {"id": 63115509, "score": 11, "vote": 0, "content": "<p>So as a full answer:</p>\n<p>If you use the python package <a href=\"https://pypi.org/project/mysqlclient/\" rel=\"noreferrer\">mysqlclient</a> you still need to install the mysql client from Oracle/MySQL. This contains the C-library that the python package uses. To make things more confusing: the python package is in fact written in C for speed increases. To install this library on MacOS:</p>\n<pre><code class=\"python\">% brew install mysql-client\n</code></pre>\n<p>There's also a <a href=\"https://pypi.org/project/PyMySQL/\" rel=\"noreferrer\">pure python package</a>, with a more attractive MIT License, which can be a solution if your company or client does not allow GPL. However, it's not officially supported and some subtle bugs can occur in between releases. YMMV.</p>\n", "abstract": "So as a full answer: If you use the python package mysqlclient you still need to install the mysql client from Oracle/MySQL. This contains the C-library that the python package uses. To make things more confusing: the python package is in fact written in C for speed increases. To install this library on MacOS: There's also a pure python package, with a more attractive MIT License, which can be a solution if your company or client does not allow GPL. However, it's not officially supported and some subtle bugs can occur in between releases. YMMV."}, {"id": 65836525, "score": 10, "vote": 0, "content": "<p>I was facing the same problem on my MacOS (Big Sur) and I fixed it by doing this\n<code>cp -r /usr/local/mysql/lib/* /usr/local/lib/</code></p>\n", "abstract": "I was facing the same problem on my MacOS (Big Sur) and I fixed it by doing this\ncp -r /usr/local/mysql/lib/* /usr/local/lib/"}, {"id": 63425945, "score": 6, "vote": 0, "content": "<p>This solved the issue for me:</p>\n<p>Since Python3 is not able to connect with Python through mysqldb, you need to install an additional module to fix things. installing mysqlclient caused me to have the same <code>NameError: : name '_mysql' is not defined</code> problem.</p>\n<p>However, by using <code>pymysql</code>, and adding the code line\n<code>pymysql.install_as_MySQLdb()</code> at the top of my <code>Flask</code> app, I managed to get it running without any errors!</p>\n<p>More info on <a href=\"https://stackoverflow.com/a/25724855/14022782\">mysql modules</a></p>\n", "abstract": "This solved the issue for me: Since Python3 is not able to connect with Python through mysqldb, you need to install an additional module to fix things. installing mysqlclient caused me to have the same NameError: : name '_mysql' is not defined problem. However, by using pymysql, and adding the code line\npymysql.install_as_MySQLdb() at the top of my Flask app, I managed to get it running without any errors! More info on mysql modules"}, {"id": 72220895, "score": 5, "vote": 0, "content": "<p>On a MacBook Pro M1 macOS Monterey, running this command didn't work:</p>\n<p><code>export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"</code></p>\n<p>But this worked for me:</p>\n<pre><code class=\"python\">export DYLD_LIBRARY_PATH=/usr/local/mysql/lib\n</code></pre>\n", "abstract": "On a MacBook Pro M1 macOS Monterey, running this command didn't work: export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\" But this worked for me:"}, {"id": 73024476, "score": 2, "vote": 0, "content": "<p>Make sure you have installed mysql client. To install it write following command.</p>\n<pre><code class=\"python\">pip install mysqlclient\n</code></pre>\n<p>Go to the settings.py file of your project and then import</p>\n<pre><code class=\"python\">import pymysql \npymysql.install_as_MySQLdb()\n</code></pre>\n<p>run the server\nIf you get no module named pymysql then in terminal run</p>\n<pre><code class=\"python\">pip install pymysql\n</code></pre>\n", "abstract": "Make sure you have installed mysql client. To install it write following command. Go to the settings.py file of your project and then import run the server\nIf you get no module named pymysql then in terminal run"}, {"id": 64294248, "score": 1, "vote": 0, "content": "<p>I just had a similar problem and couldnt find solution for hours</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; import MySQLdb\nTraceback (most recent call last):\n  File \"/{path-to-venv}/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 18, in &lt;module&gt;\n    from . import _mysql\nImportError: /{path-to-venv}/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-arm-linux-gnueabihf.so: failed to map segment from shared object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/{path-to-venv}/lib/python3.7/site-packages/MySQLdb/__init__.py\", line 24, in &lt;module&gt;\n    version_info, _mysql.version_info, _mysql.__file__\nNameError: name '_mysql' is not defined\n</code></pre>\n<p>so if anyone here is like me and has the virtualenv on a mounted partition/disk, you have to mount it with <em>exec</em>, thats the whole problem.</p>\n<p>Remount the partition with executable permission as explained in: <a href=\"https://askubuntu.com/questions/311438/how-to-make-tmp-executable\">https://askubuntu.com/questions/311438/how-to-make-tmp-executable</a>.</p>\n<p>If you mount the drive with fstab, see: <a href=\"https://askubuntu.com/questions/678857/fstab-doesnt-mount-with-exec\">https://askubuntu.com/questions/678857/fstab-doesnt-mount-with-exec</a>.</p>\n<p>(well, that was 10hours of trying and debugging well spend lol)</p>\n", "abstract": "I just had a similar problem and couldnt find solution for hours so if anyone here is like me and has the virtualenv on a mounted partition/disk, you have to mount it with exec, thats the whole problem. Remount the partition with executable permission as explained in: https://askubuntu.com/questions/311438/how-to-make-tmp-executable. If you mount the drive with fstab, see: https://askubuntu.com/questions/678857/fstab-doesnt-mount-with-exec. (well, that was 10hours of trying and debugging well spend lol)"}, {"id": 67133044, "score": 0, "vote": 0, "content": "<p>I agree with Melvyn.</p>\n<p>you can see your MySQL library link by typing like:</p>\n<pre><code class=\"python\">(quantum) chaiyudeMacBook-Pro:quantum chaiyu$ python\nPython 3.8.7 (v3.8.7:6503f05dd5, Dec 21 2020, 12:45:15) \n[Clang 6.0 (clang-600.0.57)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import MySQLdb as Database\nTraceback (most recent call last):\n  File \"/Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/__init__.py\", line 18, in &lt;module&gt;\n    from . import _mysql\nImportError: dlopen(/Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/_mysql.cpython-38-darwin.so, 2): Library not loaded: /usr/local/opt/mysql/lib/libmysqlclient.21.dylib\n  Referenced from: /Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/_mysql.cpython-38-darwin.so\n  Reason: image not found\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/__init__.py\", line 24, in &lt;module&gt;\n    version_info, _mysql.version_info, _mysql.__file__\nNameError: name '_mysql' is not defined\n</code></pre>\n<p>then type:</p>\n<pre><code class=\"python\">(quantum) chaiyudeMacBook-Pro:quantum chaiyu$ otool -L /Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/_mysql.cpython-38-darwin.so\n/Users/chaiyu/Envs/quantum/lib/python3.8/site-packages/MySQLdb/_mysql.cpython-38-darwin.so:\n    /usr/local/opt/mysql/lib/libmysqlclient.21.dylib (compatibility version 21.0.0, current version 21.0.0)\n    /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.250.1)\n(quantum) chaiyudeMacBook-Pro:quantum chaiyu$ ls -l /usr/local/opt/mysql/lib/libmysqlclient.21.dylib\nls: /usr/local/opt/mysql/lib/libmysqlclient.21.dylib: No such file or directory\n</code></pre>\n<p>and then, I found the library MySQL linked to does not exist.</p>\n", "abstract": "I agree with Melvyn. you can see your MySQL library link by typing like: then type: and then, I found the library MySQL linked to does not exist."}, {"id": 68075991, "score": 0, "vote": 0, "content": "<p>I had the same error and it was working for a while. I did an update on MacOS BigSur then it stopped working with this error.</p>\n<p>To fix this for me it was a simple uninstall reinstall of both django and mysqlclient.</p>\n<p>The uninstall/reinstall of just mysqlclient itself did not do the trick. Also the order may help. Here are the commands in the order i did it in:</p>\n<pre><code class=\"python\">pip uninstall mysqlclient\npip uninstall django\npip install django\npip install mysqlclient\n</code></pre>\n<p>Note: this will install the latest versions, so if you have specific versions, make sure you install those versions.</p>\n", "abstract": "I had the same error and it was working for a while. I did an update on MacOS BigSur then it stopped working with this error. To fix this for me it was a simple uninstall reinstall of both django and mysqlclient. The uninstall/reinstall of just mysqlclient itself did not do the trick. Also the order may help. Here are the commands in the order i did it in: Note: this will install the latest versions, so if you have specific versions, make sure you install those versions."}, {"id": 69263522, "score": 0, "vote": 0, "content": "<p>This worked for me</p>\n<pre><code class=\"python\">brew install mysql\n</code></pre>\n", "abstract": "This worked for me"}, {"id": 70224627, "score": 0, "vote": 0, "content": "<p>for mac</p>\n<h1>Assume you are activating Python 3 venv</h1>\n<pre><code class=\"python\">brew install mysql\npip install mysqlclient\n</code></pre>\n<p>for ubuntu interminal run</p>\n<pre><code class=\"python\">sudo apt-get install python3-dev default-libmysqlclient-dev build-essential\npip install mysqlclient\n</code></pre>\n<p>for # Red Hat / CentOS</p>\n<pre><code class=\"python\">sudo yum install python3-devel mysql-devel\npip install mysqlclient\n</code></pre>\n", "abstract": "for mac for ubuntu interminal run for # Red Hat / CentOS"}, {"id": 70299394, "score": 0, "vote": 0, "content": "<p>Finally after spending like decade on this issue found a good answer.</p>\n<p><code>mysqlclient</code> is a Python 3 compatible fork of the original Python MySQL driver, <code>MySQLdb</code>. It still provides a Python module called <code>MySQLdb</code>. On install, it compiles against either the MariaDB client library or MySQL client library - whichever one you have installed.</p>\n<p>Solution Found <a href=\"https://adamj.eu/tech/2020/02/04/how-to-use-pymysql-with-django/\" rel=\"nofollow noreferrer\">Here</a> By Adam(Genius Guy)</p>\n", "abstract": "Finally after spending like decade on this issue found a good answer. mysqlclient is a Python 3 compatible fork of the original Python MySQL driver, MySQLdb. It still provides a Python module called MySQLdb. On install, it compiles against either the MariaDB client library or MySQL client library - whichever one you have installed. Solution Found Here By Adam(Genius Guy)"}, {"id": 70424511, "score": 0, "vote": 0, "content": "<p>From Apple Silicon M1 Mac and a conda environment the only solution that worked for me was to use <a href=\"https://github.com/conda-forge/miniforge\" rel=\"nofollow noreferrer\">Miniforge</a> installation, which is comparable to Miniconda, but with conda-forge as the default channel. There is an Apple Silicon option that solved the problem for me: <a href=\"https://github.com/Haydnspass/miniforge#download\" rel=\"nofollow noreferrer\">https://github.com/Haydnspass/miniforge#download</a></p>\n<p>(Solution from <a href=\"https://towardsdatascience.com/using-conda-on-an-m1-mac-b2df5608a141\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/using-conda-on-an-m1-mac-b2df5608a141</a>)</p>\n", "abstract": "From Apple Silicon M1 Mac and a conda environment the only solution that worked for me was to use Miniforge installation, which is comparable to Miniconda, but with conda-forge as the default channel. There is an Apple Silicon option that solved the problem for me: https://github.com/Haydnspass/miniforge#download (Solution from https://towardsdatascience.com/using-conda-on-an-m1-mac-b2df5608a141)"}, {"id": 70829930, "score": 0, "vote": 0, "content": "<p>If you're using bash, use:</p>\n<pre><code class=\"python\">open -t .bash_profile \n</code></pre>\n<p>and add the following:</p>\n<pre><code class=\"python\">export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"\n</code></pre>\n<hr/>\n<p>If you're using Zsh, use:</p>\n<pre><code class=\"python\">open -t ~/.zshrc \n</code></pre>\n<p>and add the following:</p>\n<pre><code class=\"python\">export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"\n</code></pre>\n", "abstract": "If you're using bash, use: and add the following: If you're using Zsh, use: and add the following:"}, {"id": 71007408, "score": 0, "vote": 0, "content": "<p>although I used _mysql for 10 years, the MySQL website tells the recommended way for Python 3 at:\n<a href=\"https://dev.mysql.com/doc/connector-python/en/preface.html\" rel=\"nofollow noreferrer\">https://dev.mysql.com/doc/connector-python/en/preface.html</a></p>\n<p>Conversion may take some time, but for me less than an hour. Here an example with new code:</p>\n<pre><code class=\"python\">import mysql.connector\ncnx = mysql.connector.connect(user='mensfort', password='zhongcan',\n                              host='127.0.0.1', database='restaurant')\ncursor = cnx.cursor()\ncursor.execute('select dongle from personnel')\nfor dongle in cursor:\n    print(dongle)\ncursor.close()\ncnx.close()\n</code></pre>\n<p>Please use your own password, database name, queries, this is just an example.</p>\n<p>Please do UNINSTALL this. This is not working for me:</p>\n<pre><code class=\"python\">pip uninstall mysql-connector\n</code></pre>\n<p>Please INSTALL this first. It works great:</p>\n<pre><code class=\"python\">pip install mysql-connector-python\n</code></pre>\n", "abstract": "although I used _mysql for 10 years, the MySQL website tells the recommended way for Python 3 at:\nhttps://dev.mysql.com/doc/connector-python/en/preface.html Conversion may take some time, but for me less than an hour. Here an example with new code: Please use your own password, database name, queries, this is just an example. Please do UNINSTALL this. This is not working for me: Please INSTALL this first. It works great:"}, {"id": 73119055, "score": 0, "vote": 0, "content": "<p>For Macbook M1 this command worked</p>\n<pre><code class=\"python\">export DYLD_LIBRARY_PATH=/usr/local/mysql/lib\n</code></pre>\n", "abstract": "For Macbook M1 this command worked"}, {"id": 73653254, "score": 0, "vote": 0, "content": "<p>I faced the same issue with my django project working on ubuntu 22.04 but when i imported the code below to the setting file for the django project it works,</p>\n<ol>\n<li>import pymysql</li>\n<li>pymysql.install_as_MySQLdb()</li>\n</ol>\n<p>it will only work if you have pymysql installed but if you don't have it installed used pip install pymysql.\nThis worked for me</p>\n", "abstract": "I faced the same issue with my django project working on ubuntu 22.04 but when i imported the code below to the setting file for the django project it works, it will only work if you have pymysql installed but if you don't have it installed used pip install pymysql.\nThis worked for me"}, {"id": 64535020, "score": -3, "vote": 0, "content": "<p>Reverting back from MySQL Server 8.x.x to 5.7.x worked for me.</p>\n<blockquote>\n<p>Django supports MySQL 5.5.x - 5.7.x. MySQL 8 and later aren\u2019t supported.</p>\n</blockquote>\n<p>Found @ <a href=\"https://docs.djangoproject.com/en/1.11/ref/databases/#version-support\" rel=\"nofollow noreferrer\">Django Docs</a></p>\n", "abstract": "Reverting back from MySQL Server 8.x.x to 5.7.x worked for me. Django supports MySQL 5.5.x - 5.7.x. MySQL 8 and later aren\u2019t supported. Found @ Django Docs"}]}, {"link": "https://stackoverflow.com/questions/5833418/django-south-is-there-a-way-to-view-the-sql-it-runs", "question": {"id": "5833418", "title": "Django - South - Is There a way to view the SQL it runs?", "content": "<p>Here's what I want to do.</p>\n<p>Develop a Django project on a development server with a development database.  Run the south migrations as necessary when I change the model.</p>\n<p>Save the SQL from each migration, and apply those to the production server when I'm ready to deploy.</p>\n<p>Is such a thing possible with South?  (I'd also be curious what others do to get your development database changes on production when working with Django)</p>\n", "abstract": "Here's what I want to do. Develop a Django project on a development server with a development database.  Run the south migrations as necessary when I change the model. Save the SQL from each migration, and apply those to the production server when I'm ready to deploy. Is such a thing possible with South?  (I'd also be curious what others do to get your development database changes on production when working with Django)"}, "answers": [{"id": 5897509, "score": 50, "vote": 0, "content": "<p>You can at least inspect the sql generated by doing <code>manage.py migrate --db-dry-run --verbosity=2</code>. This will not do anything to the database and will show all the sql. I would still make a backup though, better safe than sorry. </p>\n", "abstract": "You can at least inspect the sql generated by doing manage.py migrate --db-dry-run --verbosity=2. This will not do anything to the database and will show all the sql. I would still make a backup though, better safe than sorry. "}, {"id": 34548252, "score": 7, "vote": 0, "content": "<pre><code class=\"python\"> python manage.py sqlmigrate &lt;app_label&gt; &lt;migration_name&gt;\n</code></pre>\n", "abstract": ""}, {"id": 5947567, "score": 3, "vote": 0, "content": "<p>You could try logging the SQL queries in db.connection.queries, using a management command that calls the migrate with a dry-run option:</p>\n<pre><code class=\"python\">\nfrom django.core.management.base import BaseCommand\nfrom django import db\n\nclass Command(BaseCommand):\n    help = 'Output SQL for migration'\n\n    def handle(self, *app_labels, **options):\n        # assumes DEBUG is True in settings\n        db.reset_queries()\n\n        from django.core.management import call_command\n        kw = {'db-dry-run': 1,  'verbosity': 0}\n        call_command('migrate', **kw)\n\n        for query in db.connection.queries:\n            print query['sql']\n</code></pre>\n<p>Assuming that south puts everything through the usual db interface that should work.  There will be a few extra selects in there when it queries the history table.</p>\n<p>You'd put that in a <code>management/commands/print_migration_sql.py</code> inside your app and then run it:</p>\n<pre><code class=\"python\">\npython manage.py print_migration_sql\n</code></pre>\n<p>It could probably be easily extended to run this only for specific apps etc</p>\n", "abstract": "You could try logging the SQL queries in db.connection.queries, using a management command that calls the migrate with a dry-run option: Assuming that south puts everything through the usual db interface that should work.  There will be a few extra selects in there when it queries the history table. You'd put that in a management/commands/print_migration_sql.py inside your app and then run it: It could probably be easily extended to run this only for specific apps etc"}, {"id": 22366403, "score": 3, "vote": 0, "content": "<p>When I need to see the SQL that South generates for debugging or verification I just add the following logging settings to my local_settings.LOGGING.loggers:</p>\n<pre><code class=\"python\">    'django.db.backends': {\n        'handlers': ['console'],\n        'level': 'DEBUG',\n    },\n</code></pre>\n<p>This is a complete example of the logging setting for South:</p>\n<pre><code class=\"python\">LOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'verbose': {\n            'format': '[%(asctime)s] %(levelname)s %(name)s %(lineno)d \"%(message)s\"'\n        },\n        'simple': {\n            'format': '%(levelname)s %(message)s'\n        },\n    },\n    'filters': {\n        'require_debug_false': {\n            '()': 'django.utils.log.RequireDebugFalse'\n        }\n    },\n    'handlers': {\n        'console': {\n            'level': 'DEBUG',\n            'class': 'logging.StreamHandler',\n            'formatter': 'verbose',\n        },\n    },\n    'loggers': {\n        'django': {\n            'handlers': ['console'],\n            'level': 'DEBUG',\n            'propagate': True,\n        },\n        'django.db.backends': {\n            'handlers': ['console'],\n            'level': 'DEBUG',\n        },\n    }\n}\n</code></pre>\n<p>This will output everything including the query that South runs to decide what migrations to run:</p>\n<pre><code class=\"python\">[2014-03-12 23:47:31,385] DEBUG django.db.backends 79 \"(0.001) SELECT `south_migrationhistory`.`id`, `south_migrationhistory`.`app_name`, `south_migrationhistory`.`migration`, `south_migrationhistory`.`applied` FROM `south_migrationhistory` WHERE `south_migrationhistory`.`applied` IS NOT NULL ORDER BY `south_migrationhistory`.`applied` ASC; args=()\"\n</code></pre>\n<p>That and setting verbosity to 2 or 3 is usually more than enough to get a clear picture of what's going on.</p>\n", "abstract": "When I need to see the SQL that South generates for debugging or verification I just add the following logging settings to my local_settings.LOGGING.loggers: This is a complete example of the logging setting for South: This will output everything including the query that South runs to decide what migrations to run: That and setting verbosity to 2 or 3 is usually more than enough to get a clear picture of what's going on."}, {"id": 5932967, "score": 2, "vote": 0, "content": "<p>I'd either do what Lutger suggested (and maybe write a log parser to strip out <em>just</em> the SQL), or I'd run my migration against a test database with logging enabled on the test DB.</p>\n<p>Of course, if you can run it against the test database, you're just a few steps away from validating the migration.  If it passes, run it again against production.</p>\n", "abstract": "I'd either do what Lutger suggested (and maybe write a log parser to strip out just the SQL), or I'd run my migration against a test database with logging enabled on the test DB. Of course, if you can run it against the test database, you're just a few steps away from validating the migration.  If it passes, run it again against production."}]}, {"link": "https://stackoverflow.com/questions/476017/a-queryset-by-aggregate-field-value", "question": {"id": "476017", "title": "A QuerySet by aggregate field value", "content": "<p>Let's say I have the following model:</p>\n<pre><code class=\"python\">class Contest:\n    title = models.CharField( max_length = 200 )\n    description = models.TextField()\n\nclass Image:\n    title = models.CharField( max_length = 200 )\n    description = models.TextField()\n    contest = models.ForeignKey( Contest )\n    user = models.ForeignKey( User )\n\n    def score( self ):\n        return self.vote_set.all().aggregate( models.Sum( 'value' ) )[ 'value__sum' ]\n\nclass Vote:\n    value = models.SmallIntegerField()\n    user = models.ForeignKey( User )\n    image = models.ForeignKey( Image )\n</code></pre>\n<p>The users of a site can contribute their images to several contests. Then other users can vote them up or down.</p>\n<p>Everything works fine, but now I want to display a page on which users can see all contributions to a certain contest. The images shall be ordered by their score.\nTherefore I have tried the following:</p>\n<pre><code class=\"python\">Contest.objects.get( pk = id ).image_set.order_by( 'score' )\n</code></pre>\n<p>As I feared it doesn't work since <code>'score'</code> is no database field that could be used in queries.</p>\n", "abstract": "Let's say I have the following model: The users of a site can contribute their images to several contests. Then other users can vote them up or down. Everything works fine, but now I want to display a page on which users can see all contributions to a certain contest. The images shall be ordered by their score.\nTherefore I have tried the following: As I feared it doesn't work since 'score' is no database field that could be used in queries."}, "answers": [{"id": 476033, "score": 47, "vote": 0, "content": "<p>Oh, of course I forget about new aggregation support in Django and its <code>annotate</code> functionality.</p>\n<p>So query may look like this:</p>\n<pre><code class=\"python\">Contest.objects.get(pk=id).image_set.annotate(score=Sum('vote__value')).order_by( 'score' )\n</code></pre>\n", "abstract": "Oh, of course I forget about new aggregation support in Django and its annotate functionality. So query may look like this:"}, {"id": 476024, "score": 9, "vote": 0, "content": "<p>You can write your own sort in Python very simply.</p>\n<pre><code class=\"python\">def getScore( anObject ):\n    return anObject.score()\nobjects= list(Contest.objects.get( pk = id ).image_set)\nobjects.sort( key=getScore )\n</code></pre>\n<p>This works nicely because we sorted the list, which we're going to provide to the template.</p>\n", "abstract": "You can write your own sort in Python very simply. This works nicely because we sorted the list, which we're going to provide to the template."}, {"id": 476025, "score": 2, "vote": 0, "content": "<p>The db-level <code>order_by</code> cannot sort queryset by model's python method.</p>\n<p>The solution is to introduce <code>score</code> field to <code>Image</code> model and recalculate it on every <code>Vote</code> update. Some sort of denormalization. When you will can to sort by it.</p>\n", "abstract": "The db-level order_by cannot sort queryset by model's python method. The solution is to introduce score field to Image model and recalculate it on every Vote update. Some sort of denormalization. When you will can to sort by it."}]}, {"link": "https://stackoverflow.com/questions/5832531/recommended-nosql-database-for-use-with-python", "question": {"id": "5832531", "title": "Recommended NoSQL Database for use with Python", "content": "<p>What are the <strong>popular NoSQL databases</strong> that are used with Python ? I know there are a few options as explained at <a href=\"http://nosql-database.org/\" rel=\"nofollow noreferrer\">http://nosql-database.org/</a> but which one does python programmers use/recommend the most?</p>\n", "abstract": "What are the popular NoSQL databases that are used with Python ? I know there are a few options as explained at http://nosql-database.org/ but which one does python programmers use/recommend the most?"}, "answers": [{"id": 5835099, "score": 24, "vote": 0, "content": "<p>Most of the nosql databases have python clients which are actively supported.  Pick your database based on your usage needs.  Using it from python shouldn't be a problem.<br/>\nTo name a few:<br/>\nCassandra: <a href=\"https://github.com/datastax/python-driver\" rel=\"noreferrer\">https://github.com/datastax/python-driver</a><br/>\nRiak: <a href=\"https://github.com/basho/riak-python-client\" rel=\"noreferrer\">https://github.com/basho/riak-python-client</a><br/>\nMongoDB: <a href=\"http://api.mongodb.org/python/current/\" rel=\"noreferrer\">http://api.mongodb.org/python/current/</a><br/>\nCouchDB: <a href=\"http://wiki.apache.org/couchdb/Getting_started_with_Python\" rel=\"noreferrer\">http://wiki.apache.org/couchdb/Getting_started_with_Python</a><br/>\nRedis: <a href=\"https://github.com/andymccurdy/redis-py\" rel=\"noreferrer\">https://github.com/andymccurdy/redis-py</a> </p>\n", "abstract": "Most of the nosql databases have python clients which are actively supported.  Pick your database based on your usage needs.  Using it from python shouldn't be a problem.\nTo name a few:\nCassandra: https://github.com/datastax/python-driver\nRiak: https://github.com/basho/riak-python-client\nMongoDB: http://api.mongodb.org/python/current/\nCouchDB: http://wiki.apache.org/couchdb/Getting_started_with_Python\nRedis: https://github.com/andymccurdy/redis-py "}, {"id": 5833073, "score": 14, "vote": 0, "content": "<p>I like mongodb.  Basically you can just throw a dictionary into a database, which makes it very easy to use from python.  I haven't seen a consensus on one specific nosql database.  I would suggest trying a couple of them.  Redis is pretty neat and couchdb is in the mix.</p>\n<p><a href=\"http://api.mongodb.org/python/current/tutorial.html\" rel=\"noreferrer\">http://api.mongodb.org/python/current/tutorial.html</a></p>\n", "abstract": "I like mongodb.  Basically you can just throw a dictionary into a database, which makes it very easy to use from python.  I haven't seen a consensus on one specific nosql database.  I would suggest trying a couple of them.  Redis is pretty neat and couchdb is in the mix. http://api.mongodb.org/python/current/tutorial.html"}, {"id": 5834881, "score": 5, "vote": 0, "content": "<p>Be sure to take a look at <a href=\"http://www.zodb.org/en/latest/\" rel=\"nofollow\">ZODB</a>. It's an exceedingly easy-to-use, Python-based database that covers a large range of use cases and has been used in production environments for many years.</p>\n", "abstract": "Be sure to take a look at ZODB. It's an exceedingly easy-to-use, Python-based database that covers a large range of use cases and has been used in production environments for many years."}, {"id": 18340968, "score": 4, "vote": 0, "content": "<p>In addition to the standard NOSQL databases mentioned by Zanzon you might also try:</p>\n<ul>\n<li><p><a href=\"http://docs.python.org/2/library/shelve.html?highlight=shelve#shelve\" rel=\"nofollow\">shelve</a>, which is a document database provided as part of the python standard library, and uses <a href=\"http://docs.python.org/2/library/anydbm.html\" rel=\"nofollow\">anydbm</a> as a backend. it supports anything that can be pickled.</p></li>\n<li><p><a href=\"https://pypi.python.org/pypi/shove/0.5.6\" rel=\"nofollow\">shove</a>, which is similar to <a href=\"http://docs.python.org/2/library/shelve.html?highlight=shelve#shelve\" rel=\"nofollow\">shelve</a> but with a wide choice of backends including dbm, ZODB, Redis, Mongo et al.</p></li>\n<li><p><a href=\"http://www.zodb.org/en/latest/\" rel=\"nofollow\">ZODB</a>, which is part of the Zope framework - I'd recommend using this on its own only if you also use Zope.</p></li>\n</ul>\n<p>I myself have recently started using shelve with a decent amount of success - the only caution I'd give you is that it does not play well with the default OSX version of python - in fact issues with low file size limits (hundreds of KiB) have been noted on 2.7.1&lt;=python&lt;=2.7.3 . I however have no issued on the brew version of python 2.7.5</p>\n", "abstract": "In addition to the standard NOSQL databases mentioned by Zanzon you might also try: shelve, which is a document database provided as part of the python standard library, and uses anydbm as a backend. it supports anything that can be pickled. shove, which is similar to shelve but with a wide choice of backends including dbm, ZODB, Redis, Mongo et al. ZODB, which is part of the Zope framework - I'd recommend using this on its own only if you also use Zope. I myself have recently started using shelve with a decent amount of success - the only caution I'd give you is that it does not play well with the default OSX version of python - in fact issues with low file size limits (hundreds of KiB) have been noted on 2.7.1<=python<=2.7.3 . I however have no issued on the brew version of python 2.7.5"}, {"id": 9546201, "score": 0, "vote": 0, "content": "<p>I strongly recommend <a href=\"http://www.garret.ru/dybase.html\" rel=\"nofollow noreferrer\">DyBASE</a>. It is not popular but it is an excellent Python object database before the NoSQL term existed. I answered a similar question at <a href=\"https://stackoverflow.com/questions/5106212/list-of-python-object-databases\">List of Python Object Databases</a>.</p>\n", "abstract": "I strongly recommend DyBASE. It is not popular but it is an excellent Python object database before the NoSQL term existed. I answered a similar question at List of Python Object Databases."}]}, {"link": "https://stackoverflow.com/questions/9723656/whats-causing-unable-to-connect-to-data-source-for-pyodbc", "question": {"id": "9723656", "title": "What&#39;s causing &#39;unable to connect to data source&#39; for pyodbc?", "content": "<p>I'm trying to connect to an MSSQL database from python on Linux (SLES).</p>\n<p>I have installed pyodbc and Free TDS. From the command line:</p>\n<pre><code class=\"python\">tsql -H server -p 1433 -U username -P password\n</code></pre>\n<p>Connects to the server without a problem, however, from Python:</p>\n<pre><code class=\"python\">import pyodbc\npyodbc.connect(driver='{FreeTDS}', server='server', database='database', uid='username', pwd='password')\n</code></pre>\n<p>Yields an error:</p>\n<pre><code class=\"python\">pyodbc.Error: ('08001', '[08001] [unixODBC][FreeTDS][SQL Server]Unable to connect to data source (0) (SQLDriverConnect)')\n</code></pre>\n<p>I'm finding this error unhelpfully vague. Even a suggestion to narrow down the issue would be helpful right now.</p>\n<p>Edit:\n    Looking at the TDS log dump it looks like this is where the whole thing falls apart:</p>\n<pre><code class=\"python\">token.c:328:tds_process_login_tokens()\nutil.c:331:tdserror(0x87bbeb8, 0x8861820, 20017, 115)\nodbc.c:2270:msgno 20017 20003\nutil.c:361:tdserror: client library returned TDS_INT_CANCEL(2)\nutil.c:384:tdserror: returning TDS_INT_CANCEL(2)\nutil.c:156:Changed query state from IDLE to DEAD\ntoken.c:337:looking for login token, got  0()\ntoken.c:122:tds_process_default_tokens() marker is 0()\ntoken.c:125:leaving tds_process_default_tokens() connection dead\nlogin.c:466:login packet accepted\nutil.c:331:tdserror(0x87bbeb8, 0x8861820, 20002, 0)\nodbc.c:2270:msgno 20002 20003\nutil.c:361:tdserror: client library returned TDS_INT_CANCEL(2)\nutil.c:384:tdserror: returning TDS_INT_CANCEL(2)\nmem.c:615:tds_free_all_results()\nerror.c:412:odbc_errs_add: \"Unable to connect to data source\"\n</code></pre>\n", "abstract": "I'm trying to connect to an MSSQL database from python on Linux (SLES). I have installed pyodbc and Free TDS. From the command line: Connects to the server without a problem, however, from Python: Yields an error: I'm finding this error unhelpfully vague. Even a suggestion to narrow down the issue would be helpful right now. Edit:\n    Looking at the TDS log dump it looks like this is where the whole thing falls apart:"}, "answers": [{"id": 11863041, "score": 21, "vote": 0, "content": "<p>I try with:</p>\n<ul>\n<li>MS SQL 2008 Datacenter</li>\n<li>Ubuntu 12.04 TLS (amd64)</li>\n<li>Python 2.7 </li>\n</ul>\n<p>And this works for me:</p>\n<p>Test connection:</p>\n<pre><code class=\"python\">tsql -H 10.19.4.42 -p 1433 -U DAVIDG -P 123456\n</code></pre>\n<p>on /etc/odbcinst.ini add:</p>\n<pre><code class=\"python\">[ODBC]\nTrace = Yes\nTraceFile = /tmp/odbc.log\n\n[FreeTDS]\nDescription = TDS driver (Sybase/MS SQL)\nDriver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so\nSetup =  /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so\nUsageCount = 1\n</code></pre>\n<p>on /etc/odbc.ini add:</p>\n<pre><code class=\"python\">[SQLDemo]\nDescription=my dsn\nDriver=FreeTDS\nDatabase=teste3\nServername=SQLDemo\n</code></pre>\n<p>on /etc/freetds/freetds.conf add:</p>\n<pre><code class=\"python\">[SQLDemo]\n        host = 10.19.4.42\n        port = 1433\n        tds version = 8.0\n</code></pre>\n<p>test with test.py:</p>\n<pre><code class=\"python\">#!/usr/bin/python\n\nimport pyodbc\ncnx = pyodbc.connect(\"DSN=SQLDemo;UID=DAVIDG;PWD=123456\")\n\ncursor = cnx.cursor()\ncursor.execute(\"select * from Company;\")\nfor row in cursor:\n  print row.Name\n</code></pre>\n", "abstract": "I try with: And this works for me: Test connection: on /etc/odbcinst.ini add: on /etc/odbc.ini add: on /etc/freetds/freetds.conf add: test with test.py:"}, {"id": 12370754, "score": 19, "vote": 0, "content": "<p>I had the same problem and I found out that it was missing the <code>TDS_Version</code> parameter in the call to <code>connect()</code>. The following code works for me to connect to an instance of MS SQL Server 2008:</p>\n<pre><code class=\"python\">import pyodbc\n\ndriver = '/opt/local/lib/libtdsodbc.so' # Change this to where FreeTDS installed the driver libaray!\n\nconn = pyodbc.connect(\n    driver = driver,\n    TDS_Version = '7.2', # Use for\n    server = '&lt;hostname or ip address&gt;',\n    port = 1433,\n    database = '&lt;database&gt;',\n    uid = '&lt;uid&gt;',\n    pwd = '&lt;pwd&gt;')\n</code></pre>\n", "abstract": "I had the same problem and I found out that it was missing the TDS_Version parameter in the call to connect(). The following code works for me to connect to an instance of MS SQL Server 2008:"}, {"id": 9754890, "score": 13, "vote": 0, "content": "<p>After hours of going in circles it turns out all I was missing was</p>\n<p>TDS_Version = 8.0\nin the DSN in my odbc.ini file.</p>\n<p>I had specified it elsewhere, but it needed to be here, too, apparently.</p>\n<p>Hope this helps some other poor soul.</p>\n", "abstract": "After hours of going in circles it turns out all I was missing was TDS_Version = 8.0\nin the DSN in my odbc.ini file. I had specified it elsewhere, but it needed to be here, too, apparently. Hope this helps some other poor soul."}, {"id": 32151164, "score": 5, "vote": 0, "content": "<p>Adding TDS_Version to the connection string worked for me:</p>\n<p><code>connection_string = 'DRIVER={{FreeTDS}};SERVER={server};PORT=1433;DATABASE={database};UID={uid};PWD={pwd};TDS_VERSION=8.0'</code></p>\n", "abstract": "Adding TDS_Version to the connection string worked for me: connection_string = 'DRIVER={{FreeTDS}};SERVER={server};PORT=1433;DATABASE={database};UID={uid};PWD={pwd};TDS_VERSION=8.0'"}, {"id": 9744902, "score": 4, "vote": 0, "content": "<p>Just for an extra datapoint, odbc.ini is empty on my host, and odbcinst.ini has the following lines:</p>\n<pre><code class=\"python\"># Driver from FreeTDS\n#\n[FreeTDS]\nDriver = /usr/lib64/libtdsodbc.so.0\n</code></pre>\n<p>last, the freetds.conf file has these lines:</p>\n<pre><code class=\"python\">[global]\n    host= &lt;hostname&gt;\n    port= &lt;mssql port&gt;\n    tds version = 8.0\n</code></pre>\n<p>While one can certainly specify option settings in odbc.ini, doing it this way allows the configuration options to all be managed where you'd expect them -- the freetds.conf file.</p>\n", "abstract": "Just for an extra datapoint, odbc.ini is empty on my host, and odbcinst.ini has the following lines: last, the freetds.conf file has these lines: While one can certainly specify option settings in odbc.ini, doing it this way allows the configuration options to all be managed where you'd expect them -- the freetds.conf file."}, {"id": 41047804, "score": 3, "vote": 0, "content": "<p>My problem was that on my settings file I was setting HOST to the SQL Server IP, however after hours of pulling my hair off I figured out that HOST has to be set to the Data Source Name []</p>\n", "abstract": "My problem was that on my settings file I was setting HOST to the SQL Server IP, however after hours of pulling my hair off I figured out that HOST has to be set to the Data Source Name []"}, {"id": 10652130, "score": 2, "vote": 0, "content": "<p>I was also having problems with this after upgrading my version of ubuntu to 12.04. My old freetds config <code>/etc/freetds/freetds.conf</code> was no being found so I had to move it to <code>/usr/local/etc</code> at which point it started working again.</p>\n<p>Also my driver location is <code>/usr/local/lib/libtdsodbc.so</code></p>\n<p>Hope this helps save someone a day and a half!</p>\n", "abstract": "I was also having problems with this after upgrading my version of ubuntu to 12.04. My old freetds config /etc/freetds/freetds.conf was no being found so I had to move it to /usr/local/etc at which point it started working again. Also my driver location is /usr/local/lib/libtdsodbc.so Hope this helps save someone a day and a half!"}, {"id": 48049998, "score": 1, "vote": 0, "content": "<p>You can also set an environmental variable in your python script:</p>\n<pre><code class=\"python\">os.environ['TDSVER'] = '8.0'\n</code></pre>\n", "abstract": "You can also set an environmental variable in your python script:"}, {"id": 55758919, "score": 1, "vote": 0, "content": "<p>One setting is enoug, <code>/etc/odbcinst.ini</code>:</p>\n<pre><code class=\"python\">[FreeTDS]\nDescription = FreeTDS Driver to MsSQL\nDriver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so\nUsageCount = 1\n</code></pre>\n<p>And next:</p>\n<pre><code class=\"python\">connection = pyodbc.connect(\n    'DRIVER=FreeTDS;'\n    'SERVER=&lt;host_name_or_ip&gt;;'\n    'PORT=1433;'\n    'DATABASE=&lt;database&gt;;'\n    'UID=&lt;username&gt;;'\n    'PWD=&lt;password&gt;;'\n    'TDS_VERSION=8.0;'\n)\n</code></pre>\n", "abstract": "One setting is enoug, /etc/odbcinst.ini: And next:"}, {"id": 60741788, "score": 1, "vote": 0, "content": "<p>In my case my host files are missing when i ping to that server it was not pinging. Then i noticed my host files are missing by applying sudo vi /etc/hosts command in the terminal. I added my host and ip address and worked fine for me.</p>\n", "abstract": "In my case my host files are missing when i ping to that server it was not pinging. Then i noticed my host files are missing by applying sudo vi /etc/hosts command in the terminal. I added my host and ip address and worked fine for me."}, {"id": 49243712, "score": 0, "vote": 0, "content": "<p>This worked for me, not sure but thought that it might help someone</p>\n<p>run below command to find which version of odbcinst and isql you are using</p>\n<pre><code class=\"python\"> which odbcinst\n\n which isql\n</code></pre>\n<p>Then run <code>$ odbcinst -j</code> to find which <code>odbc.ini</code> and <code>odbcinst.ini</code> is getting used.</p>\n<p>In <code>odbcinst.ini</code> add</p>\n<pre><code class=\"python\">[FreeTDS]\nDescription=FreeTDS Driver for Linux &amp; MSSQL\nDriver=/usr/local/lib/libtdsodbc.so\nSetup=/usr/local/lib/libtdsodbc.so\nUsageCount=1\n</code></pre>\n<p>And in <code>odbc.ini</code> configure your server as</p>\n<pre><code class=\"python\">[YOUR_SERVER]\nDriver = FreeTDS\nServername = &lt;YOUR_MACHINE_NAME&gt;\nDatabase = &lt;Database_You_Want_To_Connect&gt;\n</code></pre>\n<p>I found some good description at <a href=\"https://docs.snowflake.net/manuals/user-guide/odbc-linux.html#unixodbc\" rel=\"nofollow noreferrer\">https://docs.snowflake.net/manuals/user-guide/odbc-linux.html#unixodbc</a></p>\n<p>Also take a look at <a href=\"https://github.com/lionheart/django-pyodbc/wiki/Mac-setup-to-connect-to-a-MS-SQL-Server\" rel=\"nofollow noreferrer\">https://github.com/lionheart/django-pyodbc/wiki/Mac-setup-to-connect-to-a-MS-SQL-Server</a></p>\n", "abstract": "This worked for me, not sure but thought that it might help someone run below command to find which version of odbcinst and isql you are using Then run $ odbcinst -j to find which odbc.ini and odbcinst.ini is getting used. In odbcinst.ini add And in odbc.ini configure your server as I found some good description at https://docs.snowflake.net/manuals/user-guide/odbc-linux.html#unixodbc Also take a look at https://github.com/lionheart/django-pyodbc/wiki/Mac-setup-to-connect-to-a-MS-SQL-Server"}, {"id": 63562783, "score": 0, "vote": 0, "content": "<p>I found my way here after an Ubuntu 18.04 upgrade broke my pyodbc connections.  Turns out, in my //etc/odbcinst.ini file my driver description order was switched around.</p>\n<p>So when I called:</p>\n<pre><code class=\"python\">from pyodbc import connect,drivers\nconn = connect(driver=drivers()[0], ...\n</code></pre>\n<p>I should have been calling:</p>\n<pre><code class=\"python\">conn = connect(driver=drivers()[1], ...  \n</code></pre>\n<p>In other words, I was calling the wrong driver because of a simple index issue.  Hope this helps someone else.</p>\n", "abstract": "I found my way here after an Ubuntu 18.04 upgrade broke my pyodbc connections.  Turns out, in my //etc/odbcinst.ini file my driver description order was switched around. So when I called: I should have been calling: In other words, I was calling the wrong driver because of a simple index issue.  Hope this helps someone else."}, {"id": 37158113, "score": -1, "vote": 0, "content": "<p>The follow worked for me:</p>\n<p>Modify  <code>python2.7/site-packages/sql_server/pyodbc/base.py</code></p>\n<pre><code class=\"python\">def get_new_connection(self, conn_params):\n...\n-    cstr_parts['SERVERNAME'] = host\n+    cstr_parts['SERVER'] = host\n+    cstr_parts['PORT'] = str(port)\n</code></pre>\n", "abstract": "The follow worked for me: Modify  python2.7/site-packages/sql_server/pyodbc/base.py"}]}, {"link": "https://stackoverflow.com/questions/973481/dynamic-table-creation-and-orm-mapping-in-sqlalchemy", "question": {"id": "973481", "title": "Dynamic Table Creation and ORM mapping in SqlAlchemy", "content": "<p>I'm fairly new to using relational databases, so I prefer using a good ORM to simplify things.  I spent time evaluating different Python ORMs and I think SQLAlchemy is what I need.  However, I've come to a mental dead end.</p>\n<p>I need to create a new table to go along with each instance of a player I create in my app's player table. I think I know how to create the table by changing the name of the table through the metadata then calling the create function, but I have no clue on how to map it to a new dynamic class.</p>\n<p>Can someone give me some tips to help me get past my brain freeze? Is this even possible? </p>\n<p>Note: I'm open to other ORMs in Python if what I'm asking is easier to implement.Just show me how :-)</p>\n", "abstract": "I'm fairly new to using relational databases, so I prefer using a good ORM to simplify things.  I spent time evaluating different Python ORMs and I think SQLAlchemy is what I need.  However, I've come to a mental dead end. I need to create a new table to go along with each instance of a player I create in my app's player table. I think I know how to create the table by changing the name of the table through the metadata then calling the create function, but I have no clue on how to map it to a new dynamic class. Can someone give me some tips to help me get past my brain freeze? Is this even possible?  Note: I'm open to other ORMs in Python if what I'm asking is easier to implement.Just show me how :-)"}, "answers": [{"id": 973567, "score": 38, "vote": 0, "content": "<p>We are spoiled by SQLAlchemy.<br/>\nWhat follows below is taken directly from the <a href=\"https://docs.sqlalchemy.org/en/latest/orm/tutorial.html\" rel=\"noreferrer\">tutorial</a>,<br/>\nand is really easy to setup and get working.</p>\n<p>And because it is done so often,<br/>\nthe documentation <a href=\"https://github.com/sqlalchemy/sqlalchemy/commit/4abcc0da839a57513f18a7a9ea7ee6918d48e4b1#diff-968e8ba485a103281d4e8854c32110c8\" rel=\"noreferrer\">moved to full declarative</a> in Aug 2011.</p>\n<p>Setup your environment (I'm using the SQLite in-memory db to test):</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from sqlalchemy import create_engine\n&gt;&gt;&gt; engine = create_engine('sqlite:///:memory:', echo=True)\n&gt;&gt;&gt; from sqlalchemy import Table, Column, Integer, String, MetaData\n&gt;&gt;&gt; metadata = MetaData()\n</code></pre>\n<p>Define your table:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; players_table = Table('players', metadata,\n...   Column('id', Integer, primary_key=True),\n...   Column('name', String),\n...   Column('score', Integer)\n... )\n&gt;&gt;&gt; metadata.create_all(engine) # create the table\n</code></pre>\n<p>If you have logging turned on, you'll see the SQL that SQLAlchemy creates for you.</p>\n<p>Define your class:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; class Player(object):\n...     def __init__(self, name, score):\n...         self.name = name\n...         self.score = score\n...\n...     def __repr__(self):\n...        return \"&lt;Player('%s','%s')&gt;\" % (self.name, self.score)\n</code></pre>\n<p>Map the class to your table:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from sqlalchemy.orm import mapper\n&gt;&gt;&gt; mapper(Player, players_table) \n&lt;Mapper at 0x...; Player&gt;\n</code></pre>\n<p>Create a player:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; a_player = Player('monty', 0)\n&gt;&gt;&gt; a_player.name\n'monty'\n&gt;&gt;&gt; a_player.score\n0\n</code></pre>\n<p>That's it, you now have a your player table.</p>\n", "abstract": "We are spoiled by SQLAlchemy.\nWhat follows below is taken directly from the tutorial,\nand is really easy to setup and get working. And because it is done so often,\nthe documentation moved to full declarative in Aug 2011. Setup your environment (I'm using the SQLite in-memory db to test): Define your table: If you have logging turned on, you'll see the SQL that SQLAlchemy creates for you. Define your class: Map the class to your table: Create a player: That's it, you now have a your player table."}, {"id": 54844559, "score": 10, "vote": 0, "content": "<p>It's a very old question. Anyway if you prefer ORM, it's quite easy to generate table class with type:</p>\n<pre><code class=\"python\">from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\n\nBase = declarative_base()\n\nTest = type('Test', (Base,), {\n    '__tablename__': 'test',\n    'test_id': Column(Integer, primary_key=True, autoincrement=True),\n    'fldA': Column(String),  \n    ... other columns\n    }\n)\n\nBase.metadata.create_all(engine)\n\n#  passed session create with sqlalchemy\nsession.query(Test).all()\n</code></pre>\n<p>Making a class factory, it's easy to assign names to a class and database table.</p>\n", "abstract": "It's a very old question. Anyway if you prefer ORM, it's quite easy to generate table class with type: Making a class factory, it's easy to assign names to a class and database table."}, {"id": 60389478, "score": 9, "vote": 0, "content": "<p>If you are looking to create dynamic classes and tables you can use the following technique based from this tutorial URL I found here (<a href=\"http://sparrigan.github.io/sql/sqla/2016/01/03/dynamic-tables.html\" rel=\"noreferrer\">http://sparrigan.github.io/sql/sqla/2016/01/03/dynamic-tables.html</a>), I modified how he did it a bit.</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine\nengine = create_engine('sqlite:///test.db', echo=True)\nfrom sqlalchemy import Column, Integer,Float,DateTime, String, MetaData\nmetadata = MetaData()\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSession = sessionmaker(bind=engine)\nsession = Session() # create a Session\nBase = declarative_base()\n</code></pre>\n<p>First include all the needed dependencies and create your session and Base.</p>\n<p>The key to creating it dynamically is this here:</p>\n<pre><code class=\"python\">attr_dict = {'__tablename__': 'default','id': Column(Integer, primary_key=True, auto_increment=True)}\n</code></pre>\n<p>you could create a table from just this by taking advantage of the <strong>'type'</strong> function in python.</p>\n<p><strong><code>myClass = type('ClassnameHere', (Base,), attr_dict)</code></strong></p>\n<p>Note that we are passing in <strong>attr_dict</strong>, this will give the required <strong>tablename</strong> and column information to our class, but the difference is we are <strong>defining the class name through a string!</strong> This means you could create a loop for example going through an array of strings to start creating tables dynamically!</p>\n<p>Next all you have to do is simply call</p>\n<pre><code class=\"python\">Base.metadata.create_all(engine)\n</code></pre>\n<p>Because the dynamic class we created inherits from <strong>Base</strong> the command will simply create the tables!</p>\n<p>You add to this table for example like this now:</p>\n<pre><code class=\"python\">SomeRow = myClass(id='2')\nsession.add(SomeRow)\nsession.commit()\n</code></pre>\n<p>This can go even further if you you don't know the column names as well.  Just refer to the article to learn how to do that.</p>\n<p>You would essentially do something like this though:</p>\n<pre><code class=\"python\">firstColName = \"Ill_decide_later\"\nsecondColName = \"Seriously_quit_bugging_me\"\n\nnew_row_vals = myClass(**{firstColName: 14, secondColName: 33})\n</code></pre>\n<p>The ** operator takes the object and unpacks it so that firstColName and secondColName are added with assignment operators so it would essentially be the same thing as this:</p>\n<pre><code class=\"python\">new_row_vals = myClass(firstColName=14, secondColName=33)\n</code></pre>\n<p>The advantage of this technique is now you can <strong>dynamically add to the table without even having to define the column names!</strong></p>\n<p>These column names could be stored in a string array for example or whatever you wanted and you just take it from there.</p>\n", "abstract": "If you are looking to create dynamic classes and tables you can use the following technique based from this tutorial URL I found here (http://sparrigan.github.io/sql/sqla/2016/01/03/dynamic-tables.html), I modified how he did it a bit. First include all the needed dependencies and create your session and Base. The key to creating it dynamically is this here: you could create a table from just this by taking advantage of the 'type' function in python. myClass = type('ClassnameHere', (Base,), attr_dict) Note that we are passing in attr_dict, this will give the required tablename and column information to our class, but the difference is we are defining the class name through a string! This means you could create a loop for example going through an array of strings to start creating tables dynamically! Next all you have to do is simply call Because the dynamic class we created inherits from Base the command will simply create the tables! You add to this table for example like this now: This can go even further if you you don't know the column names as well.  Just refer to the article to learn how to do that. You would essentially do something like this though: The ** operator takes the object and unpacks it so that firstColName and secondColName are added with assignment operators so it would essentially be the same thing as this: The advantage of this technique is now you can dynamically add to the table without even having to define the column names! These column names could be stored in a string array for example or whatever you wanted and you just take it from there."}, {"id": 973564, "score": 5, "vote": 0, "content": "<p>Maybe look at <a href=\"https://sqlsoup.readthedocs.io\" rel=\"nofollow noreferrer\">SQLSoup</a>, which is layer over SQLAlchemy.</p>\n<p>You can also create the tables using plain SQL, and to dynamically map, use these libraries if they already don't have create table function.</p>\n<p>Or alternatively create a dynamic class and map it:</p>\n<pre><code class=\"python\">tableClass = type(str(table.fullname), (BaseTable.BaseTable,), {})\nmapper(tableClass, table)\n</code></pre>\n<p>where BaseTable can be any Python class which you want all your table classes to inherit from, e.g. such <code>Base</code> class may have some utility or common methods, e.g. basic CRUD methods:</p>\n<pre><code class=\"python\">class BaseTable(object): pass\n</code></pre>\n<p>Otherwise you need not pass any bases to <code>type(...)</code>.</p>\n", "abstract": "Maybe look at SQLSoup, which is layer over SQLAlchemy. You can also create the tables using plain SQL, and to dynamically map, use these libraries if they already don't have create table function. Or alternatively create a dynamic class and map it: where BaseTable can be any Python class which you want all your table classes to inherit from, e.g. such Base class may have some utility or common methods, e.g. basic CRUD methods: Otherwise you need not pass any bases to type(...)."}, {"id": 1052559, "score": 2, "vote": 0, "content": "<p>you can use declarative method  for dynamically creating tables in database</p>\n<pre><code class=\"python\">from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey\n\n\nBase = declarative_base()\n\nclass Language(Base):\n    __tablename__ = 'languages'\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String(20))\n    extension = Column(String(20))\n\n    def __init__(self, name, extension):\n        self.name = name\n        self.extension = extension\n</code></pre>\n", "abstract": "you can use declarative method  for dynamically creating tables in database"}, {"id": 1615689, "score": 1, "vote": 0, "content": "<p>I faced the same problem when I was trying to automate simple CRUD tasks using SQLAlchemy.\nHere is simple explanation and some code:  <a href=\"http://www.devx.com/dbzone/Article/42015\" rel=\"nofollow noreferrer\">http://www.devx.com/dbzone/Article/42015</a></p>\n", "abstract": "I faced the same problem when I was trying to automate simple CRUD tasks using SQLAlchemy.\nHere is simple explanation and some code:  http://www.devx.com/dbzone/Article/42015"}, {"id": 16454798, "score": 0, "vote": 0, "content": "<p>maybe i didn't quite understand what you want, but this recipe create identical column in different __tablename__</p>\n<pre><code class=\"python\">class TBase(object):\n    \"\"\"Base class is a 'mixin'.\n    Guidelines for declarative mixins is at:\n\n    http://www.sqlalchemy.org/docs/orm/extensions/declarative.html#mixin-classes\n\n    \"\"\"\n    id = Column(Integer, primary_key=True)\n    data = Column(String(50))\n\n    def __repr__(self):\n        return \"%s(data=%r)\" % (\n            self.__class__.__name__, self.data\n        )\n\nclass T1Foo(TBase, Base):\n    __tablename__ = 't1'\n\nclass T2Foo(TBase, Base):\n    __tablename__ = 't2'\n\nengine = create_engine('sqlite:///foo.db', echo=True)\n\nBase.metadata.create_all(engine)\n\nsess = sessionmaker(engine)()\n\nsess.add_all([T1Foo(data='t1'), T1Foo(data='t2'), T2Foo(data='t3'),\n         T1Foo(data='t4')])\n\nprint sess.query(T1Foo).all()\nprint sess.query(T2Foo).all()\nsess.commit()\n</code></pre>\n<p><a href=\"http://www.sqlalchemy.org/trac/wiki/UsageRecipes/EntityName\" rel=\"nofollow\">info in example sqlalchemy </a></p>\n", "abstract": "maybe i didn't quite understand what you want, but this recipe create identical column in different __tablename__ info in example sqlalchemy "}]}, {"link": "https://stackoverflow.com/questions/2249489/how-to-generate-data-model-from-sql-schema-in-django", "question": {"id": "2249489", "title": "How to generate data model from sql schema in Django?", "content": "<p>Our website uses a PHP front-end and a PostgreSQL database. We don't have a back-end at the moment except phpPgAdmin. The database admin has to type data into phpPgAmin manually, which is error-prone and tedious. We want to use Django to build a back-end.</p>\n<p>The database has a few dozen of tables already there. Is it possible to import the database schema into Django and create models automatically?</p>\n", "abstract": "Our website uses a PHP front-end and a PostgreSQL database. We don't have a back-end at the moment except phpPgAdmin. The database admin has to type data into phpPgAmin manually, which is error-prone and tedious. We want to use Django to build a back-end. The database has a few dozen of tables already there. Is it possible to import the database schema into Django and create models automatically?"}, "answers": [{"id": 2249559, "score": 51, "vote": 0, "content": "<p>Yes it is possible, using the <a href=\"http://docs.djangoproject.com/en/dev/ref/django-admin/#inspectdb\" rel=\"noreferrer\">inspectdb</a> command:</p>\n<pre><code class=\"python\">python manage.py inspectdb\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">python manage.py inspectdb &gt; models.py\n</code></pre>\n<p>to get them in into the file</p>\n<p>This will look at the database configured in your <code>settings.py</code> and outputs model classes to standard output.</p>\n<p>As Ignacio pointed out, there is a <a href=\"http://docs.djangoproject.com/en/dev/howto/legacy-databases/#howto-legacy-databases\" rel=\"noreferrer\">guide for your situation</a> in the documentation.</p>\n", "abstract": "Yes it is possible, using the inspectdb command: or to get them in into the file This will look at the database configured in your settings.py and outputs model classes to standard output. As Ignacio pointed out, there is a guide for your situation in the documentation."}, {"id": 2249495, "score": 2, "vote": 0, "content": "<p>If each table has an autoincrement integer PK then you can use the <a href=\"http://docs.djangoproject.com/en/dev/howto/legacy-databases/#howto-legacy-databases\" rel=\"nofollow noreferrer\">legacy database</a> instructions.</p>\n", "abstract": "If each table has an autoincrement integer PK then you can use the legacy database instructions."}]}, {"link": "https://stackoverflow.com/questions/5161164/python-in-memory-object-database-which-supports-indexing", "question": {"id": "5161164", "title": "Python: in-memory object database which supports indexing?", "content": "<p>I'm doing some data munging which would be quite a bit simpler if I could stick a bunch of dictionaries in an in-memory database, then run simply queries against it. </p>\n<p>For example, something like:</p>\n<pre><code class=\"python\">people = db([\n    {\"name\": \"Joe\", \"age\": 16},\n    {\"name\": \"Jane\", \"favourite_color\": \"red\"},\n])\nover_16 = db.filter(age__gt=16)\nwith_favorite_colors = db.filter(favorite_color__exists=True)\n</code></pre>\n<p>There are three confounding factors, though: </p>\n<ul>\n<li>Some of the values will be Python objects, and serializing them is out of the question (too slow, breaks identity). Of course, I could work around this (eg, by storing all the items in a big list, then serializing their indexes in that list\u2026 But that could take a fair bit of fiddling).</li>\n<li>There will be thousands of data, and I will be running lookup-heavy operations (like graph traversals) against them, so it <em>must</em> be possible to perform efficient (ie, indexed) queries.</li>\n<li>As in the example, the data is <em>unstructured</em>, so systems which require me to predefine a schema would be tricky.</li>\n</ul>\n<p>So, does such a thing exist? Or will I need to kludge something together?</p>\n", "abstract": "I'm doing some data munging which would be quite a bit simpler if I could stick a bunch of dictionaries in an in-memory database, then run simply queries against it.  For example, something like: There are three confounding factors, though:  So, does such a thing exist? Or will I need to kludge something together?"}, "answers": [{"id": 5161225, "score": 10, "vote": 0, "content": "<p>What about using an in-memory SQLite database via the <a href=\"http://docs.python.org/library/sqlite3.html\">sqlite3 standard library module</a>, using the special value <code>:memory:</code> for the connection? If you don't want to write your on SQL statements, you can always use an ORM, like <a href=\"http://www.sqlalchemy.org/docs/dialects/sqlite.html#connect-strings\">SQLAlchemy</a>, to access an in-memory SQLite database.</p>\n<p><strong>EDIT</strong>: I noticed you stated that the values may be Python objects, and also that you require avoiding serialization. Requiring arbitrary Python objects be stored in a database also necessitates serialization.</p>\n<p>Can I propose a practical solution if you must keep those two requirements? Why not just use Python dictionaries as indices into your collection of Python dictionaries? It sounds like you will have idiosyncratic needs for building each of your indices; figure out what values you're going to query on, then write a function to generate and index for each. The possible values for one key in your list of dicts will be the keys for an index; the values of the index will be a list of dictionaries. Query the index by giving the value you're looking for as the key.</p>\n<pre><code class=\"python\">import collections\nimport itertools\n\ndef make_indices(dicts):\n    color_index = collections.defaultdict(list)\n    age_index = collections.defaultdict(list)\n    for d in dicts:\n        if 'favorite_color' in d:\n            color_index[d['favorite_color']].append(d)\n        if 'age' in d:\n            age_index[d['age']].append(d)\n    return color_index, age_index\n\n\ndef make_data_dicts():\n    ...\n\n\ndata_dicts = make_data_dicts()\ncolor_index, age_index = make_indices(data_dicts)\n# Query for those with a favorite color is simply values\nwith_color_dicts = list(\n        itertools.chain.from_iterable(color_index.values()))\n# Query for people over 16\nover_16 = list(\n        itertools.chain.from_iterable(\n            v for k, v in age_index.items() if age &gt; 16)\n)\n</code></pre>\n", "abstract": "What about using an in-memory SQLite database via the sqlite3 standard library module, using the special value :memory: for the connection? If you don't want to write your on SQL statements, you can always use an ORM, like SQLAlchemy, to access an in-memory SQLite database. EDIT: I noticed you stated that the values may be Python objects, and also that you require avoiding serialization. Requiring arbitrary Python objects be stored in a database also necessitates serialization. Can I propose a practical solution if you must keep those two requirements? Why not just use Python dictionaries as indices into your collection of Python dictionaries? It sounds like you will have idiosyncratic needs for building each of your indices; figure out what values you're going to query on, then write a function to generate and index for each. The possible values for one key in your list of dicts will be the keys for an index; the values of the index will be a list of dictionaries. Query the index by giving the value you're looking for as the key."}, {"id": 5161655, "score": 5, "vote": 0, "content": "<p>If the in memory database solution ends up being too much work, here is a method for filtering it yourself that you may find useful.</p>\n<p>The <code>get_filter</code> function takes in arguments to define how you want to filter a dictionary, and returns a function that can be passed into the built in <code>filter</code> function to filter a list of dictionaries.</p>\n<pre><code class=\"python\">import operator\n\ndef get_filter(key, op=None, comp=None, inverse=False):\n    # This will invert the boolean returned by the function 'op' if 'inverse == True'\n    result = lambda x: not x if inverse else x\n    if op is None:\n        # Without any function, just see if the key is in the dictionary\n        return lambda d: result(key in d)\n\n    if comp is None:\n        # If 'comp' is None, assume the function takes one argument\n        return lambda d: result(op(d[key])) if key in d else False\n\n    # Use 'comp' as the second argument to the function provided\n    return lambda d: result(op(d[key], comp)) if key in d else False\n\npeople = [{'age': 16, 'name': 'Joe'}, {'name': 'Jane', 'favourite_color': 'red'}]\n\nprint filter(get_filter(\"age\", operator.gt, 15), people)\n# [{'age': 16, 'name': 'Joe'}]\nprint filter(get_filter(\"name\", operator.eq, \"Jane\"), people)\n# [{'name': 'Jane', 'favourite_color': 'red'}]\nprint filter(get_filter(\"favourite_color\", inverse=True), people)\n# [{'age': 16, 'name': 'Joe'}]\n</code></pre>\n<p>This is pretty easily extensible to more complex filtering, for example to filter based on whether or not a value is matched by a regex:</p>\n<pre><code class=\"python\">p = re.compile(\"[aeiou]{2}\") # matches two lowercase vowels in a row\nprint filter(get_filter(\"name\", p.search), people)\n# [{'age': 16, 'name': 'Joe'}]\n</code></pre>\n", "abstract": "If the in memory database solution ends up being too much work, here is a method for filtering it yourself that you may find useful. The get_filter function takes in arguments to define how you want to filter a dictionary, and returns a function that can be passed into the built in filter function to filter a list of dictionaries. This is pretty easily extensible to more complex filtering, for example to filter based on whether or not a value is matched by a regex:"}, {"id": 5161883, "score": 5, "vote": 0, "content": "<p>The only solution I know is a package I stumbled across a few years ago on PyPI, <a href=\"http://www.pydblite.net/en/PyDbLite.html\" rel=\"noreferrer\">PyDbLite</a>.  It's okay, but there are few issues:</p>\n<ol>\n<li>It still wants to serialize everything to disk, as a pickle file.  But that was simple enough for me to rip out.  (It's also unnecessary.  If the objects inserted are serializable, so is the collection as a whole.)</li>\n<li>The basic record type is a dictionary, into which it inserts its own metadata, two ints under keys <code>__id__</code> and <code>__version__</code>.</li>\n<li>The indexing is very simple, based only on value of the record dictionary.  If you want something more complicated, like based on a the attribute of a object in the record, you'll have to code it yourself.  (Something I've meant to do myself, but never got around to.)</li>\n</ol>\n<p>The author does seem to be working on it occasionally.  There's some new features from when I used it, including some nice syntax for complex queries.</p>\n<p>Assuming you rip out the pickling (and I can tell you what I did), your example would be (untested code):</p>\n<pre><code class=\"python\">from PyDbLite import Base\n\ndb = Base()\ndb.create(\"name\", \"age\", \"favourite_color\")\n\n# You can insert records as either named parameters\n# or in the order of the fields\ndb.insert(name=\"Joe\", age=16, favourite_color=None)\ndb.insert(\"Jane\", None, \"red\")\n\n# These should return an object you can iterate over\n# to get the matching records.  These are unindexed queries.\n#\n# The first might throw because of the None in the second record\nover_16 = db(\"age\") &gt; 16\nwith_favourite_colors = db(\"favourite_color\") != None\n\n# Or you can make an index for faster queries\ndb.create_index(\"favourite_color\")\nwith_favourite_color_red = db._favourite_color[\"red\"]\n</code></pre>\n<p>Hopefully it will be enough to get you started.</p>\n", "abstract": "The only solution I know is a package I stumbled across a few years ago on PyPI, PyDbLite.  It's okay, but there are few issues: The author does seem to be working on it occasionally.  There's some new features from when I used it, including some nice syntax for complex queries. Assuming you rip out the pickling (and I can tell you what I did), your example would be (untested code): Hopefully it will be enough to get you started."}, {"id": 5162218, "score": 3, "vote": 0, "content": "<p>As far as \"identity\" anything that is hashable you should be able to compare, to keep track of object identity.</p>\n<p>Zope Object Database (ZODB):\n<a href=\"http://www.zodb.org/\" rel=\"nofollow\">http://www.zodb.org/</a></p>\n<p>PyTables works well:\n<a href=\"http://www.pytables.org/moin\" rel=\"nofollow\">http://www.pytables.org/moin</a></p>\n<p>Also Metakit for Python works well: \n<a href=\"http://equi4.com/metakit/python.html\" rel=\"nofollow\">http://equi4.com/metakit/python.html</a><br/>\n<code>supports columns, and sub-columns but not unstructured data</code></p>\n<p>Research \"Stream Processing\", if your data sets are extremely large this may be useful:\n<a href=\"http://www.trinhhaianh.com/stream.py/\" rel=\"nofollow\">http://www.trinhhaianh.com/stream.py/</a></p>\n<p>Any in-memory database, that can be serialized (written to disk) is going to have your identity problem.  I would suggest representing the data you want to store as native types (list, dict) instead of objects if at all possible.</p>\n<p>Keep in mind NumPy was designed to perform complex operations on in-memory data structures, and could possibly be apart of your solution if you decide to roll your own.</p>\n", "abstract": "As far as \"identity\" anything that is hashable you should be able to compare, to keep track of object identity. Zope Object Database (ZODB):\nhttp://www.zodb.org/ PyTables works well:\nhttp://www.pytables.org/moin Also Metakit for Python works well: \nhttp://equi4.com/metakit/python.html\nsupports columns, and sub-columns but not unstructured data Research \"Stream Processing\", if your data sets are extremely large this may be useful:\nhttp://www.trinhhaianh.com/stream.py/ Any in-memory database, that can be serialized (written to disk) is going to have your identity problem.  I would suggest representing the data you want to store as native types (list, dict) instead of objects if at all possible. Keep in mind NumPy was designed to perform complex operations on in-memory data structures, and could possibly be apart of your solution if you decide to roll your own."}, {"id": 13356491, "score": 2, "vote": 0, "content": "<p>I wrote a simple module called <a href=\"http://code.dealmeida.net/jsonstore\" rel=\"nofollow\">Jsonstore</a> that solves (2) and (3). Here's how your example would go:</p>\n<pre><code class=\"python\">from jsonstore import EntryManager\nfrom jsonstore.operators import GreaterThan, Exists\n\ndb = EntryManager(':memory:')\ndb.create(name='Joe', age=16)\ndb.create({'name': 'Jane', 'favourite_color': 'red'})  # alternative syntax\n\ndb.search({'age': GreaterThan(16)})\ndb.search(favourite_color=Exists())  # again, 2 different syntaxes\n</code></pre>\n", "abstract": "I wrote a simple module called Jsonstore that solves (2) and (3). Here's how your example would go:"}, {"id": 42352586, "score": 1, "vote": 0, "content": "<p>Not sure if it complies with all your requirements, but TinyDB (using in-memory storage) is also probably worth the try:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from tinydb import TinyDB, Query\n&gt;&gt;&gt; from tinydb.storages import MemoryStorage\n&gt;&gt;&gt; db = TinyDB(storage=MemoryStorage)\n&gt;&gt;&gt; db.insert({'name': 'John', 'age': 22})\n&gt;&gt;&gt; User = Query()\n&gt;&gt;&gt; db.search(User.name == 'John')\n[{'name': 'John', 'age': 22}]\n</code></pre>\n<p>Its simplicity and powerful query engine makes it a very interesting tool for some use cases. See <a href=\"http://tinydb.readthedocs.io/\" rel=\"nofollow noreferrer\">http://tinydb.readthedocs.io/</a> for more details.</p>\n", "abstract": "Not sure if it complies with all your requirements, but TinyDB (using in-memory storage) is also probably worth the try: Its simplicity and powerful query engine makes it a very interesting tool for some use cases. See http://tinydb.readthedocs.io/ for more details."}, {"id": 5161231, "score": 0, "vote": 0, "content": "<p>If you are willing to work around serializing, MongoDB could work for you. PyMongo provides an interface almost identical to what you describe. If you decide to serialize, the hit won't be as bad since Mongodb is memory mapped. </p>\n", "abstract": "If you are willing to work around serializing, MongoDB could work for you. PyMongo provides an interface almost identical to what you describe. If you decide to serialize, the hit won't be as bad since Mongodb is memory mapped. "}, {"id": 5161451, "score": 0, "vote": 0, "content": "<p>It should be possible to do what you are wanting to do with just isinstance(), hasattr(), getattr() and setattr().</p>\n<p>However, things are going to get fairly complicated before you are done! </p>\n<p>I suppose one could store all the objects in a big list, then run a query on each object, determining what it is and looking for a given attribute or value, then return the value and the object as a list of tuples. Then you could sort on your return values pretty easily. copy.deepcopy will be your best friend and your worst enemy. </p>\n<p>Sounds like fun! Good luck!</p>\n", "abstract": "It should be possible to do what you are wanting to do with just isinstance(), hasattr(), getattr() and setattr(). However, things are going to get fairly complicated before you are done!  I suppose one could store all the objects in a big list, then run a query on each object, determining what it is and looking for a given attribute or value, then return the value and the object as a list of tuples. Then you could sort on your return values pretty easily. copy.deepcopy will be your best friend and your worst enemy.  Sounds like fun! Good luck!"}, {"id": 20426664, "score": 0, "vote": 0, "content": "<p>I started developing one yesterday and it isn't published yet. It indexes your objects and allows you to run fast queries. All data is kept in RAM and I'm thinking about smart load and save methods. For testing purposes it is loading and saving through cPickle.</p>\n<p>Let me know if you are still interested.</p>\n", "abstract": "I started developing one yesterday and it isn't published yet. It indexes your objects and allows you to run fast queries. All data is kept in RAM and I'm thinking about smart load and save methods. For testing purposes it is loading and saving through cPickle. Let me know if you are still interested."}, {"id": 72964102, "score": 0, "vote": 0, "content": "<p><a href=\"https://ducks.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">ducks</a> is exactly what you are describing.</p>\n<ul>\n<li>It builds indexes on Python objects</li>\n<li>It does not serialize or persist anything</li>\n<li>Missing attributes are handled correctly</li>\n<li>It uses C libraries so it's very fast and RAM-efficient</li>\n</ul>\n<p><code>pip install ducks</code></p>\n<pre><code class=\"python\">from ducks import Dex, ANY\n\nobjects = [\n    {\"name\": \"Joe\", \"age\": 16},\n    {\"name\": \"Jane\", \"favourite_color\": \"red\"},\n]\n\n\n# Build the index\ndex = Dex(objects, ['name', 'age', 'favourite_color'])\n\n# Look up by any combination of attributes\ndex[{'age': {'&gt;=': 16}}]  # Returns Joe\n\n# Match the special value ANY to find all objects with the attribute\ndex[{'favourite_color': ANY}] # Returns Jane\n\n</code></pre>\n<p>This example uses dicts, but ducks works on any object type.</p>\n", "abstract": "ducks is exactly what you are describing. pip install ducks This example uses dicts, but ducks works on any object type."}]}, {"link": "https://stackoverflow.com/questions/1961013/are-there-any-tools-for-schema-migration-for-nosql-databases", "question": {"id": "1961013", "title": "Are there any tools for schema migration for NoSQL databases?", "content": "<p>I'm looking a way to automate schema migration for such databases like MongoDB or CouchDB.</p>\n<p>Preferably, this instument should be written in python, but any other language is ok.</p>\n", "abstract": "I'm looking a way to automate schema migration for such databases like MongoDB or CouchDB. Preferably, this instument should be written in python, but any other language is ok."}, "answers": [{"id": 3007620, "score": 19, "vote": 0, "content": "<p>Since a nosql database can contain huge amounts of data you can not migrate it in the regular rdbms sence. Actually you can't do it for rdbms as well as soon as your data passes some size threshold. It is impractical to bring your site down for a day to add a field to an existing table, and so with rdbms you end up doing ugly patches like adding new tables just for the field and doing joins to get to the data.\nIn nosql world you can do several things.</p>\n<ul>\n<li>As others suggested you can write your code so that it will handle different 'versions' of the possible schema. this is usually simpler then it looks. Many kinds of schema changes are trivial to code around. for example if you want to add a new field to the schema, you just add it to all new records and it will be empty on the all old records (you will not get \"field doesn't exist\" errors or anything ;). if you need a 'default' value for the field in the old records it is too trivially done in code.</li>\n<li>Another option and actually the only sane option going forward with non-trivial schema changes like field renames and structural changes is to store schema_version in EACH record, and to have code to migrate data from any version to the next on <em>READ</em>. i.e. if your current schema version is 10 and you read a record from the database with the version of 7, then your db layer should call migrate_8, migrate_9, and migrate_10. This way the data that is accessed will be gradually migrated to the new version. and if it is not accessed, then who cares which version is it;)</li>\n</ul>\n", "abstract": "Since a nosql database can contain huge amounts of data you can not migrate it in the regular rdbms sence. Actually you can't do it for rdbms as well as soon as your data passes some size threshold. It is impractical to bring your site down for a day to add a field to an existing table, and so with rdbms you end up doing ugly patches like adding new tables just for the field and doing joins to get to the data.\nIn nosql world you can do several things."}, {"id": 1961090, "score": 2, "vote": 0, "content": "<p>One of the supposed benefits of these databases is that they are schemaless, and therefore don't need schema migration tools.  Instead, you write your data handling code to deal with the variety of data stored in the db.</p>\n", "abstract": "One of the supposed benefits of these databases is that they are schemaless, and therefore don't need schema migration tools.  Instead, you write your data handling code to deal with the variety of data stored in the db."}, {"id": 1966375, "score": 2, "vote": 0, "content": "<p>If your data are sufficiently big, you will probably find that you cannot <em>EVER</em> migrate the data, or that it is not beneficial to do so. This means that when you do a schema change, the code needs to continue to be backwards compatible with the old formats forever.</p>\n<p>Of course if your data \"age\" and eventually expire anyway, this can do schema migration for you - simply change the format for newly added data, then wait for all data in the old format to expire - you can then retire the backward-compatibility code.</p>\n", "abstract": "If your data are sufficiently big, you will probably find that you cannot EVER migrate the data, or that it is not beneficial to do so. This means that when you do a schema change, the code needs to continue to be backwards compatible with the old formats forever. Of course if your data \"age\" and eventually expire anyway, this can do schema migration for you - simply change the format for newly added data, then wait for all data in the old format to expire - you can then retire the backward-compatibility code."}, {"id": 3007685, "score": 1, "vote": 0, "content": "<p>When a project has a need for a schema migration in regards to a NoSQL database makes me think that you are still thinking in a Relational database manner, but using a NoSQL database.</p>\n<p>If anybody is going to start working with NoSQL databases, you need to realize that most of the 'rules' for a RDBMS (i.e. MySQL) need to go out the window too.  Things like strict schemas, normalization, using many relationships between objects.  NoSQL exists to solve problems that don't need all the extra 'features' provided by a RDBMS.</p>\n<p>I would urge you to write your code in a manner that doesn't expect or need a hard schema for your NoSQL database - you should support an old schema and convert a document record on the fly when you access if if you really want more schema fields on that record.</p>\n<p><strong>Please keep in mind that NoSQL storage works best when you think and design differently compared to when using a RDBMS</strong></p>\n", "abstract": "When a project has a need for a schema migration in regards to a NoSQL database makes me think that you are still thinking in a Relational database manner, but using a NoSQL database. If anybody is going to start working with NoSQL databases, you need to realize that most of the 'rules' for a RDBMS (i.e. MySQL) need to go out the window too.  Things like strict schemas, normalization, using many relationships between objects.  NoSQL exists to solve problems that don't need all the extra 'features' provided by a RDBMS. I would urge you to write your code in a manner that doesn't expect or need a hard schema for your NoSQL database - you should support an old schema and convert a document record on the fly when you access if if you really want more schema fields on that record. Please keep in mind that NoSQL storage works best when you think and design differently compared to when using a RDBMS"}]}, {"link": "https://stackoverflow.com/questions/2580497/database-on-the-fly-with-scripting-languages", "question": {"id": "2580497", "title": "Database on the fly with scripting languages", "content": "<p>I have a set of .csv files that I want to process. It would be far easier to process it with SQL queries. I wonder if there is some way to load a .csv file and use SQL language to look into it with a scripting language like python or ruby. Loading it with something similar to ActiveRecord would be awesome. </p>\n<p>The problem is that I don't want to have to run a database somewhere prior to running my script. I souldn't have additionnal installations needed outside of the scripting language and some modules.</p>\n<p>My question is which language and what modules should I use for this task. I looked around and can't find anything that suits my need. Is it even possible?</p>\n", "abstract": "I have a set of .csv files that I want to process. It would be far easier to process it with SQL queries. I wonder if there is some way to load a .csv file and use SQL language to look into it with a scripting language like python or ruby. Loading it with something similar to ActiveRecord would be awesome.  The problem is that I don't want to have to run a database somewhere prior to running my script. I souldn't have additionnal installations needed outside of the scripting language and some modules. My question is which language and what modules should I use for this task. I looked around and can't find anything that suits my need. Is it even possible?"}, "answers": [{"id": 2580543, "score": 67, "vote": 0, "content": "<p>There's <a href=\"http://docs.python.org/library/sqlite3\" rel=\"noreferrer\"><code>sqlite3</code></a>, included into python. With it you can create a database (<strong>on memory</strong>) and add rows to it, and perform SQL queries.</p>\n<p>If you want neat ActiveRecord-like functionality you should add an external ORM, like <a href=\"http://sqlalchemy.org\" rel=\"noreferrer\">sqlalchemy</a>. That's a separate download though</p>\n<p>Quick example using sqlalchemy:</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine, Column, String, Integer, MetaData, Table\nfrom sqlalchemy.orm import mapper, create_session\nimport csv\nCSV_FILE = 'foo.csv'\nengine = create_engine('sqlite://') # memory-only database\n\ntable = None\nmetadata = MetaData(bind=engine)\nwith open(CSV_FILE) as f:\n    # assume first line is header\n    cf = csv.DictReader(f, delimiter=',')\n    for row in cf:\n        if table is None:\n            # create the table\n            table = Table('foo', metadata, \n                Column('id', Integer, primary_key=True),\n                *(Column(rowname, String()) for rowname in row.keys()))\n            table.create()\n        # insert data into the table\n        table.insert().values(**row).execute()\n\nclass CsvTable(object): pass\nmapper(CsvTable, table)\nsession = create_session(bind=engine, autocommit=False, autoflush=True)\n</code></pre>\n<p>Now you can query the database, filtering by any field, etc.</p>\n<p>Suppose you run the code above on this csv:</p>\n<pre><code class=\"python\">name,age,nickname\nnosklo,32,nosklo\nAfila Tun,32,afilatun\nFoo Bar,33,baz\n</code></pre>\n<p>That will create and populate a table in memory with fields <code>name</code>, <code>age</code>, <code>nickname</code>. You can then query the table:</p>\n<pre><code class=\"python\">for r in session.query(CsvTable).filter(CsvTable.age == '32'):\n    print r.name, r.age, r.nickname\n</code></pre>\n<p>That will automatically create and run a <code>SELECT</code> query and return the correct rows.</p>\n<p>Another advantage of using sqlalchemy is that, if you decide to use another, more powerful database in the future, you can do so pratically without changing the code.</p>\n", "abstract": "There's sqlite3, included into python. With it you can create a database (on memory) and add rows to it, and perform SQL queries. If you want neat ActiveRecord-like functionality you should add an external ORM, like sqlalchemy. That's a separate download though Quick example using sqlalchemy: Now you can query the database, filtering by any field, etc. Suppose you run the code above on this csv: That will create and populate a table in memory with fields name, age, nickname. You can then query the table: That will automatically create and run a SELECT query and return the correct rows. Another advantage of using sqlalchemy is that, if you decide to use another, more powerful database in the future, you can do so pratically without changing the code."}, {"id": 2580540, "score": 4, "vote": 0, "content": "<p>Use a DB in a library like <a href=\"http://sqlite.org/\" rel=\"nofollow noreferrer\">SQLite</a>. \nThere are <a href=\"http://docs.python.org/library/sqlite3.html\" rel=\"nofollow noreferrer\">Python</a> and <a href=\"http://sqlite-ruby.rubyforge.org/\" rel=\"nofollow noreferrer\">Ruby</a> versions .</p>\n<p>Load your CSV into table, there might be modules/libraries to help you here too. Then SQL away.</p>\n", "abstract": "Use a DB in a library like SQLite. \nThere are Python and Ruby versions . Load your CSV into table, there might be modules/libraries to help you here too. Then SQL away."}, {"id": 2580548, "score": 4, "vote": 0, "content": "<p>Looked at Perl and and Text::CSV and DBI? There are many modules on CPAN to do exactly this. Here is an example (from <a href=\"http://perlmeme.org/tutorials/parsing_csv.html\" rel=\"nofollow noreferrer\">HERE</a>):</p>\n<pre><code class=\"python\">#!/usr/bin/perl\nuse strict;\nuse warnings;\nuse DBI;\n\n# Connect to the database, (the directory containing our csv file(s))\n\nmy $dbh = DBI-&gt;connect(\"DBI:CSV:f_dir=.;csv_eol=\\n;\");\n\n# Associate our csv file with the table name 'prospects'\n\n$dbh-&gt;{'csv_tables'}-&gt;{'prospects'} = { 'file' =&gt; 'prospects.csv'};\n\n# Output the name and contact field from each row\n\nmy $sth = $dbh-&gt;prepare(\"SELECT * FROM prospects WHERE name LIKE 'G%'\");\n$sth-&gt;execute();\nwhile (my $row = $sth-&gt;fetchrow_hashref) {\n     print(\"name = \", $row-&gt;{'Name'}, \"  contact = \", $row-&gt;{'Contact'}. \"\\n\");\n}\n$sth-&gt;finish();\n\nname = Glenhuntly Pharmacy  contact = Paul\nname = Gilmour's Shoes  contact = Ringo\n</code></pre>\n<p>Just type perldoc DBI   and perldoc Text::CSV at the command prompt for more.</p>\n", "abstract": "Looked at Perl and and Text::CSV and DBI? There are many modules on CPAN to do exactly this. Here is an example (from HERE): Just type perldoc DBI   and perldoc Text::CSV at the command prompt for more."}, {"id": 2580542, "score": 3, "vote": 0, "content": "<p>CSV files are not databases--they have no indices--and any SQL simulation you imposed upon them would amount to little more than searching through the entire thing over and over again. </p>\n", "abstract": "CSV files are not databases--they have no indices--and any SQL simulation you imposed upon them would amount to little more than searching through the entire thing over and over again. "}, {"id": 2580549, "score": 3, "vote": 0, "content": "<p>You could use either scripting language to parse the CSV file and store the data into <a href=\"http://sqlite.org/\" rel=\"nofollow noreferrer\">SQLite</a>, which just uses a single file for storage. From there you have it in a database and can run queries against it.</p>\n<p>Alternatively, on windows you can setup an ODBC data source as a CSV file. But it may be difficult to automate this.</p>\n", "abstract": "You could use either scripting language to parse the CSV file and store the data into SQLite, which just uses a single file for storage. From there you have it in a database and can run queries against it. Alternatively, on windows you can setup an ODBC data source as a CSV file. But it may be difficult to automate this."}, {"id": 9156660, "score": 2, "vote": 0, "content": "<p>I used nosklo's solution (thanks!) but I already had a primary key (passed in as pk_col) within the column line (first line of csv). So I thought I'd share my modification. I used a ternary. </p>\n<pre><code class=\"python\">table = Table(tablename, metadata,\n    *((Column(pk_col, Integer, primary_key=True)) if rowname == pk_col else (Column(rowname, String())) for rowname in row.keys()))\ntable.create()\n</code></pre>\n", "abstract": "I used nosklo's solution (thanks!) but I already had a primary key (passed in as pk_col) within the column line (first line of csv). So I thought I'd share my modification. I used a ternary. "}, {"id": 2580913, "score": 1, "vote": 0, "content": "<p>PHP FlatfileDB available <a href=\"http://sourceforge.net/projects/flatfiledb/\" rel=\"nofollow noreferrer\">here</a> is a very good option if you are building a web app</p>\n", "abstract": "PHP FlatfileDB available here is a very good option if you are building a web app"}, {"id": 72688593, "score": 0, "vote": 0, "content": "<p>To put the main thing of some of the other answers a bit clearer, you can create columns dynamically by adding them to a list and then doing</p>\n<pre><code class=\"python\">table = Table(\n    table_name,\n    meta,\n    *columns\n)\n</code></pre>\n<p>But this is perhaps a python thing more than a sqlalchemy thing, to realize you can unpack the list into function arguments with <code>*</code>.</p>\n", "abstract": "To put the main thing of some of the other answers a bit clearer, you can create columns dynamically by adding them to a list and then doing But this is perhaps a python thing more than a sqlalchemy thing, to realize you can unpack the list into function arguments with *."}]}, {"link": "https://stackoverflow.com/questions/10784254/non-blocking-orm-for-tornado", "question": {"id": "10784254", "title": "Non-blocking ORM for Tornado?", "content": "<p>Is there any asynchronous Python ORM other than <a href=\"http://findingscience.com/twistar/\">Twistar</a>? </p>\n<p>I'm looking for lightweight ORM for non-blocking API, built on top of tornado. Of course, I can write raw SQL queries using momoko, but I'd like to work with objects. </p>\n", "abstract": "Is there any asynchronous Python ORM other than Twistar?  I'm looking for lightweight ORM for non-blocking API, built on top of tornado. Of course, I can write raw SQL queries using momoko, but I'd like to work with objects. "}, "answers": [{"id": 26663553, "score": 15, "vote": 0, "content": "<p>Sure, it is! Look at <a href=\"https://github.com/coleifer/peewee\" rel=\"noreferrer\">peewee</a> and <a href=\"https://github.com/05bit/peewee-async\" rel=\"noreferrer\">peewee-async</a> extension. Disclaimer: extension is only for PostgreSQL at the moment and I'm an author of extension :)</p>\n<p>It's not specifically for Tornado, but Tornado can run on asyncio event loop.</p>\n", "abstract": "Sure, it is! Look at peewee and peewee-async extension. Disclaimer: extension is only for PostgreSQL at the moment and I'm an author of extension :) It's not specifically for Tornado, but Tornado can run on asyncio event loop."}, {"id": 45646324, "score": 5, "vote": 0, "content": "<p>It's been 5 years, and a lot changed. We wrote <a href=\"https://github.com/fantix/gino\" rel=\"noreferrer\">GINO</a> to be a lightweight ORM on top of <a href=\"https://magicstack.github.io/asyncpg/current/\" rel=\"noreferrer\">asyncpg</a> and <a href=\"https://docs.sqlalchemy.org/en/rel_1_1/#sqlalchemy-core\" rel=\"noreferrer\">SQLAlchemy core</a>. It is for asyncio and PostgreSQL only. GINO as \"GINO Is Not ORM\", because it applied almost none usual ORM patterns, in order to be explicit and simple.</p>\n", "abstract": "It's been 5 years, and a lot changed. We wrote GINO to be a lightweight ORM on top of asyncpg and SQLAlchemy core. It is for asyncio and PostgreSQL only. GINO as \"GINO Is Not ORM\", because it applied almost none usual ORM patterns, in order to be explicit and simple."}, {"id": 53479387, "score": 5, "vote": 0, "content": "<p>Have a look at Tortoise <a href=\"https://github.com/tortoise/tortoise-orm\" rel=\"noreferrer\">ORM</a></p>\n<p>Its aiming to be a full-featured <code>ORM</code> inspired by <code>Django syntax</code>, but asycnio only.\nSince <code>Tornado 5.0</code> runs on asyncio, it should just work.</p>\n", "abstract": "Have a look at Tortoise ORM Its aiming to be a full-featured ORM inspired by Django syntax, but asycnio only.\nSince Tornado 5.0 runs on asyncio, it should just work."}, {"id": 10857990, "score": 2, "vote": 0, "content": "<p>None exist. The only ORM that could even consider coming close to being lightweight is <a href=\"https://github.com/coleifer/peewee\" rel=\"nofollow\">PeeWee</a>, and that isn't async. ORMs are hard to write, and even harder to write <strong>well</strong>. It needs to have a nice, clean API, expose many features of the underlying DB, <em>and</em> be efficient. A tall order!</p>\n<p>There aren't many ORMs for Python, and even fewer async ones. Sorry.</p>\n", "abstract": "None exist. The only ORM that could even consider coming close to being lightweight is PeeWee, and that isn't async. ORMs are hard to write, and even harder to write well. It needs to have a nice, clean API, expose many features of the underlying DB, and be efficient. A tall order! There aren't many ORMs for Python, and even fewer async ones. Sorry."}, {"id": 10920314, "score": 1, "vote": 0, "content": "<p>If using mongo you can look into Asyncmongo (not an orm but let's you access your data Async)\n<a href=\"https://github.com/bitly/asyncmongo\" rel=\"nofollow\">https://github.com/bitly/asyncmongo</a></p>\n<p>if it is of interest see video and slides from this webminar: \"Asynchronous MongoDB with Python and Tornado\" <a href=\"http://www.10gen.com/presentations/webinar/Asynchronous-MongoDB-with-Python-and-Tornado\" rel=\"nofollow\">http://www.10gen.com/presentations/webinar/Asynchronous-MongoDB-with-Python-and-Tornado</a></p>\n", "abstract": "If using mongo you can look into Asyncmongo (not an orm but let's you access your data Async)\nhttps://github.com/bitly/asyncmongo if it is of interest see video and slides from this webminar: \"Asynchronous MongoDB with Python and Tornado\" http://www.10gen.com/presentations/webinar/Asynchronous-MongoDB-with-Python-and-Tornado"}, {"id": 21486806, "score": 1, "vote": 0, "content": "<p>You may want to have a look at <a href=\"https://github.com/shiyanhui/monguo\" rel=\"nofollow\">Monguo</a>, a \"full-featured, asynchronous MongoDB ORM with Motor driver for Tornado applications\" as it describes itself.</p>\n", "abstract": "You may want to have a look at Monguo, a \"full-featured, asynchronous MongoDB ORM with Motor driver for Tornado applications\" as it describes itself."}, {"id": 28000155, "score": 0, "vote": 0, "content": "<p>You may want to have a look at umysqldb(<a href=\"https://github.com/hongqn/umysqldb\" rel=\"nofollow\">https://github.com/hongqn/umysqldb</a>), A MySQLdb compatible wrapper around ultramysql. ultramysql compatible with gevent through monkey patching.</p>\n", "abstract": "You may want to have a look at umysqldb(https://github.com/hongqn/umysqldb), A MySQLdb compatible wrapper around ultramysql. ultramysql compatible with gevent through monkey patching."}]}, {"link": "https://stackoverflow.com/questions/18208492/sqlalchemy-exc-operationalerror-operationalerror-unable-to-open-database-file", "question": {"id": "18208492", "title": "sqlalchemy.exc.OperationalError: (OperationalError) unable to open database file None None", "content": "<p>I am running a program from another person who are inconvenience ask for help from. The program is a website. Server end is written by python and flask (module, <a href=\"http://flask.pocoo.org/\" rel=\"noreferrer\">http://flask.pocoo.org/</a>). The program has been successfully run on the server. What I need to do is modify something on it. Since the production server is not allowed for test, I tested it in development server locally via flask. However, I could not run even the original program. Below is from python.</p>\n<pre><code class=\"python\">(venv)kevin@ubuntu:~/python/public_html$ python index.wsgi \n</code></pre>\n<blockquote>\n<p>Traceback (most recent call last):\n      File \"index.wsgi\", line 6, in \n      from app import app as application</p>\n</blockquote>\n<pre><code class=\"python\">File \"/home/kevin/python/public_html/app.py\", line 27, in &lt;module&gt;\napp = create_app()\n\nFile \"/home/kevin/python/public_html/app.py\", line 12, in create_app\ndatabase.init_db()\n\nFile \"/home/kevin/python/public_html/database.py\", line 24, in init_db\nBase.metadata.create_all(engine)\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/schema.py\", line 2793, in create_all\n  tables=tables)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1478, in _run_visitor\nwith self._optional_conn_ctx_manager(connection) as conn:\n\nFile \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\nreturn self.gen.next()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1471, in _optional_conn_ctx_manager\nwith self.contextual_connect() as conn:\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1661, in contextual_connect\nself.pool.connect(),\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 272, in connect\nreturn _ConnectionFairy(self).checkout()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 425, in __init__\nrec = self._connection_record = pool._do_get()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 857, in _do_get\nreturn self._create_connection()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 225, in _create_connection\nreturn _ConnectionRecord(self)\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 318, in __init__\nself.connection = self.__connect()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/pool.py\", line 368, in __connect\nconnection = self.__pool._creator()\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/strategies.py\", line 80, in connect\nreturn dialect.connect(*cargs, **cparams)\n\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py\", line 283, in connect\nreturn self.dbapi.connect(*cargs, **cparams)\n\nsqlalchemy.exc.OperationalError: (OperationalError) unable to open database file None None\n</code></pre>\n<p>In the config.py file</p>\n<blockquote>\n<p>LOGFILE = '/tmp/ate.log'\n      DEBUG = True\n      TESTING = True\n      THREADED = True\n      DATABASE_URI = 'sqlite:////tmp/ate.db'\n      SECRET_KEY = os.urandom(24)</p>\n</blockquote>\n<p>Hence, I created a folder called \"tmp\" under my user and an empty file called \"ate.db\". Then, ran it again. It said</p>\n<blockquote>\n<p>IOError: [Errno 2] No such file or directory: '/home/kevin/log/ate.log'</p>\n</blockquote>\n<p>Then, I created the log folder and the log file. Run it, but nothing happened like</p>\n<blockquote>\n<p>(venv)kevin@ubuntu:~/python/public_html$ python index.wsgi \n  (venv)kevin@ubuntu:~/python/public_html$ python index.wsgi \n  (venv)kevin@ubuntu:~/python/public_html$ </p>\n</blockquote>\n<p>If it is successful, the website should be available on <code>http://127.0.0.1:5000</code>/. However, it did not work. Does anybody know why and how to solve it? The codes should be fine since it is now available online. The problem should be a local problem. Thank you so much for your help. </p>\n<p>The code of where the program is stuck</p>\n<pre><code class=\"python\">from sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\nengine = None\ndb_session = None\nBase = declarative_base()\n\n\ndef init_engine(uri, **kwards):\n    global engine\n    engine = create_engine(uri, **kwards)\n    return engine\n\n\ndef init_db():\n    global db_session\n    db_session = scoped_session(sessionmaker(bind=engine))\n    # import all modules here that might define models so that\n    # they will be registered properly on the metadata.  Otherwise\n    # you will have to import them first before calling init_db()\n    import models\n    Base.metadata.create_all(engine)\n</code></pre>\n", "abstract": "I am running a program from another person who are inconvenience ask for help from. The program is a website. Server end is written by python and flask (module, http://flask.pocoo.org/). The program has been successfully run on the server. What I need to do is modify something on it. Since the production server is not allowed for test, I tested it in development server locally via flask. However, I could not run even the original program. Below is from python. Traceback (most recent call last):\n      File \"index.wsgi\", line 6, in \n      from app import app as application In the config.py file LOGFILE = '/tmp/ate.log'\n      DEBUG = True\n      TESTING = True\n      THREADED = True\n      DATABASE_URI = 'sqlite:////tmp/ate.db'\n      SECRET_KEY = os.urandom(24) Hence, I created a folder called \"tmp\" under my user and an empty file called \"ate.db\". Then, ran it again. It said IOError: [Errno 2] No such file or directory: '/home/kevin/log/ate.log' Then, I created the log folder and the log file. Run it, but nothing happened like (venv)kevin@ubuntu:~/python/public_html$ python index.wsgi \n  (venv)kevin@ubuntu:~/python/public_html$ python index.wsgi \n  (venv)kevin@ubuntu:~/python/public_html$  If it is successful, the website should be available on http://127.0.0.1:5000/. However, it did not work. Does anybody know why and how to solve it? The codes should be fine since it is now available online. The problem should be a local problem. Thank you so much for your help.  The code of where the program is stuck"}, "answers": [{"id": 44687471, "score": 46, "vote": 0, "content": "<p>Replace:</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////dbdir/test.db'\n</code></pre>\n<p>With:</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///dbdir/test.db'\n</code></pre>\n", "abstract": "Replace: With:"}, {"id": 45260782, "score": 18, "vote": 0, "content": "<p>finally figured it out, had help tho</p>\n<pre><code class=\"python\"> import os\n\nfile_path = os.path.abspath(os.getcwd())+\"\\database.db\"\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///'+file_path\n db = SQLAlchemy(app)\n</code></pre>\n", "abstract": "finally figured it out, had help tho"}, {"id": 28568622, "score": 8, "vote": 0, "content": "<p>I had this issue with sqlite. The process trying to open the database file needs to have write access to the directory as it creates temporary/lock files.</p>\n<p>The following structure worked for me to allow www-data to use the database.</p>\n<pre><code class=\"python\">%&gt; ls -l\ndrwxrwxr-x  2 fmlheureux www-data     4096 Feb 17 13:24 database-dir\n\n%&gt; ls -l database-dir/\n-rw-rw-r-- 1 fmlheureux www-data 40960 Feb 17 13:28 database.sqlite\n</code></pre>\n", "abstract": "I had this issue with sqlite. The process trying to open the database file needs to have write access to the directory as it creates temporary/lock files. The following structure worked for me to allow www-data to use the database."}, {"id": 33379594, "score": 6, "vote": 0, "content": "<p>My database URI started rocking after adding one dot in between <code>////</code>. Working on windows 7. I had directory and db-file created prior to calling this.</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///./dbdir/test.db'\n</code></pre>\n", "abstract": "My database URI started rocking after adding one dot in between ////. Working on windows 7. I had directory and db-file created prior to calling this."}, {"id": 20060134, "score": 1, "vote": 0, "content": "<p>I think I've seen errors like this where file permissions were wrong for the .db file or its parent directory. You might make sure that the process trying to access the database can do so by appropriate use of  <code>chown</code> or <code>chmod</code>.</p>\n<p>This is specifically about Django, but maybe still relevant: <a href=\"https://serverfault.com/questions/57596/why-do-i-get-sqlite-error-unable-to-open-database-file\">https://serverfault.com/questions/57596/why-do-i-get-sqlite-error-unable-to-open-database-file</a></p>\n", "abstract": "I think I've seen errors like this where file permissions were wrong for the .db file or its parent directory. You might make sure that the process trying to access the database can do so by appropriate use of  chown or chmod. This is specifically about Django, but maybe still relevant: https://serverfault.com/questions/57596/why-do-i-get-sqlite-error-unable-to-open-database-file"}, {"id": 33797900, "score": 1, "vote": 0, "content": "<p>I just met this same problem and found that I make a stupid circular reference . </p>\n<p>./data_model.py</p>\n<blockquote>\n<pre><code class=\"python\">from flask.ext.sqlalchemy import SQLAlchemy\nfrom api.src.app import app\n\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////database/user.db')\n\ndb = SQLAlchemy(app)\n</code></pre>\n</blockquote>\n<p>./app.py</p>\n<blockquote>\n<pre><code class=\"python\">...\nfrom api.src.data_model import db\ndb.init_app(app)\n</code></pre>\n</blockquote>\n<p>Then I removed the app.py/db and it works.</p>\n", "abstract": "I just met this same problem and found that I make a stupid circular reference .  ./data_model.py ./app.py Then I removed the app.py/db and it works."}, {"id": 62265747, "score": 1, "vote": 0, "content": "<p>For those looking for a solution to the <code>OperationalError</code>, not necessarily caused by being <em>unable to open database file None None</em> - you might try adding a <code>pool_pre_ping=True</code> argument to <code>create_engine</code>, i.e.</p>\n<pre><code class=\"python\">engine = create_engine(\"mysql+pymysql://user:pw@host/db\", pool_pre_ping=True)\n</code></pre>\n<p>see <a href=\"https://docs.sqlalchemy.org/en/13/core/pooling.html#dealing-with-disconnects\" rel=\"nofollow noreferrer\">sqlalchemy documentation</a>:</p>\n<blockquote>\n<p><em>Pessimistic testing of connections upon checkout is achievable by using the <code>Pool.pre_ping</code> argument, available from <code>create_engine()</code> via the <code>create_engine.pool_pre_ping</code> argument</em></p>\n<p><em>The \u201cpre ping\u201d feature will normally emit SQL equivalent to \u201cSELECT 1\u201d each time a connection is checked out from the pool; if an error is raised that is detected as a \u201cdisconnect\u201d situation, the connection will be immediately recycled, and all other pooled connections older than the current time are invalidated, so that the next time they are checked out, they will also be recycled before use.</em></p>\n</blockquote>\n", "abstract": "For those looking for a solution to the OperationalError, not necessarily caused by being unable to open database file None None - you might try adding a pool_pre_ping=True argument to create_engine, i.e. see sqlalchemy documentation: Pessimistic testing of connections upon checkout is achievable by using the Pool.pre_ping argument, available from create_engine() via the create_engine.pool_pre_ping argument The \u201cpre ping\u201d feature will normally emit SQL equivalent to \u201cSELECT 1\u201d each time a connection is checked out from the pool; if an error is raised that is detected as a \u201cdisconnect\u201d situation, the connection will be immediately recycled, and all other pooled connections older than the current time are invalidated, so that the next time they are checked out, they will also be recycled before use."}, {"id": 61066628, "score": 0, "vote": 0, "content": "<p>You're not managing to find the path to the database from your current level. What you need to do is the following:</p>\n<pre><code class=\"python\">DATABASE_URI = 'sqlite:///../tmp/ate.db'\n</code></pre>\n<p>That means go up to the root level <code>..</code> and then navigate down to the database (the relative path is <code>/tmp/ate.db</code> in this case).</p>\n", "abstract": "You're not managing to find the path to the database from your current level. What you need to do is the following: That means go up to the root level .. and then navigate down to the database (the relative path is /tmp/ate.db in this case)."}, {"id": 69829269, "score": 0, "vote": 0, "content": "<p>I had this same issue when trying to start the central scheduler for luigi (python module) with task history enabled.</p>\n<pre><code class=\"python\">sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file\n</code></pre>\n<p>I was attempting to use the following configuration from their documentation:</p>\n<pre><code class=\"python\">[task_history]\ndb_connection = sqlite:////user/local/var/luigi-task-hist.db\n</code></pre>\n<p>However, <strong>/user/local/</strong>* did not exist on my machine and I had to change the configuration to:</p>\n<pre><code class=\"python\">[task_history]\ndb_connection = sqlite:////usr/local/var/luigi-task-hist.db\n</code></pre>\n<p>Kind of a dumb mistake, but easily overlooked. Might save someone some time. This change got rid of the error in my case and <strong>luigid</strong> started with no errors.</p>\n", "abstract": "I had this same issue when trying to start the central scheduler for luigi (python module) with task history enabled. I was attempting to use the following configuration from their documentation: However, /user/local/* did not exist on my machine and I had to change the configuration to: Kind of a dumb mistake, but easily overlooked. Might save someone some time. This change got rid of the error in my case and luigid started with no errors."}, {"id": 71312831, "score": 0, "vote": 0, "content": "<p>I am doing a course of Python and I have the same problem. Affortunately in the course put the right way to determined the path of the database URI</p>\n<p>So it works for me even in the 2022 year.</p>\n<p>You need to change:</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'\n</code></pre>\n<p>to:</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///&lt;name of database&gt;.db'\n</code></pre>\n<p>I hope that it works for someone.</p>\n", "abstract": "I am doing a course of Python and I have the same problem. Affortunately in the course put the right way to determined the path of the database URI So it works for me even in the 2022 year. You need to change: to: I hope that it works for someone."}, {"id": 72212480, "score": 0, "vote": 0, "content": "<p>I was able to overcome the same error by running sudo python :)</p>\n", "abstract": "I was able to overcome the same error by running sudo python :)"}, {"id": 73635851, "score": 0, "vote": 0, "content": "<p>This is the problem related to your file path. If you want to save your file in your root directory itself, then write <em>file_name</em> itself right after '/' -</p>\n<pre><code class=\"python\">app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///file_name.db'\n</code></pre>\n", "abstract": "This is the problem related to your file path. If you want to save your file in your root directory itself, then write file_name itself right after '/' -"}]}, {"link": "https://stackoverflow.com/questions/6586552/django-how-to-do-get-or-create-in-a-threadsafe-way", "question": {"id": "6586552", "title": "Django: how to do get_or_create() in a threadsafe way?", "content": "<p>In my Django app very often I need to do something similar to <code>get_or_create()</code>. E.g.,</p>\n<blockquote>\n<p>User submits a tag. Need to see if\n  that tag already is in the database.\n  If not, create a new record for it. If\n  it is, just update the existing\n  record.</p>\n</blockquote>\n<p>But looking into the doc for <code>get_or_create()</code> it looks like it's not threadsafe. Thread A checks and finds Record X does not exist. Then Thread B checks and finds that Record X does not exist. Now both Thread A and Thread B will create a new Record X.</p>\n<p>This must be a very common situation. How do I handle it in a threadsafe way? </p>\n", "abstract": "In my Django app very often I need to do something similar to get_or_create(). E.g., User submits a tag. Need to see if\n  that tag already is in the database.\n  If not, create a new record for it. If\n  it is, just update the existing\n  record. But looking into the doc for get_or_create() it looks like it's not threadsafe. Thread A checks and finds Record X does not exist. Then Thread B checks and finds that Record X does not exist. Now both Thread A and Thread B will create a new Record X. This must be a very common situation. How do I handle it in a threadsafe way? "}, "answers": [{"id": 22095136, "score": 46, "vote": 0, "content": "<p>Since 2013 or so, get_or_create is atomic, so it handles concurrency nicely:</p>\n<blockquote>\n<p>This method is atomic assuming correct usage, correct database\n  configuration, and correct behavior of the underlying database.\n  However, if uniqueness is not enforced at the database level for the\n  kwargs used in a get_or_create call (see unique or unique_together),\n  this method is prone to a race-condition which can result in multiple\n  rows with the same parameters being inserted simultaneously.</p>\n<p>If you are using MySQL, be sure to use the READ COMMITTED isolation\n  level rather than REPEATABLE READ (the default), otherwise you may see\n  cases where get_or_create will raise an IntegrityError but the object\n  won\u2019t appear in a subsequent get() call.</p>\n</blockquote>\n<p>From: <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#get-or-create\" rel=\"noreferrer\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#get-or-create</a></p>\n<p>Here's an example of how you could do it:</p>\n<p>Define a model with either unique=True:</p>\n<pre><code class=\"python\">class MyModel(models.Model):\n    slug = models.SlugField(max_length=255, unique=True)\n    name = models.CharField(max_length=255)\n\nMyModel.objects.get_or_create(slug=&lt;user_slug_here&gt;, defaults={\"name\": &lt;user_name_here&gt;})\n</code></pre>\n<p>... or by using unique_togheter:</p>\n<pre><code class=\"python\">class MyModel(models.Model):\n    prefix = models.CharField(max_length=3)\n    slug = models.SlugField(max_length=255)\n    name = models.CharField(max_length=255)\n\n    class Meta:\n        unique_together = (\"prefix\", \"slug\")\n\nMyModel.objects.get_or_create(prefix=&lt;user_prefix_here&gt;, slug=&lt;user_slug_here&gt;, defaults={\"name\": &lt;user_name_here&gt;})\n</code></pre>\n<p>Note how the non-unique fields are in the defaults dict, NOT among the unique fields in get_or_create. This will ensure your creates are atomic.</p>\n<p>Here's how it's implemented in Django: <a href=\"https://github.com/django/django/blob/fd60e6c8878986a102f0125d9cdf61c717605cf1/django/db/models/query.py#L466\" rel=\"noreferrer\">https://github.com/django/django/blob/fd60e6c8878986a102f0125d9cdf61c717605cf1/django/db/models/query.py#L466</a> - Try creating an object, catch an eventual IntegrityError, and return the copy in that case. In other words: handle atomicity in the database.</p>\n", "abstract": "Since 2013 or so, get_or_create is atomic, so it handles concurrency nicely: This method is atomic assuming correct usage, correct database\n  configuration, and correct behavior of the underlying database.\n  However, if uniqueness is not enforced at the database level for the\n  kwargs used in a get_or_create call (see unique or unique_together),\n  this method is prone to a race-condition which can result in multiple\n  rows with the same parameters being inserted simultaneously. If you are using MySQL, be sure to use the READ COMMITTED isolation\n  level rather than REPEATABLE READ (the default), otherwise you may see\n  cases where get_or_create will raise an IntegrityError but the object\n  won\u2019t appear in a subsequent get() call. From: https://docs.djangoproject.com/en/dev/ref/models/querysets/#get-or-create Here's an example of how you could do it: Define a model with either unique=True: ... or by using unique_togheter: Note how the non-unique fields are in the defaults dict, NOT among the unique fields in get_or_create. This will ensure your creates are atomic. Here's how it's implemented in Django: https://github.com/django/django/blob/fd60e6c8878986a102f0125d9cdf61c717605cf1/django/db/models/query.py#L466 - Try creating an object, catch an eventual IntegrityError, and return the copy in that case. In other words: handle atomicity in the database."}, {"id": 6586594, "score": 11, "vote": 0, "content": "<blockquote>\n<p>This must be a very common situation. How do I handle it in a threadsafe way?</p>\n</blockquote>\n<p>Yes.</p>\n<p>The \"standard\" solution in SQL is to simply attempt to create the record.  If it works, that's good.  Keep going.</p>\n<p>If an attempt to create a record gets a \"duplicate\" exception from the RDBMS, then do a SELECT and keep going.</p>\n<p>Django, however, has an ORM layer, with it's own cache.  So the logic is inverted to make the common case work directly and quickly and the uncommon case (the duplicate) raise a rare exception.</p>\n", "abstract": "This must be a very common situation. How do I handle it in a threadsafe way? Yes. The \"standard\" solution in SQL is to simply attempt to create the record.  If it works, that's good.  Keep going. If an attempt to create a record gets a \"duplicate\" exception from the RDBMS, then do a SELECT and keep going. Django, however, has an ORM layer, with it's own cache.  So the logic is inverted to make the common case work directly and quickly and the uncommon case (the duplicate) raise a rare exception."}, {"id": 14084100, "score": 3, "vote": 0, "content": "<p>try transaction.commit_on_success decorator for callable where you are trying get_or_create(**kwargs)</p>\n<p>\"Use the commit_on_success decorator to use a single transaction for all the work done in a function.If the function returns successfully, then Django will commit all work done within the function at that point. If the function raises an exception, though, Django will roll back the transaction.\"</p>\n<p>apart from it, in concurrent calls to get_or_create, both the threads try to get the object with argument passed to it (except for \"defaults\" arg which is a dict used during create call in case get() fails to retrieve any object). in case of failure both the threads try to create the object resulting in multiple duplicate objects unless some unique/unique together is implemented at database level with field(s) used in get()'s call.</p>\n<p>it is similar to this post\n<a href=\"https://stackoverflow.com/questions/2235318/how-do-i-deal-with-this-race-condition-in-django\">How do I deal with this race condition in django?</a></p>\n", "abstract": "try transaction.commit_on_success decorator for callable where you are trying get_or_create(**kwargs) \"Use the commit_on_success decorator to use a single transaction for all the work done in a function.If the function returns successfully, then Django will commit all work done within the function at that point. If the function raises an exception, though, Django will roll back the transaction.\" apart from it, in concurrent calls to get_or_create, both the threads try to get the object with argument passed to it (except for \"defaults\" arg which is a dict used during create call in case get() fails to retrieve any object). in case of failure both the threads try to create the object resulting in multiple duplicate objects unless some unique/unique together is implemented at database level with field(s) used in get()'s call. it is similar to this post\nHow do I deal with this race condition in django?"}, {"id": 66907374, "score": 2, "vote": 0, "content": "<p>So many years have passed, but nobody has written about <code>threading.Lock</code>. If you don't have the opportunity to make migrations for <code>unique together</code>, for legacy reasons, you can use locks or <code>threading.Semaphore</code> objects. Here is the pseudocode:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from concurrent.futures import ThreadPoolExecutor\nfrom threading import Lock\n\n_lock = Lock()\n\n\ndef get_staff(data: dict):\n    _lock.acquire()\n    try:\n        staff, created = MyModel.objects.get_or_create(**data)\n        return staff\n    finally:\n        _lock.release()\n\n\nwith ThreadPoolExecutor(max_workers=50) as pool:\n    pool.map(get_staff, get_list_of_some_data())\n</code></pre>\n", "abstract": "So many years have passed, but nobody has written about threading.Lock. If you don't have the opportunity to make migrations for unique together, for legacy reasons, you can use locks or threading.Semaphore objects. Here is the pseudocode:"}]}, {"link": "https://stackoverflow.com/questions/50927740/sqlalchemy-create-schema-if-not-exists", "question": {"id": "50927740", "title": "SQLAlchemy: &quot;create schema if not exists&quot;", "content": "<p>I want to do the \"CREATE SCHEMA IF NOT EXISTS\" query in SQLAlchemy.\nIs there a better way than this:</p>\n<pre><code class=\"python\">    engine = sqlalchemy.create_engine(connstr)\n\n    schema_name = config.get_config_value('db', 'schema_name')\n\n    #Create schema; if it already exists, skip this\n    try:\n        engine.execute(CreateSchema(schema_name))\n    except sqlalchemy.exc.ProgrammingError:\n        pass\n</code></pre>\n<p>I am using Python 3.5.</p>\n", "abstract": "I want to do the \"CREATE SCHEMA IF NOT EXISTS\" query in SQLAlchemy.\nIs there a better way than this: I am using Python 3.5."}, "answers": [{"id": 55345404, "score": 37, "vote": 0, "content": "<p>I had the same question and the answer, which I found, is:</p>\n<pre><code class=\"python\">if not engine.dialect.has_schema(engine, schema_name):\n    engine.execute(sqlalchemy.schema.CreateSchema(schema_name))\n</code></pre>\n<p>We can also check schema without engine instance, but using connection</p>\n<pre><code class=\"python\">conn = engine.connect()\nif conn.dialect.has_schema(conn, schema_name):\n    ...\n</code></pre>\n", "abstract": "I had the same question and the answer, which I found, is: We can also check schema without engine instance, but using connection"}, {"id": 61274497, "score": 5, "vote": 0, "content": "<p>For MS Sql users, there's no <code>has_schema()</code> but this seems to work:</p>\n<pre><code class=\"python\">if schemaname not in conn.dialect.get_schema_names(conn):\n   conn.execute(schema.CreateSchema(schemaname))\n</code></pre>\n", "abstract": "For MS Sql users, there's no has_schema() but this seems to work:"}, {"id": 58382390, "score": 2, "vote": 0, "content": "<p>You can use the excellent <a href=\"https://sqlalchemy-utils.readthedocs.io/\" rel=\"nofollow noreferrer\"><code>sqlalchemy_utils</code></a> package to do just that, in a very neat way.</p>\n<p>First, install the package:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">pip install sqlalchemy_utils\n</code></pre>\n<p>Then use it like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from sqlalchemy_utils.functions import database_exists, create_database\n\nengin_uri = 'postgres://postgres@localhost/name'\n\nif not database_exists(engin_uri):\n    create_database(engin_uri)\n</code></pre>\n<p>The example from the official docs has used PostgreSQL and I have personally used it on MySQL 8.</p>\n", "abstract": "You can use the excellent sqlalchemy_utils package to do just that, in a very neat way. First, install the package: Then use it like this: The example from the official docs has used PostgreSQL and I have personally used it on MySQL 8."}, {"id": 73647390, "score": 1, "vote": 0, "content": "<p>My preferred way of doing that task:</p>\n<pre><code class=\"python\">from sqlalchemy import inspect\n&lt;You might need a few more SQLAlchemy imports\n\ndef setup_schemas(engine, metadata):\n    inspector = inspect(engine)\n    all_schemas = inspector.get_schema_names()\n    for schema in metadata._schemas:\n        if schema not in all_schemas:\n            _create_schema(engine, schema)\n    \ndef _create_schema(engine, schema) -&gt; None:\n    stmt = text(f\"CREATE SCHEMA {schema}\")\n    with engine.connect() as conn:\n        conn.execute(stmt)\n        conn.commit()\n</code></pre>\n", "abstract": "My preferred way of doing that task:"}]}, {"link": "https://stackoverflow.com/questions/4836327/do-i-need-to-add-a-db-index-to-this-django-model", "question": {"id": "4836327", "title": "Do I need to add a db_index to this Django model?", "content": "<pre><code class=\"python\">class Comments(models.Model):\n    content = models.ForeignKey(Content)\n</code></pre>\n<p>Do I need to add a db_index to \"content\"? Or would that automatically be indexed because it's a foreign key?</p>\n", "abstract": "Do I need to add a db_index to \"content\"? Or would that automatically be indexed because it's a foreign key?"}, "answers": [{"id": 4836354, "score": 44, "vote": 0, "content": "<p>Unless specified otherwise, an index will be created for a <code>ForeignKey</code>.  Relevant source code:</p>\n<pre><code class=\"python\">class ForeignKey(RelatedField, Field):\n    # snip\n    def __init__(self, to, to_field=None, rel_class=ManyToOneRel, **kwargs):\n        # snip\n        if 'db_index' not in kwargs:\n            kwargs['db_index'] = True\n</code></pre>\n", "abstract": "Unless specified otherwise, an index will be created for a ForeignKey.  Relevant source code:"}]}]