[{"link": "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents", "question": {"id": "8897593", "title": "How to compute the similarity between two text documents?", "content": "<p>I am looking at working on an NLP project, in any programming language (though Python will be my preference).</p>\n<p>I want to take two documents and determine how similar they are.</p>\n", "abstract": "I am looking at working on an NLP project, in any programming language (though Python will be my preference). I want to take two documents and determine how similar they are."}, "answers": [{"id": 8897648, "score": 373, "vote": 0, "content": "<p>The common way of doing this is to transform the documents into TF-IDF vectors and then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. <a href=\"http://www-nlp.stanford.edu/IR-book/\" rel=\"noreferrer\"><em>Introduction to Information Retrieval</em></a>, which is free and available online.</p>\n<h3 id=\"computing-pairwise-similarities-m9ec\">Computing Pairwise Similarities</h3>\n<p>TF-IDF (and similar text transformations) are implemented in the Python packages <a href=\"http://radimrehurek.com/gensim/\" rel=\"noreferrer\">Gensim</a> and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\" rel=\"noreferrer\">scikit-learn</a>. In the latter package, computing cosine similarities is as easy as</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">from sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = [open(f).read() for f in text_files]\ntfidf = TfidfVectorizer().fit_transform(documents)\n# no need to normalize, since Vectorizer will return normalized tf-idf\npairwise_similarity = tfidf * tfidf.T\n</code></pre>\n<p>or, if the documents are plain strings,</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; corpus = [\"I'd like an apple\", \n...           \"An apple a day keeps the doctor away\", \n...           \"Never compare an apple to an orange\", \n...           \"I prefer scikit-learn to Orange\", \n...           \"The scikit-learn docs are Orange and Blue\"]                                                                                                                                                                                                   \n&gt;&gt;&gt; vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n&gt;&gt;&gt; tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n&gt;&gt;&gt; pairwise_similarity = tfidf * tfidf.T \n</code></pre>\n<p>though Gensim may have more options for this kind of task.</p>\n<p>See also <a href=\"https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python\">this question</a>.</p>\n<p>[Disclaimer: I was involved in the scikit-learn TF-IDF implementation.]</p>\n<h3 id=\"interpreting-the-results-1v7i\">Interpreting the Results</h3>\n<p>From above, <code>pairwise_similarity</code> is a Scipy <a href=\"https://docs.scipy.org/doc/scipy/reference/sparse.html\" rel=\"noreferrer\">sparse matrix</a> that is square in shape, with the number of rows and columns equal to the number of documents in the corpus.</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; pairwise_similarity                                                                                                                                                                                                                                      \n&lt;5x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 17 stored elements in Compressed Sparse Row format&gt;\n</code></pre>\n<p>You can convert the sparse array to a NumPy array via <code>.toarray()</code> or <code>.A</code>:</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; pairwise_similarity.toarray()                                                                                                                                                                                                                            \narray([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],\n       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],\n       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],\n       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],\n       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])\n</code></pre>\n<p>Let's say we want to find the document most similar to the final document, \"The scikit-learn docs are Orange and Blue\".  This document has index 4 in <code>corpus</code>.  You can find the index of the most similar document by <strong>taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself</strong>.  You can do the latter through <code>np.fill_diagonal()</code>, and the former through <code>np.nanargmax()</code>:</p>\n<pre class=\"lang-python prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; import numpy as np     \n                                                                                                                                                                                                                                  \n&gt;&gt;&gt; arr = pairwise_similarity.toarray()     \n&gt;&gt;&gt; np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            \n                                                                                                                                                                                                                 \n&gt;&gt;&gt; input_doc = \"The scikit-learn docs are Orange and Blue\"                                                                                                                                                                                                  \n&gt;&gt;&gt; input_idx = corpus.index(input_doc)                                                                                                                                                                                                                      \n&gt;&gt;&gt; input_idx                                                                                                                                                                                                                                                \n4\n\n&gt;&gt;&gt; result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                \n&gt;&gt;&gt; corpus[result_idx]                                                                                                                                                                                                                                       \n'I prefer scikit-learn to Orange'\n</code></pre>\n<p>Note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus &amp; vocabulary.  Instead of converting to a NumPy array, you could do:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; n, _ = pairwise_similarity.shape                                                                                                                                                                                                                         \n&gt;&gt;&gt; pairwise_similarity[np.arange(n), np.arange(n)] = -1.0\n&gt;&gt;&gt; pairwise_similarity[input_idx].argmax()                                                                                                                                                                                                                  \n3\n</code></pre>\n", "abstract": "The common way of doing this is to transform the documents into TF-IDF vectors and then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. Introduction to Information Retrieval, which is free and available online. TF-IDF (and similar text transformations) are implemented in the Python packages Gensim and scikit-learn. In the latter package, computing cosine similarities is as easy as or, if the documents are plain strings, though Gensim may have more options for this kind of task. See also this question. [Disclaimer: I was involved in the scikit-learn TF-IDF implementation.] From above, pairwise_similarity is a Scipy sparse matrix that is square in shape, with the number of rows and columns equal to the number of documents in the corpus. You can convert the sparse array to a NumPy array via .toarray() or .A: Let's say we want to find the document most similar to the final document, \"The scikit-learn docs are Orange and Blue\".  This document has index 4 in corpus.  You can find the index of the most similar document by taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself.  You can do the latter through np.fill_diagonal(), and the former through np.nanargmax(): Note: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus & vocabulary.  Instead of converting to a NumPy array, you could do:"}, {"id": 24129170, "score": 105, "vote": 0, "content": "<p>Identical to @larsman, but with some preprocessing</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import nltk, string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnltk.download('punkt') # if necessary...\n\n\nstemmer = nltk.stem.porter.PorterStemmer()\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\ndef stem_tokens(tokens):\n    return [stemmer.stem(item) for item in tokens]\n\n'''remove punctuation, lowercase, stem'''\ndef normalize(text):\n    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n\nvectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]\n\n\nprint cosine_sim('a little bird', 'a little bird')\nprint cosine_sim('a little bird', 'a little bird chirps')\nprint cosine_sim('a little bird', 'a big dog barks')\n</code></pre>\n", "abstract": "Identical to @larsman, but with some preprocessing"}, {"id": 44102463, "score": 71, "vote": 0, "content": "<p>It's an old question, but I found this can be done easily with <a href=\"https://spacy.io/usage/vectors-similarity\" rel=\"noreferrer\">Spacy</a>. Once the document is read, a simple api <code>similarity</code> can be used to find the cosine similarity between the document vectors.</p>\n<p>Start by installing the package and downloading the model:</p>\n<pre><code class=\"python\">pip install spacy\npython -m spacy download en_core_web_sm\n</code></pre>\n<p>Then use like so:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc1 = nlp(u'Hello hi there!')\ndoc2 = nlp(u'Hello hi there!')\ndoc3 = nlp(u'Hey whatsup?')\n\nprint (doc1.similarity(doc2)) # 0.999999954642\nprint (doc2.similarity(doc3)) # 0.699032527716\nprint (doc1.similarity(doc3)) # 0.699032527716\n</code></pre>\n", "abstract": "It's an old question, but I found this can be done easily with Spacy. Once the document is read, a simple api similarity can be used to find the cosine similarity between the document vectors. Start by installing the package and downloading the model: Then use like so:"}, {"id": 55732255, "score": 34, "vote": 0, "content": "<p>If you are looking for something very accurate, you need to use some better tool than tf-idf. <a href=\"https://arxiv.org/abs/1803.11175\" rel=\"noreferrer\">Universal sentence encoder</a> is one of the most accurate ones to find the similarity between any two pieces of text. Google provided pretrained models that you can use for your own application without a need to train from scratch anything. First, you have to install tensorflow and tensorflow-hub:</p>\n<pre><code class=\"python\">    pip install tensorflow\n    pip install tensorflow_hub\n</code></pre>\n<p>The code below lets you convert any text to a fixed length vector representation and then you can use the dot product to find out the similarity between them</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import tensorflow_hub as hub\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n\n# Import the Universal Sentence Encoder's TF Hub module\nembed = hub.Module(module_url)\n\n# sample text\nmessages = [\n# Smartphones\n\"My phone is not good.\",\n\"Your cellphone looks great.\",\n\n# Weather\n\"Will it snow tomorrow?\",\n\"Recently a lot of hurricanes have hit the US\",\n\n# Food and health\n\"An apple a day, keeps the doctors away\",\n\"Eating strawberries is healthy\",\n]\n\nsimilarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\nsimilarity_message_encodings = embed(similarity_input_placeholder)\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})\n\n    corr = np.inner(message_embeddings_, message_embeddings_)\n    print(corr)\n    heatmap(messages, messages, corr)\n</code></pre>\n<p>and the code for plotting:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">def heatmap(x_labels, y_labels, values):\n    fig, ax = plt.subplots()\n    im = ax.imshow(values)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(x_labels)))\n    ax.set_yticks(np.arange(len(y_labels)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(x_labels)\n    ax.set_yticklabels(y_labels)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10,\n         rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(y_labels)):\n        for j in range(len(x_labels)):\n            text = ax.text(j, i, \"%.2f\"%values[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\", \nfontsize=6)\n\n    fig.tight_layout()\n    plt.show()\n</code></pre>\n<p>the result would be:\n<a href=\"https://i.stack.imgur.com/ftGqC.png\" rel=\"noreferrer\"><img alt=\"the similarity matrix between pairs of texts\" src=\"https://i.stack.imgur.com/ftGqC.png\"/></a></p>\n<p>as you can see the most similarity is between texts with themselves and then with their close texts in meaning.</p>\n<p><strong>IMPORTANT</strong>: the first time you run the code it will be slow because it needs to download the model. if you want to prevent it from downloading the model again and use the local model you have to create a folder for cache and add it to the environment variable and then after the first time running use that path:\n</p>\n<pre><code class=\"python\">tf_hub_cache_dir = \"universal_encoder_cached/\"\nos.environ[\"TFHUB_CACHE_DIR\"] = tf_hub_cache_dir\n\n# pointing to the folder inside cache dir, it will be unique on your system\nmodule_url = tf_hub_cache_dir+\"/d8fbeb5c580e50f975ef73e80bebba9654228449/\"\nembed = hub.Module(module_url)\n</code></pre>\n<p>More information: <a href=\"https://tfhub.dev/google/universal-sentence-encoder/2\" rel=\"noreferrer\">https://tfhub.dev/google/universal-sentence-encoder/2</a></p>\n", "abstract": "If you are looking for something very accurate, you need to use some better tool than tf-idf. Universal sentence encoder is one of the most accurate ones to find the similarity between any two pieces of text. Google provided pretrained models that you can use for your own application without a need to train from scratch anything. First, you have to install tensorflow and tensorflow-hub: The code below lets you convert any text to a fixed length vector representation and then you can use the dot product to find out the similarity between them and the code for plotting: the result would be:\n as you can see the most similarity is between texts with themselves and then with their close texts in meaning. IMPORTANT: the first time you run the code it will be slow because it needs to download the model. if you want to prevent it from downloading the model again and use the local model you have to create a folder for cache and add it to the environment variable and then after the first time running use that path:\n More information: https://tfhub.dev/google/universal-sentence-encoder/2"}, {"id": 8897723, "score": 17, "vote": 0, "content": "<p>Generally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use <a href=\"http://lucene.apache.org/java/docs/index.html\">Lucene</a> (if your collection is pretty large) or <a href=\"http://alias-i.com/lingpipe/\">LingPipe</a> to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors. The libraries do provide several improvements over this general approach, e.g. using inverse document frequencies and calculating tf-idf vectors. If you are looking to do something copmlex, LingPipe also provides methods to calculate LSA similarity between documents which gives better results than cosine similarity. \nFor Python, you can use <a href=\"http://www.nltk.org/\">NLTK</a>.</p>\n", "abstract": "Generally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use Lucene (if your collection is pretty large) or LingPipe to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors. The libraries do provide several improvements over this general approach, e.g. using inverse document frequencies and calculating tf-idf vectors. If you are looking to do something copmlex, LingPipe also provides methods to calculate LSA similarity between documents which gives better results than cosine similarity. \nFor Python, you can use NLTK."}, {"id": 58854567, "score": 6, "vote": 0, "content": "<p>For Syntactic Similarity \nThere can be 3 easy ways of detecting similarity.</p>\n<ul>\n<li>Word2Vec</li>\n<li>Glove</li>\n<li>Tfidf or countvectorizer</li>\n</ul>\n<p>For Semantic Similarity\nOne can use BERT Embedding and try a different word pooling strategies to get document embedding and then apply cosine similarity on document embedding. </p>\n<p>An advanced methodology can use BERT SCORE to get similarity. \n<a href=\"https://i.stack.imgur.com/Yg8c0.png\" rel=\"noreferrer\"><img alt=\"BERT SCORE\" src=\"https://i.stack.imgur.com/Yg8c0.png\"/></a></p>\n<p>Research Paper Link: <a href=\"https://arxiv.org/abs/1904.09675\" rel=\"noreferrer\">https://arxiv.org/abs/1904.09675</a></p>\n", "abstract": "For Syntactic Similarity \nThere can be 3 easy ways of detecting similarity. For Semantic Similarity\nOne can use BERT Embedding and try a different word pooling strategies to get document embedding and then apply cosine similarity on document embedding.  An advanced methodology can use BERT SCORE to get similarity. \n Research Paper Link: https://arxiv.org/abs/1904.09675"}, {"id": 11277680, "score": 5, "vote": 0, "content": "<p>Here's a little app to get you started...</p>\n<pre><code class=\"python\">import difflib as dl\n\na = file('file').read()\nb = file('file1').read()\n\nsim = dl.get_close_matches\n\ns = 0\nwa = a.split()\nwb = b.split()\n\nfor i in wa:\n    if sim(i, wb):\n        s += 1\n\nn = float(s) / float(len(wa))\nprint '%d%% similarity' % int(n * 100)\n</code></pre>\n", "abstract": "Here's a little app to get you started..."}, {"id": 61250013, "score": 5, "vote": 0, "content": "<p>To find sentence similarity with very less dataset and to get high accuracy you can use below python package which is using pre-trained BERT models,</p>\n<pre><code class=\"python\">pip install similar-sentences\n</code></pre>\n", "abstract": "To find sentence similarity with very less dataset and to get high accuracy you can use below python package which is using pre-trained BERT models,"}, {"id": 48553202, "score": 3, "vote": 0, "content": "<p>If you are more interested in measuring semantic similarity of two pieces of text, I suggest take a look at <a href=\"https://gitlab.com/mysilver/semantic-text-similarity\" rel=\"nofollow noreferrer\">this gitlab project</a>. You can run it as a server, there is also a pre-built model which you can use easily to measure the similarity of two pieces of text; even though it is mostly trained for measuring the similarity of two sentences, you can still use it in your case.It is written in java but you can run it as a RESTful service. </p>\n<p>Another option also is <a href=\"https://dkpro.github.io/dkpro-similarity/\" rel=\"nofollow noreferrer\">DKPro Similarity</a> which is a library with various algorithm to measure the similarity of texts. However, it is also written in java. </p>\n<p>code example:</p>\n<pre><code class=\"python\">// this similarity measure is defined in the dkpro.similarity.algorithms.lexical-asl package\n// you need to add that to your .pom to make that example work\n// there are some examples that should work out of the box in dkpro.similarity.example-gpl \nTextSimilarityMeasure measure = new WordNGramJaccardMeasure(3);    // Use word trigrams\n\nString[] tokens1 = \"This is a short example text .\".split(\" \");   \nString[] tokens2 = \"A short example text could look like that .\".split(\" \");\n\ndouble score = measure.getSimilarity(tokens1, tokens2);\n\nSystem.out.println(\"Similarity: \" + score);\n</code></pre>\n", "abstract": "If you are more interested in measuring semantic similarity of two pieces of text, I suggest take a look at this gitlab project. You can run it as a server, there is also a pre-built model which you can use easily to measure the similarity of two pieces of text; even though it is mostly trained for measuring the similarity of two sentences, you can still use it in your case.It is written in java but you can run it as a RESTful service.  Another option also is DKPro Similarity which is a library with various algorithm to measure the similarity of texts. However, it is also written in java.  code example:"}, {"id": 14831884, "score": 2, "vote": 0, "content": "<p>You might want to try this online service  for cosine document similarity <a href=\"http://www.scurtu.it/documentSimilarity.html\" rel=\"nofollow\">http://www.scurtu.it/documentSimilarity.html</a></p>\n<pre><code class=\"python\">import urllib,urllib2\nimport json\nAPI_URL=\"http://www.scurtu.it/apis/documentSimilarity\"\ninputDict={}\ninputDict['doc1']='Document with some text'\ninputDict['doc2']='Other document with some text'\nparams = urllib.urlencode(inputDict)    \nf = urllib2.urlopen(API_URL, params)\nresponse= f.read()\nresponseObject=json.loads(response)  \nprint responseObject\n</code></pre>\n", "abstract": "You might want to try this online service  for cosine document similarity http://www.scurtu.it/documentSimilarity.html"}, {"id": 65825950, "score": 1, "vote": 0, "content": "<p>I am combining the solutions from answers of @FredFoo and @Renaud. My solution is able to apply @Renaud's preprocessing on the text corpus of @FredFoo and then display pairwise similarities where the similarity is greater than 0. I ran this code on Windows by installing python and pip first. pip is installed as part of python but you may have to explicitly do it by re-running the installation package, choosing modify and then choosing pip. I use the command line to execute my python code saved in a file \"similarity.py\". I had to execute the following commands:</p>\n<pre><code class=\"python\">&gt;set PYTHONPATH=%PYTHONPATH%;C:\\_location_of_python_lib_\n&gt;python -m pip install sklearn\n&gt;python -m pip install nltk\n&gt;py similarity.py\n</code></pre>\n<p>The code for similarity.py is as follows:</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk, string\nimport numpy as np\nnltk.download('punkt') # if necessary...\n\nstemmer = nltk.stem.porter.PorterStemmer()\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\ndef stem_tokens(tokens):\n    return [stemmer.stem(item) for item in tokens]\n\ndef normalize(text):\n    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n\ncorpus = [\"I'd like an apple\", \n           \"An apple a day keeps the doctor away\", \n           \"Never compare an apple to an orange\", \n           \"I prefer scikit-learn to Orange\", \n           \"The scikit-learn docs are Orange and Blue\"]  \n\nvect = TfidfVectorizer(tokenizer=normalize, stop_words='english')\ntfidf = vect.fit_transform(corpus)   \n                                                                                                                                                                                                                    \npairwise_similarity = tfidf * tfidf.T\n\n#view the pairwise similarities \nprint(pairwise_similarity)\n\n#check how a string is normalized\nprint(normalize(\"The scikit-learn docs are Orange and Blue\"))\n</code></pre>\n", "abstract": "I am combining the solutions from answers of @FredFoo and @Renaud. My solution is able to apply @Renaud's preprocessing on the text corpus of @FredFoo and then display pairwise similarities where the similarity is greater than 0. I ran this code on Windows by installing python and pip first. pip is installed as part of python but you may have to explicitly do it by re-running the installation package, choosing modify and then choosing pip. I use the command line to execute my python code saved in a file \"similarity.py\". I had to execute the following commands: The code for similarity.py is as follows:"}, {"id": 73147527, "score": 1, "vote": 0, "content": "<p>We can use sentencetransformer for this task\n<a href=\"https://www.sbert.net/docs/usage/semantic_textual_similarity.html\" rel=\"nofollow noreferrer\">link</a></p>\n<p>A simple example from sbert as below:</p>\n<pre><code class=\"python\">from sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n# Two lists of sentences\nsentences1 = ['The cat sits outside']\nsentences2 = ['The dog plays in the garden']\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n#Compute cosine-similarities\ncosine_scores = util.cos_sim(embeddings1, embeddings2)\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n   print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], \n         sentences2[i], cosine_scores[i][i]))\n</code></pre>\n", "abstract": "We can use sentencetransformer for this task\nlink A simple example from sbert as below:"}, {"id": 73908280, "score": 1, "vote": 0, "content": "<p>Creator of the <a href=\"https://github.com/brianrisk/simphile\" rel=\"nofollow noreferrer\">Simphile NLP</a> text similarity Python package here. Simphile contains several text similarity methods that are language agnostic and less CPU-intensive than language embeddings.</p>\n<p>Install:</p>\n<pre><code class=\"python\">pip install simphile\n</code></pre>\n<p>Choose your favorite method.  This example shows three:</p>\n<pre><code class=\"python\">from simphile import jaccard_similarity, euclidian_similarity, compression_similarity\n\ntext_a = \"I love dogs\"\ntext_b = \"I love cats\"\n\nprint(f\"Jaccard Similarity: {jaccard_similarity(text_a, text_b)}\")\nprint(f\"Euclidian Similarity: {euclidian_similarity(text_a, text_b)}\")\nprint(f\"Compression Similarity: {compression_similarity(text_a, text_b)}\")\n</code></pre>\n<ul>\n<li><strong>Compression Similairty</strong> \u2013 leverages the pattern recognition of compression algorithms</li>\n<li><strong>Euclidian Similarity</strong> \u2013 Treats text like points in multi-dimensional space and calculates their closeness</li>\n<li><strong>Jaccard Similairy</strong> \u2013 Texts are more similar the more their words overlap</li>\n</ul>\n", "abstract": "Creator of the Simphile NLP text similarity Python package here. Simphile contains several text similarity methods that are language agnostic and less CPU-intensive than language embeddings. Install: Choose your favorite method.  This example shows three:"}]}, {"link": "https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do", "question": {"id": "34870614", "title": "What does tf.nn.embedding_lookup function do?", "content": "<pre><code class=\"python\">tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)\n</code></pre>\n<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>\n<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>\n", "abstract": "I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)? For instance, in the skip-gram model if we use tf.nn.embedding_lookup(embeddings, train_inputs), then for each train_input it finds the correspond embedding?"}, "answers": [{"id": 41922877, "score": 230, "vote": 0, "content": "<p>Yes, this function is hard to understand, until you get the point.</p>\n<p>In its simplest form, it is similar to <code>tf.gather</code>. It returns the elements of <code>params</code> according to the indexes specified by <code>ids</code>.</p>\n<p>For example (assuming you are inside <code>tf.InteractiveSession()</code>)</p>\n<pre><code class=\"python\">params = tf.constant([10,20,30,40])\nids = tf.constant([0,1,2,3])\nprint tf.nn.embedding_lookup(params,ids).eval()\n</code></pre>\n<p>would return <code>[10 20 30 40]</code>, because the first element (index 0) of params is <code>10</code>, the second element of params (index 1) is <code>20</code>, etc.</p>\n<p>Similarly, </p>\n<pre><code class=\"python\">params = tf.constant([10,20,30,40])\nids = tf.constant([1,1,3])\nprint tf.nn.embedding_lookup(params,ids).eval()\n</code></pre>\n<p>would return <code>[20 20 40]</code>.</p>\n<p>But <code>embedding_lookup</code> is more than that. The <code>params</code> argument can be a <strong>list</strong> of tensors, rather than a single tensor.</p>\n<pre><code class=\"python\">params1 = tf.constant([1,2])\nparams2 = tf.constant([10,20])\nids = tf.constant([2,0,2,1,2,3])\nresult = tf.nn.embedding_lookup([params1, params2], ids)\n</code></pre>\n<p>In such a case, the indexes, specified in <code>ids</code>, correspond to elements of tensors according to a <strong>partition strategy</strong>, where the default partition strategy is 'mod'.</p>\n<p>In the 'mod' strategy, index 0 corresponds to the first element of the first tensor in the list. Index 1 corresponds to the <strong>first</strong> element of the <strong>second</strong> tensor. Index 2 corresponds to the <strong>first</strong> element of the <strong>third</strong> tensor, and so on. Simply index <code>i</code> corresponds to the first element of the (i+1)th tensor , for all the indexes <code>0..(n-1)</code>, assuming params is a list of <code>n</code> tensors.</p>\n<p>Now, index <code>n</code> cannot correspond to tensor n+1, because the list <code>params</code> contains only <code>n</code> tensors. So index <code>n</code> corresponds to the <strong>second</strong> element of the first tensor. Similarly, index <code>n+1</code> corresponds to the second element of the second tensor, etc.</p>\n<p>So, in the code</p>\n<pre><code class=\"python\">params1 = tf.constant([1,2])\nparams2 = tf.constant([10,20])\nids = tf.constant([2,0,2,1,2,3])\nresult = tf.nn.embedding_lookup([params1, params2], ids)\n</code></pre>\n<p>index 0 corresponds to the first element of the first tensor: 1</p>\n<p>index 1 corresponds to the first element of the second tensor: 10</p>\n<p>index 2 corresponds to the second element of the first tensor: 2</p>\n<p>index 3 corresponds to the second element of the second tensor: 20</p>\n<p>Thus, the result would be:</p>\n<pre><code class=\"python\">[ 2  1  2 10  2 20]\n</code></pre>\n", "abstract": "Yes, this function is hard to understand, until you get the point. In its simplest form, it is similar to tf.gather. It returns the elements of params according to the indexes specified by ids. For example (assuming you are inside tf.InteractiveSession()) would return [10 20 30 40], because the first element (index 0) of params is 10, the second element of params (index 1) is 20, etc. Similarly,  would return [20 20 40]. But embedding_lookup is more than that. The params argument can be a list of tensors, rather than a single tensor. In such a case, the indexes, specified in ids, correspond to elements of tensors according to a partition strategy, where the default partition strategy is 'mod'. In the 'mod' strategy, index 0 corresponds to the first element of the first tensor in the list. Index 1 corresponds to the first element of the second tensor. Index 2 corresponds to the first element of the third tensor, and so on. Simply index i corresponds to the first element of the (i+1)th tensor , for all the indexes 0..(n-1), assuming params is a list of n tensors. Now, index n cannot correspond to tensor n+1, because the list params contains only n tensors. So index n corresponds to the second element of the first tensor. Similarly, index n+1 corresponds to the second element of the second tensor, etc. So, in the code index 0 corresponds to the first element of the first tensor: 1 index 1 corresponds to the first element of the second tensor: 10 index 2 corresponds to the second element of the first tensor: 2 index 3 corresponds to the second element of the second tensor: 20 Thus, the result would be:"}, {"id": 34877590, "score": 150, "vote": 0, "content": "<p><code>embedding_lookup</code> function retrieves rows of the <code>params</code> tensor. The behavior is similar to using indexing with arrays in numpy. E.g.</p>\n<pre><code class=\"python\">matrix = np.random.random([1024, 64])  # 64-dimensional embeddings\nids = np.array([0, 5, 17, 33])\nprint matrix[ids]  # prints a matrix of shape [4, 64] \n</code></pre>\n<p><code>params</code> argument can be also a list of tensors in which case the <code>ids</code> will be distributed among the tensors. For example, given a list of 3 tensors <code>[2, 64]</code>, the default behavior is that they will represent <code>ids</code>: <code>[0, 3]</code>, <code>[1, 4]</code>, <code>[2, 5]</code>. </p>\n<p><code>partition_strategy</code> controls the way how the <code>ids</code> are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.</p>\n", "abstract": "embedding_lookup function retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy. E.g. params argument can be also a list of tensors in which case the ids will be distributed among the tensors. For example, given a list of 3 tensors [2, 64], the default behavior is that they will represent ids: [0, 3], [1, 4], [2, 5].  partition_strategy controls the way how the ids are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece."}, {"id": 48438325, "score": 48, "vote": 0, "content": "<p>Yes, the purpose of <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup\" rel=\"noreferrer\"><strong><code>tf.nn.embedding_lookup()</code></strong></a> function is to perform a <em>lookup</em> in the <em>embedding matrix</em> and return the embeddings (or in simple terms the vector representation) of words.</p>\n<p>A simple embedding matrix (of shape: <strong><code>vocabulary_size x embedding_dimension</code></strong>) would look like below. (i.e. each <em>word</em> will be represented by a <em>vector</em> of numbers; hence the name <em>word2vec</em>)</p>\n<hr/>\n<p><strong>Embedding Matrix</strong></p>\n<pre><code class=\"python\">the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862\nlike 0.36808 0.20834 -0.22319 0.046283 0.20098 0.27515 -0.77127 -0.76804\nbetween 0.7503 0.71623 -0.27033 0.20059 -0.17008 0.68568 -0.061672 -0.054638\ndid 0.042523 -0.21172 0.044739 -0.19248 0.26224 0.0043991 -0.88195 0.55184\njust 0.17698 0.065221 0.28548 -0.4243 0.7499 -0.14892 -0.66786 0.11788\nnational -1.1105 0.94945 -0.17078 0.93037 -0.2477 -0.70633 -0.8649 -0.56118\nday 0.11626 0.53897 -0.39514 -0.26027 0.57706 -0.79198 -0.88374 0.30119\ncountry -0.13531 0.15485 -0.07309 0.034013 -0.054457 -0.20541 -0.60086 -0.22407\nunder 0.13721 -0.295 -0.05916 -0.59235 0.02301 0.21884 -0.34254 -0.70213\nsuch 0.61012 0.33512 -0.53499 0.36139 -0.39866 0.70627 -0.18699 -0.77246\nsecond -0.29809 0.28069 0.087102 0.54455 0.70003 0.44778 -0.72565 0.62309 \n</code></pre>\n<hr/>\n<p>I split the above embedding matrix and loaded only the <em>words</em> in <code>vocab</code> which will be our vocabulary and the corresponding vectors in <code>emb</code> array.</p>\n<pre><code class=\"python\">vocab = ['the','like','between','did','just','national','day','country','under','such','second']\n\nemb = np.array([[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862],\n   [0.36808, 0.20834, -0.22319, 0.046283, 0.20098, 0.27515, -0.77127, -0.76804],\n   [0.7503, 0.71623, -0.27033, 0.20059, -0.17008, 0.68568, -0.061672, -0.054638],\n   [0.042523, -0.21172, 0.044739, -0.19248, 0.26224, 0.0043991, -0.88195, 0.55184],\n   [0.17698, 0.065221, 0.28548, -0.4243, 0.7499, -0.14892, -0.66786, 0.11788],\n   [-1.1105, 0.94945, -0.17078, 0.93037, -0.2477, -0.70633, -0.8649, -0.56118],\n   [0.11626, 0.53897, -0.39514, -0.26027, 0.57706, -0.79198, -0.88374, 0.30119],\n   [-0.13531, 0.15485, -0.07309, 0.034013, -0.054457, -0.20541, -0.60086, -0.22407],\n   [ 0.13721, -0.295, -0.05916, -0.59235, 0.02301, 0.21884, -0.34254, -0.70213],\n   [ 0.61012, 0.33512, -0.53499, 0.36139, -0.39866, 0.70627, -0.18699, -0.77246 ],\n   [ -0.29809, 0.28069, 0.087102, 0.54455, 0.70003, 0.44778, -0.72565, 0.62309 ]])\n\n\nemb.shape\n# (11, 8)\n</code></pre>\n<hr/>\n<p><strong>Embedding Lookup in TensorFlow</strong></p>\n<p>Now we will see how can we perform <em>embedding lookup</em> for some arbitrary input sentence.</p>\n<pre><code class=\"python\">In [54]: from collections import OrderedDict\n\n# embedding as TF tensor (for now constant; could be tf.Variable() during training)\nIn [55]: tf_embedding = tf.constant(emb, dtype=tf.float32)\n\n# input for which we need the embedding\nIn [56]: input_str = \"like the country\"\n\n# build index based on our `vocabulary`\nIn [57]: word_to_idx = OrderedDict({w:vocab.index(w) for w in input_str.split() if w in vocab})\n\n# lookup in embedding matrix &amp; return the vectors for the input words\nIn [58]: tf.nn.embedding_lookup(tf_embedding, list(word_to_idx.values())).eval()\nOut[58]: \narray([[ 0.36807999,  0.20834   , -0.22318999,  0.046283  ,  0.20097999,\n         0.27515   , -0.77126998, -0.76804   ],\n       [ 0.41800001,  0.24968   , -0.41242   ,  0.1217    ,  0.34527001,\n        -0.044457  , -0.49687999, -0.17862   ],\n       [-0.13530999,  0.15485001, -0.07309   ,  0.034013  , -0.054457  ,\n        -0.20541   , -0.60086   , -0.22407   ]], dtype=float32)\n</code></pre>\n<p>Observe how we got the <em>embeddings</em> from our original embedding matrix (with words) using the <em>indices of words</em> in our vocabulary.</p>\n<p>Usually, such an embedding lookup is performed by the first layer (called <em>Embedding layer</em>) which then passes these embeddings to RNN/LSTM/GRU layers for further processing.</p>\n<hr/>\n<p><em>Side Note</em>: Usually the vocabulary will also have a special <code>unk</code> token. So, if a token from our input sentence is not present in our vocabulary, then the index corresponding to <strong><code>unk</code></strong> will be looked up in the embedding matrix.</p>\n<hr/>\n<p><strong>P.S.</strong> Note that <code>embedding_dimension</code> is a hyperparameter that one has to tune for their application but popular models like <strong><a href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"noreferrer\">Word2Vec</a></strong> and <strong><a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">GloVe</a></strong> uses <code>300</code> dimension vector for representing each word.</p>\n<p><strong>Bonus Reading</strong> <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\" rel=\"noreferrer\">word2vec skip-gram model</a></p>\n", "abstract": "Yes, the purpose of tf.nn.embedding_lookup() function is to perform a lookup in the embedding matrix and return the embeddings (or in simple terms the vector representation) of words. A simple embedding matrix (of shape: vocabulary_size x embedding_dimension) would look like below. (i.e. each word will be represented by a vector of numbers; hence the name word2vec) Embedding Matrix I split the above embedding matrix and loaded only the words in vocab which will be our vocabulary and the corresponding vectors in emb array. Embedding Lookup in TensorFlow Now we will see how can we perform embedding lookup for some arbitrary input sentence. Observe how we got the embeddings from our original embedding matrix (with words) using the indices of words in our vocabulary. Usually, such an embedding lookup is performed by the first layer (called Embedding layer) which then passes these embeddings to RNN/LSTM/GRU layers for further processing. Side Note: Usually the vocabulary will also have a special unk token. So, if a token from our input sentence is not present in our vocabulary, then the index corresponding to unk will be looked up in the embedding matrix. P.S. Note that embedding_dimension is a hyperparameter that one has to tune for their application but popular models like Word2Vec and GloVe uses 300 dimension vector for representing each word. Bonus Reading word2vec skip-gram model"}, {"id": 51694027, "score": 18, "vote": 0, "content": "<p>Here's an image depicting the process of embedding lookup. </p>\n<p><img alt=\"Image: Embedding lookup process\" src=\"https://i.stack.imgur.com/5wFii.jpg\"/>\n<br/>\n<br/>\nConcisely, it gets the corresponding rows of a embedding layer, specified by a list of IDs and provide that as a tensor. It is achieved through the following process.</p>\n<ol>\n<li>Define a placeholder <code>lookup_ids = tf.placeholder([10])</code></li>\n<li>Define a embedding layer <code>embeddings = tf.Variable([100,10],...)</code></li>\n<li>Define the tensorflow operation <code>embed_lookup = tf.embedding_lookup(embeddings, lookup_ids)</code></li>\n<li>Get the results by running <code>lookup = session.run(embed_lookup, feed_dict={lookup_ids:[95,4,14]})</code></li>\n</ol>\n", "abstract": "Here's an image depicting the process of embedding lookup.  \n\n\nConcisely, it gets the corresponding rows of a embedding layer, specified by a list of IDs and provide that as a tensor. It is achieved through the following process."}, {"id": 47482287, "score": 7, "vote": 0, "content": "<p>When the params tensor is in high dimensions, the ids only refers to top dimension. Maybe it's obvious to most of people but I have to run the following code to understand that:</p>\n<pre><code class=\"python\">embeddings = tf.constant([[[1,1],[2,2],[3,3],[4,4]],[[11,11],[12,12],[13,13],[14,14]],\n                          [[21,21],[22,22],[23,23],[24,24]]])\nids=tf.constant([0,2,1])\nembed = tf.nn.embedding_lookup(embeddings, ids, partition_strategy='div')\n\nwith tf.Session() as session:\n    result = session.run(embed)\n    print (result)\n</code></pre>\n<p>Just trying the 'div' strategy and for one tensor, it makes no difference.</p>\n<p>Here is the output:</p>\n<pre><code class=\"python\">[[[ 1  1]\n  [ 2  2]\n  [ 3  3]\n  [ 4  4]]\n\n [[21 21]\n  [22 22]\n  [23 23]\n  [24 24]]\n\n [[11 11]\n  [12 12]\n  [13 13]\n  [14 14]]]\n</code></pre>\n", "abstract": "When the params tensor is in high dimensions, the ids only refers to top dimension. Maybe it's obvious to most of people but I have to run the following code to understand that: Just trying the 'div' strategy and for one tensor, it makes no difference. Here is the output:"}, {"id": 46965588, "score": 3, "vote": 0, "content": "<p>Another way to look at it is , assume that you flatten out the tensors to one dimensional array, and then you are performing a lookup</p>\n<p>(eg) Tensor0=[1,2,3], Tensor1=[4,5,6], Tensor2=[7,8,9]</p>\n<p>The flattened out tensor will be as follows\n[1,4,7,2,5,8,3,6,9]</p>\n<p>Now when you do a lookup of [0,3,4,1,7] it will yeild [1,2,5,4,6]</p>\n<p>(i,e) if lookup value is 7 for example , and we have 3 tensors (or a tensor with 3 rows) then, </p>\n<p>7 / 3 : (Reminder is 1, Quotient is 2) So 2nd element of Tensor1 will be shown, which is 6 </p>\n", "abstract": "Another way to look at it is , assume that you flatten out the tensors to one dimensional array, and then you are performing a lookup (eg) Tensor0=[1,2,3], Tensor1=[4,5,6], Tensor2=[7,8,9] The flattened out tensor will be as follows\n[1,4,7,2,5,8,3,6,9] Now when you do a lookup of [0,3,4,1,7] it will yeild [1,2,5,4,6] (i,e) if lookup value is 7 for example , and we have 3 tensors (or a tensor with 3 rows) then,  7 / 3 : (Reminder is 1, Quotient is 2) So 2nd element of Tensor1 will be shown, which is 6 "}, {"id": 48572802, "score": 3, "vote": 0, "content": "<p>Since I was also intrigued by this function, I'll give my two cents.</p>\n<p>The way I see it in the 2D case is just as a matrix multiplication (it's easy to generalize to other dimensions).</p>\n<p>Consider a vocabulary with N symbols.\nThen, you can represent a symbol <strong><em>x</em></strong> as a vector of dimensions Nx1, one-hot-encoded.</p>\n<p>But you want a representation of this symbol not as a vector of Nx1, but as one with dimensions Mx1, called <strong><em>y</em></strong>.</p>\n<p>So, to transform <strong><em>x</em></strong> into <strong><em>y</em></strong>, you can use and embedding matrix <strong>E</strong>, with dimensions MxN: </p>\n<p><strong><em>y</em></strong> = <strong>E</strong> <strong><em>x</em></strong>.</p>\n<p>This is essentially what tf.nn.embedding_lookup(params, ids, ...) is doing, with the nuance that <em>ids</em> are just one number that represents the position of the 1 in the one-hot-encoded vector <strong><em>x</em></strong>.</p>\n", "abstract": "Since I was also intrigued by this function, I'll give my two cents. The way I see it in the 2D case is just as a matrix multiplication (it's easy to generalize to other dimensions). Consider a vocabulary with N symbols.\nThen, you can represent a symbol x as a vector of dimensions Nx1, one-hot-encoded. But you want a representation of this symbol not as a vector of Nx1, but as one with dimensions Mx1, called y. So, to transform x into y, you can use and embedding matrix E, with dimensions MxN:  y = E x. This is essentially what tf.nn.embedding_lookup(params, ids, ...) is doing, with the nuance that ids are just one number that represents the position of the 1 in the one-hot-encoded vector x."}, {"id": 45724451, "score": 0, "vote": 0, "content": "<p>Adding to Asher Stern's answer,\n <code>params</code> is\n  interpreted as a <strong>partitioning</strong> of a large embedding tensor. It can be a  single tensor representing the complete embedding tensor,\n      or a list of X tensors all of same shape except for the first dimension,\n      representing sharded embedding tensors. </p>\n<p>The function <code>tf.nn.embedding_lookup</code> is written considering the fact that embedding (params) will be large. Therefore we need <code>partition_strategy</code>.</p>\n", "abstract": "Adding to Asher Stern's answer,\n params is\n  interpreted as a partitioning of a large embedding tensor. It can be a  single tensor representing the complete embedding tensor,\n      or a list of X tensors all of same shape except for the first dimension,\n      representing sharded embedding tensors.  The function tf.nn.embedding_lookup is written considering the fact that embedding (params) will be large. Therefore we need partition_strategy."}, {"id": 73501463, "score": 0, "vote": 0, "content": "<p>The existing explanations are not enough.\nThe main purpose of this function is to efficiently retrieve the vectors for each word in a given sequence of word indices. Suppose we have the following matrix of embeddings:</p>\n<pre><code class=\"python\">embds = np.array([[0.2, 0.32,0.9],\n        [0.8, 0.62,0.19],\n        [0.0, -0.22,-1.9],\n        [1.2, 2.32,6.0],\n        [0.11, 0.10,5.9]])\n</code></pre>\n<p>Let's say we have the following sequences of word indices:</p>\n<pre><code class=\"python\">data=[[0,1],\n     [3,4]]\n</code></pre>\n<p>Now to get the corresponding embedding for each word in our data:</p>\n<pre><code class=\"python\">tf.nn.embedding_lookup(\n    embds, data\n)\n</code></pre>\n<p>out:</p>\n<pre><code class=\"python\">array([[[0.2 , 0.32, 0.9 ],\n        [0.8 , 0.62, 0.19]],\n\n       [[1.2 , 2.32, 6.  ],\n        [0.11, 0.1 , 5.9 ]]])&gt;\n</code></pre>\n<p><strong>Note</strong> If embds are not an array or tensor, the output will not be like this (I won't go into details). For example, if embds were a list, the output would be:</p>\n<pre><code class=\"python\">array([[0.2 , 0.32],\n       [0.8 , 0.62]], dtype=float32)&gt;\n</code></pre>\n", "abstract": "The existing explanations are not enough.\nThe main purpose of this function is to efficiently retrieve the vectors for each word in a given sequence of word indices. Suppose we have the following matrix of embeddings: Let's say we have the following sequences of word indices: Now to get the corresponding embedding for each word in our data: out: Note If embds are not an array or tensor, the output will not be like this (I won't go into details). For example, if embds were a list, the output would be:"}]}, {"link": "https://stackoverflow.com/questions/39142778/how-to-determine-the-language-of-a-piece-of-text", "question": {"id": "39142778", "title": "How to determine the language of a piece of text?", "content": "<p>I want to get this:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">Input text: \"\u0440\u0443\u0301\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u0301\u043a\"\nOutput text: \"Russian\" \n\nInput text: \"\u4e2d\u6587\"\nOutput text: \"Chinese\" \n\nInput text: \"\u306b\u307b\u3093\u3054\"\nOutput text: \"Japanese\" \n\nInput text: \"\u0627\u0644\u0639\u064e\u0631\u064e\u0628\u0650\u064a\u064e\u0651\u0629\"\nOutput text: \"Arabic\"\n</code></pre>\n<p>How can I do it in python?</p>\n", "abstract": "I want to get this: How can I do it in python?"}, "answers": [{"id": 47106810, "score": 344, "vote": 0, "content": "<h1>1. <a href=\"https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.BaseBlob.detect_language\" rel=\"noreferrer\">TextBlob</a>.</h1>\n<p>Requires NLTK package, uses Google.</p>\n<pre><code class=\"python\">from textblob import TextBlob\nb = TextBlob(\"bonjour\")\nb.detect_language()\n</code></pre>\n<p><code>pip install textblob</code></p>\n<p>Note: This solution requires internet access and Textblob is using <a href=\"https://github.com/sloria/TextBlob/blob/e733d597db9908f192c5f5cb472523ad4de7d6b9/textblob/translate.py\" rel=\"noreferrer\">Google Translate's language detector by calling the API</a>.</p>\n<h1>2. <a href=\"https://polyglot.readthedocs.io/en/latest/Installation.html\" rel=\"noreferrer\">Polyglot</a>.</h1>\n<p>Requires numpy and some arcane libraries, <s>unlikely to get it work for Windows</s>. (For Windows, get an appropriate versions of <strong>PyICU</strong>, <strong>Morfessor</strong> and <strong>PyCLD2</strong> from <a href=\"https://www.lfd.uci.edu/%7Egohlke/pythonlibs/\" rel=\"noreferrer\">here</a>, then just <code>pip install downloaded_wheel.whl</code>.) Able to detect texts with mixed languages.</p>\n<pre><code class=\"python\">from polyglot.detect import Detector\n\nmixed_text = u\"\"\"\nChina (simplified Chinese: \u4e2d\u56fd; traditional Chinese: \u4e2d\u570b),\nofficially the People's Republic of China (PRC), is a sovereign state\nlocated in East Asia.\n\"\"\"\nfor language in Detector(mixed_text).languages:\n        print(language)\n\n# name: English     code: en       confidence:  87.0 read bytes:  1154\n# name: Chinese     code: zh_Hant  confidence:   5.0 read bytes:  1755\n# name: un          code: un       confidence:   0.0 read bytes:     0\n</code></pre>\n<p><code>pip install polyglot</code></p>\n<p>To install the dependencies, run:\n<code>sudo apt-get install python-numpy libicu-dev</code></p>\n<p>Note: Polyglot is using <code>pycld2</code>, see <a href=\"https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72\" rel=\"noreferrer\">https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72</a> for details.</p>\n<h1>3. <a href=\"https://chardet.readthedocs.io/en/latest/usage.html\" rel=\"noreferrer\">chardet</a></h1>\n<p>Chardet has also a feature of detecting languages if there are character bytes in range (127-255]:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; chardet.detect(\"\u042f \u043b\u044e\u0431\u043b\u044e \u0432\u043a\u0443\u0441\u043d\u044b\u0435 \u043f\u0430\u043c\u043f\u0443\u0448\u043a\u0438\".encode('cp1251'))\n{'encoding': 'windows-1251', 'confidence': 0.9637267119204621, 'language': 'Russian'}\n</code></pre>\n<p><code>pip install chardet</code></p>\n<h1>4. <a href=\"https://pypi.python.org/pypi/langdetect?\" rel=\"noreferrer\">langdetect</a></h1>\n<p>Requires large portions of text. It uses non-deterministic approach under the hood. That means you get different results for the same text sample. Docs say you have to use following code to make it determined:</p>\n<pre><code class=\"python\">from langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\ndetect('\u4eca\u4e00\u306f\u304a\u524d\u3055\u3093')\n</code></pre>\n<p><code>pip install langdetect</code></p>\n<h1>5. <a href=\"https://bitbucket.org/spirit/guess_language\" rel=\"noreferrer\">guess_language</a></h1>\n<p>Can detect very short samples by using <a href=\"https://pythonhosted.org/pyenchant/\" rel=\"noreferrer\">this</a> spell checker with dictionaries.</p>\n<p><code>pip install guess_language-spirit</code></p>\n<h1>6. <a href=\"https://github.com/saffsd/langid.py\" rel=\"noreferrer\">langid</a></h1>\n<p>langid.py provides both a module</p>\n<pre><code class=\"python\">import langid\nlangid.classify(\"This is a test\")\n# ('en', -54.41310358047485)\n</code></pre>\n<p>and a command-line tool:</p>\n<pre><code class=\"python\">$ langid &lt; README.md\n</code></pre>\n<p><code>pip install langid</code></p>\n<h1>7. <a href=\"https://fasttext.cc\" rel=\"noreferrer\">FastText</a></h1>\n<p>FastText is a text classifier, can be used to recognize 176 languages with a proper <a href=\"https://fasttext.cc/docs/en/language-identification.html\" rel=\"noreferrer\">models for language classification</a>. Download <a href=\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\" rel=\"noreferrer\">this model</a>, then:</p>\n<pre><code class=\"python\">import fasttext\nmodel = fasttext.load_model('lid.176.ftz')\nprint(model.predict('\u0627\u0644\u0634\u0645\u0633 \u062a\u0634\u0631\u0642', k=2))  # top 2 matching languages\n\n(('__label__ar', '__label__fa'), array([0.98124713, 0.01265871]))\n</code></pre>\n<p><code>pip install fasttext</code></p>\n<h1>8. <a href=\"https://github.com/bsolomon1124/pycld3\" rel=\"noreferrer\">pyCLD3</a></h1>\n<p>pycld3 is a neural network model for language identification. This package contains the inference code and a trained model.</p>\n<pre><code class=\"python\">import cld3\ncld3.get_language(\"\u5f71\u97ff\u5305\u542b\u5c0d\u6c23\u5019\u7684\u8b8a\u5316\u4ee5\u53ca\u81ea\u7136\u8cc7\u6e90\u7684\u67af\u7aed\u7a0b\u5ea6\")\n\nLanguagePrediction(language='zh', probability=0.999969482421875, is_reliable=True, proportion=1.0)\n</code></pre>\n<p><code>pip install pycld3</code></p>\n", "abstract": "Requires NLTK package, uses Google. pip install textblob Note: This solution requires internet access and Textblob is using Google Translate's language detector by calling the API. Requires numpy and some arcane libraries, unlikely to get it work for Windows. (For Windows, get an appropriate versions of PyICU, Morfessor and PyCLD2 from here, then just pip install downloaded_wheel.whl.) Able to detect texts with mixed languages. pip install polyglot To install the dependencies, run:\nsudo apt-get install python-numpy libicu-dev Note: Polyglot is using pycld2, see https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72 for details. Chardet has also a feature of detecting languages if there are character bytes in range (127-255]: pip install chardet Requires large portions of text. It uses non-deterministic approach under the hood. That means you get different results for the same text sample. Docs say you have to use following code to make it determined: pip install langdetect Can detect very short samples by using this spell checker with dictionaries. pip install guess_language-spirit langid.py provides both a module and a command-line tool: pip install langid FastText is a text classifier, can be used to recognize 176 languages with a proper models for language classification. Download this model, then: pip install fasttext pycld3 is a neural network model for language identification. This package contains the inference code and a trained model. pip install pycld3"}, {"id": 39143059, "score": 78, "vote": 0, "content": "<p>Have you had a look at <a href=\"https://pypi.python.org/pypi/langdetect?\" rel=\"noreferrer\">langdetect</a>?</p>\n<pre><code class=\"python\">from langdetect import detect\n\nlang = detect(\"Ein, zwei, drei, vier\")\n\nprint lang\n#output: de\n</code></pre>\n", "abstract": "Have you had a look at langdetect?"}, {"id": 67238956, "score": 30, "vote": 0, "content": "<p>@Rabash had a good list of tools on <a href=\"https://stackoverflow.com/a/47106810/610569\">https://stackoverflow.com/a/47106810/610569</a></p>\n<p>And @toto_tico did a nice job in presenting the speed comparison.</p>\n<p>Here's a summary to complete the great answers above (as of 2021)</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Language ID software</th>\n<th style=\"text-align: left;\">Used by</th>\n<th style=\"text-align: center;\">Open Source / Model</th>\n<th style=\"text-align: center;\">Rule-based</th>\n<th style=\"text-align: center;\">Stats-based</th>\n<th style=\"text-align: center;\">Can train/tune</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://cloud.google.com/translate/docs/basic/detecting-language\" rel=\"noreferrer\">Google Translate Language Detection</a></td>\n<td style=\"text-align: left;\"><a href=\"https://github.com/sloria/TextBlob/blob/e733d597db9908f192c5f5cb472523ad4de7d6b9/textblob/translate.py\" rel=\"noreferrer\">TextBlob</a> (limited usage)</td>\n<td style=\"text-align: center;\">\u2715</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">-</td>\n<td style=\"text-align: center;\">\u2715</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://github.com/chardet/chardet\" rel=\"noreferrer\">Chardet</a></td>\n<td style=\"text-align: left;\">-</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2715</td>\n<td style=\"text-align: center;\">\u2715</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://pypi.org/project/guess-language/\" rel=\"noreferrer\">Guess Language</a> (non-active development)</td>\n<td style=\"text-align: left;\"><a href=\"https://github.com/alvations/spirit-guess\" rel=\"noreferrer\">spirit-guess</a> (updated rewrite)</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Minimally</td>\n<td style=\"text-align: center;\">\u2715</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://github.com/aboSamoor/pycld2\" rel=\"noreferrer\">pyCLD2</a></td>\n<td style=\"text-align: left;\"><a href=\"https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72\" rel=\"noreferrer\">Polyglot</a></td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Somewhat</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Not sure</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://github.com/bsolomon1124/pycld3\" rel=\"noreferrer\">CLD3</a></td>\n<td style=\"text-align: left;\">-</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2715</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Possibly</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://github.com/saffsd/langid.py\" rel=\"noreferrer\">langid-py</a></td>\n<td style=\"text-align: left;\">-</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Not sure</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2713</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://github.com/Mimino666/langdetect\" rel=\"noreferrer\">langdetect</a></td>\n<td style=\"text-align: left;\"><a href=\"https://github.com/Abhijit-2592/spacy-langdetect\" rel=\"noreferrer\">SpaCy-langdetect</a></td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2715</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2713</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><a href=\"https://fasttext.cc/docs/en/language-identification.html\" rel=\"noreferrer\">FastText</a></td>\n<td style=\"text-align: left;\"><a href=\"https://github.com/indix/whatthelang\" rel=\"noreferrer\">What The Lang</a></td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">\u2715</td>\n<td style=\"text-align: center;\">\u2713</td>\n<td style=\"text-align: center;\">Not sure</td>\n</tr>\n</tbody>\n</table>\n</div>", "abstract": "@Rabash had a good list of tools on https://stackoverflow.com/a/47106810/610569 And @toto_tico did a nice job in presenting the speed comparison. Here's a summary to complete the great answers above (as of 2021)"}, {"id": 62115619, "score": 27, "vote": 0, "content": "<p>If you are looking for <strong><em>a library that is fast with long texts</em></strong>, <code>polyglot</code> and <code>fastext</code> are doing the best job here.</p>\n<p>I sampled 10000 documents from a collection of dirty and random HTMLs, and here are the results:</p>\n<pre><code class=\"python\">+------------+----------+\n| Library    | Time     |\n+------------+----------+\n| polyglot   | 3.67 s   |\n+------------+----------+\n| fasttext   | 6.41     |\n+------------+----------+\n| cld3       | 14 s     |\n+------------+----------+\n| langid     | 1min 8s  |\n+------------+----------+\n| langdetect | 2min 53s |\n+------------+----------+\n| chardet    | 4min 36s |\n+------------+----------+\n</code></pre>\n<hr/>\n<p>I have noticed that a lot of the methods focus on short texts, probably because it is the hard problem to solve: if you have a lot of text, it is really easy to detect languages (e.g. one could just use a dictionary!). However, this makes it difficult to find for an easy and suitable method for long texts. </p>\n", "abstract": "If you are looking for a library that is fast with long texts, polyglot and fastext are doing the best job here. I sampled 10000 documents from a collection of dirty and random HTMLs, and here are the results: I have noticed that a lot of the methods focus on short texts, probably because it is the hard problem to solve: if you have a lot of text, it is really easy to detect languages (e.g. one could just use a dictionary!). However, this makes it difficult to find for an easy and suitable method for long texts. "}, {"id": 56118039, "score": 10, "vote": 0, "content": "<p>There is an issue with <code>langdetect</code> when it is being used for parallelization and it fails. But <code>spacy_langdetect</code> is a wrapper for that and you can use it for that purpose. You can use the following snippet as well:</p>\n<pre><code class=\"python\">import spacy\nfrom spacy_langdetect import LanguageDetector\n\nnlp = spacy.load(\"en\")\nnlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\ntext = \"This is English text Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los d\u00edas en el parque. Je m'appelle Ang\u00e9lica Summer, j'ai 12 ans et je suis canadienne.\"\ndoc = nlp(text)\n# document level language detection. Think of it like average language of document!\nprint(doc._.language['language'])\n# sentence level language detection\nfor i, sent in enumerate(doc.sents):\n    print(sent, sent._.language)\n</code></pre>\n", "abstract": "There is an issue with langdetect when it is being used for parallelization and it fails. But spacy_langdetect is a wrapper for that and you can use it for that purpose. You can use the following snippet as well:"}, {"id": 62942199, "score": 7, "vote": 0, "content": "<p>You can use <a href=\"https://github.com/ssut/py-googletrans#language-detection\" rel=\"noreferrer\">Googletrans</a> (unofficial) a free and unlimited Google translate API for Python.</p>\n<p>You can make as many requests as you want, there are no limits</p>\n<p><strong>Installation:</strong></p>\n<pre><code class=\"python\">$ pip install googletrans\n</code></pre>\n<p><strong>Language detection:</strong></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from googletrans import Translator\n&gt;&gt;&gt; t = Translator().detect(\"hello world!\")\n&gt;&gt;&gt; t.lang\n'en'\n&gt;&gt;&gt; t.confidence\n0.8225234\n</code></pre>\n", "abstract": "You can use Googletrans (unofficial) a free and unlimited Google translate API for Python. You can make as many requests as you want, there are no limits Installation: Language detection:"}, {"id": 58862145, "score": 2, "vote": 0, "content": "<h2>Pretrained Fast Text Model Worked Best For My Similar Needs</h2>\n<p>I arrived at your question with a very similar need. I found the most help from Rabash's answers for my specific needs.</p>\n<p>After experimenting to find what worked best among his recommendations, which was making sure that text files were in English in 60,000+ text files, I found that fasttext was an excellent tool for such a task.</p>\n<p>With a little work, I had a tool that worked very fast over many files. But it could be easily modified for something like your case, because fasttext works over a list of lines easily. </p>\n<p>My code with comments is among the answers on <a href=\"https://stackoverflow.com/questions/43377265/determine-if-text-is-in-english/58861912#58861912\">THIS</a> post. I believe that you and others can easily modify this code for other specific needs.</p>\n", "abstract": "I arrived at your question with a very similar need. I found the most help from Rabash's answers for my specific needs. After experimenting to find what worked best among his recommendations, which was making sure that text files were in English in 60,000+ text files, I found that fasttext was an excellent tool for such a task. With a little work, I had a tool that worked very fast over many files. But it could be easily modified for something like your case, because fasttext works over a list of lines easily.  My code with comments is among the answers on THIS post. I believe that you and others can easily modify this code for other specific needs."}, {"id": 59588429, "score": 2, "vote": 0, "content": "<p>Depending on the case, you might be interested in using one of the following methods:</p>\n<p><strong>Method 0: Use an API or library</strong></p>\n<ul>\n<li><a href=\"https://pypi.org/project/cld2-cffi/\" rel=\"nofollow noreferrer\">cld2-cffi</a></li>\n<li><a href=\"https://cloud.google.com/translate/docs/basic/detecting-language\" rel=\"nofollow noreferrer\">Google Cloud Translation - Basic (v2)</a></li>\n<li><a href=\"https://textblob.readthedocs.io/en/dev/\" rel=\"nofollow noreferrer\">TextBlob</a></li>\n<li><a href=\"https://github.com/Mimino666/langdetect\" rel=\"nofollow noreferrer\">langdetect</a></li>\n<li>etc.</li>\n</ul>\n<p>Usually, there are a few problems with these libraries because some of them are not accurate for small texts, some languages are missing, are slow, require internet connection, are non-free,... But generally speaking, they will suit most needs.</p>\n<p><strong>Method 1: Language models</strong></p>\n<p>A language model gives us the probability of a sequence of words. This is important because it allows us to robustly detect the language of a text, even when the text contains words in other languages (e.g.: <em>\"'Hola' means 'hello' in spanish\"</em>).</p>\n<p>You can use N language models (one per language), to score your text. The detected language will be the language of the model that gave you the highest score.</p>\n<p>If you want to build a simple language model for this, I'd go for 1-grams. To do this, you only need to count the number of times each word from a big text (e.g. Wikipedia Corpus in \"X\" language) has appeared.</p>\n<p>Then, the probability of a word will be its frequency divided by the total number of words analyzed (sum of all frequencies).</p>\n<pre><code class=\"python\">the 23135851162\nof  13151942776\nand 12997637966\nto  12136980858\na   9081174698\nin  8469404971\nfor 5933321709\n...\n\n=&gt; P(\"'Hola' means 'hello' in spanish\") = P(\"hola\") * P(\"means\") * P(\"hello\") * P(\"in\") * P(\"spanish\")\n</code></pre>\n<p>If the text to detect is quite big, I recommend sampling N random words and then use the sum of logarithms instead of multiplications to avoid floating-point precision problems.</p>\n<pre><code class=\"python\">P(s) = 0.03 * 0.01 * 0.014 = 0.0000042\nP(s) = log10(0.03) + log10(0.01) + log10(0.014) = -5.376\n</code></pre>\n<p><strong>Method 2: Intersecting sets</strong></p>\n<p>An even simpler approach is to prepare N sets (one per language) with the top M most frequent words. Then intersect your text with each set. The set with the highest number of intersections will be your detected language.</p>\n<pre><code class=\"python\">spanish_set = {\"de\", \"hola\", \"la\", \"casa\",...}\nenglish_set = {\"of\", \"hello\", \"the\", \"house\",...}\nczech_set = {\"z\", \"ahoj\", \"z\u00e1v\u011brky\", \"d\u016fm\",...}\n...\n\ntext_set = {\"hola\", \"means\", \"hello\", \"in\", \"spanish\"}\n\nspanish_votes = text_set.intersection(spanish_set)  # 1\nenglish_votes = text_set.intersection(english_set)  # 4\nczech_votes = text_set.intersection(czech_set)  # 0\n...\n</code></pre>\n<p><strong>Method 3: Zip compression</strong></p>\n<p>This more a curiosity than anything else, but here it goes... You can compress your text (e.g LZ77) and then measure the zip-distance with regards to a reference compressed text (target language). Personally, I didn't like it because it's slower, less accurate and less descriptive than other methods. Nevertheless, there might be interesting applications for this method. \nTo read more: <a href=\"https://arxiv.org/abs/cond-mat/0108530\" rel=\"nofollow noreferrer\">Language Trees and Zipping</a></p>\n", "abstract": "Depending on the case, you might be interested in using one of the following methods: Method 0: Use an API or library Usually, there are a few problems with these libraries because some of them are not accurate for small texts, some languages are missing, are slow, require internet connection, are non-free,... But generally speaking, they will suit most needs. Method 1: Language models A language model gives us the probability of a sequence of words. This is important because it allows us to robustly detect the language of a text, even when the text contains words in other languages (e.g.: \"'Hola' means 'hello' in spanish\"). You can use N language models (one per language), to score your text. The detected language will be the language of the model that gave you the highest score. If you want to build a simple language model for this, I'd go for 1-grams. To do this, you only need to count the number of times each word from a big text (e.g. Wikipedia Corpus in \"X\" language) has appeared. Then, the probability of a word will be its frequency divided by the total number of words analyzed (sum of all frequencies). If the text to detect is quite big, I recommend sampling N random words and then use the sum of logarithms instead of multiplications to avoid floating-point precision problems. Method 2: Intersecting sets An even simpler approach is to prepare N sets (one per language) with the top M most frequent words. Then intersect your text with each set. The set with the highest number of intersections will be your detected language. Method 3: Zip compression This more a curiosity than anything else, but here it goes... You can compress your text (e.g LZ77) and then measure the zip-distance with regards to a reference compressed text (target language). Personally, I didn't like it because it's slower, less accurate and less descriptive than other methods. Nevertheless, there might be interesting applications for this method. \nTo read more: Language Trees and Zipping"}, {"id": 73308182, "score": 2, "vote": 0, "content": "<p>I like the approach offered by TextBlob for language detection. Its quite simple and easy to implement and uses fewer lines of code. before you begin. you will need to install the textblob python library for the below code to work.</p>\n<pre><code class=\"python\">from textblob import TextBlob\ntext = \"\u044d\u0442\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0439 \u043f\u043e\u0440\u0442\u0430\u043b \u0434\u043b\u044f \u0433\u0438\u043a\u043e\u0432.\"\nlang = TextBlob(text)\nprint(lang.detect_language())\n</code></pre>\n<p>On the other hand, if you have a combination of various languages used, you might want to try pycld2 that allows language detection by defining parts of the sentence or paragraph with accuracy.</p>\n", "abstract": "I like the approach offered by TextBlob for language detection. Its quite simple and easy to implement and uses fewer lines of code. before you begin. you will need to install the textblob python library for the below code to work. On the other hand, if you have a combination of various languages used, you might want to try pycld2 that allows language detection by defining parts of the sentence or paragraph with accuracy."}, {"id": 39143700, "score": 1, "vote": 0, "content": "<p>You can try determining the Unicode group of chars in input string to point out type of language, (Cyrillic for Russian, for example), and then search for language-specific symbols in text.</p>\n", "abstract": "You can try determining the Unicode group of chars in input string to point out type of language, (Cyrillic for Russian, for example), and then search for language-specific symbols in text."}, {"id": 73233887, "score": 1, "vote": 0, "content": "<p>If the language you want to detect is among these...</p>\n<ul>\n<li>arabic (ar)</li>\n<li>bulgarian (bg)</li>\n<li>german (de)</li>\n<li>modern greek (el)</li>\n<li>english (en)</li>\n<li>spanish (es)</li>\n<li>french (fr)</li>\n<li>hindi (hi)</li>\n<li>italian (it)</li>\n<li>japanese (ja)</li>\n<li>dutch (nl)</li>\n<li>polish (pl)</li>\n<li>portuguese (pt)</li>\n<li>russian (ru)</li>\n<li>swahili (sw)</li>\n<li>thai (th)</li>\n<li>turkish (tr)</li>\n<li>urdu (ur)</li>\n<li>vietnamese (vi)</li>\n<li>chinese (zh)</li>\n</ul>\n<p>...then it is relatively easy with HuggingFace libraries and models (Deep Learning Natural Language Processing, if you are not familiar with it):</p>\n<pre><code class=\"python\"># Import libraries\nfrom transformers import pipeline\n# Load pipeline\nclassifier = pipeline(\"text-classification\", model = \"papluca/xlm-roberta-base-language-detection\")\n# Example sentence\nsentence1 = 'Ciao, come stai?'\n# Get language\nclassifier(sentence1)\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">[{'label': 'it', 'score': 0.9948362112045288}]\n</code></pre>\n<p><code>label</code> is the predicted language, and <code>score</code> is the assigned score to it: you can think of it as a confidence measure.\nSome details:</p>\n<blockquote>\n<p>The training set contains 70k samples, while the validation and test\nsets 10k each. The average accuracy on the test set is 99.6%</p>\n</blockquote>\n<p>You can finde more info <a href=\"https://huggingface.co/papluca/xlm-roberta-base-language-detection?text=I%20like%20you.%20I%20love%20you\" rel=\"nofollow noreferrer\">at the model's page</a>, and I suppose that you could find other models that fit you needs.</p>\n", "abstract": "If the language you want to detect is among these... ...then it is relatively easy with HuggingFace libraries and models (Deep Learning Natural Language Processing, if you are not familiar with it): Output: label is the predicted language, and score is the assigned score to it: you can think of it as a confidence measure.\nSome details: The training set contains 70k samples, while the validation and test\nsets 10k each. The average accuracy on the test set is 99.6% You can finde more info at the model's page, and I suppose that you could find other models that fit you needs."}, {"id": 64530175, "score": 0, "vote": 0, "content": "<p>I have tried all the libraries out there, and i concluded that pycld2 is the best one, fast and accurate.</p>\n<p>you can install it like this:</p>\n<pre><code class=\"python\">python -m pip install -U pycld2\n</code></pre>\n<p>you can use it like this:</p>\n<pre><code class=\"python\">isReliable, textBytesFound, details = cld2.detect(your_sentence)\n\nprint(isReliable, details[0][1]) # reliablity(bool),lang abbrev.(en/es/de...)   \n</code></pre>\n", "abstract": "I have tried all the libraries out there, and i concluded that pycld2 is the best one, fast and accurate. you can install it like this: you can use it like this:"}, {"id": 70666829, "score": 0, "vote": 0, "content": "<p>Polygot or Cld2 are among the best suggestions because they can detect multiple language in text. But, they are not easy to be installed on Windows because of \"building wheel fail\".</p>\n<p>A solution that worked for me ( I am using Windows 10 ) is installing <a href=\"https://github.com/GregBowyer/cld2-cffi\" rel=\"nofollow noreferrer\">CLD2-CFFI</a></p>\n<p>so first install cld2-cffi</p>\n<pre><code class=\"python\">pip install cld2-cffi\n</code></pre>\n<p>and then use it like this:</p>\n<pre><code class=\"python\">text_content = \"\"\" A acc\u00e8s aux chiens et aux frontaux qui lui ont \u00e9t\u00e9 il peut \nconsulter et modifier ses collections et exporter Cet article concerne le pays \neurop\u00e9en aujourd\u2019hui appel\u00e9 R\u00e9publique fran\u00e7aise. \nPour d\u2019autres usages du nom France, Pour une aide rapide et effective, veuiller \ntrouver votre aide dans le menu ci-dessus. \nWelcome, to this world of Data Scientist. Today is a lovely day.\"\"\"\n\nimport cld2\n\nisReliable, textBytesFound, details = cld2.detect(text_content)\nprint('  reliable: %s' % (isReliable != 0))\nprint('  textBytes: %s' % textBytesFound)\nprint('  details: %s' % str(details))\n</code></pre>\n<p>Th output is like this:</p>\n<pre><code class=\"python\">reliable: True\ntextBytes: 377\ndetails: (Detection(language_name='FRENCH', language_code='fr', percent=74, \nscore=1360.0), Detection(language_name='ENGLISH', language_code='en', \npercent=25, score=1141.0), Detection(language_name='Unknown', \nlanguage_code='un', percent=0, score=0.0))\n</code></pre>\n", "abstract": "Polygot or Cld2 are among the best suggestions because they can detect multiple language in text. But, they are not easy to be installed on Windows because of \"building wheel fail\". A solution that worked for me ( I am using Windows 10 ) is installing CLD2-CFFI so first install cld2-cffi and then use it like this: Th output is like this:"}, {"id": 73304367, "score": 0, "vote": 0, "content": "<p>You can install the <code>pycld2</code> python library</p>\n<pre><code class=\"python\">pip install pycld2\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">python -m pip install -U pycld2\n</code></pre>\n<p>for the below code to work.</p>\n<pre><code class=\"python\">import pycld2 as cld2\n\nisReliable, textBytesFound, details = cld2.detect(\n    \"\u0430 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0434\u043d \u043d\u0430\u0437\u0430\u0434\"\n)\n\nprint(isReliable)\n# True\ndetails[0]\n# ('RUSSIAN', 'ru', 98, 404.0)\n\nfr_en_Latn = \"\"\"\\\nFrance is the largest country in Western Europe and the third-largest in Europe as a whole.\nA acc\u00e8s aux chiens et aux frontaux qui lui ont \u00e9t\u00e9 il peut consulter et modifier ses collections\net exporter Cet article concerne le pays europ\u00e9en aujourd\u2019hui appel\u00e9 R\u00e9publique fran\u00e7aise.\nPour d\u2019autres usages du nom France, Pour une aide rapide et effective, veuiller trouver votre aide\ndans le menu ci-dessus.\nMotoring events began soon after the construction of the first successful gasoline-fueled automobiles.\nThe quick brown fox jumped over the lazy dog.\"\"\"\n\nisReliable, textBytesFound, details, vectors = cld2.detect(\n    fr_en_Latn, returnVectors=True\n)\nprint(vectors)\n# ((0, 94, 'ENGLISH', 'en'), (94, 329, 'FRENCH', 'fr'), (423, 139, 'ENGLISH', 'en'))\n</code></pre>\n<p>Pycld2 python library is a python binding for the Compact Language Detect 2 (CLD2). You can explore the different functionality of Pycld2. Know about the Pycld2 <a href=\"https://pypi.org/project/pycld2/\" rel=\"nofollow noreferrer\">here</a>.</p>\n", "abstract": "You can install the pycld2 python library or for the below code to work. Pycld2 python library is a python binding for the Compact Language Detect 2 (CLD2). You can explore the different functionality of Pycld2. Know about the Pycld2 here."}, {"id": 73770695, "score": 0, "vote": 0, "content": "<p>I would say <a href=\"https://github.com/pemistahl/lingua-py\" rel=\"nofollow noreferrer\">lingua.py</a> all the way. It is much faster and more accurate than <code>fasttext</code>. Definately deserves to be listed here.</p>\n<h2>Installation</h2>\n<pre><code class=\"python\">poety add lingua-language-detector\n</code></pre>\n<h2>Usage</h2>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from typing import List\nfrom lingua.language import Language\nfrom lingua.builder import LanguageDetectorBuilder\nlanguages: List[Language] = [Language.ENGLISH, Language.TURKISH, Language.PERSIAN]\ndetector = LanguageDetectorBuilder.from_languages(*languages).build()\n\nif __name__ == \"__main__\":\n    print(detector.detect_language_of(\"Ben de iyiyim. Tesekkurler.\")) # Language.TURKISH\n    print(detector.detect_language_of(\"I'm fine and you?\")) # Language.ENGLISH\n    print(detector.detect_language_of(\"\u062d\u0627\u0644 \u0645\u0646 \u062e\u0648\u0628\u0647\u061f \u0634\u0645\u0627 \u0686\u0637\u0648\u0631\u06cc\u062f\u061f\")) # Language.PERSIAN\n</code></pre>\n", "abstract": "I would say lingua.py all the way. It is much faster and more accurate than fasttext. Definately deserves to be listed here."}]}, {"link": "https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group", "question": {"id": "52455774", "title": "googletrans stopped working with error &#39;NoneType&#39; object has no attribute &#39;group&#39;", "content": "<p>I was trying <code>googletrans</code> and it was working quite well. Since this morning I started getting below error. I went through multiple posts from stackoverflow and other sites and found probably my ip is banned to use the service for sometime. I tried using multiple service provider internet that has different ip and stil facing the same issue ? I also tried to use <code>googletrans</code> on different laptops , still same issue ..Is <code>googletrans</code> package broken or something google did at their end ?</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from googletrans import Translator\n&gt;&gt;&gt; translator = Translator()\n&gt;&gt;&gt; translator.translate('\uc548\ub155\ud558\uc138\uc694.')\n\nTraceback (most recent call last):\n  File \"&lt;pyshell#2&gt;\", line 1, in &lt;module&gt;\n    translator.translate('\uc548\ub155\ud558\uc138\uc694.')\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 172, in translate\n    data = self._translate(text, dest, src)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 75, in _translate\n    token = self.token_acquirer.do(text)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 180, in do\n    self._update()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 59, in _update\n    code = unicode(self.RE_TKK.search(r.text).group(1)).replace('var ', '')\nAttributeError: 'NoneType' object has no attribute 'group'\n</code></pre>\n", "abstract": "I was trying googletrans and it was working quite well. Since this morning I started getting below error. I went through multiple posts from stackoverflow and other sites and found probably my ip is banned to use the service for sometime. I tried using multiple service provider internet that has different ip and stil facing the same issue ? I also tried to use googletrans on different laptops , still same issue ..Is googletrans package broken or something google did at their end ?"}, "answers": [{"id": 65109346, "score": 173, "vote": 0, "content": "<h3><strong>Update 06.12.20: A new 'official' alpha version of googletrans with a fix was released</strong></h3>\n<p>Install the alpha version like this:</p>\n<pre><code class=\"python\">pip install googletrans==3.1.0a0\n</code></pre>\n<p>Translation example:</p>\n<pre><code class=\"python\">translator = Translator()\ntranslation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\nprint(translation.text)\n#output: 'The sky is blue and I like bananas'\n</code></pre>\n<p>In case it does not work, try to specify the service url like this:</p>\n<pre><code class=\"python\">from googletrans import Translator\ntranslator = Translator(service_urls=['translate.googleapis.com'])\ntranslator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\n</code></pre>\n<p>See the discussion here for details and updates: <a href=\"https://github.com/ssut/py-googletrans/pull/237\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/pull/237</a></p>\n<h3><strong>Update 10.12.20: Another fix was released</strong></h3>\n<p>As pointed out by @DesiKeki and @Ahmed Breem, there is another fix which seems to work for several people:</p>\n<pre><code class=\"python\">pip install googletrans==4.0.0-rc1\n</code></pre>\n<p>Github discussion here: <a href=\"https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612</a></p>\n<h3>In case the fixes above don't work for you</h3>\n<p>If the above doesn't work for you, <code>google_trans_new</code> seems to be a good alternative that works for some people. It's unclear why the fix above works for some and doesn't for others. See details on installation and usage here: <a href=\"https://github.com/lushan88a/google_trans_new\" rel=\"noreferrer\">https://github.com/lushan88a/google_trans_new</a></p>\n<pre><code class=\"python\">#pip install google_trans_new\n\nfrom google_trans_new import google_translator  \ntranslator = google_translator()  \ntranslate_text = translator.translate('\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e08\u0e35\u0e19',lang_tgt='en')  \nprint(translate_text)\n#output: Hello china\n</code></pre>\n", "abstract": "Install the alpha version like this: Translation example: In case it does not work, try to specify the service url like this: See the discussion here for details and updates: https://github.com/ssut/py-googletrans/pull/237 As pointed out by @DesiKeki and @Ahmed Breem, there is another fix which seems to work for several people: Github discussion here: https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612 If the above doesn't work for you, google_trans_new seems to be a good alternative that works for some people. It's unclear why the fix above works for some and doesn't for others. See details on installation and usage here: https://github.com/lushan88a/google_trans_new"}, {"id": 52456197, "score": 64, "vote": 0, "content": "<p><strong>Update 01/12/2020:</strong> This issue re-emerged lately, (apparently) caused once again by some changes on the Google translation API.</p>\n<p>A solution is being discussed (again) in this <a href=\"https://github.com/ssut/py-googletrans/issues/234\" rel=\"noreferrer\">Github issue</a>. Although there is not a definitive solution yet a Pull Request seem to be solving the problem: <a href=\"https://github.com/ssut/py-googletrans/pull/237\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/pull/237</a>.</p>\n<p>While we wait for it to be approved it can be installed like this:</p>\n<pre><code class=\"python\">$ pip uninstall googletrans\n$ git clone https://github.com/alainrouillon/py-googletrans.git\n$ cd ./py-googletrans\n$ git checkout origin/feature/enhance-use-of-direct-api\n$ python setup.py install\n</code></pre>\n<p><strong>Original Answer</strong>:</p>\n<p>Apparently it's a recent and widespread problem on Google's side.\nQuoting various Github discussions, it happens when Google sends you directly the raw token.</p>\n<p>It's being discussed right now and there is already a pull request to fix it, so it should be resolved in the next few days.</p>\n<p>For reference, see:</p>\n<p><a href=\"https://github.com/ssut/py-googletrans/issues/48\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/issues/48</a> &lt;-- exact same problem reported on the Github repo\n<a href=\"https://github.com/pndurette/gTTS/issues/60\" rel=\"noreferrer\">https://github.com/pndurette/gTTS/issues/60</a> &lt;-- seemingly same problem on a text-to-speech library\n<a href=\"https://github.com/ssut/py-googletrans/pull/78\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/pull/78</a> &lt;-- pull request to fix the issue</p>\n<p>To apply this patch (without waiting for the pull request to be accepted) simply install the library from the forked repo <a href=\"https://github.com/BoseCorp/py-googletrans.git\" rel=\"noreferrer\">https://github.com/BoseCorp/py-googletrans.git</a> (uninstall the official library first):</p>\n<pre><code class=\"python\">$ pip uninstall googletrans\n$ git clone https://github.com/BoseCorp/py-googletrans.git\n$ cd ./py-googletrans\n$ python setup.py install\n</code></pre>\n<p>You can clone it anywhere on your system and install it globally or while inside a <code>virtualenv</code>.</p>\n", "abstract": "Update 01/12/2020: This issue re-emerged lately, (apparently) caused once again by some changes on the Google translation API. A solution is being discussed (again) in this Github issue. Although there is not a definitive solution yet a Pull Request seem to be solving the problem: https://github.com/ssut/py-googletrans/pull/237. While we wait for it to be approved it can be installed like this: Original Answer: Apparently it's a recent and widespread problem on Google's side.\nQuoting various Github discussions, it happens when Google sends you directly the raw token. It's being discussed right now and there is already a pull request to fix it, so it should be resolved in the next few days. For reference, see: https://github.com/ssut/py-googletrans/issues/48 <-- exact same problem reported on the Github repo\nhttps://github.com/pndurette/gTTS/issues/60 <-- seemingly same problem on a text-to-speech library\nhttps://github.com/ssut/py-googletrans/pull/78 <-- pull request to fix the issue To apply this patch (without waiting for the pull request to be accepted) simply install the library from the forked repo https://github.com/BoseCorp/py-googletrans.git (uninstall the official library first): You can clone it anywhere on your system and install it globally or while inside a virtualenv."}, {"id": 65113191, "score": 45, "vote": 0, "content": "<p>Try google_trans_new. It solved the problem for me\n<a href=\"https://github.com/lushan88a/google_trans_new\" rel=\"noreferrer\">https://github.com/lushan88a/google_trans_new</a></p>\n<blockquote>\n<p><code>pip install google_trans_new</code></p>\n</blockquote>\n<pre><code class=\"python\">from google_trans_new import google_translator  \n  \ntranslator = google_translator()  \ntranslate_text = translator.translate('Hola mundo!', lang_src='es', lang_tgt='en')  \nprint(translate_text)\n-&gt; Hello world!\n</code></pre>\n", "abstract": "Try google_trans_new. It solved the problem for me\nhttps://github.com/lushan88a/google_trans_new pip install google_trans_new"}, {"id": 69271089, "score": 18, "vote": 0, "content": "<p>Updated Answer as of 2021 Sept</p>\n<pre><code class=\"python\">pip uninstall googletrans==4.0.0-rc1\n\npip install googletrans==3.1.0a0\n</code></pre>\n<p>The 3.1.0a0 version works with bulk translation too!</p>\n", "abstract": "Updated Answer as of 2021 Sept The 3.1.0a0 version works with bulk translation too!"}, {"id": 65386909, "score": 17, "vote": 0, "content": "<p><strong>Update 10.12.20: New Alpha Version Release (Stable Release Candidate) is released: 4.0.0-rc1</strong></p>\n<p>It can be installed as follows:</p>\n<pre><code class=\"python\">pip install googletrans==4.0.0-rc1\n</code></pre>\n<p>Usage:</p>\n<pre><code class=\"python\">translation = translator.translate('\uc774 \ubb38\uc7a5\uc740 \ud55c\uae00\ub85c \uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.', dest='en')\nprint(translation.text)\n&gt;&gt;This sentence is written in Korean.\ndetected_lang = translator.detect('mein english me hindi likh raha hoon')\nprint(detected_lang)\n&gt;&gt;Detected(lang=hi, confidence=None)\ndetected_lang = translator.detect('\uc774 \ubb38\uc7a5\uc740 \ud55c\uae00\ub85c \uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.')\nprint(detected_lang)\n&gt;&gt;Detected(lang=ko, confidence=None)\n</code></pre>\n", "abstract": "Update 10.12.20: New Alpha Version Release (Stable Release Candidate) is released: 4.0.0-rc1 It can be installed as follows: Usage:"}, {"id": 52487148, "score": 11, "vote": 0, "content": "<p>Here is an unofficial fix to this problem as Darkblader24 stated in: <a href=\"https://github.com/ssut/py-googletrans/pull/78\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/pull/78</a></p>\n<p>Update gtoken.py like this: </p>\n<pre><code class=\"python\">    RE_TKK = re.compile(r'TKK=eval\\(\\'\\(\\(function\\(\\)\\{(.+?)\\}\\)\\(\\)\\)\\'\\);',\n                        re.DOTALL)\n    RE_RAWTKK = re.compile(r'TKK=\\'([^\\']*)\\';',re.DOTALL)\n\n    def __init__(self, tkk='0', session=None, host='translate.google.com'):\n        self.session = session or requests.Session()\n        self.tkk = tkk\n        self.host = host if 'http' in host else 'https://' + host\n\n    def _update(self):\n        \"\"\"update tkk\n        \"\"\"\n        # we don't need to update the base TKK value when it is still valid\n        now = math.floor(int(time.time() * 1000) / 3600000.0)\n        if self.tkk and int(self.tkk.split('.')[0]) == now:\n            return\n\n        r = self.session.get(self.host)\n\n        rawtkk = self.RE_RAWTKK.search(r.text)\n        if rawtkk:\n            self.tkk = rawtkk.group(1)\n            return\n</code></pre>\n", "abstract": "Here is an unofficial fix to this problem as Darkblader24 stated in: https://github.com/ssut/py-googletrans/pull/78 Update gtoken.py like this: "}, {"id": 67874518, "score": 11, "vote": 0, "content": "<p>By the time of this answer, you can solve it with the following:</p>\n<p>Uninstall your installed version of</p>\n<pre><code class=\"python\">pip uninstall googletrans\n</code></pre>\n<p>Install the following version</p>\n<pre><code class=\"python\">pip install googletrans==4.0.0rc1\n</code></pre>\n<p>I hope this will works for you as it worked for me.</p>\n<p>You can try it now:</p>\n<pre><code class=\"python\">from googletrans import Translator\ntranslator = Translator()\nar = translator.translate('\u0645\u0631\u062d\u0628\u0627').text\nprint(ar)\n</code></pre>\n", "abstract": "By the time of this answer, you can solve it with the following: Uninstall your installed version of Install the following version I hope this will works for you as it worked for me. You can try it now:"}, {"id": 65353677, "score": 7, "vote": 0, "content": "<p>This worked for me:</p>\n<pre><code class=\"python\">pip install googletrans==4.0.0-rc1\n</code></pre>\n<p>Original answer can be found here:\n<a href=\"https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612\" rel=\"noreferrer\">https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612</a></p>\n", "abstract": "This worked for me: Original answer can be found here:\nhttps://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612"}, {"id": 52885859, "score": 6, "vote": 0, "content": "<pre><code class=\"python\">pip uninstall googletrans googletrans-temp\npip install googletrans-temp\n</code></pre>\n<p>Worked for me in Win10 and Ubuntu 16 (Python 3.6) as of 2019.2.24 -- Refer to one of the replies in <a href=\"https://github.com/ssut/py-googletrans/issues/94\" rel=\"nofollow noreferrer\">https://github.com/ssut/py-googletrans/issues/94</a>. The old fix <code>pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade</code> does not work any more over here.</p>\n", "abstract": "Worked for me in Win10 and Ubuntu 16 (Python 3.6) as of 2019.2.24 -- Refer to one of the replies in https://github.com/ssut/py-googletrans/issues/94. The old fix pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade does not work any more over here."}, {"id": 53362009, "score": 5, "vote": 0, "content": "<p>Fixed is here <a href=\"https://pypi.org/project/py-translator/\" rel=\"noreferrer\">https://pypi.org/project/py-translator/</a></p>\n<p>$ pip3 install py_translator==1.8.9 </p>\n<pre><code class=\"python\">from py_translator import Translator\ns = Translator().translate(text='Hello my friend', dest='es').text\nprint(s)\n</code></pre>\n<blockquote>\n<p>out:Hola mi amigo</p>\n</blockquote>\n", "abstract": "Fixed is here https://pypi.org/project/py-translator/ $ pip3 install py_translator==1.8.9  out:Hola mi amigo"}, {"id": 72808431, "score": 5, "vote": 0, "content": "<p>Unfortunately, I could get neither <code>googletrans</code> nor <code>google_trans_new</code> to work, despite the many proposed fixes that are around.</p>\n<p>My solution was to switch to the <code>deep_translator</code> package:</p>\n<pre><code class=\"python\">pip install -U deep-translator\n</code></pre>\n<p>Then you can use it like this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from deep_translator import GoogleTranslator\n&gt;&gt;&gt; GoogleTranslator(source='auto', target='de').translate(\"keep it up, you are awesome\") \n'weiter so, du bist toll'\n</code></pre>\n<p>See <a href=\"https://deep-translator.readthedocs.io/en/latest/README.html\" rel=\"noreferrer\">documentation</a> for more info.</p>\n", "abstract": "Unfortunately, I could get neither googletrans nor google_trans_new to work, despite the many proposed fixes that are around. My solution was to switch to the deep_translator package: Then you can use it like this: See documentation for more info."}, {"id": 68471570, "score": 2, "vote": 0, "content": "<p>googletrans is not supported in latest python so you need to unistall it</p>\n<p>install new googletrans ( pip install googletrans==3.1.0a0)</p>\n", "abstract": "googletrans is not supported in latest python so you need to unistall it install new googletrans ( pip install googletrans==3.1.0a0)"}, {"id": 69548749, "score": 2, "vote": 0, "content": "<p>This is how I fixed my problem.</p>\n<pre><code class=\"python\">pip3 uninstall googletrans\npip3 install googletrans==3.1.0a0\n</code></pre>\n<p>First you need to uninstall the previous version and the install the 3.1.0 version.</p>\n", "abstract": "This is how I fixed my problem. First you need to uninstall the previous version and the install the 3.1.0 version."}, {"id": 71099343, "score": 2, "vote": 0, "content": "<h2>Use the translators package from <a href=\"https://pypi.org/project/translate-api/\" rel=\"nofollow noreferrer\">here</a></h2>\n<ol>\n<li>It works (;</li>\n<li>Supports more then google</li>\n</ol>\n<h2>Installation:</h2>\n<p><code>pip install translators --upgrade</code></p>\n<h2>Usage:</h2>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">\n    &gt;&gt;&gt; import translators as ts\n    Using Israel server backend.\n    &gt;&gt;&gt; ts.google('\u05e9\u05dc\u05d5\u05dd' , to_language = 'es')\n    'Hola'\n    \n\n</code></pre>\n", "abstract": "pip install translators --upgrade"}, {"id": 55402301, "score": 1, "vote": 0, "content": "<p>Making the following change to gtoken made it work for me:</p>\n<pre><code class=\"python\">RE_TKK = re.compile(r'tkk:\\'(.+?)\\'')      \n\ndef __init__(self, tkk='0', session=None, host='translate.google.com'):\n    self.session = session or requests.Session()\n    self.tkk = tkk\n    self.host = host if 'http' in host else 'https://' + host\n\ndef _update(self):\n    \"\"\"update tkk\n    \"\"\"\n    # we don't need to update the base TKK value when it is still valid\n    r = self.session.get(self.host)        \n\n    self.tkk = self.RE_TKK.findall(r.text)[0]\n\n    now = math.floor(int(time.time() * 1000) / 3600000.0)\n    if self.tkk and int(self.tkk.split('.')[0]) == now:\n        return\n\n    # this will be the same as python code after stripping out a reserved word 'var'\n    code = unicode(self.RE_TKK.search(r.text).group(1)).replace('var ', '')\n    # unescape special ascii characters such like a \\x3d(=)\n</code></pre>\n<p>I obtained this snippet from the ticket <a href=\"https://github.com/ssut/py-googletrans/pull/78\" rel=\"nofollow noreferrer\">here</a>. </p>\n<p>Note that this is slightly different from other change suggested earlier by Kerem. </p>\n<p>For other uninitiated folks like me, gtoken.py can be found within AppData\\Local\\Continuum\\anaconda3\\site-packages\\googletrans on a Windows machine using Anaconda. To find AppData, go into the address bar in file explorer, type '%AppData%', and hit Enter. </p>\n", "abstract": "Making the following change to gtoken made it work for me: I obtained this snippet from the ticket here.  Note that this is slightly different from other change suggested earlier by Kerem.  For other uninitiated folks like me, gtoken.py can be found within AppData\\Local\\Continuum\\anaconda3\\site-packages\\googletrans on a Windows machine using Anaconda. To find AppData, go into the address bar in file explorer, type '%AppData%', and hit Enter. "}, {"id": 61858275, "score": 1, "vote": 0, "content": "<p>It turns out putting the call whithin a try/except block solved the problem for me</p>\n<pre><code class=\"python\">try:\n    langs = translator.detect(update.message.text)\n    if langs.lang == 'en':\n        foo(translator.translate(update.message.text,dest='zh-cn').text)\n    else:\n        bar(translator.translate(update.message.text,dest='en').text)\nexcept Exception as e:\n    print(e)\n</code></pre>\n", "abstract": "It turns out putting the call whithin a try/except block solved the problem for me"}, {"id": 73748983, "score": 0, "vote": 0, "content": "<p>Try - pip install googletrans==3.1.0a0</p>\n", "abstract": "Try - pip install googletrans==3.1.0a0"}]}, {"link": "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer", "question": {"id": "15547409", "title": "How to get rid of punctuation using NLTK tokenizer?", "content": "<p>I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use <code>nltk.word_tokenize()</code>, I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also <code>word_tokenize</code> doesn't work with multiple sentences: dots are added to the last word.</p>\n", "abstract": "I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use nltk.word_tokenize(), I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also word_tokenize doesn't work with multiple sentences: dots are added to the last word."}, "answers": [{"id": 15555162, "score": 202, "vote": 0, "content": "<p>Take a look at the other tokenizing options that nltk provides <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">here</a>. For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else:</p>\n<pre><code class=\"python\">from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']\n</code></pre>\n", "abstract": "Take a look at the other tokenizing options that nltk provides here. For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else: Output:"}, {"id": 32684992, "score": 53, "vote": 0, "content": "<p>You do not really need NLTK to remove punctuation. You can remove it with simple python. For strings:</p>\n<pre><code class=\"python\">import string\ns = '... some string with punctuation ...'\ns = s.translate(None, string.punctuation)\n</code></pre>\n<p>Or for unicode:</p>\n<pre><code class=\"python\">import string\ntranslate_table = dict((ord(char), None) for char in string.punctuation)   \ns.translate(translate_table)\n</code></pre>\n<p>and then use this string in your tokenizer.</p>\n<p><strong>P.S.</strong> string module have some other sets of elements that can be removed (like digits).</p>\n", "abstract": "You do not really need NLTK to remove punctuation. You can remove it with simple python. For strings: Or for unicode: and then use this string in your tokenizer. P.S. string module have some other sets of elements that can be removed (like digits)."}, {"id": 41024113, "score": 38, "vote": 0, "content": "<p>Below code will remove all punctuation marks as well as non alphabetic characters. Copied from their book.</p>\n<p><a href=\"http://www.nltk.org/book/ch01.html\" rel=\"noreferrer\">http://www.nltk.org/book/ch01.html</a> </p>\n<pre><code class=\"python\">import nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time. @ sd  4 232\"\n\nwords = nltk.word_tokenize(s)\n\nwords=[word.lower() for word in words if word.isalpha()]\n\nprint(words)\n</code></pre>\n<p>output</p>\n<pre><code class=\"python\">['i', 'ca', 'do', 'this', 'now', 'because', 'i', 'so', 'tired', 'please', 'give', 'me', 'some', 'time', 'sd']\n</code></pre>\n", "abstract": "Below code will remove all punctuation marks as well as non alphabetic characters. Copied from their book. http://www.nltk.org/book/ch01.html  output"}, {"id": 15554045, "score": 18, "vote": 0, "content": "<p>As noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). </p>\n<pre><code class=\"python\">from nltk.tokenize import word_tokenize, sent_tokenize\n\ntext = '''It is a blue, small, and extraordinary ball. Like no other'''\ntokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\nprint filter(lambda word: word not in ',-', tokens)\n</code></pre>\n", "abstract": "As noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). "}, {"id": 56847275, "score": 13, "vote": 0, "content": "<p>Sincerely asking, what is a word? If your assumption is that a word consists of alphabetic characters only, you are wrong since words such as <code>can't</code> will be destroyed into pieces (such as <code>can</code> and <code>t</code>) <strong>if you remove punctuation before tokenisation</strong>, which is very likely to affect your program negatively.</p>\n<p>Hence the solution is to <strong>tokenise and then remove punctuation tokens</strong>.</p>\n<pre><code class=\"python\">import string\n\nfrom nltk.tokenize import word_tokenize\n\ntokens = word_tokenize(\"I'm a southern salesman.\")\n# ['I', \"'m\", 'a', 'southern', 'salesman', '.']\n\ntokens = list(filter(lambda token: token not in string.punctuation, tokens))\n# ['I', \"'m\", 'a', 'southern', 'salesman']\n</code></pre>\n<p>...and then if you wish, you can replace certain tokens such as <code>'m</code> with <code>am</code>.</p>\n", "abstract": "Sincerely asking, what is a word? If your assumption is that a word consists of alphabetic characters only, you are wrong since words such as can't will be destroyed into pieces (such as can and t) if you remove punctuation before tokenisation, which is very likely to affect your program negatively. Hence the solution is to tokenise and then remove punctuation tokens. ...and then if you wish, you can replace certain tokens such as 'm with am."}, {"id": 30180102, "score": 12, "vote": 0, "content": "<p>I just used the following code, which removed all the punctuation:</p>\n<pre><code class=\"python\">tokens = nltk.wordpunct_tokenize(raw)\n\ntype(tokens)\n\ntext = nltk.Text(tokens)\n\ntype(text)  \n\nwords = [w.lower() for w in text if w.isalpha()]\n</code></pre>\n", "abstract": "I just used the following code, which removed all the punctuation:"}, {"id": 38734861, "score": 6, "vote": 0, "content": "<p>I think you need some sort of regular expression matching (the following code is in Python 3):</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import string\nimport re\nimport nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time.\"\nl = nltk.word_tokenize(s)\nll = [x for x in l if not re.fullmatch('[' + string.punctuation + ']+', x)]\nprint(l)\nprint(ll)\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">['I', 'ca', \"n't\", 'do', 'this', 'now', ',', 'because', 'I', \"'m\", 'so', 'tired', '.', 'Please', 'give', 'me', 'some', 'time', '.']\n['I', 'ca', \"n't\", 'do', 'this', 'now', 'because', 'I', \"'m\", 'so', 'tired', 'Please', 'give', 'me', 'some', 'time']\n</code></pre>\n<p>Should work well in most cases since it removes punctuation while preserving tokens like \"n't\", which can't be obtained from regex tokenizers such as <code>wordpunct_tokenize</code>.</p>\n", "abstract": "I think you need some sort of regular expression matching (the following code is in Python 3): Output: Should work well in most cases since it removes punctuation while preserving tokens like \"n't\", which can't be obtained from regex tokenizers such as wordpunct_tokenize."}, {"id": 32404965, "score": 4, "vote": 0, "content": "<p>I use this code to remove punctuation:</p>\n<pre><code class=\"python\">import nltk\ndef getTerms(sentences):\n    tokens = nltk.word_tokenize(sentences)\n    words = [w.lower() for w in tokens if w.isalnum()]\n    print tokens\n    print words\n\ngetTerms(\"hh, hh3h. wo shi 2 4 A . fdffdf. A&amp;&amp;B \")\n</code></pre>\n<p>And If you want to check whether a token is a valid English word or not, you may need <a href=\"http://pythonhosted.org/pyenchant/\" rel=\"noreferrer\">PyEnchant</a></p>\n<p>Tutorial:</p>\n<pre><code class=\"python\"> import enchant\n d = enchant.Dict(\"en_US\")\n d.check(\"Hello\")\n d.check(\"Helo\")\n d.suggest(\"Helo\")\n</code></pre>\n", "abstract": "I use this code to remove punctuation: And If you want to check whether a token is a valid English word or not, you may need PyEnchant Tutorial:"}, {"id": 59026599, "score": 3, "vote": 0, "content": "<p>You can do it in one line without nltk (python 3.x).</p>\n<pre><code class=\"python\">import string\nstring_text= string_text.translate(str.maketrans('','',string.punctuation))\n</code></pre>\n", "abstract": "You can do it in one line without nltk (python 3.x)."}, {"id": 51772568, "score": 1, "vote": 0, "content": "<p>Remove punctuaion(It will remove . as well as part of punctuation handling using below code)</p>\n<pre><code class=\"python\">        tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n        text_string = text_string.translate(tbl) #text_string don't have punctuation\n        w = word_tokenize(text_string)  #now tokenize the string \n</code></pre>\n<p>Sample Input/Output:</p>\n<pre><code class=\"python\">direct flat in oberoi esquire. 3 bhk 2195 saleable 1330 carpet. rate of 14500 final plus 1% floor rise. tax approx 9% only. flat cost with parking 3.89 cr plus taxes plus possession charger. middle floor. north door. arey and oberoi woods facing. 53% paymemt due. 1% transfer charge with buyer. total cost around 4.20 cr approx plus possession charges. rahul soni\n</code></pre>\n<p><code>['direct', 'flat', 'oberoi', 'esquire', '3', 'bhk', '2195', 'saleable', '1330', 'carpet', 'rate', '14500', 'final', 'plus', '1', 'floor', 'rise', 'tax', 'approx', '9', 'flat', 'cost', 'parking', '389', 'cr', 'plus', 'taxes', 'plus', 'possession', 'charger', 'middle', 'floor', 'north', 'door', 'arey', 'oberoi', 'woods', 'facing', '53', 'paymemt', 'due', '1', 'transfer', 'charge', 'buyer', 'total', 'cost', 'around', '420', 'cr', 'approx', 'plus', 'possession', 'charges', 'rahul', 'soni']\n</code></p>\n", "abstract": "Remove punctuaion(It will remove . as well as part of punctuation handling using below code) Sample Input/Output: ['direct', 'flat', 'oberoi', 'esquire', '3', 'bhk', '2195', 'saleable', '1330', 'carpet', 'rate', '14500', 'final', 'plus', '1', 'floor', 'rise', 'tax', 'approx', '9', 'flat', 'cost', 'parking', '389', 'cr', 'plus', 'taxes', 'plus', 'possession', 'charger', 'middle', 'floor', 'north', 'door', 'arey', 'oberoi', 'woods', 'facing', '53', 'paymemt', 'due', '1', 'transfer', 'charge', 'buyer', 'total', 'cost', 'around', '420', 'cr', 'approx', 'plus', 'possession', 'charges', 'rahul', 'soni']\n"}, {"id": 57419516, "score": 1, "vote": 0, "content": "<p>Just adding to the solution by @rmalouf, this will not include any numbers because \\w+ is equivalent to [a-zA-Z0-9_]</p>\n<pre><code class=\"python\">from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'[a-zA-Z]')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n</code></pre>\n", "abstract": "Just adding to the solution by @rmalouf, this will not include any numbers because \\w+ is equivalent to [a-zA-Z0-9_]"}]}, {"link": "https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case", "question": {"id": "31421413", "title": "How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?", "content": "<p>I'm working in a sentiment analysis problem the data looks like this:</p>\n<pre><code class=\"python\">label instances\n    5    1190\n    4     838\n    3     239\n    1     204\n    2     127\n</code></pre>\n<p>So my data is unbalanced since 1190 <code>instances</code> are labeled with <code>5</code>. For the classification Im using scikit's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\" rel=\"noreferrer\">SVC</a>. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:</p>\n<p>First:</p>\n<pre><code class=\"python\">    wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})\n    wclf.fit(X, y)\n    weighted_prediction = wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, weighted_prediction)\nprint 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')\nprint 'Recall:', recall_score(y_test, weighted_prediction,\n                              average='weighted')\nprint 'Precision:', precision_score(y_test, weighted_prediction,\n                                    average='weighted')\nprint '\\n clasification report:\\n', classification_report(y_test, weighted_prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, weighted_prediction)\n</code></pre>\n<p>Second:</p>\n<pre><code class=\"python\">auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')\nauto_wclf.fit(X, y)\nauto_weighted_prediction = auto_wclf.predict(X_test)\n\nprint 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)\n\nprint 'F1 score:', f1_score(y_test, auto_weighted_prediction,\n                            average='weighted')\n\nprint 'Recall:', recall_score(y_test, auto_weighted_prediction,\n                              average='weighted')\n\nprint 'Precision:', precision_score(y_test, auto_weighted_prediction,\n                                    average='weighted')\n\nprint '\\n clasification report:\\n', classification_report(y_test,auto_weighted_prediction)\n\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, auto_weighted_prediction)\n</code></pre>\n<p>Third:</p>\n<pre><code class=\"python\">clf = SVC(kernel='linear', C= 1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\nprint 'Accuracy:', accuracy_score(y_test, prediction)\nprint 'F1 score:', f1_score(y_test, prediction)\nprint 'Recall:', recall_score(y_test, prediction)\nprint 'Precision:', precision_score(y_test, prediction)\nprint '\\n clasification report:\\n', classification_report(y_test,prediction)\nprint '\\n confussion matrix:\\n',confusion_matrix(y_test, prediction)\n\n\nF1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n  sample_weight=sample_weight)\n 0.930416613529\n</code></pre>\n<p>However, Im getting warnings like this:</p>\n<pre><code class=\"python\">/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:\nDeprecationWarning: The default `weighted` averaging is deprecated,\nand from version 0.18, use of precision, recall or F-score with \nmulticlass or multilabel data or pos_label=None will result in an \nexception. Please set an explicit value for `average`, one of (None, \n'micro', 'macro', 'weighted', 'samples'). In cross validation use, for \ninstance, scoring=\"f1_weighted\" instead of scoring=\"f1\"\n</code></pre>\n<p>How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?</p>\n", "abstract": "I'm working in a sentiment analysis problem the data looks like this: So my data is unbalanced since 1190 instances are labeled with 5. For the classification Im using scikit's SVC. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches: First: Second: Third: However, Im getting warnings like this: How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?"}, "answers": [{"id": 31575870, "score": 194, "vote": 0, "content": "<p>I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;).</p>\n<h2>Class weights</h2>\n<p>The weights from the <code>class_weight</code> parameter are used to <strong>train the classifier</strong>.\nThey <strong>are not used in the calculation of any of the metrics you are using</strong>: with different class weights, the numbers will be different simply because the classifier is different.</p>\n<p>Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.<br/>\nHow they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.</p>\n<h2>The metrics</h2>\n<p>Once you have a classifier, you want to know how well it is performing. \nHere you can use the metrics you mentioned: <code>accuracy</code>, <code>recall_score</code>, <code>f1_score</code>...</p>\n<p>Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class. </p>\n<p>I will not detail all these metrics but note that, with the exception of <code>accuracy</code>, they are naturally applied at the class level: as you can see in this <code>print</code> of a classification report they are defined for each class. They rely on concepts such as <code>true positives</code> or <code>false negative</code> that require defining which class is the <em>positive</em> one.</p>\n<pre><code class=\"python\">             precision    recall  f1-score   support\n\n          0       0.65      1.00      0.79        17\n          1       0.57      0.75      0.65        16\n          2       0.33      0.06      0.10        17\navg / total       0.52      0.60      0.51        50\n</code></pre>\n<h2>The warning</h2>\n<pre><code class=\"python\">F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The \ndefault `weighted` averaging is deprecated, and from version 0.18, \nuse of precision, recall or F-score with multiclass or multilabel data  \nor pos_label=None will result in an exception. Please set an explicit \nvalue for `average`, one of (None, 'micro', 'macro', 'weighted', \n'samples'). In cross validation use, for instance, \nscoring=\"f1_weighted\" instead of scoring=\"f1\".\n</code></pre>\n<p>You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!\nThe question could be rephrased: from the above classification report, how do you output <strong>one</strong> global number for the f1-score?\nYou could:</p>\n<ol>\n<li>Take the average of the f1-score for each class: that's the <code>avg / total</code> result above. It's also called <em>macro</em> averaging.</li>\n<li>Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka <em>micro</em> averaging.</li>\n<li>Compute a weighted average of the f1-score. Using <code>'weighted'</code> in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.</li>\n</ol>\n<p>These are 3 of the options in scikit-learn, the warning is there to say you <strong>have to pick one</strong>. So you have to specify an <code>average</code> argument for the score method.  </p>\n<p>Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.</p>\n<p>The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.</p>\n<h2>Computing scores</h2>\n<p>Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier <strong>has never seen</strong>. \nThis is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.</p>\n<p>Here's a way to do it using <code>StratifiedShuffleSplit</code>, which gives you a random splits of your data (after shuffling) that preserve the label distribution.</p>\n<pre><code class=\"python\">from sklearn.datasets import make_classification\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n# We use a utility to generate artificial classification data.\nX, y = make_classification(n_samples=100, n_informative=10, n_classes=3)\nsss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0)\nfor train_idx, test_idx in sss:\n    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    print(f1_score(y_test, y_pred, average=\"macro\"))\n    print(precision_score(y_test, y_pred, average=\"macro\"))\n    print(recall_score(y_test, y_pred, average=\"macro\"))    \n</code></pre>\n<p>Hope this helps.</p>\n", "abstract": "I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;). The weights from the class_weight parameter are used to train the classifier.\nThey are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different. Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.\nHow they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it. Once you have a classifier, you want to know how well it is performing. \nHere you can use the metrics you mentioned: accuracy, recall_score, f1_score... Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.  I will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one. You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!\nThe question could be rephrased: from the above classification report, how do you output one global number for the f1-score?\nYou could: These are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an average argument for the score method.   Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5. The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it. Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen. \nThis is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant. Here's a way to do it using StratifiedShuffleSplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution. Hope this helps."}, {"id": 31587532, "score": 93, "vote": 0, "content": "<p>Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns:</p>\n<ol>\n<li>How to I score a multiclass problem?</li>\n<li>How do I deal with unbalanced data?</li>\n</ol>\n<h2>1.</h2>\n<p>You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.:</p>\n<pre><code class=\"python\">from sklearn.metrics import precision_recall_fscore_support as score\n\npredicted = [1,2,3,4,5,1,2,1,1,4,5] \ny_test = [1,2,3,4,5,1,2,1,1,4,1]\n\nprecision, recall, fscore, support = score(y_test, predicted)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\n</code></pre>\n<p>This way you end up with tangible and interpretable numbers for each of the classes.</p>\n<pre><code class=\"python\">| Label | Precision | Recall | FScore | Support |\n|-------|-----------|--------|--------|---------|\n| 1     | 94%       | 83%    | 0.88   | 204     |\n| 2     | 71%       | 50%    | 0.54   | 127     |\n| ...   | ...       | ...    | ...    | ...     |\n| 4     | 80%       | 98%    | 0.89   | 838     |\n| 5     | 93%       | 81%    | 0.91   | 1190    |\n</code></pre>\n<p>Then...</p>\n<h2>2.</h2>\n<p>... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread.\nHowever, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing.</p>\n", "abstract": "Lot of very detailed answers here but I don't think you are answering the right questions. As I understand the question, there are two concerns: You can use most of the scoring functions in scikit-learn with both multiclass problem as with single class problems. Ex.: This way you end up with tangible and interpretable numbers for each of the classes. Then... ... you can tell if the unbalanced data is even a problem. If the scoring for the less represented classes (class 1 and 2) are lower than for the classes with more training samples (class 4 and 5) then you know that the unbalanced data is in fact a problem, and you can act accordingly, as described in some of the other answers in this thread.\nHowever, if the same class distribution is present in the data you want to predict on, your unbalanced training data is a good representative of the data, and hence, the unbalance is a good thing."}, {"id": 31570518, "score": 18, "vote": 0, "content": "<p><strong>Posed question</strong></p>\n<p>Responding to the question 'what metric should be used for multi-class classification with imbalanced data': Macro-F1-measure. \nMacro Precision and Macro Recall can be also used, but they are not so easily interpretable as for binary classificaion, they are already incorporated into F-measure, and excess metrics complicate methods comparison, parameters tuning, and so on. </p>\n<p>Micro averaging are sensitive to class imbalance: if your method, for example, works good for the most common labels and totally messes others, micro-averaged metrics show good results.</p>\n<p>Weighting averaging isn't well suited for imbalanced data, because it weights by counts of labels. Moreover, it is too hardly interpretable and unpopular: for instance, there is no mention of such an averaging in the following very detailed <a href=\"http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf\">survey</a> I strongly recommend to look through:</p>\n<blockquote>\n<p>Sokolova, Marina, and Guy Lapalme. \"A systematic analysis of\n  performance measures for classification tasks.\" Information Processing\n  &amp; Management 45.4 (2009): 427-437.</p>\n</blockquote>\n<p><strong>Application-specific question</strong></p>\n<p>However, returning to your task, I'd research 2 topics:</p>\n<ol>\n<li>metrics commonly used for your specific task - it lets (a) to\ncompare your method with others and understand if you do something\nwrong, and (b) to not explore this by yourself and reuse someone\nelse's findings; </li>\n<li>cost of different errors of your methods - for\nexample, use-case of your application may rely on 4- and 5-star\nreviewes only - in this case, good metric should count only these 2\nlabels.</li>\n</ol>\n<p><strong><em>Commonly used metrics.</em></strong>\nAs I can infer after looking through literature, there are 2 main evaluation metrics:</p>\n<ol>\n<li><strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">Accuracy</a></strong>, which is used, e.g. in </li>\n</ol>\n<blockquote>\n<p>Yu, April, and Daryl Chang. \"Multiclass Sentiment Prediction using\n  Yelp Business.\"</p>\n</blockquote>\n<p>(<a href=\"http://cs224d.stanford.edu/reports/YuApril.pdf\">link</a>) - note that the authors work with almost the same distribution of ratings, see Figure 5.</p>\n<blockquote>\n<p>Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class\n  relationships for sentiment categorization with respect to rating\n  scales.\" Proceedings of the 43rd Annual Meeting on Association for\n  Computational Linguistics. Association for Computational Linguistics,\n  2005.</p>\n</blockquote>\n<p>(<a href=\"http://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf\">link</a>)</p>\n<ol start=\"2\">\n<li><strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\">MSE</a></strong> (or, less often, Mean Absolute Error - <strong><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\">MAE</a></strong>) - see, for example,</li>\n</ol>\n<blockquote>\n<p>Lee, Moontae, and R. Grafe. \"Multiclass sentiment analysis with\n  restaurant reviews.\" Final Projects from CS N 224 (2010).</p>\n</blockquote>\n<p>(<a href=\"http://nlp.stanford.edu/courses/cs224n/2010/reports/pgrafe-moontae.pdf\">link</a>) - they explore both accuracy and MSE, considering the latter to be better</p>\n<blockquote>\n<p>Pappas, Nikolaos, Rue Marconi, and Andrei Popescu-Belis. \"Explaining\n  the Stars: Weighted Multiple-Instance Learning for Aspect-Based\n  Sentiment Analysis.\" Proceedings of the 2014 Conference on Empirical\n  Methods In Natural Language Processing. No. EPFL-CONF-200899. 2014.</p>\n</blockquote>\n<p>(<a href=\"http://www.aclweb.org/anthology/D14-1052\">link</a>) - they utilize scikit-learn for evaluation and baseline approaches and state that their code is available; however, I can't find it, so if you need it, write a letter to the authors, the work is pretty new and seems to be written in Python.</p>\n<p><strong><em>Cost of different errors</em>.</strong>\nIf you care more about avoiding gross blunders, e.g. assinging 1-star to 5-star review or something like that, look at MSE; \nif difference matters, but not so much, try MAE, since it doesn't square diff; \notherwise stay with Accuracy.</p>\n<p><strong>About approaches, not metrics</strong></p>\n<p>Try regression approaches, e.g. <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\">SVR</a>, since they generally outperforms Multiclass classifiers like SVC or OVA SVM.</p>\n", "abstract": "Posed question Responding to the question 'what metric should be used for multi-class classification with imbalanced data': Macro-F1-measure. \nMacro Precision and Macro Recall can be also used, but they are not so easily interpretable as for binary classificaion, they are already incorporated into F-measure, and excess metrics complicate methods comparison, parameters tuning, and so on.  Micro averaging are sensitive to class imbalance: if your method, for example, works good for the most common labels and totally messes others, micro-averaged metrics show good results. Weighting averaging isn't well suited for imbalanced data, because it weights by counts of labels. Moreover, it is too hardly interpretable and unpopular: for instance, there is no mention of such an averaging in the following very detailed survey I strongly recommend to look through: Sokolova, Marina, and Guy Lapalme. \"A systematic analysis of\n  performance measures for classification tasks.\" Information Processing\n  & Management 45.4 (2009): 427-437. Application-specific question However, returning to your task, I'd research 2 topics: Commonly used metrics.\nAs I can infer after looking through literature, there are 2 main evaluation metrics: Yu, April, and Daryl Chang. \"Multiclass Sentiment Prediction using\n  Yelp Business.\" (link) - note that the authors work with almost the same distribution of ratings, see Figure 5. Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class\n  relationships for sentiment categorization with respect to rating\n  scales.\" Proceedings of the 43rd Annual Meeting on Association for\n  Computational Linguistics. Association for Computational Linguistics,\n  2005. (link) Lee, Moontae, and R. Grafe. \"Multiclass sentiment analysis with\n  restaurant reviews.\" Final Projects from CS N 224 (2010). (link) - they explore both accuracy and MSE, considering the latter to be better Pappas, Nikolaos, Rue Marconi, and Andrei Popescu-Belis. \"Explaining\n  the Stars: Weighted Multiple-Instance Learning for Aspect-Based\n  Sentiment Analysis.\" Proceedings of the 2014 Conference on Empirical\n  Methods In Natural Language Processing. No. EPFL-CONF-200899. 2014. (link) - they utilize scikit-learn for evaluation and baseline approaches and state that their code is available; however, I can't find it, so if you need it, write a letter to the authors, the work is pretty new and seems to be written in Python. Cost of different errors.\nIf you care more about avoiding gross blunders, e.g. assinging 1-star to 5-star review or something like that, look at MSE; \nif difference matters, but not so much, try MAE, since it doesn't square diff; \notherwise stay with Accuracy. About approaches, not metrics Try regression approaches, e.g. SVR, since they generally outperforms Multiclass classifiers like SVC or OVA SVM."}, {"id": 31558398, "score": 15, "vote": 0, "content": "<p>First of all it's a little bit harder using just counting analysis to tell if your data is unbalanced or not. For example: 1 in 1000 positive observation is just a noise, error or a breakthrough in science? You never know.<br/>\nSo it's always better to use all your available knowledge and choice its status with all wise.</p>\n<p><strong>Okay, what if it's really unbalanced?</strong><br/>\nOnce again \u2014 look to your data. Sometimes you can find one or two observation multiplied by hundred times. Sometimes it's useful to create this fake one-class-observations.<br/>\nIf all the data is clean next step is to use class weights in prediction model.</p>\n<p><strong>So what about multiclass metrics?</strong><br/>\nIn my experience none of your metrics is usually used. There are two main reasons.<br/>\nFirst: it's always better to work with probabilities than with solid prediction (because how else could you separate models with 0.9 and 0.6 prediction if they both give you the same class?)<br/>\nAnd second: it's much easier to compare your prediction models and build new ones depending on only one good metric.<br/>\nFrom my experience I could recommend <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\" rel=\"noreferrer\">logloss</a> or <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\" rel=\"noreferrer\">MSE</a> (or just mean squared error).</p>\n<p><strong>How to fix sklearn warnings?</strong><br/>\nJust simply (as yangjie noticed) overwrite <code>average</code> parameter with one of these \nvalues: <code>'micro'</code> (calculate metrics globally), <code>'macro'</code> (calculate metrics for each label) or <code>'weighted'</code> (same as macro but with auto weights).</p>\n<pre><code class=\"python\">f1_score(y_test, prediction, average='weighted')\n</code></pre>\n<p>All your Warnings came after calling metrics functions with default <code>average</code> value <code>'binary'</code> which is inappropriate for multiclass prediction.<br/>\nGood luck and have fun with machine learning!</p>\n<p><strong>Edit:</strong><br/>\nI found another answerer recommendation to switch to regression approaches (e.g. SVR) with which I cannot agree. As far as I remember there is no even such a thing as multiclass regression. Yes there is multilabel regression which is far different and yes it's possible in some cases switch between regression and classification (if classes somehow sorted) but it pretty rare.</p>\n<p>What I would recommend (in scope of scikit-learn) is to try another very powerful classification tools: <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\" rel=\"noreferrer\">gradient boosting</a>, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">random forest</a> (my favorite), <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" rel=\"noreferrer\">KNeighbors</a> and many more.</p>\n<p>After that you can calculate arithmetic or geometric mean between predictions and most of the time you'll get even better result.</p>\n<pre><code class=\"python\">final_prediction = (KNNprediction * RFprediction) ** 0.5\n</code></pre>\n", "abstract": "First of all it's a little bit harder using just counting analysis to tell if your data is unbalanced or not. For example: 1 in 1000 positive observation is just a noise, error or a breakthrough in science? You never know.\nSo it's always better to use all your available knowledge and choice its status with all wise. Okay, what if it's really unbalanced?\nOnce again \u2014 look to your data. Sometimes you can find one or two observation multiplied by hundred times. Sometimes it's useful to create this fake one-class-observations.\nIf all the data is clean next step is to use class weights in prediction model. So what about multiclass metrics?\nIn my experience none of your metrics is usually used. There are two main reasons.\nFirst: it's always better to work with probabilities than with solid prediction (because how else could you separate models with 0.9 and 0.6 prediction if they both give you the same class?)\nAnd second: it's much easier to compare your prediction models and build new ones depending on only one good metric.\nFrom my experience I could recommend logloss or MSE (or just mean squared error). How to fix sklearn warnings?\nJust simply (as yangjie noticed) overwrite average parameter with one of these \nvalues: 'micro' (calculate metrics globally), 'macro' (calculate metrics for each label) or 'weighted' (same as macro but with auto weights). All your Warnings came after calling metrics functions with default average value 'binary' which is inappropriate for multiclass prediction.\nGood luck and have fun with machine learning! Edit:\nI found another answerer recommendation to switch to regression approaches (e.g. SVR) with which I cannot agree. As far as I remember there is no even such a thing as multiclass regression. Yes there is multilabel regression which is far different and yes it's possible in some cases switch between regression and classification (if classes somehow sorted) but it pretty rare. What I would recommend (in scope of scikit-learn) is to try another very powerful classification tools: gradient boosting, random forest (my favorite), KNeighbors and many more. After that you can calculate arithmetic or geometric mean between predictions and most of the time you'll get even better result."}]}, {"link": "https://stackoverflow.com/questions/22904025/java-or-python-for-natural-language-processing", "question": {"id": "22904025", "title": "Java or Python for Natural Language Processing", "content": "<p>I would like to know which programming language is better for natural language processing. <em>Java</em> or <em>Python</em>? I have found lots of questions and answers regarding about it. But I am still lost in choosing which one to use.</p>\n<p>And I want to know which NLP library to use for Java since there are lots of libraries (LingPipe, GATE, OpenNLP, StandfordNLP). For Python, most programmers recommend NLTK.</p>\n<p>But if I am to do some text processing or information extraction from <strong>unstructured data</strong> (just free formed plain English text) to get some useful information, what is the best option? Java or Python? Suitable library?</p>\n<p><strong>Updated</strong></p>\n<p>What I want to do is to extract useful product information from unstructured data (E.g. users make different forms of advertisement about mobiles or laptops with not very standard English language)</p>\n", "abstract": "I would like to know which programming language is better for natural language processing. Java or Python? I have found lots of questions and answers regarding about it. But I am still lost in choosing which one to use. And I want to know which NLP library to use for Java since there are lots of libraries (LingPipe, GATE, OpenNLP, StandfordNLP). For Python, most programmers recommend NLTK. But if I am to do some text processing or information extraction from unstructured data (just free formed plain English text) to get some useful information, what is the best option? Java or Python? Suitable library? Updated What I want to do is to extract useful product information from unstructured data (E.g. users make different forms of advertisement about mobiles or laptops with not very standard English language)"}, "answers": [{"id": 22905260, "score": 142, "vote": 0, "content": "<p>Java vs Python for NLP is very much a preference or necessity. Depending on the company/projects you'll need to use one or the other and often there isn't much of a choice unless you're heading a project.</p>\n<p>Other than <strong><code>NLTK</code></strong> (www.nltk.org), there are actually other libraries for text processing in <strong><code>python</code></strong>:</p>\n<ul>\n<li><strong>TextBlob</strong>: <a href=\"http://textblob.readthedocs.org/en/dev/\" rel=\"noreferrer\">http://textblob.readthedocs.org/en/dev/</a></li>\n<li><strong>Gensim</strong>: <a href=\"http://radimrehurek.com/gensim/\" rel=\"noreferrer\">http://radimrehurek.com/gensim/</a></li>\n<li><strong>Pattern</strong>: <a href=\"http://www.clips.ua.ac.be/pattern\" rel=\"noreferrer\">http://www.clips.ua.ac.be/pattern</a></li>\n<li><strong>Spacy</strong>:: <a href=\"http://spacy.io\" rel=\"noreferrer\">http://spacy.io</a></li>\n<li><strong>Orange</strong>: <a href=\"http://orange.biolab.si/features/\" rel=\"noreferrer\">http://orange.biolab.si/features/</a></li>\n<li><strong>Pineapple</strong>: <a href=\"https://github.com/proycon/pynlpl\" rel=\"noreferrer\">https://github.com/proycon/pynlpl</a></li>\n</ul>\n<p>(for more, see <a href=\"https://pypi.python.org/pypi?%3Aaction=search&amp;term=natural+language+processing&amp;submit=search\" rel=\"noreferrer\">https://pypi.python.org/pypi?%3Aaction=search&amp;term=natural+language+processing&amp;submit=search</a>)</p>\n<p>For <strong><code>Java</code></strong>, there're tonnes of others but here's another list:</p>\n<ul>\n<li><strong>Freeling</strong>: <a href=\"http://nlp.lsi.upc.edu/freeling/\" rel=\"noreferrer\">http://nlp.lsi.upc.edu/freeling/</a></li>\n<li><strong>OpenNLP</strong>: <a href=\"http://opennlp.apache.org/\" rel=\"noreferrer\">http://opennlp.apache.org/</a></li>\n<li><strong>LingPipe</strong>: <a href=\"http://alias-i.com/lingpipe/\" rel=\"noreferrer\">http://alias-i.com/lingpipe/</a></li>\n<li><strong>Stanford CoreNLP</strong>: <a href=\"http://stanfordnlp.github.io/CoreNLP/\" rel=\"noreferrer\">http://stanfordnlp.github.io/CoreNLP/</a> (comes with wrappers for other languages, python included)</li>\n<li><strong>CogComp NLP</strong>: <a href=\"https://github.com/CogComp/cogcomp-nlp\" rel=\"noreferrer\">https://github.com/CogComp/cogcomp-nlp</a></li>\n</ul>\n<p>This is a nice comparison for basic string processing, see <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/nlp-python.html\" rel=\"noreferrer\">http://nltk.googlecode.com/svn/trunk/doc/howto/nlp-python.html</a></p>\n<p>A useful comparison of GATE vs UIMA vs OpenNLP, see <a href=\"https://www.assembla.com/spaces/extraction-of-cost-data/wiki/Gate-vs-UIMA-vs-OpenNLP?version=4\" rel=\"noreferrer\">https://www.assembla.com/spaces/extraction-of-cost-data/wiki/Gate-vs-UIMA-vs-OpenNLP?version=4</a></p>\n<p>If you're uncertain, which is the language to go for NLP, personally i say, \"any language that will give you the desired analysis/output\", see <a href=\"https://stackoverflow.com/questions/18558665/which-language-or-tools-to-learn-for-natural-language-processing?rq=1\">Which language or tools to learn for natural language processing?</a></p>\n<p>Here's a pretty recent (2017) of NLP tools: <a href=\"https://github.com/alvations/awesome-community-curated-nlp\" rel=\"noreferrer\">https://github.com/alvations/awesome-community-curated-nlp</a></p>\n<p>An older list of NLP tools (2013): <a href=\"http://web.archive.org/web/20130703190201/http://yauhenklimovich.wordpress.com/2013/05/20/tools-nlp\" rel=\"noreferrer\">http://web.archive.org/web/20130703190201/http://yauhenklimovich.wordpress.com/2013/05/20/tools-nlp</a></p>\n<hr/>\n<p>Other than language processing tools, you would very much need <strong><code>machine learning</code></strong> tools to incorporate into <code>NLP</code> pipelines. </p>\n<p>There's a whole range in <code>Python</code> and <code>Java</code>, and once again it's up to preference and whether the libraries are user-friendly enough:</p>\n<p>Machine Learning libraries in python:</p>\n<ul>\n<li><strong>Sklearn</strong> (Scikit-learn): <a href=\"http://scikit-learn.org/stable/\" rel=\"noreferrer\">http://scikit-learn.org/stable/</a></li>\n<li><strong>Milk</strong>: <a href=\"http://luispedro.org/software/milk\" rel=\"noreferrer\">http://luispedro.org/software/milk</a></li>\n<li><strong>Scipy</strong>: <a href=\"http://www.scipy.org/\" rel=\"noreferrer\">http://www.scipy.org/</a></li>\n<li><strong>Theano</strong>: <a href=\"http://deeplearning.net/software/theano/\" rel=\"noreferrer\">http://deeplearning.net/software/theano/</a></li>\n<li><strong>PyML</strong>: <a href=\"http://pyml.sourceforge.net/\" rel=\"noreferrer\">http://pyml.sourceforge.net/</a></li>\n<li><strong>pyBrain</strong>: <a href=\"http://pybrain.org/\" rel=\"noreferrer\">http://pybrain.org/</a></li>\n<li><strong>Graphlab Create</strong> (Commerical tool but free academic license for 1 year): <a href=\"https://dato.com/products/create/\" rel=\"noreferrer\">https://dato.com/products/create/</a></li>\n</ul>\n<p>(for more, see <a href=\"https://pypi.python.org/pypi?%3Aaction=search&amp;term=machine+learning&amp;submit=search\" rel=\"noreferrer\">https://pypi.python.org/pypi?%3Aaction=search&amp;term=machine+learning&amp;submit=search</a>)</p>\n<ul>\n<li><strong>Weka</strong>: <a href=\"http://www.cs.waikato.ac.nz/ml/weka/index.html\" rel=\"noreferrer\">http://www.cs.waikato.ac.nz/ml/weka/index.html</a></li>\n<li><strong>Mallet</strong>: <a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">http://mallet.cs.umass.edu/</a></li>\n<li><strong>Mahout</strong>: <a href=\"https://mahout.apache.org/\" rel=\"noreferrer\">https://mahout.apache.org/</a></li>\n</ul>\n<hr/>\n<p>With the recent (2015) <a href=\"http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239\" rel=\"noreferrer\">deep learning tsunami in NLP</a>, possibly you could consider: <a href=\"https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a></p>\n<p>I'll avoid listing deep learning tools out of non-favoritism / neutrality. </p>\n<hr/>\n<p>Other Stackoverflow questions that also asked for NLP/ML tools:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/2233435/machine-learning-and-natural-language-processing\">Machine Learning and Natural Language Processing</a></li>\n<li><a href=\"https://stackoverflow.com/questions/212219/what-are-good-starting-points-for-someone-interested-in-natural-language-process\">What are good starting points for someone interested in natural language processing?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/4115526/natural-language-processing\">Natural language processing</a></li>\n<li><a href=\"https://stackoverflow.com/questions/11116390/natural-language-processing-in-java-nlp\">Natural Language Processing in Java (NLP)</a></li>\n<li><a href=\"https://stackoverflow.com/questions/870460/is-there-a-good-natural-language-processing-library\">Is there a good natural language processing library</a></li>\n<li><a href=\"https://stackoverflow.com/questions/5833030/simple-natural-language-processing-startup-for-java\">Simple Natural Language Processing Startup for Java</a></li>\n<li><a href=\"https://stackoverflow.com/questions/11849134/what-libraries-offer-basic-or-advanced-nlp-methods\">What libraries offer basic or advanced NLP methods?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/3886913/latest-good-languages-and-books-for-natural-language-processing-the-basics?rq=1\">Latest good languages and books for Natural Language Processing, the basics</a></li>\n<li>(For NER) <a href=\"https://stackoverflow.com/questions/7455188/entity-extraction-recognition-with-free-tools-while-feeding-lucene-index\">Entity Extraction/Recognition with free tools while feeding Lucene Index</a></li>\n<li>(With PHP) <a href=\"https://stackoverflow.com/questions/4457830/nlp-programming-tools-using-php\">NLP programming tools using PHP?</a></li>\n<li>(With Ruby) <a href=\"https://stackoverflow.com/questions/3776361/ruby-nlp-libraries\">https://stackoverflow.com/questions/3776361/ruby-nlp-libraries</a></li>\n</ul>\n", "abstract": "Java vs Python for NLP is very much a preference or necessity. Depending on the company/projects you'll need to use one or the other and often there isn't much of a choice unless you're heading a project. Other than NLTK (www.nltk.org), there are actually other libraries for text processing in python: (for more, see https://pypi.python.org/pypi?%3Aaction=search&term=natural+language+processing&submit=search) For Java, there're tonnes of others but here's another list: This is a nice comparison for basic string processing, see http://nltk.googlecode.com/svn/trunk/doc/howto/nlp-python.html A useful comparison of GATE vs UIMA vs OpenNLP, see https://www.assembla.com/spaces/extraction-of-cost-data/wiki/Gate-vs-UIMA-vs-OpenNLP?version=4 If you're uncertain, which is the language to go for NLP, personally i say, \"any language that will give you the desired analysis/output\", see Which language or tools to learn for natural language processing? Here's a pretty recent (2017) of NLP tools: https://github.com/alvations/awesome-community-curated-nlp An older list of NLP tools (2013): http://web.archive.org/web/20130703190201/http://yauhenklimovich.wordpress.com/2013/05/20/tools-nlp Other than language processing tools, you would very much need machine learning tools to incorporate into NLP pipelines.  There's a whole range in Python and Java, and once again it's up to preference and whether the libraries are user-friendly enough: Machine Learning libraries in python: (for more, see https://pypi.python.org/pypi?%3Aaction=search&term=machine+learning&submit=search) With the recent (2015) deep learning tsunami in NLP, possibly you could consider: https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software I'll avoid listing deep learning tools out of non-favoritism / neutrality.  Other Stackoverflow questions that also asked for NLP/ML tools:"}, {"id": 22904195, "score": 42, "vote": 0, "content": "<p>The question is very open ended. That said, rather than choose one, below is a comparison depending on the language that you would like to use (since there are good libraries available in both languages).</p>\n<p><strong>Python</strong></p>\n<p>In terms of Python, the first place you should look at is the <a href=\"http://www.nltk.org/\">Python Natural Language Toolkit</a>. As they note in their description, NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.</p>\n<p>There is also some excellent code that you can look up that originated out of Google's Natural Language Toolkit project that is Python based. You can find a link to that code here <a href=\"https://github.com/nltk/nltk\">on GitHub</a>.</p>\n<p><strong>Java</strong></p>\n<p>The first place to look would be Stanford's <a href=\"http://nlp.stanford.edu/software/index.shtml\">Natural Language Processing Group</a>. All of software that is distributed there is written in Java. All recent distributions require Oracle Java 6+ or OpenJDK 7+. Distribution packages include components for command-line invocation, jar files, a Java API, and source code.</p>\n<p>Another great option that you see in a lot of machine learning environments here (general option), is <a href=\"http://www.cs.waikato.ac.nz/ml/weka/\">Weka</a>. Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.</p>\n", "abstract": "The question is very open ended. That said, rather than choose one, below is a comparison depending on the language that you would like to use (since there are good libraries available in both languages). Python In terms of Python, the first place you should look at is the Python Natural Language Toolkit. As they note in their description, NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. There is also some excellent code that you can look up that originated out of Google's Natural Language Toolkit project that is Python based. You can find a link to that code here on GitHub. Java The first place to look would be Stanford's Natural Language Processing Group. All of software that is distributed there is written in Java. All recent distributions require Oracle Java 6+ or OpenJDK 7+. Distribution packages include components for command-line invocation, jar files, a Java API, and source code. Another great option that you see in a lot of machine learning environments here (general option), is Weka. Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes."}]}, {"link": "https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do", "question": {"id": "51956000", "title": "What does Keras Tokenizer method exactly do?", "content": "<p>On occasion, circumstances require us to do the following:</p>\n<pre><code class=\"python\">from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=my_max)\n</code></pre>\n<p>Then, invariably, we chant this mantra:</p>\n<pre><code class=\"python\">tokenizer.fit_on_texts(text) \nsequences = tokenizer.texts_to_sequences(text)\n</code></pre>\n<p>While I (more or less) understand what the total effect is, I can't figure out what each one does separately, regardless of how much research I do (including, obviously, the documentation). I don't think I've ever seen one without the other. </p>\n<p>So what does each do? Are there any circumstances where you would use either one without the other? If not, why aren't they simply combined into something like:</p>\n<pre><code class=\"python\">sequences = tokenizer.fit_on_texts_to_sequences(text)\n</code></pre>\n<p>Apologies if I'm missing something obvious, but I'm pretty new at this.</p>\n", "abstract": "On occasion, circumstances require us to do the following: Then, invariably, we chant this mantra: While I (more or less) understand what the total effect is, I can't figure out what each one does separately, regardless of how much research I do (including, obviously, the documentation). I don't think I've ever seen one without the other.  So what does each do? Are there any circumstances where you would use either one without the other? If not, why aren't they simply combined into something like: Apologies if I'm missing something obvious, but I'm pretty new at this."}, "answers": [{"id": 51956230, "score": 170, "vote": 0, "content": "<p>From the <a href=\"https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py\" rel=\"noreferrer\">source code</a>:</p>\n<ol>\n<li><code>fit_on_texts</code> <em>Updates internal vocabulary based on a list of texts.</em> This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. <code>word_index[\"the\"] = 1; word_index[\"cat\"] = 2</code> it is word -&gt; index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).</li>\n<li><code>texts_to_sequences</code> <em>Transforms each text in texts to a sequence of integers.</em> So it basically takes each word in the text and replaces it with its corresponding integer value from the <code>word_index</code> dictionary. Nothing more, nothing less, certainly no magic involved.</li>\n</ol>\n<p><strong>Why don't combine them?</strong> Because you almost always fit <em>once</em> and convert to sequences <em>many times</em>. You will fit on your training corpus once and use that exact same <code>word_index</code> dictionary at train / eval / testing / prediction time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.</p>\n", "abstract": "From the source code: Why don't combine them? Because you almost always fit once and convert to sequences many times. You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate."}, {"id": 53921105, "score": 71, "vote": 0, "content": "<p>Adding more to above answers with examples will help in better understanding:</p>\n<p><strong>Example 1</strong>:</p>\n<pre><code class=\"python\">t  = Tokenizer()\nfit_text = \"The earth is an awesome place live\"\nt.fit_on_texts(fit_text)\ntest_text = \"The earth is an great place live\"\nsequences = t.texts_to_sequences(test_text)\n\nprint(\"sequences : \",sequences,'\\n')\n\nprint(\"word_index : \",t.word_index)\n#[] specifies : 1. space b/w the words in the test_text    2. letters that have not occured in fit_text\n\nOutput :\n\n       sequences :  [[3], [4], [1], [], [1], [2], [8], [3], [4], [], [5], [6], [], [2], [9], [], [], [8], [1], [2], [3], [], [13], [7], [2], [14], [1], [], [7], [5], [15], [1]] \n\n       word_index :  {'e': 1, 'a': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'l': 7, 'r': 8, 'n': 9, 'w': 10, 'o': 11, 'm': 12, 'p': 13, 'c': 14, 'v': 15}\n</code></pre>\n<p><strong>Example 2</strong>:</p>\n<pre><code class=\"python\">t  = Tokenizer()\nfit_text = [\"The earth is an awesome place live\"]\nt.fit_on_texts(fit_text)\n\n#fit_on_texts fits on sentences when list of sentences is passed to fit_on_texts() function. \n#ie - fit_on_texts( [ sent1, sent2, sent3,....sentN ] )\n\n#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\ntest_text1 = \"The earth is an great place live\"\ntest_text2 = \"The is my program\"\nsequences = t.texts_to_sequences([test_text1, test_text2])\n\nprint('sequences : ',sequences,'\\n')\n\nprint('word_index : ',t.word_index)\n#texts_to_sequences() returns list of list. ie - [ [] ]\n\nOutput:\n\n        sequences :  [[1, 2, 3, 4, 6, 7], [1, 3]] \n\n        word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n</code></pre>\n", "abstract": "Adding more to above answers with examples will help in better understanding: Example 1: Example 2:"}, {"id": 53512348, "score": 14, "vote": 0, "content": "<p>Lets see what this line of code does.</p>\n<pre><code class=\"python\">tokenizer.fit_on_texts(text) \n</code></pre>\n<p>For example, consider the sentence <code>\" The earth is an awesome place live\"</code></p>\n<p><code>tokenizer.fit_on_texts(\"The earth is an awesome place live\")</code> fits <code>[[1,2,3,4,5,6,7]]</code> where 3 -&gt; \"is\" , 6 -&gt; \"place\", so on.</p>\n<pre><code class=\"python\">sequences = tokenizer.texts_to_sequences(\"The earth is an great place live\")\n</code></pre>\n<p>returns <code>[[1,2,3,4,6,7]]</code>.</p>\n<p>You see what happened here. The word \"great\" is not fit initially, so it does not recognize the word \"great\". Meaning, fit_on_text can be used independently on train data and then the fitted vocabulary index can be used to represent a completely new set of word sequence. These are two different processes. Hence the two lines of code.</p>\n", "abstract": "Lets see what this line of code does. For example, consider the sentence \" The earth is an awesome place live\" tokenizer.fit_on_texts(\"The earth is an awesome place live\") fits [[1,2,3,4,5,6,7]] where 3 -> \"is\" , 6 -> \"place\", so on. returns [[1,2,3,4,6,7]]. You see what happened here. The word \"great\" is not fit initially, so it does not recognize the word \"great\". Meaning, fit_on_text can be used independently on train data and then the fitted vocabulary index can be used to represent a completely new set of word sequence. These are two different processes. Hence the two lines of code."}, {"id": 69643477, "score": 3, "vote": 0, "content": "<p><a href=\"https://stackoverflow.com/a/51956230/5404074\">nuric</a> already satistified the question, but I would add something.</p>\n<p>Please focus on both word <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments\" rel=\"nofollow noreferrer\">frequency-based encoding</a> and OOV in this example:</p>\n<pre><code class=\"python\">from tensorflow.keras.preprocessing.text        import Tokenizer\n\ncorpus =['The', 'cat', 'is', 'on', 'the', 'table', 'a', 'very', 'long', 'table']\n\ntok_obj = Tokenizer(num_words=10, oov_token='&lt;OOV&gt;')\ntok_obj.fit_on_texts(corpus)\n</code></pre>\n<p>[TL;DR] The tokenizer will include the first <code>10</code> words appearing in the corpus. Here <code>10</code> words, but only <em>8</em> are unique. The most frequent <code>10</code> words will be encoded, if they are more than this number they will go OOV (Out Of Vocabulary).</p>\n<p><strong>Built dictionary</strong>:</p>\n<p><em>Please note the frequency</em></p>\n<pre><code class=\"python\">{'&lt;OOV&gt;': 1, 'the': 2, 'table': 3, 'cat': 4, 'is': 5, 'on': 6, 'a': 7, 'very': 8, 'long': 9}\n</code></pre>\n<p><strong>Sentence(s) processing</strong>:</p>\n<pre><code class=\"python\">processed_seq = tok_obj.texts_to_sequences(['The dog is on the bed'])\n</code></pre>\n<p>Which gives:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; processed_seq\n    [[2, 1, 5, 6, 2, 1]]\n</code></pre>\n<p><strong>How to retrieve the sentence?</strong></p>\n<p>Build the dictionary <code>inv_map</code> and use It! list comprehension can be used below to compress the code.</p>\n<pre><code class=\"python\">inv_map = {v: k for k, v in tok_obj.word_index.items()}\n\nfor seq in processed_seq:\n    for tok in seq:\n        print(inv_map[tok])\n</code></pre>\n<p>which gives:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; the\n&lt;OOV&gt;\nis\non\nthe\n&lt;OOV&gt;\n</code></pre>\n<p>because <em>dog</em> and <em>bed</em> are not in the dictionary.</p>\n<p><strong>List comprehension</strong> can be used to compress the code. Here obtaining a list as output.</p>\n<pre><code class=\"python\">[inv_map[tok] for seq in processed_seq for tok in seq]\n</code></pre>\n<p>which gives:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; ['the', '&lt;OOV&gt;', 'is', 'on', 'the', '&lt;OOV&gt;']\n</code></pre>\n", "abstract": "nuric already satistified the question, but I would add something. Please focus on both word frequency-based encoding and OOV in this example: [TL;DR] The tokenizer will include the first 10 words appearing in the corpus. Here 10 words, but only 8 are unique. The most frequent 10 words will be encoded, if they are more than this number they will go OOV (Out Of Vocabulary). Built dictionary: Please note the frequency Sentence(s) processing: Which gives: How to retrieve the sentence? Build the dictionary inv_map and use It! list comprehension can be used below to compress the code. which gives: because dog and bed are not in the dictionary. List comprehension can be used to compress the code. Here obtaining a list as output. which gives:"}]}, {"link": "https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer", "question": {"id": "27697766", "title": "Understanding min_df and max_df in scikit CountVectorizer", "content": "<p>I have five text files that I input to a CountVectorizer. When specifying <code>min_df</code> and <code>max_df</code> to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)?</p>\n<p>What are the differences when <code>min_df</code> and <code>max_df</code> are provided as integers or as floats?</p>\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer\" rel=\"noreferrer\">The documentation</a> doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating <code>min_df</code> and <code>max_df</code>?</p>\n", "abstract": "I have five text files that I input to a CountVectorizer. When specifying min_df and max_df to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)? What are the differences when min_df and max_df are provided as integers or as floats? The documentation doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating min_df and max_df?"}, "answers": [{"id": 35615151, "score": 318, "vote": 0, "content": "<p><code>max_df</code> is used for removing terms that appear <strong>too frequently</strong>, also known as \"corpus-specific stop words\". For example:</p>\n<ul>\n<li><code>max_df = 0.50</code> means \"ignore terms that appear in <strong>more than 50% of the documents</strong>\".</li>\n<li><code>max_df = 25</code> means \"ignore terms that appear in <strong>more than 25 documents</strong>\".</li>\n</ul>\n<p>The default <code>max_df</code> is <code>1.0</code>, which means \"ignore terms that appear in <strong>more than 100% of the documents</strong>\". Thus, the default setting does not ignore any terms.</p>\n<hr/>\n<p><code>min_df</code> is used for removing terms that appear <strong>too infrequently</strong>. For example:</p>\n<ul>\n<li><code>min_df = 0.01</code> means \"ignore terms that appear in <strong>less than 1% of the documents</strong>\".</li>\n<li><code>min_df = 5</code> means \"ignore terms that appear in <strong>less than 5 documents</strong>\".</li>\n</ul>\n<p>The default <code>min_df</code> is <code>1</code>, which means \"ignore terms that appear in <strong>less than 1 document</strong>\". Thus, the default setting does not ignore any terms.</p>\n", "abstract": "max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example: The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms. min_df is used for removing terms that appear too infrequently. For example: The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms."}, {"id": 46699495, "score": 13, "vote": 0, "content": "<p>I would add this point also for understanding <code>min_df</code> and <code>max_df</code> in tf-idf better.</p>\n<p>If you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time. </p>\n<p>BUT the quality of your clustering should NOT be reduced. </p>\n<p>One might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents).</p>\n<p>So to sum it up, pruning the terms via <code>min_df</code> and <code>max_df</code> is to improve the performance, not the quality of clusters (as an example).</p>\n<p>And the crucial point is that if you set the <code>min</code> and <code>max</code> mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the <code>min</code>, <code>max</code> parameters unchanged.</p>\n", "abstract": "I would add this point also for understanding min_df and max_df in tf-idf better. If you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time.  BUT the quality of your clustering should NOT be reduced.  One might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents). So to sum it up, pruning the terms via min_df and max_df is to improve the performance, not the quality of clusters (as an example). And the crucial point is that if you set the min and max mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the min, max parameters unchanged."}, {"id": 27697863, "score": 11, "vote": 0, "content": "<p>As per the <code>CountVectorizer</code> documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" rel=\"noreferrer\">here</a>.</p>\n<p>When using a float in the range <code>[0.0, 1.0]</code> they refer to the <strong>document</strong> frequency. That is the percentage of documents that contain the term.</p>\n<p>When using an int it refers to absolute number of documents that hold this term.</p>\n<p>Consider the example where you have 5 text files (or documents). If you set <code>max_df = 0.6</code> then that would translate to <code>0.6*5=3</code> documents. If you set <code>max_df = 2</code> then that would simply translate to 2 documents.</p>\n<p>The source code example below is copied from Github <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\" rel=\"noreferrer\">here</a> and shows how the <code>max_doc_count</code> is constructed from the <code>max_df</code>. The code for <code>min_df</code> is similar and can be found on the GH page.</p>\n<pre><code class=\"python\">max_doc_count = (max_df\n                 if isinstance(max_df, numbers.Integral)\n                 else max_df * n_doc)\n</code></pre>\n<p>The defaults for <code>min_df</code> and <code>max_df</code> are 1 and 1.0, respectively. This basically says <em>\"If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored.\"</em></p>\n<p><code>max_df</code> and <code>min_df</code> are both used internally to calculate <code>max_doc_count</code> and <code>min_doc_count</code>, the maximum and minimum number of documents that a term must be found in. This is then passed to <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L665\" rel=\"noreferrer\"><code>self._limit_features</code></a> as the keyword arguments <code>high</code> and <code>low</code> respectively, the docstring for <code>self._limit_features</code> is</p>\n<pre><code class=\"python\">\"\"\"Remove too rare or too common features.\n\nPrune features that are non zero in more samples than high or less\ndocuments than low, modifying the vocabulary, and restricting it to\nat most the limit most frequent.\n\nThis does not prune samples with zero features.\n\"\"\"\n</code></pre>\n", "abstract": "As per the CountVectorizer documentation here. When using a float in the range [0.0, 1.0] they refer to the document frequency. That is the percentage of documents that contain the term. When using an int it refers to absolute number of documents that hold this term. Consider the example where you have 5 text files (or documents). If you set max_df = 0.6 then that would translate to 0.6*5=3 documents. If you set max_df = 2 then that would simply translate to 2 documents. The source code example below is copied from Github here and shows how the max_doc_count is constructed from the max_df. The code for min_df is similar and can be found on the GH page. The defaults for min_df and max_df are 1 and 1.0, respectively. This basically says \"If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored.\" max_df and min_df are both used internally to calculate max_doc_count and min_doc_count, the maximum and minimum number of documents that a term must be found in. This is then passed to self._limit_features as the keyword arguments high and low respectively, the docstring for self._limit_features is"}, {"id": 34852324, "score": 4, "vote": 0, "content": "<p>The defaults for min_df and max_df are 1 and 1.0, respectively. These defaults really don't do anything at all.  </p>\n<p>That being said, I believe the currently accepted answer by @Ffisegydd answer isn't quite correct.</p>\n<p>For example, run this using the defaults, to see that when <code>min_df=1</code> and <code>max_df=1.0</code>, then </p>\n<p>1) all tokens that appear in at least one document are used  (e.g., all tokens!)</p>\n<p>2) all tokens that appear in all documents are used (we'll test with one candidate: everywhere).  </p>\n<pre><code class=\"python\">cv = CountVectorizer(min_df=1, max_df=1.0, lowercase=True) \n# here is just a simple list of 3 documents.\ncorpus = ['one two three everywhere', 'four five six everywhere', 'seven eight nine everywhere']\n# below we call fit_transform on the corpus and get the feature names.\nX = cv.fit_transform(corpus)\nvocab = cv.get_feature_names()\nprint vocab\nprint X.toarray()\nprint cv.stop_words_\n</code></pre>\n<p>We get:  </p>\n<pre><code class=\"python\">[u'eight', u'everywhere', u'five', u'four', u'nine', u'one', u'seven', u'six', u'three', u'two']\n[[0 1 0 0 0 1 0 0 1 1]\n [0 1 1 1 0 0 0 1 0 0]\n [1 1 0 0 1 0 1 0 0 0]]\nset([])\n</code></pre>\n<p>All tokens are kept. There are no stopwords.  </p>\n<p>Further messing around with the arguments will clarify other configurations.  </p>\n<p>For fun and insight, I'd also recommend playing around with <code>stop_words = 'english'</code> and seeing that, peculiarly, all the words except 'seven' are removed! Including `everywhere'.</p>\n", "abstract": "The defaults for min_df and max_df are 1 and 1.0, respectively. These defaults really don't do anything at all.   That being said, I believe the currently accepted answer by @Ffisegydd answer isn't quite correct. For example, run this using the defaults, to see that when min_df=1 and max_df=1.0, then  1) all tokens that appear in at least one document are used  (e.g., all tokens!) 2) all tokens that appear in all documents are used (we'll test with one candidate: everywhere).   We get:   All tokens are kept. There are no stopwords.   Further messing around with the arguments will clarify other configurations.   For fun and insight, I'd also recommend playing around with stop_words = 'english' and seeing that, peculiarly, all the words except 'seven' are removed! Including `everywhere'."}, {"id": 59216706, "score": 2, "vote": 0, "content": "<p>The goal of <code>MIN_DF</code> is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis. Similarly, you can ignore words that are too common with <code>MAX_DF</code>.</p>\n<p>Instead of using a minimum/maximum term frequency (total occurrences of a word) to eliminate words, <code>MIN_DF</code> and <code>MAX_DF</code> look at how many documents contained a term, better known as document frequency. The threshold values can be an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) .</p>\n<p>See <a href=\"https://kavita-ganesan.com/how-to-use-countvectorizer/#CountVectorizer-and-Stop-Words\" rel=\"nofollow noreferrer\">some usage examples here</a>.</p>\n", "abstract": "The goal of MIN_DF is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis. Similarly, you can ignore words that are too common with MAX_DF. Instead of using a minimum/maximum term frequency (total occurrences of a word) to eliminate words, MIN_DF and MAX_DF look at how many documents contained a term, better known as document frequency. The threshold values can be an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) . See some usage examples here."}, {"id": 67580156, "score": 0, "vote": 0, "content": "<p>I just looked at the documentation for sklearn CountVectorizer.This is how I think about it.</p>\n<p>Common words have higher frequency values, while rare words have lower frequency values. The frequency values range between <code>0 - 1</code> as fractions.</p>\n<p><code>max_df</code> is the upper ceiling value of the frequency values, while <code>min_df</code> is just the lower cutoff value of the frequency values.</p>\n<p>If we want to remove more common words, we set <code>max_df</code> to a lower ceiling value between 0 and 1. If we want to remove more rare words, we set <code>min_df</code> to a higher cutoff value between 0 and 1. We keep everything between <code>max_df</code> and <code>min_df</code>.</p>\n<p>Let me know, not sure if this makes sense.</p>\n", "abstract": "I just looked at the documentation for sklearn CountVectorizer.This is how I think about it. Common words have higher frequency values, while rare words have lower frequency values. The frequency values range between 0 - 1 as fractions. max_df is the upper ceiling value of the frequency values, while min_df is just the lower cutoff value of the frequency values. If we want to remove more common words, we set max_df to a lower ceiling value between 0 and 1. If we want to remove more rare words, we set min_df to a higher cutoff value between 0 and 1. We keep everything between max_df and min_df. Let me know, not sure if this makes sense."}]}, {"link": "https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anacon", "question": {"id": "54334304", "title": "spacy Can&#39;t find model &#39;en_core_web_sm&#39; on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "content": "<p>what is difference between <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code>? <a href=\"https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod\">This link</a> explains different model sizes. But i am still not clear how <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code> differ</p>\n<p><code>spacy.load('en')</code> runs fine for me. But the <code>spacy.load('en_core_web_sm')</code> throws error</p>\n<p>i have installed <code>spacy</code>as below. when i go to jupyter notebook and run command <code>nlp = spacy.load('en_core_web_sm')</code> I get the below error </p>\n<pre><code class=\"python\">---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n&lt;ipython-input-4-b472bef03043&gt; in &lt;module&gt;()\n      1 # Import spaCy and load the language library\n      2 import spacy\n----&gt; 3 nlp = spacy.load('en_core_web_sm')\n      4 \n      5 # Create a Doc object\n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\__init__.py in load(name, **overrides)\n     13     if depr_path not in (True, False, None):\n     14         deprecation_warning(Warnings.W001.format(path=depr_path))\n---&gt; 15     return util.load_model(name, **overrides)\n     16 \n     17 \n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n    117     elif hasattr(name, 'exists'):  # Path or Path-like to model data\n    118         return load_model_from_path(name, **overrides)\n--&gt; 119     raise IOError(Errors.E050.format(name=name))\n    120 \n    121 \n\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n</code></pre>\n<p>how I installed Spacy ---</p>\n<pre><code class=\"python\">(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;conda install -c conda-forge spacy\nFetching package metadata .............\nSolving package specifications: .\n\nPackage plan for installation in environment C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder:\n\nThe following NEW packages will be INSTALLED:\n\n    blas:           1.0-mkl\n    cymem:          1.31.2-py35h6538335_0    conda-forge\n    dill:           0.2.8.2-py35_0           conda-forge\n    msgpack-numpy:  0.4.4.2-py_0             conda-forge\n    murmurhash:     0.28.0-py35h6538335_1000 conda-forge\n    plac:           0.9.6-py_1               conda-forge\n    preshed:        1.0.0-py35h6538335_0     conda-forge\n    pyreadline:     2.1-py35_1000            conda-forge\n    regex:          2017.11.09-py35_0        conda-forge\n    spacy:          2.0.12-py35h830ac7b_0    conda-forge\n    termcolor:      1.1.0-py_2               conda-forge\n    thinc:          6.10.3-py35h830ac7b_2    conda-forge\n    tqdm:           4.29.1-py_0              conda-forge\n    ujson:          1.35-py35hfa6e2cd_1001   conda-forge\n\nThe following packages will be UPDATED:\n\n    msgpack-python: 0.4.8-py35_0                         --&gt; 0.5.6-py35he980bc4_3 conda-forge\n\nThe following packages will be DOWNGRADED:\n\n    freetype:       2.7-vc14_2               conda-forge --&gt; 2.5.5-vc14_2\n\nProceed ([y]/n)? y\n\nblas-1.0-mkl.t 100% |###############################| Time: 0:00:00   0.00  B/s\ncymem-1.31.2-p 100% |###############################| Time: 0:00:00   1.65 MB/s\nmsgpack-python 100% |###############################| Time: 0:00:00   5.37 MB/s\nmurmurhash-0.2 100% |###############################| Time: 0:00:00   1.49 MB/s\nplac-0.9.6-py_ 100% |###############################| Time: 0:00:00   0.00  B/s\npyreadline-2.1 100% |###############################| Time: 0:00:00   4.62 MB/s\nregex-2017.11. 100% |###############################| Time: 0:00:00   3.31 MB/s\ntermcolor-1.1. 100% |###############################| Time: 0:00:00 187.81 kB/s\ntqdm-4.29.1-py 100% |###############################| Time: 0:00:00   2.51 MB/s\nujson-1.35-py3 100% |###############################| Time: 0:00:00   1.66 MB/s\ndill-0.2.8.2-p 100% |###############################| Time: 0:00:00   4.34 MB/s\nmsgpack-numpy- 100% |###############################| Time: 0:00:00   0.00  B/s\npreshed-1.0.0- 100% |###############################| Time: 0:00:00   0.00  B/s\nthinc-6.10.3-p 100% |###############################| Time: 0:00:00   5.49 MB/s\nspacy-2.0.12-p 100% |###############################| Time: 0:00:10   7.42 MB/s\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;python -V\nPython 3.5.3 :: Anaconda custom (64-bit)\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;python -m spacy download en\nCollecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |################################| 37.4MB ...\nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm ... done\nSuccessfully installed en-core-web-sm-2.0.0\n\n    Linking successful\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\en_core_web_sm\n    --&gt;\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\data\\en\n\n    You can now load the model via spacy.load('en')\n\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz&gt;\n</code></pre>\n", "abstract": "what is difference between spacy.load('en_core_web_sm') and spacy.load('en')? This link explains different model sizes. But i am still not clear how spacy.load('en_core_web_sm') and spacy.load('en') differ spacy.load('en') runs fine for me. But the spacy.load('en_core_web_sm') throws error i have installed spacyas below. when i go to jupyter notebook and run command nlp = spacy.load('en_core_web_sm') I get the below error  how I installed Spacy ---"}, "answers": [{"id": 57989297, "score": 148, "vote": 0, "content": "<p>Initially I downloaded two en packages using following statements in anaconda prompt.</p>\n<pre><code class=\"python\">python -m spacy download en_core_web_lg\npython -m spacy download en_core_web_sm\n</code></pre>\n<p>But, I kept on getting linkage error and finally running below command helped me to establish link and solved error.</p>\n<pre><code class=\"python\">python -m spacy download en\n</code></pre>\n<p>Also make sure you to restart your runtime if working with Jupyter.\n-PS : If you get linkage error try giving admin previlages.</p>\n", "abstract": "Initially I downloaded two en packages using following statements in anaconda prompt. But, I kept on getting linkage error and finally running below command helped me to establish link and solved error. Also make sure you to restart your runtime if working with Jupyter.\n-PS : If you get linkage error try giving admin previlages."}, {"id": 54409674, "score": 83, "vote": 0, "content": "<p>The answer to your misunderstanding is a Unix concept, <strong>softlinks</strong> which we could say that in Windows are similar to shortcuts. Let's explain this.</p>\n<p>When you <code>spacy download en</code>, spaCy tries to find the best <strong>small</strong> model that matches your spaCy distribution. The small model that I am talking about defaults to <code>en_core_web_sm</code> which can be found in different variations which correspond to the different spaCy versions (for example <code>spacy</code>, <code>spacy-nightly</code> have <code>en_core_web_sm</code> of different sizes). </p>\n<p>When spaCy finds the best model for you, it downloads it and then <strong>links</strong> the name <code>en</code> to the package it downloaded, e.g. <code>en_core_web_sm</code>. That basically means that whenever you refer to <code>en</code> you will be referring to <code>en_core_web_sm</code>. In other words, <code>en</code> after linking is not a \"real\" package, is just a name for <code>en_core_web_sm</code>.</p>\n<p>However, it doesn't work the other way. You can't refer directly to <code>en_core_web_sm</code> because your system doesn't know you have it installed. When you did <code>spacy download en</code> you basically did a pip install. So pip knows that you have a package named <code>en</code> installed for your python distribution, but knows nothing about the package <code>en_core_web_sm</code>. This package is just replacing package <code>en</code> when you import it, which means that package <code>en</code> is just a softlink to <code>en_core_web_sm</code>.</p>\n<p>Of course, you can directly download <code>en_core_web_sm</code>, using the command: <code>python -m spacy download en_core_web_sm</code>, or you can even link the name <code>en</code> to other models as well. For example, you could do <code>python -m spacy download en_core_web_lg</code> and then <code>python -m spacy link en_core_web_lg en</code>. That would make \n<code>en</code> a name for <code>en_core_web_lg</code>, which is a large spaCy model for the English language.</p>\n<p>Hope it is clear now :) </p>\n", "abstract": "The answer to your misunderstanding is a Unix concept, softlinks which we could say that in Windows are similar to shortcuts. Let's explain this. When you spacy download en, spaCy tries to find the best small model that matches your spaCy distribution. The small model that I am talking about defaults to en_core_web_sm which can be found in different variations which correspond to the different spaCy versions (for example spacy, spacy-nightly have en_core_web_sm of different sizes).  When spaCy finds the best model for you, it downloads it and then links the name en to the package it downloaded, e.g. en_core_web_sm. That basically means that whenever you refer to en you will be referring to en_core_web_sm. In other words, en after linking is not a \"real\" package, is just a name for en_core_web_sm. However, it doesn't work the other way. You can't refer directly to en_core_web_sm because your system doesn't know you have it installed. When you did spacy download en you basically did a pip install. So pip knows that you have a package named en installed for your python distribution, but knows nothing about the package en_core_web_sm. This package is just replacing package en when you import it, which means that package en is just a softlink to en_core_web_sm. Of course, you can directly download en_core_web_sm, using the command: python -m spacy download en_core_web_sm, or you can even link the name en to other models as well. For example, you could do python -m spacy download en_core_web_lg and then python -m spacy link en_core_web_lg en. That would make \nen a name for en_core_web_lg, which is a large spaCy model for the English language. Hope it is clear now :) "}, {"id": 60197737, "score": 40, "vote": 0, "content": "<p>The below worked for me :</p>\n<pre><code class=\"python\">import en_core_web_sm\n\nnlp = en_core_web_sm.load()\n</code></pre>\n", "abstract": "The below worked for me :"}, {"id": 58432391, "score": 16, "vote": 0, "content": "<p>For those who are still facing problems even after installing it as administrator from Anaconda prompt, here's a quick fix:</p>\n<ol>\n<li><p>Got to the path where it is downloaded. For e.g.</p>\n<pre><code class=\"python\">C:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\n</code></pre>\n</li>\n<li><p>Copy the path.</p>\n</li>\n<li><p>Paste it in:</p>\n<pre><code class=\"python\">nlp = spacy.load(r'C:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0')\n</code></pre>\n</li>\n<li><p>Works like a charm :)</p>\n</li>\n</ol>\n<p>PS: Check for spacy version</p>\n", "abstract": "For those who are still facing problems even after installing it as administrator from Anaconda prompt, here's a quick fix: Got to the path where it is downloaded. For e.g. Copy the path. Paste it in: Works like a charm :) PS: Check for spacy version"}, {"id": 62858576, "score": 15, "vote": 0, "content": "<p>Using the Spacy language model in Colab requires only the following two steps:</p>\n<ol>\n<li><em>Download the model (change the name according to the size of the model)</em></li>\n</ol>\n<pre><code class=\"python\">!python -m spacy download en_core_web_lg \n</code></pre>\n<ol start=\"2\">\n<li><em>Restart the colab runtime!</em>\nPerform shortcut key\uff1a <strong>Ctrl + M + .</strong></li>\n</ol>\n<p>Test</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load(\"en_core_web_lg\")\n</code></pre>\n<p>successful!!!</p>\n", "abstract": "Using the Spacy language model in Colab requires only the following two steps: Test successful!!!"}, {"id": 67404153, "score": 11, "vote": 0, "content": "<p>Try this method as this worked like a charm to me:</p>\n<p>In your Anaconda Prompt, run the command:</p>\n<pre><code class=\"python\">!python -m spacy download en\n</code></pre>\n<p>After running the above command, you should be able to execute the below in your jupyter notebook:</p>\n<pre><code class=\"python\">spacy.load('en_core_web_sm')\n</code></pre>\n", "abstract": "Try this method as this worked like a charm to me: In your Anaconda Prompt, run the command: After running the above command, you should be able to execute the below in your jupyter notebook:"}, {"id": 65177235, "score": 5, "vote": 0, "content": "<p>First of all, install spacy using the following command for jupyter notebook\n<code>pip install -U spacy</code></p>\n<p>Then write the following code:</p>\n<pre><code class=\"python\">import en_core_web_sm\nnlp = en_core_web_sm.load()\n</code></pre>\n", "abstract": "First of all, install spacy using the following command for jupyter notebook\npip install -U spacy Then write the following code:"}, {"id": 66424662, "score": 5, "vote": 0, "content": "<p>I am running <code>Jupyter</code> Notebook on Windows.</p>\n<p>Finally, its a version issue, Need to execute below commands in conda cmd prompt( open as admin)</p>\n<ul>\n<li><p>pip install spacy==2.3.5</p>\n</li>\n<li><p>python -m spacy download en_core_web_sm</p>\n</li>\n<li><p>python -m spacy download en</p>\n</li>\n</ul>\n<pre><code class=\"python\">from chatterbot import ChatBot\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nChatBot(\"hello\")\n</code></pre>\n<p>Output -\n<img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/7cBOr.png\"/></p>\n", "abstract": "I am running Jupyter Notebook on Windows. Finally, its a version issue, Need to execute below commands in conda cmd prompt( open as admin) pip install spacy==2.3.5 python -m spacy download en_core_web_sm python -m spacy download en Output -\n"}, {"id": 69744857, "score": 5, "vote": 0, "content": "<p>Don't run <code>!python -m spacy download en_core_web_lg</code> from inside jupyter.\nDo this instead:</p>\n<pre><code class=\"python\">import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\n</code></pre>\n<p>You may need to restart the kernel before running the above two commands for it to work.</p>\n", "abstract": "Don't run !python -m spacy download en_core_web_lg from inside jupyter.\nDo this instead: You may need to restart the kernel before running the above two commands for it to work."}, {"id": 63829222, "score": 4, "vote": 0, "content": "<pre><code class=\"python\">import spacy\n\nnlp = spacy.load('/opt/anaconda3/envs/NLPENV/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\n</code></pre>\n<p>Try giving the absolute path of the package with the version as shown in the image.</p>\n<p>It works perfectly fine.</p>\n", "abstract": "Try giving the absolute path of the package with the version as shown in the image. It works perfectly fine."}, {"id": 66503526, "score": 4, "vote": 0, "content": "<p>a simple solution for this which I saw on spacy.io</p>\n<pre><code class=\"python\">from spacy.lang.en import English\nnlp=English()\n</code></pre>\n<p><a href=\"https://course.spacy.io/en/chapter1\" rel=\"nofollow noreferrer\">https://course.spacy.io/en/chapter1</a></p>\n", "abstract": "a simple solution for this which I saw on spacy.io https://course.spacy.io/en/chapter1"}, {"id": 61971415, "score": 3, "vote": 0, "content": "<p>As for Windows based Anaconda,</p>\n<ol>\n<li><p>Open Anaconda Prompt</p>\n</li>\n<li><p>Activate your environment. Ex: active myspacyenv</p>\n</li>\n<li><p><code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz</code></p>\n</li>\n<li><p><code>python -m spacy download en_core_web_sm</code></p>\n</li>\n<li><p>Open Jupyter Notebook ex: active myspacyenv and then jupyter notebook on Anaconda Promt</p>\n</li>\n</ol>\n<blockquote>\n<p><code>import spacy spacy.load('en_core_web_sm')</code></p>\n</blockquote>\n<p>and it will run peacefully!</p>\n", "abstract": "As for Windows based Anaconda, Open Anaconda Prompt Activate your environment. Ex: active myspacyenv pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz python -m spacy download en_core_web_sm Open Jupyter Notebook ex: active myspacyenv and then jupyter notebook on Anaconda Promt import spacy spacy.load('en_core_web_sm') and it will run peacefully!"}, {"id": 64521502, "score": 3, "vote": 0, "content": "<p>Steps to load up modules based on different versions of spacy</p>\n<p>download the best-matching version of a specific model for your spaCy installation</p>\n<pre><code class=\"python\">python -m spacy download en_core_web_sm\npip install .tar.gz archive from path or URL\npip install /Users/you/en_core_web_sm-2.2.0.tar.gz\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n</code></pre>\n<p>Add to your requirements file or environment yaml file. Theres range of version that one spacy version is comptable with you can view more under <a href=\"https://github.com/explosion/spacy-models/releases\" rel=\"nofollow noreferrer\">https://github.com/explosion/spacy-models/releases</a></p>\n<p>if your not sure running below code</p>\n<pre><code class=\"python\">nlp = spacy.load('en_core_web_sm') \n</code></pre>\n<p>will give off a warning telling what version model will be compatible with your installed spacy verion</p>\n<p>enironment.yml example</p>\n<pre><code class=\"python\">name: root\nchannels:\n  - defaults\n  - conda-forge\n  - anaconda\ndependencies:\n  - python=3.8.3\n  - pip\n  - spacy=2.3.2\n  - scikit-learn=0.23.2\n  - pip:\n    - https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm\n</code></pre>\n", "abstract": "Steps to load up modules based on different versions of spacy download the best-matching version of a specific model for your spaCy installation or Add to your requirements file or environment yaml file. Theres range of version that one spacy version is comptable with you can view more under https://github.com/explosion/spacy-models/releases if your not sure running below code will give off a warning telling what version model will be compatible with your installed spacy verion enironment.yml example"}, {"id": 58427838, "score": 2, "vote": 0, "content": "<p>Open Anaconda Navigator. Click on any IDE. Run the code: </p>\n<pre><code class=\"python\">!pip install -U spacy download en_core_web_sm\n!pip install -U spacy download en_core_web_sm\n</code></pre>\n<p>It will work. If you are open IDE directly close it and follow this procedure once.</p>\n", "abstract": "Open Anaconda Navigator. Click on any IDE. Run the code:  It will work. If you are open IDE directly close it and follow this procedure once."}, {"id": 61967756, "score": 2, "vote": 0, "content": "<p>Loading the module using the different syntax worked for me.</p>\n<pre><code class=\"python\">import en_core_web_sm\nnlp = en_core_web_sm.load()\n</code></pre>\n", "abstract": "Loading the module using the different syntax worked for me."}, {"id": 62142676, "score": 2, "vote": 0, "content": "<p>Anaconda Users</p>\n<ol>\n<li><p>If you're using a conda virtual environment, <strong><em>be sure that its the same version of Python as that in your base environment</em></strong>. To verify this, run <code>python --version</code> in each environment. If not the same, create a new virtual environment with that version of Python (Ex. <code>conda create --name myenv python=x.x.x</code>).</p></li>\n<li><p>Activate the virtual environment (<code>conda activate myenv</code>)</p></li>\n<li><code>conda install -c conda-forge spacy</code></li>\n<li><code>python -m spacy download en_core_web_sm</code></li>\n</ol>\n<p>I just ran into this issue, and the above worked for me. This addresses the issue of the download occurring in an area that is not accessible to your current virtual environment.</p>\n<p>You should then be able to run the following:</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n</code></pre>\n", "abstract": "Anaconda Users If you're using a conda virtual environment, be sure that its the same version of Python as that in your base environment. To verify this, run python --version in each environment. If not the same, create a new virtual environment with that version of Python (Ex. conda create --name myenv python=x.x.x). Activate the virtual environment (conda activate myenv) I just ran into this issue, and the above worked for me. This addresses the issue of the download occurring in an area that is not accessible to your current virtual environment. You should then be able to run the following:"}, {"id": 63595314, "score": 2, "vote": 0, "content": "<p>Open command prompt or terminal and execute the below code:</p>\n<pre><code class=\"python\">pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n</code></pre>\n<p>Execute the below chunk in your Jupiter notebook.</p>\n<p><code>import spacy</code></p>\n<p><code>nlp = spacy.load('en_core_web_sm')</code></p>\n<p>Hope the above code works for all:)</p>\n", "abstract": "Open command prompt or terminal and execute the below code: Execute the below chunk in your Jupiter notebook. import spacy nlp = spacy.load('en_core_web_sm') Hope the above code works for all:)"}, {"id": 65022770, "score": 2, "vote": 0, "content": "<p>I had also same issue as I couldnt load module using '''spacy.load()'''\nYou can follow below steps to solve this on windows:</p>\n<ol>\n<li>download using <i>!python -m spacy download en_core_web_sm</i></li>\n<li>import en_core_web_sm as <i>import en_core_web_sm</i></li>\n<li>load using <i>en_core_web_sm.load()</i> to some variable</li>\n</ol>\n<p>Complete code will be:</p>\n<pre><code class=\"python\">python -m spacy download en_core_web_sm\n\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\n</code></pre>\n", "abstract": "I had also same issue as I couldnt load module using '''spacy.load()'''\nYou can follow below steps to solve this on windows: Complete code will be:"}, {"id": 65879078, "score": 2, "vote": 0, "content": "<p>This works with colab:</p>\n<pre><code class=\"python\">!python -m spacy download en\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n</code></pre>\n<p>Or for the medium:</p>\n<pre><code class=\"python\">import en_core_web_md\nnlp = en_core_web_md.load()\n</code></pre>\n", "abstract": "This works with colab: Or for the medium:"}, {"id": 66528792, "score": 2, "vote": 0, "content": "<p>Instead of any of the above, this solved my error.</p>\n<p><code>conda install -c conda-forge spacy-model-en_core_web_sm</code></p>\n<p>If you are an anaconda user, this is the solution.</p>\n", "abstract": "Instead of any of the above, this solved my error. conda install -c conda-forge spacy-model-en_core_web_sm If you are an anaconda user, this is the solution."}, {"id": 66678429, "score": 1, "vote": 0, "content": "<p>I'm running PyCharm on MacOS and while none of the above answers completely worked for me, they did provide enough clues and I was finally able to everything working.  I am connecting to an ec2 instance and have configured PyCharm such that I can edit on my Mac and it automatically updates the files on my ec2 instance.  Thus, the problem was on the ec2 side where it was not finding Spacy even though I installed it several different times and ways.  If I ran my python script from the command line, everything worked fine.  However, from within PyCharm, it was initially not finding Spacy and the models.  I eventually fixed the \"finding\" spacy issue using the above recommendation of adding a \"requirements.txt\" file.  But the models were still not recognized.</p>\n<p><strong>My solution:</strong> <em>download the models manually and place them in the file system on the ec2 instance and explicitly point to them when loaded.  I downloaded the files from here:</em></p>\n<p><a href=\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\" rel=\"nofollow noreferrer\">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz</a></p>\n<p><a href=\"https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz\" rel=\"nofollow noreferrer\">https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz</a></p>\n<p>After downloading, I dropped moved them to my ec2 instance, decompressed and untared them in my filesystem, e.g. <code>/path_to_models/en_core_web_lg-3.0.0/</code></p>\n<p>I then load a model using the explicit path and it worked from within PyCharm (note the path used goes all the way to <code>en_core_web_lg-3.0.0</code>; you will get an error if you do not use the folder with the <code>config.cfg</code> file):</p>\n<pre><code class=\"python\">nlpObject = spacy.load('/path_to_models/en_core_web_lg-3.0.0/en_core_web_lg/en_core_web_lg-3.0.0')\n</code></pre>\n", "abstract": "I'm running PyCharm on MacOS and while none of the above answers completely worked for me, they did provide enough clues and I was finally able to everything working.  I am connecting to an ec2 instance and have configured PyCharm such that I can edit on my Mac and it automatically updates the files on my ec2 instance.  Thus, the problem was on the ec2 side where it was not finding Spacy even though I installed it several different times and ways.  If I ran my python script from the command line, everything worked fine.  However, from within PyCharm, it was initially not finding Spacy and the models.  I eventually fixed the \"finding\" spacy issue using the above recommendation of adding a \"requirements.txt\" file.  But the models were still not recognized. My solution: download the models manually and place them in the file system on the ec2 instance and explicitly point to them when loaded.  I downloaded the files from here: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz After downloading, I dropped moved them to my ec2 instance, decompressed and untared them in my filesystem, e.g. /path_to_models/en_core_web_lg-3.0.0/ I then load a model using the explicit path and it worked from within PyCharm (note the path used goes all the way to en_core_web_lg-3.0.0; you will get an error if you do not use the folder with the config.cfg file):"}, {"id": 69345097, "score": 1, "vote": 0, "content": "<p>Check installed version of spacy\n<code>pip show spacy</code>\nYou will get something like this:</p>\n<p>Name: spacy\nVersion: 3.1.3\nSummary: Industrial-strength Natural Language Processing (NLP) in Python</p>\n<p>Install the relevant version of the model using:\n<code>!pip install -U https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz</code></p>\n", "abstract": "Check installed version of spacy\npip show spacy\nYou will get something like this: Name: spacy\nVersion: 3.1.3\nSummary: Industrial-strength Natural Language Processing (NLP) in Python Install the relevant version of the model using:\n!pip install -U https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"}, {"id": 64608678, "score": 0, "vote": 0, "content": "<p>I tried all the above answers but could not succeed. Below worked for me :</p>\n<p>(Specific to WINDOWS os)</p>\n<ol>\n<li>Run anaconda command prompt with admin privilege(Important)</li>\n<li>Then run below commands:</li>\n</ol>\n<pre><code class=\"python\">  pip install -U --user spacy    \n  python -m spacy download en\n</code></pre>\n<ol start=\"3\">\n<li>Try below command for verification:</li>\n</ol>\n<pre><code class=\"python\">import spacy\nspacy.load('en')\n</code></pre>\n<ol start=\"4\">\n<li>It might work for others versions as well:\n<a href=\"https://i.stack.imgur.com/AZQTP.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/AZQTP.png\"/></a></li>\n</ol>\n", "abstract": "I tried all the above answers but could not succeed. Below worked for me : (Specific to WINDOWS os)"}, {"id": 65228029, "score": 0, "vote": 0, "content": "<p>If you have already downloaded <code>spacy</code> and the language model (E.g., <code>en_core_web_sm</code> or <code>en_core_web_md</code>), then you can follow these steps:</p>\n<ol>\n<li><p>Open Anaconda prompt as admin</p>\n</li>\n<li><p>Then type : <code>python -m spacy link</code> [package name or path] [shortcut]</p>\n<p>For E.g., <code>python -m spacy link /Users/you/model en</code></p>\n</li>\n</ol>\n<p>This will create a symlink to the your language model. Now you can load the model using <code>spacy.load(\"en\")</code> in your notebooks or scripts</p>\n", "abstract": "If you have already downloaded spacy and the language model (E.g., en_core_web_sm or en_core_web_md), then you can follow these steps: Open Anaconda prompt as admin Then type : python -m spacy link [package name or path] [shortcut] For E.g., python -m spacy link /Users/you/model en This will create a symlink to the your language model. Now you can load the model using spacy.load(\"en\") in your notebooks or scripts"}, {"id": 65589372, "score": 0, "vote": 0, "content": "<p>This is what I did:</p>\n<ol>\n<li><p>Went to the virtual environment where I was working on Anaconda Prompt / Command Line</p>\n</li>\n<li><p>Ran this: python -m spacy download en_core_web_sm</p>\n</li>\n</ol>\n<p>And was done</p>\n", "abstract": "This is what I did: Went to the virtual environment where I was working on Anaconda Prompt / Command Line Ran this: python -m spacy download en_core_web_sm And was done"}, {"id": 68267692, "score": 0, "vote": 0, "content": "<p>TRY THIS :-<br/>\n!python -m spacy download en_core_web_md</p>\n", "abstract": "TRY THIS :-\n!python -m spacy download en_core_web_md"}, {"id": 68757345, "score": 0, "vote": 0, "content": "<p>Even I faced similar issue. How I resolved it</p>\n<ol>\n<li>start anaconda prompt in admin mode.</li>\n<li>installed both\npython -m spacy download en\nand\npython -m spacy download en_core_web_sm\nafter above steps only I started jupyter notebook where I am accessing this package.\nNow I can access both\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nor\nnlp = spacy.load('en')\nBoth are working for me.</li>\n</ol>\n", "abstract": "Even I faced similar issue. How I resolved it"}, {"id": 68785584, "score": 0, "vote": 0, "content": "<p>I faced a similar issue. I installed spacy and en_core_web_sm from a specific conda environment. However, I got two(02) differents issues as following:</p>\n<p>[Errno 2] No such file or directory: '....\\en_core_web_sm\\en_core_web_sm-2.3.1\\vocab\\lexemes.bin'\nor\nOSError: [E050] Can't find model 'en_core_web_sm'.... It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.</p>\n<p>I did the following:</p>\n<ol>\n<li>Open Command Prompt as Administrator</li>\n<li>Go to c:&gt;</li>\n<li>Activate my Conda environment (If you work in a specific conda environment):</li>\n</ol>\n<pre><code class=\"python\">c:\\&gt;activate &lt;conda environment name&gt;\n</code></pre>\n<ol start=\"4\">\n<li><code>(conda environment name)c:\\&gt;python -m spacy download en</code></li>\n<li>Return to Jupyter Notebook and you can load the language library:</li>\n</ol>\n<pre><code class=\"python\">nlp = en_core_web_sm.load()\n</code></pre>\n<p>For me, it works :)</p>\n", "abstract": "I faced a similar issue. I installed spacy and en_core_web_sm from a specific conda environment. However, I got two(02) differents issues as following: [Errno 2] No such file or directory: '....\\en_core_web_sm\\en_core_web_sm-2.3.1\\vocab\\lexemes.bin'\nor\nOSError: [E050] Can't find model 'en_core_web_sm'.... It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory. I did the following: For me, it works :)"}, {"id": 68826631, "score": 0, "vote": 0, "content": "<p>Download <code>en_core_web_sm</code> tar file</p>\n<p>Open terminal from anaconda or open anaconda evn.\nRun this:</p>\n<pre><code class=\"python\">pip3 install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">pip install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n</code></pre>\n<p>Restart jupyter, it will work.</p>\n", "abstract": "Download en_core_web_sm tar file Open terminal from anaconda or open anaconda evn.\nRun this: or Restart jupyter, it will work."}, {"id": 69465033, "score": 0, "vote": 0, "content": "<p>Run this in os console:</p>\n<pre><code class=\"python\">python -m spacy download en\npython -m spacy link en_core_web_sm en_core_web_sm\n</code></pre>\n<p>Then run this in python console or on your python IDE:</p>\n<pre><code class=\"python\">import spacy\nspacy.load('en_core_web_sm')\n</code></pre>\n", "abstract": "Run this in os console: Then run this in python console or on your python IDE:"}, {"id": 69849687, "score": 0, "vote": 0, "content": "<p>This worked for me:\nconda install -c conda-forge spacy-model-en_core_web_sm</p>\n", "abstract": "This worked for me:\nconda install -c conda-forge spacy-model-en_core_web_sm"}]}, {"link": "https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code", "question": {"id": "3522372", "title": "How to config nltk data directory from code?", "content": "<p>How to config nltk data directory from code?</p>\n", "abstract": "How to config nltk data directory from code?"}, "answers": [{"id": 3903787, "score": 80, "vote": 0, "content": "<p>Just change items of <code>nltk.data.path</code>, it's a simple list.</p>\n", "abstract": "Just change items of nltk.data.path, it's a simple list."}, {"id": 22987374, "score": 52, "vote": 0, "content": "<p>From the code, <a href=\"http://www.nltk.org/_modules/nltk/data.html\">http://www.nltk.org/_modules/nltk/data.html</a>: </p>\n<blockquote>\n<pre><code class=\"python\">``nltk:path``: Specifies the file stored in the NLTK data\n package at *path*.  NLTK will search for these files in the\n directories specified by ``nltk.data.path``.\n</code></pre>\n</blockquote>\n<p>Then within the code:</p>\n<pre><code class=\"python\">######################################################################\n# Search Path\n######################################################################\n\npath = []\n\"\"\"A list of directories where the NLTK data package might reside.\n   These directories will be checked in order when looking for a\n   resource in the data package.  Note that this allows users to\n   substitute in their own versions of resources, if they have them\n   (e.g., in their home directory under ~/nltk_data).\"\"\"\n\n# User-specified locations:\npath += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]\nif os.path.expanduser('~/') != '~/':\n    path.append(os.path.expanduser(str('~/nltk_data')))\n\nif sys.platform.startswith('win'):\n    # Common locations on Windows:\n    path += [\n        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n        os.path.join(sys.prefix, str('nltk_data')),\n        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n    ]\nelse:\n    # Common locations on UNIX &amp; OS X:\n    path += [\n        str('/usr/share/nltk_data'),\n        str('/usr/local/share/nltk_data'),\n        str('/usr/lib/nltk_data'),\n        str('/usr/local/lib/nltk_data')\n    ]\n</code></pre>\n<p>To modify the path, simply append to the list of possible paths:</p>\n<pre><code class=\"python\">import nltk\nnltk.data.path.append(\"/home/yourusername/whateverpath/\")\n</code></pre>\n<p>Or in windows:</p>\n<pre><code class=\"python\">import nltk\nnltk.data.path.append(\"C:\\somewhere\\farfar\\away\\path\")\n</code></pre>\n", "abstract": "From the code, http://www.nltk.org/_modules/nltk/data.html:  Then within the code: To modify the path, simply append to the list of possible paths: Or in windows:"}, {"id": 22979193, "score": 32, "vote": 0, "content": "<p>I use append, example</p>\n<pre><code class=\"python\">nltk.data.path.append('/libs/nltk_data/')\n</code></pre>\n", "abstract": "I use append, example"}, {"id": 49608594, "score": 21, "vote": 0, "content": "<p>Instead of adding <code>nltk.data.path.append('your/path/to/nltk_data')</code> to every script, NLTK accepts NLTK_DATA environment variable. (<a href=\"https://github.com/nltk/nltk/blob/develop/nltk/data.py#L88\" rel=\"noreferrer\">code link</a>)</p>\n<p>Open <code>~/.bashrc</code> (or <code>~/.profile</code>) with text editor (e.g. <code>nano</code>, <code>vim</code>, <code>gedit</code>), and add following line:  </p>\n<pre><code class=\"python\">export NLTK_DATA=\"your/path/to/nltk_data\"\n</code></pre>\n<p>Execute <code>source</code> to load environmental variable  </p>\n<pre><code class=\"python\">source ~/.bashrc\n</code></pre>\n<p><br/></p>\n<h3>Test</h3>\n<p>Open python and execute following lines</p>\n<pre><code class=\"python\">import nltk\nnltk.data.path\n</code></pre>\n<p>Your can see your nltk data path already in there.</p>\n<p>Reference: @alvations's answer on\n<a href=\"https://github.com/nltk/nltk/issues/1997#issuecomment-377885461\" rel=\"noreferrer\">nltk/nltk #1997</a> </p>\n", "abstract": "Instead of adding nltk.data.path.append('your/path/to/nltk_data') to every script, NLTK accepts NLTK_DATA environment variable. (code link) Open ~/.bashrc (or ~/.profile) with text editor (e.g. nano, vim, gedit), and add following line:   Execute source to load environmental variable    Open python and execute following lines Your can see your nltk data path already in there. Reference: @alvations's answer on\nnltk/nltk #1997 "}, {"id": 44662705, "score": 1, "vote": 0, "content": "<p>For those using uwsgi: </p>\n<p>I was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to <code>myapp_uwsgi.ini</code>:</p>\n<pre><code class=\"python\">env = NLTK_DATA=/home/myuser/nltk_data/\n</code></pre>\n<p>This sets the environment variable <code>NLTK_DATA</code>, as suggested by @schemacs.<br/>\nYou may need to restart your uwsgi process after making this change. </p>\n", "abstract": "For those using uwsgi:  I was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to myapp_uwsgi.ini: This sets the environment variable NLTK_DATA, as suggested by @schemacs.\nYou may need to restart your uwsgi process after making this change. "}, {"id": 64285461, "score": 1, "vote": 0, "content": "<p>Using fnjn's advice above on printing out the path:</p>\n<pre><code class=\"python\">print(nltk.data.path)\n</code></pre>\n<p>I saw the path strings in this type of format on windows:</p>\n<pre><code class=\"python\">C:\\\\Users\\\\my_user_name\\\\AppData\\\\Roaming\\\\SPB_Data\n</code></pre>\n<p>So I switched my path from the python type forward slash '/', to a double backslash '\\\\' when I used path.append:</p>\n<pre><code class=\"python\">nltk.data.path.append(\"C:\\\\workspace\\\\my_project\\\\data\\\\nltk_books\")\n</code></pre>\n<p>The exception went away.</p>\n", "abstract": "Using fnjn's advice above on printing out the path: I saw the path strings in this type of format on windows: So I switched my path from the python type forward slash '/', to a double backslash '\\\\' when I used path.append: The exception went away."}, {"id": 56460309, "score": 0, "vote": 0, "content": "<p>Another solution is to get ahead of it. </p>\n<p>try \n    import nltk \n    nltk.download()  </p>\n<p>When the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. </p>\n", "abstract": "Another solution is to get ahead of it.  try \n    import nltk \n    nltk.download()   When the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. "}]}, {"link": "https://stackoverflow.com/questions/13883277/how-to-use-stanford-parser-in-nltk-using-python", "question": {"id": "13883277", "title": "How to use Stanford Parser in NLTK using Python", "content": "<p>Is it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)</p>\n", "abstract": "Is it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)"}, "answers": [{"id": 22269678, "score": 95, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>Sure, try the following in Python:</p>\n<pre><code class=\"python\">import os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = '/path/to/standford/jars'\nos.environ['STANFORD_MODELS'] = '/path/to/standford/jars'\n\nparser = stanford.StanfordParser(model_path=\"/location/of/the/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n\n# GUI\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n</code></pre>\n<p>Output:</p>\n<blockquote>\n<p>[Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]),\n  Tree(',', [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN',\n  ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ',\n  ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ',\n  [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ',\n  ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]),\n  Tree('.', ['?'])])])]</p>\n</blockquote>\n<p><strong>Note 1:</strong>\nIn this example both the parser &amp; model jars are in the same folder.</p>\n<p><strong>Note 2:</strong></p>\n<ul>\n<li>File name of stanford parser is: stanford-parser.jar </li>\n<li>File name of stanford models is: stanford-parser-x.x.x-models.jar</li>\n</ul>\n<p><strong>Note 3:</strong>\nThe englishPCFG.ser.gz file can be found <strong><em>inside</em></strong> the models.jar file (/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz). Please use come archive manager to 'unzip' the models.jar file.</p>\n<p><strong>Note 4:</strong>\nBe sure you are using Java JRE (Runtime Environment) <strong>1.8</strong> also known as Oracle JDK 8. Otherwise you will get: Unsupported major.minor version 52.0.</p>\n<h1>Installation</h1>\n<ol>\n<li><p>Download NLTK v3 from: <a href=\"https://github.com/nltk/nltk\" rel=\"noreferrer\">https://github.com/nltk/nltk</a>. And install NLTK:</p>\n<p>sudo python setup.py install</p></li>\n<li><p>You can use the NLTK downloader to get Stanford Parser, using Python:</p>\n<pre><code class=\"python\">import nltk\nnltk.download()\n</code></pre></li>\n<li><p>Try my example! (don't forget the change the jar paths and change the model path to the ser.gz location)</p></li>\n</ol>\n<p><strong>OR:</strong></p>\n<ol>\n<li><p>Download and install NLTK v3, same as above.</p></li>\n<li><p>Download the latest version from (<strong>current version</strong> filename is stanford-parser-full-2015-01-29.zip):\n<a href=\"http://nlp.stanford.edu/software/lex-parser.shtml#Download\" rel=\"noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml#Download</a></p></li>\n<li><p>Extract the standford-parser-full-20xx-xx-xx.zip. </p></li>\n<li><p>Create a new folder ('jars' in my example). Place the extracted files into this jar folder:  stanford-parser-3.x.x-models.jar and stanford-parser.jar.</p>\n<p>As shown above you can use the environment variables (STANFORD_PARSER &amp; STANFORD_MODELS) to point to this 'jars' folder. I'm using Linux, so if you use Windows please use something like: C://folder//jars.</p></li>\n<li><p>Open the stanford-parser-3.x.x-models.jar using an Archive manager (7zip).</p></li>\n<li><p>Browse inside the jar file; edu/stanford/nlp/models/lexparser. Again, extract the file called 'englishPCFG.ser.gz'. Remember the location where you extract this ser.gz file.</p></li>\n<li><p>When creating a StanfordParser instance, you can provide the model path as parameter. This is the complete path to the model, in our case /location/of/englishPCFG.ser.gz.</p></li>\n<li><p>Try my example! (don't forget the change the jar paths and change the model path to the ser.gz location)</p></li>\n</ol>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. Sure, try the following in Python: Output: [Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]),\n  Tree(',', [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN',\n  ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ',\n  ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ',\n  [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ',\n  ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]),\n  Tree('.', ['?'])])])] Note 1:\nIn this example both the parser & model jars are in the same folder. Note 2: Note 3:\nThe englishPCFG.ser.gz file can be found inside the models.jar file (/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz). Please use come archive manager to 'unzip' the models.jar file. Note 4:\nBe sure you are using Java JRE (Runtime Environment) 1.8 also known as Oracle JDK 8. Otherwise you will get: Unsupported major.minor version 52.0. Download NLTK v3 from: https://github.com/nltk/nltk. And install NLTK: sudo python setup.py install You can use the NLTK downloader to get Stanford Parser, using Python: Try my example! (don't forget the change the jar paths and change the model path to the ser.gz location) OR: Download and install NLTK v3, same as above. Download the latest version from (current version filename is stanford-parser-full-2015-01-29.zip):\nhttp://nlp.stanford.edu/software/lex-parser.shtml#Download Extract the standford-parser-full-20xx-xx-xx.zip.  Create a new folder ('jars' in my example). Place the extracted files into this jar folder:  stanford-parser-3.x.x-models.jar and stanford-parser.jar. As shown above you can use the environment variables (STANFORD_PARSER & STANFORD_MODELS) to point to this 'jars' folder. I'm using Linux, so if you use Windows please use something like: C://folder//jars. Open the stanford-parser-3.x.x-models.jar using an Archive manager (7zip). Browse inside the jar file; edu/stanford/nlp/models/lexparser. Again, extract the file called 'englishPCFG.ser.gz'. Remember the location where you extract this ser.gz file. When creating a StanfordParser instance, you can provide the model path as parameter. This is the complete path to the model, in our case /location/of/englishPCFG.ser.gz. Try my example! (don't forget the change the jar paths and change the model path to the ser.gz location)"}, {"id": 34112695, "score": 78, "vote": 0, "content": "<h2>Deprecated Answer</h2>\n<p>The answer below is deprecated, please use the solution on <a href=\"https://stackoverflow.com/a/51981566/610569\">https://stackoverflow.com/a/51981566/610569</a> for NLTK v3.3 and above.</p>\n<hr/>\n<h2>EDITED</h2>\n<p>Note: The following answer will only work on:</p>\n<ul>\n<li>NLTK version &gt;=3.2.4</li>\n<li>Stanford Tools compiled since 2015-04-20</li>\n<li>Python 2.7, 3.4 and 3.5 (Python 3.6 is not yet officially supported)</li>\n</ul>\n<p>As both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.</p>\n<p><strong>Always refer to <a href=\"https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software\" rel=\"noreferrer\">https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software</a> for the latest instruction on how to interface Stanford NLP tools using NLTK!!</strong> </p>\n<hr/>\n<h2>TL;DR</h2>\n<pre><code class=\"python\">cd $HOME\n\n# Update / Install NLTK\npip install -U nltk\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n\n\nexport STANFORDTOOLSDIR=$HOME\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n</code></pre>\n<p>Then:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.tag.stanford import StanfordPOSTagger\n&gt;&gt;&gt; st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n&gt;&gt;&gt; st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n&gt;&gt;&gt; from nltk.tag import StanfordNERTagger\n&gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n&gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n&gt;&gt;&gt; from nltk.parse.stanford import StanfordParser\n&gt;&gt;&gt; parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n&gt;&gt;&gt; list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n\n&gt;&gt;&gt; from nltk.parse.stanford import StanfordDependencyParser\n&gt;&gt;&gt; dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n&gt;&gt;&gt; print [parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")]\n[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]\n</code></pre>\n<hr/>\n<h2>In Long:</h2>\n<p><br/></p>\n<p><strong>Firstly</strong>, one must note that <strong><em>the Stanford NLP tools are written in Java</em></strong> and <strong><em>NLTK is written in Python</em></strong>. The way NLTK is interfacing the tool is through the call the Java tool through the command line interface. </p>\n<p><strong>Secondly</strong>, the <code>NLTK</code> API to the Stanford NLP tools have changed quite a lot since the version 3.1. So it is advisable to update your NLTK package to v3.1.</p>\n<p><strong>Thirdly</strong>, the <code>NLTK</code> API to Stanford NLP Tools wraps around the individual NLP tools, e.g. <a href=\"http://nlp.stanford.edu/software/tagger.shtml\" rel=\"noreferrer\">Stanford POS tagger</a>, <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"noreferrer\">Stanford NER Tagger</a>, <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">Stanford Parser</a>. </p>\n<p>For the POS and NER tagger, it <strong>DOES NOT</strong> wrap around the <a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"noreferrer\"><strong>Stanford Core NLP package</strong></a>. </p>\n<p>For the Stanford Parser, it's a special case where it wraps around both the Stanford Parser and the Stanford Core NLP (personally, I have not used the latter using NLTK, i would rather follow @dimazest's demonstration on <a href=\"http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html\" rel=\"noreferrer\">http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html</a> )</p>\n<p><strong>Note that as of NLTK v3.1, the <code>STANFORD_JAR</code> and <code>STANFORD_PARSER</code> variables is deprecated and NO LONGER used</strong></p>\n<hr/>\n<h2>In Longer:</h2>\n<p><br/></p>\n<h2>STEP 1</h2>\n<p><strong>Assuming that you have installed Java appropriately on your OS.</strong></p>\n<p>Now, install/update your NLTK version (see <a href=\"http://www.nltk.org/install.html\" rel=\"noreferrer\">http://www.nltk.org/install.html</a>):</p>\n<ul>\n<li><strong>Using pip</strong>: <code>sudo pip install -U nltk</code></li>\n<li><strong>Debian distro</strong> (using apt-get): <code>sudo apt-get install python-nltk</code></li>\n</ul>\n<p><strong>For Windows</strong> (Use the 32-bit binary installation):</p>\n<ol>\n<li>Install Python 3.4: <a href=\"http://www.python.org/downloads/\" rel=\"noreferrer\">http://www.python.org/downloads/</a> (avoid the 64-bit versions)</li>\n<li>Install Numpy (optional): <a href=\"http://sourceforge.net/projects/numpy/files/NumPy/\" rel=\"noreferrer\">http://sourceforge.net/projects/numpy/files/NumPy/</a> (the version that specifies pythnon3.4)</li>\n<li>Install NLTK: <a href=\"http://pypi.python.org/pypi/nltk\" rel=\"noreferrer\">http://pypi.python.org/pypi/nltk</a></li>\n<li>Test installation: Start&gt;Python34, then type import nltk</li>\n</ol>\n<p>(<strong>Why not 64 bit?</strong> See <a href=\"https://github.com/nltk/nltk/issues/1079\" rel=\"noreferrer\">https://github.com/nltk/nltk/issues/1079</a>)</p>\n<hr/>\n<p>Then out of paranoia, recheck your <code>nltk</code> version inside python:</p>\n<pre><code class=\"python\">from __future__ import print_function\nimport nltk\nprint(nltk.__version__)\n</code></pre>\n<p>Or on the command line:</p>\n<pre><code class=\"python\">python3 -c \"import nltk; print(nltk.__version__)\"\n</code></pre>\n<p>Make sure that you see <code>3.1</code> as the output.</p>\n<p>For even more paranoia, check that all your favorite Stanford NLP tools API are available:</p>\n<pre><code class=\"python\">from nltk.parse.stanford import StanfordParser\nfrom nltk.parse.stanford import StanfordDependencyParser\nfrom nltk.parse.stanford import StanfordNeuralDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\nfrom nltk.tokenize.stanford import StanfordTokenizer\n</code></pre>\n<p>(<strong>Note</strong>: The imports above will <strong>ONLY</strong> ensure that you are using a correct NLTK version that contains these APIs. Not seeing errors in the import doesn't mean that you have successfully configured the NLTK API to use the Stanford Tools)</p>\n<hr/>\n<h2>STEP 2</h2>\n<p>Now that you have checked that you have the correct version of NLTK that contains the necessary Stanford NLP tools interface. You need to download and extract all the necessary Stanford NLP tools.</p>\n<p><strong>TL;DR</strong>, in Unix:</p>\n<pre><code class=\"python\">cd $HOME\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n</code></pre>\n<p>In Windows / Mac:</p>\n<ul>\n<li>Download and unzip the parser from <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml#Download\" rel=\"noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml#Download</a></li>\n<li>Download and unizp the <strong>FULL VERSION</strong> tagger from <a href=\"http://nlp.stanford.edu/software/tagger.shtml#Download\" rel=\"noreferrer\">http://nlp.stanford.edu/software/tagger.shtml#Download</a></li>\n<li>Download and unizp the NER tagger from <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml#Download\" rel=\"noreferrer\">http://nlp.stanford.edu/software/CRF-NER.shtml#Download</a></li>\n</ul>\n<hr/>\n<h2>STEP 3</h2>\n<p>Setup the environment variables such that NLTK can find the relevant file path automatically. You have to set the following variables:</p>\n<ul>\n<li><p>Add the appropriate Stanford NLP <code>.jar</code> file to the  <code>CLASSPATH</code> environment variable.</p>\n<ul>\n<li>e.g. for the NER, it will be <code>stanford-ner-2015-04-20/stanford-ner.jar</code></li>\n<li>e.g. for the POS, it will be <code>stanford-postagger-full-2015-04-20/stanford-postagger.jar</code></li>\n<li>e.g. for the parser, it will be <code>stanford-parser-full-2015-04-20/stanford-parser.jar</code> and the parser model jar file, <code>stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar</code></li>\n</ul></li>\n<li><p>Add the appropriate model directory to the <code>STANFORD_MODELS</code> variable (i.e. the directory where you can find where the pre-trained models are saved)</p>\n<ul>\n<li>e.g. for the NER, it will be in <code>stanford-ner-2015-04-20/classifiers/</code></li>\n<li>e.g. for the POS, it will be in <code>stanford-postagger-full-2015-04-20/models/</code></li>\n<li>e.g. for the Parser, there won't be a model directory.</li>\n</ul></li>\n</ul>\n<p>In the code, see that it searches for the <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L55\" rel=\"noreferrer\"><code>STANFORD_MODELS</code></a> directory before appending the model name. Also see that, the API also automatically tries to search the OS environments for the <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L115\" rel=\"noreferrer\">`CLASSPATH</a>)</p>\n<p><strong>Note that as of NLTK v3.1, the <code>STANFORD_JAR</code> variables is deprecated and NO LONGER used</strong>. Code snippets found in the following Stackoverflow questions might not work:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/34053021/stanford-dependency-parser-setup-and-nltk\">Stanford Dependency Parser Setup and NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/21652251/nltk-interface-to-stanford-parser\">nltk interface to stanford parser</a></li>\n<li><a href=\"https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk\">trouble importing stanford pos tagger into nltk</a></li>\n<li><a href=\"https://stackoverflow.com/questions/30745714/stanford-entity-recognizer-caseless-in-python-nltk\">Stanford Entity Recognizer (caseless) in Python Nltk</a></li>\n<li><a href=\"https://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk\">How to improve speed with Stanford NLP Tagger and NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/19694106/how-can-i-get-the-stanford-nltk-python-module\">How can I get the stanford NLTK python module?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/29200007/stanford-parser-and-nltk-windows\">Stanford Parser and NLTK windows</a></li>\n<li><a href=\"https://stackoverflow.com/questions/18371092/stanford-named-entity-recognizer-ner-functionality-with-nltk\">Stanford Named Entity Recognizer (NER) functionality with NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/32056719/stanford-parser-with-nltk-produces-empty-output\">Stanford parser with NLTK produces empty output</a></li>\n<li><a href=\"https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\">Extract list of Persons and Organizations using Stanford NER Tagger in NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python\">Error using Stanford POS Tagger in NLTK Python</a></li>\n</ul>\n<p><strong>TL;DR for STEP 3 on Ubuntu</strong></p>\n<pre><code class=\"python\">export STANFORDTOOLSDIR=/home/path/to/stanford/tools/\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n</code></pre>\n<p>(<strong>For Windows</strong>: See <a href=\"https://stackoverflow.com/a/17176423/610569\">https://stackoverflow.com/a/17176423/610569</a> for instructions for setting environment variables)</p>\n<p>You <strong>MUST</strong> set the variables as above before starting python, then:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.tag.stanford import StanfordPOSTagger\n&gt;&gt;&gt; st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n&gt;&gt;&gt; st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n&gt;&gt;&gt; from nltk.tag import StanfordNERTagger\n&gt;&gt;&gt; st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n&gt;&gt;&gt; st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n&gt;&gt;&gt; from nltk.parse.stanford import StanfordParser\n&gt;&gt;&gt; parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n&gt;&gt;&gt; list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n</code></pre>\n<hr/>\n<p>Alternatively, you could try add the environment variables inside python, as the previous answers have suggested but you can also directly tell the parser/tagger to initialize to the direct path where you kept the <code>.jar</code> file and your models. </p>\n<p>There is NO need to set the environment variables if you use the following method <strong>BUT</strong> when the API changes its parameter names, you will need to change accordingly. <strong><em>That is why it is MORE advisable to set the environment variables than to modify your python code to suit the NLTK version.</em></strong></p>\n<p>For example (<strong>without setting any environment variables</strong>):</p>\n<pre><code class=\"python\"># POS tagging:\n\nfrom nltk.tag import StanfordPOSTagger\n\nstanford_pos_dir = '/home/alvas/stanford-postagger-full-2015-04-20/'\neng_model_filename= stanford_pos_dir + 'models/english-left3words-distsim.tagger'\nmy_path_to_jar= stanford_pos_dir + 'stanford-postagger.jar'\n\nst = StanfordPOSTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('What is the airspeed of an unladen swallow ?'.split())\n\n\n# NER Tagging:\nfrom nltk.tag import StanfordNERTagger\n\nstanford_ner_dir = '/home/alvas/stanford-ner/'\neng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\nmy_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n\nst = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n\n# Parsing:\nfrom nltk.parse.stanford import StanfordParser\n\nstanford_parser_dir = '/home/alvas/stanford-parser/'\neng_model_path = stanford_parser_dir  + \"edu/stanford/nlp/models/lexparser/englishRNN.ser.gz\"\nmy_path_to_models_jar = stanford_parser_dir  + \"stanford-parser-3.5.2-models.jar\"\nmy_path_to_jar = stanford_parser_dir  + \"stanford-parser.jar\"\n\nparser=StanfordParser(model_path=eng_model_path, path_to_models_jar=my_path_to_models_jar, path_to_jar=my_path_to_jar)\n</code></pre>\n", "abstract": "The answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above. Note: The following answer will only work on: As both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix. Always refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!!  Then:  Firstly, one must note that the Stanford NLP tools are written in Java and NLTK is written in Python. The way NLTK is interfacing the tool is through the call the Java tool through the command line interface.  Secondly, the NLTK API to the Stanford NLP tools have changed quite a lot since the version 3.1. So it is advisable to update your NLTK package to v3.1. Thirdly, the NLTK API to Stanford NLP Tools wraps around the individual NLP tools, e.g. Stanford POS tagger, Stanford NER Tagger, Stanford Parser.  For the POS and NER tagger, it DOES NOT wrap around the Stanford Core NLP package.  For the Stanford Parser, it's a special case where it wraps around both the Stanford Parser and the Stanford Core NLP (personally, I have not used the latter using NLTK, i would rather follow @dimazest's demonstration on http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html ) Note that as of NLTK v3.1, the STANFORD_JAR and STANFORD_PARSER variables is deprecated and NO LONGER used  Assuming that you have installed Java appropriately on your OS. Now, install/update your NLTK version (see http://www.nltk.org/install.html): For Windows (Use the 32-bit binary installation): (Why not 64 bit? See https://github.com/nltk/nltk/issues/1079) Then out of paranoia, recheck your nltk version inside python: Or on the command line: Make sure that you see 3.1 as the output. For even more paranoia, check that all your favorite Stanford NLP tools API are available: (Note: The imports above will ONLY ensure that you are using a correct NLTK version that contains these APIs. Not seeing errors in the import doesn't mean that you have successfully configured the NLTK API to use the Stanford Tools) Now that you have checked that you have the correct version of NLTK that contains the necessary Stanford NLP tools interface. You need to download and extract all the necessary Stanford NLP tools. TL;DR, in Unix: In Windows / Mac: Setup the environment variables such that NLTK can find the relevant file path automatically. You have to set the following variables: Add the appropriate Stanford NLP .jar file to the  CLASSPATH environment variable. Add the appropriate model directory to the STANFORD_MODELS variable (i.e. the directory where you can find where the pre-trained models are saved) In the code, see that it searches for the STANFORD_MODELS directory before appending the model name. Also see that, the API also automatically tries to search the OS environments for the `CLASSPATH) Note that as of NLTK v3.1, the STANFORD_JAR variables is deprecated and NO LONGER used. Code snippets found in the following Stackoverflow questions might not work: TL;DR for STEP 3 on Ubuntu (For Windows: See https://stackoverflow.com/a/17176423/610569 for instructions for setting environment variables) You MUST set the variables as above before starting python, then: Alternatively, you could try add the environment variables inside python, as the previous answers have suggested but you can also directly tell the parser/tagger to initialize to the direct path where you kept the .jar file and your models.  There is NO need to set the environment variables if you use the following method BUT when the API changes its parameter names, you will need to change accordingly. That is why it is MORE advisable to set the environment variables than to modify your python code to suit the NLTK version. For example (without setting any environment variables):"}, {"id": 51981566, "score": 26, "vote": 0, "content": "<p>As of NLTK v3.3, users should <strong>avoid</strong> the Stanford NER or POS taggers from <code>nltk.tag</code>, and <strong>avoid</strong> Stanford tokenizer/segmenter from <code>nltk.tokenize</code>.</p>\n<p>Instead use the new <code>nltk.parse.corenlp.CoreNLPParser</code> API. </p>\n<p>Please see <a href=\"https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\" rel=\"noreferrer\">https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK</a></p>\n<hr/>\n<p>(Avoiding link only answer, I've pasted the docs from NLTK github wiki below)</p>\n<p>First, update your NLTK</p>\n<pre><code class=\"python\">pip3 install -U nltk # Make sure is &gt;=3.3\n</code></pre>\n<p>Then download the necessary CoreNLP packages:</p>\n<pre><code class=\"python\">cd ~\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\nunzip stanford-corenlp-full-2018-02-27.zip\ncd stanford-corenlp-full-2018-02-27\n\n# Get the Chinese model \nwget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties \n\n# Get the Arabic model\nwget http://nlp.stanford.edu/software/stanford-arabic-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-arabic.properties \n\n# Get the French model\nwget http://nlp.stanford.edu/software/stanford-french-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties \n\n# Get the German model\nwget http://nlp.stanford.edu/software/stanford-german-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-german.properties \n\n\n# Get the Spanish model\nwget http://nlp.stanford.edu/software/stanford-spanish-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties \n</code></pre>\n<h1>English</h1>\n<p>Still in the <code>stanford-corenlp-full-2018-02-27</code> directory, start the server:</p>\n<pre><code class=\"python\">java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,ner,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000 &amp; \n</code></pre>\n<p>Then in Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.parse import CoreNLPParser\n\n# Lexical Parser\n&gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000')\n\n# Parse tokenized text.\n&gt;&gt;&gt; list(parser.parse('What is the airspeed of an unladen swallow ?'.split()))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Parse raw string.\n&gt;&gt;&gt; list(parser.raw_parse('What is the airspeed of an unladen swallow ?'))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Neural Dependency Parser\n&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPDependencyParser\n&gt;&gt;&gt; dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n&gt;&gt;&gt; parses = dep_parser.parse('What is the airspeed of an unladen swallow ?'.split())\n&gt;&gt;&gt; [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n[[(('What', 'WP'), 'cop', ('is', 'VBZ')), (('What', 'WP'), 'nsubj', ('airspeed', 'NN')), (('airspeed', 'NN'), 'det', ('the', 'DT')), (('airspeed', 'NN'), 'nmod', ('swallow', 'VB')), (('swallow', 'VB'), 'case', ('of', 'IN')), (('swallow', 'VB'), 'det', ('an', 'DT')), (('swallow', 'VB'), 'amod', ('unladen', 'JJ')), (('What', 'WP'), 'punct', ('?', '.'))]]\n\n\n# Tokenizer\n&gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000')\n&gt;&gt;&gt; list(parser.tokenize('What is the airspeed of an unladen swallow?'))\n['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n\n# POS Tagger\n&gt;&gt;&gt; pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n&gt;&gt;&gt; list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split()))\n[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\n# NER Tagger\n&gt;&gt;&gt; ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n&gt;&gt;&gt; list(ner_tagger.tag(('Rami Eid is studying at Stony Brook University in NY'.split())))\n[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]\n</code></pre>\n<h1>Chinese</h1>\n<p>Start the server a little differently, still from the `stanford-corenlp-full-2018-02-27 directory:</p>\n<pre><code class=\"python\">java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-chinese.properties \\\n-preload tokenize,ssplit,pos,lemma,ner,parse \\\n-status_port 9001  -port 9001 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; parser = CoreNLPParser('http://localhost:9001')\n&gt;&gt;&gt; list(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002'))\n['\u6211\u5bb6', '\u6ca1\u6709', '\u7535\u8111', '\u3002']\n\n&gt;&gt;&gt; list(parser.parse(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002')))\n[Tree('ROOT', [Tree('IP', [Tree('IP', [Tree('NP', [Tree('NN', ['\u6211\u5bb6'])]), Tree('VP', [Tree('VE', ['\u6ca1\u6709']), Tree('NP', [Tree('NN', ['\u7535\u8111'])])])]), Tree('PU', ['\u3002'])])])]\n</code></pre>\n<h1>Arabic</h1>\n<p>Start the server:</p>\n<pre><code class=\"python\">java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-arabic.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9005  -port 9005 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.parse import CoreNLPParser\n&gt;&gt;&gt; parser = CoreNLPParser('http://localhost:9005')\n&gt;&gt;&gt; text = u'\u0627\u0646\u0627 \u062d\u0627\u0645\u0644'\n\n# Parser.\n&gt;&gt;&gt; parser.raw_parse(text)\n&lt;list_iterator object at 0x7f0d894c9940&gt;\n&gt;&gt;&gt; list(parser.raw_parse(text))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n&gt;&gt;&gt; list(parser.parse(parser.tokenize(text)))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n\n# Tokenizer / Segmenter.\n&gt;&gt;&gt; list(parser.tokenize(text))\n['\u0627\u0646\u0627', '\u062d\u0627\u0645\u0644']\n\n# POS tagg\n&gt;&gt;&gt; pos_tagger = CoreNLPParser('http://localhost:9005', tagtype='pos')\n&gt;&gt;&gt; list(pos_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'PRP'), ('\u062d\u0627\u0645\u0644', 'NN')]\n\n\n# NER tag\n&gt;&gt;&gt; ner_tagger = CoreNLPParser('http://localhost:9005', tagtype='ner')\n&gt;&gt;&gt; list(ner_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'O'), ('\u062d\u0627\u0645\u0644', 'O')]\n</code></pre>\n<h1>French</h1>\n<p>Start the server:</p>\n<pre><code class=\"python\">java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-french.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9004  -port 9004 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; parser = CoreNLPParser('http://localhost:9004')\n&gt;&gt;&gt; list(parser.parse('Je suis enceinte'.split()))\n[Tree('ROOT', [Tree('SENT', [Tree('NP', [Tree('PRON', ['Je']), Tree('VERB', ['suis']), Tree('AP', [Tree('ADJ', ['enceinte'])])])])])]\n&gt;&gt;&gt; pos_tagger = CoreNLPParser('http://localhost:9004', tagtype='pos')\n&gt;&gt;&gt; pos_tagger.tag('Je suis enceinte'.split())\n[('Je', 'PRON'), ('suis', 'VERB'), ('enceinte', 'ADJ')]\n</code></pre>\n<h1>German</h1>\n<p>Start the server:</p>\n<pre><code class=\"python\">java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-german.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9002  -port 9002 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; parser = CoreNLPParser('http://localhost:9002')\n&gt;&gt;&gt; list(parser.raw_parse('Ich bin schwanger'))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n&gt;&gt;&gt; list(parser.parse('Ich bin schwanger'.split()))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n\n\n&gt;&gt;&gt; pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n&gt;&gt;&gt; pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n&gt;&gt;&gt; pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n&gt;&gt;&gt; pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n&gt;&gt;&gt; ner_tagger = CoreNLPParser('http://localhost:9002', tagtype='ner')\n&gt;&gt;&gt; ner_tagger.tag('Donald Trump besuchte Angela Merkel in Berlin.'.split())\n[('Donald', 'PERSON'), ('Trump', 'PERSON'), ('besuchte', 'O'), ('Angela', 'PERSON'), ('Merkel', 'PERSON'), ('in', 'O'), ('Berlin', 'LOCATION'), ('.', 'O')]\n</code></pre>\n<h1>Spanish</h1>\n<p>Start the server:</p>\n<pre><code class=\"python\">java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-spanish.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9003  -port 9003 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; pos_tagger = CoreNLPParser('http://localhost:9003', tagtype='pos')\n&gt;&gt;&gt; pos_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PROPN'), ('Obama', 'PROPN'), ('sali\u00f3', 'VERB'), ('con', 'ADP'), ('Michael', 'PROPN'), ('Jackson', 'PROPN'), ('.', 'PUNCT')]\n&gt;&gt;&gt; ner_tagger = CoreNLPParser('http://localhost:9003', tagtype='ner')\n&gt;&gt;&gt; ner_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('sali\u00f3', 'O'), ('con', 'O'), ('Michael', 'PERSON'), ('Jackson', 'PERSON'), ('.', 'O')]\n</code></pre>\n", "abstract": "As of NLTK v3.3, users should avoid the Stanford NER or POS taggers from nltk.tag, and avoid Stanford tokenizer/segmenter from nltk.tokenize. Instead use the new nltk.parse.corenlp.CoreNLPParser API.  Please see https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK (Avoiding link only answer, I've pasted the docs from NLTK github wiki below) First, update your NLTK Then download the necessary CoreNLP packages: Still in the stanford-corenlp-full-2018-02-27 directory, start the server: Then in Python: Start the server a little differently, still from the `stanford-corenlp-full-2018-02-27 directory: In Python: Start the server: In Python: Start the server: In Python: Start the server: In Python: Start the server: In Python:"}, {"id": 14376410, "score": 23, "vote": 0, "content": "<h2>Deprecated Answer</h2>\n<p>The answer below is deprecated, please use the solution on <a href=\"https://stackoverflow.com/a/51981566/610569\">https://stackoverflow.com/a/51981566/610569</a> for NLTK v3.3 and above.</p>\n<hr/>\n<h2>Edited</h2>\n<p>As of the current Stanford parser (2015-04-20), the default output for the <code>lexparser.sh</code> has changed so the script below will not work.</p>\n<p>But this answer is kept for legacy sake, it will still work with <a href=\"http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip</a> though.</p>\n<hr/>\n<h2>Original Answer</h2>\n<p>I suggest you don't mess with Jython, JPype. Let python do python stuff and let java do java stuff, get the Stanford Parser output through the console.</p>\n<p>After you've installed the <a href=\"http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip\" rel=\"nofollow noreferrer\">Stanford Parser</a> in your home directory <code>~/</code>, just use this python recipe to get the flat bracketed parse:</p>\n<pre><code class=\"python\">import os\nsentence = \"this is a foo bar i want to parse.\"\n\nos.popen(\"echo '\"+sentence+\"' &gt; ~/stanfordtemp.txt\")\nparser_out = os.popen(\"~/stanford-parser-2012-11-12/lexparser.sh ~/stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parser_out if i.strip()[0] == \"(\"] )\nprint bracketed_parse\n</code></pre>\n", "abstract": "The answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above. As of the current Stanford parser (2015-04-20), the default output for the lexparser.sh has changed so the script below will not work. But this answer is kept for legacy sake, it will still work with http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip though. I suggest you don't mess with Jython, JPype. Let python do python stuff and let java do java stuff, get the Stanford Parser output through the console. After you've installed the Stanford Parser in your home directory ~/, just use this python recipe to get the flat bracketed parse:"}, {"id": 13939013, "score": 7, "vote": 0, "content": "<p>There is python interface for stanford parser</p>\n<p><a href=\"http://projects.csail.mit.edu/spatial/Stanford_Parser\">http://projects.csail.mit.edu/spatial/Stanford_Parser</a></p>\n", "abstract": "There is python interface for stanford parser http://projects.csail.mit.edu/spatial/Stanford_Parser"}, {"id": 18366016, "score": 7, "vote": 0, "content": "<p>The Stanford Core NLP software page has a list of python wrappers:</p>\n<p><a href=\"http://nlp.stanford.edu/software/corenlp.shtml#Extensions\">http://nlp.stanford.edu/software/corenlp.shtml#Extensions</a></p>\n", "abstract": "The Stanford Core NLP software page has a list of python wrappers: http://nlp.stanford.edu/software/corenlp.shtml#Extensions"}, {"id": 13883479, "score": 6, "vote": 0, "content": "<p>If I remember well, the Stanford parser is a java library, therefore you must have a Java interpreter running on your server/computer.</p>\n<p>I used it once a server, combined with a php script. The script used php's exec() function to make a command-line call to the parser like so:</p>\n<pre><code class=\"python\">&lt;?php\n\nexec( \"java -cp /pathTo/stanford-parser.jar -mx100m edu.stanford.nlp.process.DocumentPreprocessor /pathTo/fileToParse &gt; /pathTo/resultFile 2&gt;/dev/null\" );\n\n?&gt;\n</code></pre>\n<p>I don't remember all the details of this command, it basically opened the fileToParse, parsed it, and wrote the output in the resultFile. PHP would then open the result file for further use.</p>\n<p>The end of the command directs the parser's verbose to NULL, to prevent unnecessary command line information from disturbing the script.</p>\n<p>I don't know much about Python, but there might be a way to make command line calls.</p>\n<p>It might not be the exact route you were hoping for, but hopefully it'll give you some inspiration. Best of luck.</p>\n", "abstract": "If I remember well, the Stanford parser is a java library, therefore you must have a Java interpreter running on your server/computer. I used it once a server, combined with a php script. The script used php's exec() function to make a command-line call to the parser like so: I don't remember all the details of this command, it basically opened the fileToParse, parsed it, and wrote the output in the resultFile. PHP would then open the result file for further use. The end of the command directs the parser's verbose to NULL, to prevent unnecessary command line information from disturbing the script. I don't know much about Python, but there might be a way to make command line calls. It might not be the exact route you were hoping for, but hopefully it'll give you some inspiration. Best of luck."}, {"id": 25778313, "score": 6, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>Here is an adaptation of danger98's code that works with nltk3.0.0 on windoze, and presumably the other platforms as well, adjust directory names as appropriate for your setup:</p>\n<pre><code class=\"python\">import os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'd:/stanford-parser'\nos.environ['STANFORD_MODELS'] = 'd:/stanford-parser'\nos.environ['JAVAHOME'] = 'c:/Program Files/java/jre7/bin'\n\nparser = stanford.StanfordParser(model_path=\"d:/stanford-grammars/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n</code></pre>\n<p>Note that the parsing command has changed (see the source code at www.nltk.org/_modules/nltk/parse/stanford.html), and that you need to define the JAVAHOME variable.  I tried to get it to read the grammar file in situ in the jar, but have so far failed to do that.</p>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. Here is an adaptation of danger98's code that works with nltk3.0.0 on windoze, and presumably the other platforms as well, adjust directory names as appropriate for your setup: Note that the parsing command has changed (see the source code at www.nltk.org/_modules/nltk/parse/stanford.html), and that you need to define the JAVAHOME variable.  I tried to get it to read the grammar file in situ in the jar, but have so far failed to do that."}, {"id": 17999231, "score": 4, "vote": 0, "content": "<p>You can use the Stanford Parsers output to create a Tree in nltk (nltk.tree.Tree).</p>\n<p>Assuming the stanford parser gives you a file in which there is exactly one parse tree for every sentence.\nThen this example works, though it might not look very pythonic:</p>\n<pre><code class=\"python\">f = open(sys.argv[1]+\".output\"+\".30\"+\".stp\", \"r\")\nparse_trees_text=[]\ntree = \"\"\nfor line in f:\n  if line.isspace():\n    parse_trees_text.append(tree)\ntree = \"\"\n  elif \"(. ...))\" in line:\n#print \"YES\"\ntree = tree+')'\nparse_trees_text.append(tree)\ntree = \"\"\n  else:\ntree = tree + line\n\nparse_trees=[]\nfor t in parse_trees_text:\n  tree = nltk.Tree(t)\n  tree.__delitem__(len(tree)-1) #delete \"(. .))\" from tree (you don't need that)\n  s = traverse(tree)\n  parse_trees.append(tree)\n</code></pre>\n", "abstract": "You can use the Stanford Parsers output to create a Tree in nltk (nltk.tree.Tree). Assuming the stanford parser gives you a file in which there is exactly one parse tree for every sentence.\nThen this example works, though it might not look very pythonic:"}, {"id": 36504961, "score": 4, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>Since nobody really mentioned and it's somehow troubled me a lot, here is an alternative way to use Stanford parser in python:</p>\n<pre><code class=\"python\">stanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                        path_to_models_jar=stanford_model_jar)\n</code></pre>\n<p>in this way, you don't need to worry about the path thing anymore.</p>\n<p>For those who cannot use it properly on Ubuntu or run the code in Eclipse.</p>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. Since nobody really mentioned and it's somehow troubled me a lot, here is an alternative way to use Stanford parser in python: in this way, you don't need to worry about the path thing anymore. For those who cannot use it properly on Ubuntu or run the code in Eclipse."}, {"id": 27415031, "score": 3, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>Here is the windows version of alvas's answer</p>\n<pre><code class=\"python\">sentences = ('. '.join(['this is sentence one without a period','this is another foo bar sentence '])+'.').encode('ascii',errors = 'ignore')\ncatpath =r\"YOUR CURRENT FILE PATH\"\n\nf = open('stanfordtemp.txt','w')\nf.write(sentences)\nf.close()\n\nparse_out = os.popen(catpath+r\"\\nlp_tools\\stanford-parser-2010-08-20\\lexparser.bat \"+catpath+r\"\\stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parse_out if i.strip() if i.strip()[0] == \"(\"] )\nbracketed_parse = \"\\n(ROOT\".join(bracketed_parse.split(\" (ROOT\")).split('\\n')\naa = map(lambda x :ParentedTree.fromstring(x),bracketed_parse)\n</code></pre>\n<p><strong>NOTES:</strong></p>\n<ul>\n<li><p>In <code>lexparser.bat</code>  you need to change all the paths into absolute path to avoid java errors such as \"class not found\"</p></li>\n<li><p>I strongly recommend you to apply this method under windows since I Tried several answers on the page and   all the methods communicates python with Java fails.</p></li>\n<li><p>wish to hear from you if you succeed on windows and wish you can tell me how you overcome all these problems.</p></li>\n<li><p>search python wrapper for stanford coreNLP to get the python version</p></li>\n</ul>\n<hr/>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. Here is the windows version of alvas's answer NOTES: In lexparser.bat  you need to change all the paths into absolute path to avoid java errors such as \"class not found\" I strongly recommend you to apply this method under windows since I Tried several answers on the page and   all the methods communicates python with Java fails. wish to hear from you if you succeed on windows and wish you can tell me how you overcome all these problems. search python wrapper for stanford coreNLP to get the python version"}, {"id": 28903231, "score": 3, "vote": 0, "content": "<p>I am on a windows machine and you can simply run the parser normally as you do from the command like but as in a different directory so you don't need to edit the lexparser.bat file. Just put in the full path. </p>\n<pre><code class=\"python\">cmd = r'java -cp \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30 edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat \"typedDependencies\" \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30\\stanford-parser-3.5.1-models\\edu\\stanford\\nlp\\models\\lexparser\\englishFactored.ser.gz stanfordtemp.txt'\nparse_out = os.popen(cmd).readlines()\n</code></pre>\n<p>The tricky part for me was realizing how to run a java program from a different path. There must be a better way but this works.</p>\n", "abstract": "I am on a windows machine and you can simply run the parser normally as you do from the command like but as in a different directory so you don't need to edit the lexparser.bat file. Just put in the full path.  The tricky part for me was realizing how to run a java program from a different path. There must be a better way but this works."}, {"id": 32268939, "score": 3, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>A slight update (or simply alternative) on danger89's comprehensive answer on using Stanford Parser in NLTK and Python</p>\n<p>With stanford-parser-full-2015-04-20, JRE 1.8 and nltk 3.0.4 (python 2.7.6), it appears that you no longer need to extract the englishPCFG.ser.gz from stanford-parser-x.x.x-models.jar or setting up any os.environ</p>\n<pre><code class=\"python\">from nltk.parse.stanford import StanfordParser\n\nenglish_parser = StanfordParser('path/stanford-parser.jar', 'path/stanford-parser-3.5.2-models.jar')\n\ns = \"The real voyage of discovery consists not in seeking new landscapes, but in having new eyes.\"\n\nsentences = english_parser.raw_parse_sents((s,))\nprint sentences #only print &lt;listiterator object&gt; for this version\n\n#draw the tree\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n</code></pre>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. A slight update (or simply alternative) on danger89's comprehensive answer on using Stanford Parser in NLTK and Python With stanford-parser-full-2015-04-20, JRE 1.8 and nltk 3.0.4 (python 2.7.6), it appears that you no longer need to extract the englishPCFG.ser.gz from stanford-parser-x.x.x-models.jar or setting up any os.environ"}, {"id": 40097793, "score": 2, "vote": 0, "content": "<p>Note that this answer applies to NLTK v 3.0, and not to more recent versions.</p>\n<p>I cannot leave this as a comment because of reputation, but since I spent (wasted?) some time solving this I would rather share my problem/solution to get this parser to work in NLTK.</p>\n<p>In the <strong>excellent</strong> <a href=\"https://stackoverflow.com/a/34112695/5303618\">answer from alvas</a>, it is mentioned that:</p>\n<blockquote>\n<p>e.g. for the Parser, there won't be a model directory.</p>\n</blockquote>\n<p>This led me wrongly to:</p>\n<ul>\n<li>not be careful to the value I put to <code>STANFORD_MODELS</code>  (and only care about my <code>CLASSPATH</code>)</li>\n<li>leave <code>../path/tostanford-parser-full-2015-2012-09/models directory</code> * virtually empty* (or with a jar file whose name did not match nltk regex)!</li>\n</ul>\n<p>If the OP, like me, just wanted to use the parser, it may be confusing that when not downloading anything else (no POStagger, no NER,...) and following all these instructions, we still get an error.</p>\n<p>Eventually, for any <code>CLASSPATH</code> given (following examples and explanations in answers from this thread) I would still get the error:</p>\n<blockquote>\n<p>NLTK was unable to find stanford-parser-(\\d+)(.(\\d+))+-models.jar!\n  Set the CLASSPATH environment variable. For more information, on\n  stanford-parser-(\\d+)(.(\\d+))+-models.jar,</p>\n</blockquote>\n<p>see:\n    <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml</a></p>\n<p>OR:</p>\n<blockquote>\n<p>NLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable. For more information, on stanford-parser.jar,\n  see: <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/lex-parser.shtml</a></p>\n</blockquote>\n<p><em>Though</em>, importantly, I could correctly load and use the parser if I called the function with all arguments and path fully specified, as in:</p>\n<pre><code class=\"python\">stanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanfor-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                    path_to_models_jar=stanford_model_jar)\n</code></pre>\n<h2>Solution for Parser alone:</h2>\n<p>Therefore the error came from <code>NLTK</code> and how it is looking for jars using the supplied <code>STANFORD_MODELS</code> and <code>CLASSPATH</code> environment variables. To solve this, the <code>*-models.jar</code>, with the correct formatting (to match the regex in <code>NLTK</code> code, so no -corenlp-....jar) must be located in the folder designated by <code>STANFORD_MODELS</code>.</p>\n<p>Namely, I first created:</p>\n<pre><code class=\"python\">mkdir stanford-parser-full-2015-12-09/models\n</code></pre>\n<p>Then added in <code>.bashrc</code>:</p>\n<pre><code class=\"python\">export STANFORD_MODELS=/path/to/stanford-parser-full-2015-12-09/models\n</code></pre>\n<p>And finally, by copying <code>stanford-parser-3.6.0-models.jar</code> (or corresponding version), into:</p>\n<pre><code class=\"python\">path/to/stanford-parser-full-2015-12-09/models/\n</code></pre>\n<p>I could get <code>StanfordParser</code> to load smoothly in python with the classic <code>CLASSPATH</code> that points to <code>stanford-parser.jar</code>. Actually, as such, you can call <code>StanfordParser</code> with no parameters, the default will just work.</p>\n", "abstract": "Note that this answer applies to NLTK v 3.0, and not to more recent versions. I cannot leave this as a comment because of reputation, but since I spent (wasted?) some time solving this I would rather share my problem/solution to get this parser to work in NLTK. In the excellent answer from alvas, it is mentioned that: e.g. for the Parser, there won't be a model directory. This led me wrongly to: If the OP, like me, just wanted to use the parser, it may be confusing that when not downloading anything else (no POStagger, no NER,...) and following all these instructions, we still get an error. Eventually, for any CLASSPATH given (following examples and explanations in answers from this thread) I would still get the error: NLTK was unable to find stanford-parser-(\\d+)(.(\\d+))+-models.jar!\n  Set the CLASSPATH environment variable. For more information, on\n  stanford-parser-(\\d+)(.(\\d+))+-models.jar, see:\n    http://nlp.stanford.edu/software/lex-parser.shtml OR: NLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable. For more information, on stanford-parser.jar,\n  see: http://nlp.stanford.edu/software/lex-parser.shtml Though, importantly, I could correctly load and use the parser if I called the function with all arguments and path fully specified, as in: Therefore the error came from NLTK and how it is looking for jars using the supplied STANFORD_MODELS and CLASSPATH environment variables. To solve this, the *-models.jar, with the correct formatting (to match the regex in NLTK code, so no -corenlp-....jar) must be located in the folder designated by STANFORD_MODELS. Namely, I first created: Then added in .bashrc: And finally, by copying stanford-parser-3.6.0-models.jar (or corresponding version), into: I could get StanfordParser to load smoothly in python with the classic CLASSPATH that points to stanford-parser.jar. Actually, as such, you can call StanfordParser with no parameters, the default will just work."}, {"id": 41180959, "score": 2, "vote": 0, "content": "<p>I took many hours and finally found a simple solution for Windows users. Basically its summarized version of an <a href=\"https://stackoverflow.com/a/34112695/4566277\">existing answer</a> by alvas, but made easy to follow(hopefully) for those who are new to stanford NLP and are Window users.</p>\n<p><strong>1)</strong> Download the module you want to use, such as NER, POS etc. In my case i wanted to use NER, so i downloaded the module from <a href=\"http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\" rel=\"nofollow noreferrer\">http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip</a></p>\n<p><strong>2)</strong> Unzip the file.</p>\n<p><strong>3)</strong> Set the environment variables(classpath and stanford_modules) from the unzipped folder.</p>\n<pre><code class=\"python\">import os\nos.environ['CLASSPATH'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/stanford-ner.jar\"\nos.environ['STANFORD_MODELS'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/classifiers/\"\n</code></pre>\n<p><strong>4)</strong> set the environment variables for JAVA, as in where you have JAVA installed. for me it was below</p>\n<pre><code class=\"python\">os.environ['JAVAHOME'] = \"C:/Program Files/Java/jdk1.8.0_102/bin/java.exe\"\n</code></pre>\n<p><strong>5)</strong> import the module you want</p>\n<pre><code class=\"python\">from nltk.tag import StanfordNERTagger\n</code></pre>\n<p><strong>6)</strong> call the pretrained model which is present in classifier folder in the unzipped folder. add \".gz\" in the end for file extension. for me the model i wanted to use was <code>english.all.3class.distsim.crf.ser</code></p>\n<pre><code class=\"python\">st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n</code></pre>\n<p><strong>7)</strong> Now execute the parser!! and we are done!!</p>\n<pre><code class=\"python\">st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n</code></pre>\n", "abstract": "I took many hours and finally found a simple solution for Windows users. Basically its summarized version of an existing answer by alvas, but made easy to follow(hopefully) for those who are new to stanford NLP and are Window users. 1) Download the module you want to use, such as NER, POS etc. In my case i wanted to use NER, so i downloaded the module from http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip 2) Unzip the file. 3) Set the environment variables(classpath and stanford_modules) from the unzipped folder. 4) set the environment variables for JAVA, as in where you have JAVA installed. for me it was below 5) import the module you want 6) call the pretrained model which is present in classifier folder in the unzipped folder. add \".gz\" in the end for file extension. for me the model i wanted to use was english.all.3class.distsim.crf.ser 7) Now execute the parser!! and we are done!!"}, {"id": 46215645, "score": 2, "vote": 0, "content": "<p>I am using nltk version 3.2.4. And following code worked for me.</p>\n<pre><code class=\"python\">from nltk.internals import find_jars_within_path\nfrom nltk.tag import StanfordPOSTagger\nfrom nltk import word_tokenize\n\n# Alternatively to setting the CLASSPATH add the jar and model via their \npath:\njar = '/home/ubuntu/stanford-postagger-full-2017-06-09/stanford-postagger.jar'\nmodel = '/home/ubuntu/stanford-postagger-full-2017-06-09/models/english-left3words-distsim.tagger'\n\npos_tagger = StanfordPOSTagger(model, jar)\n\n# Add other jars from Stanford directory\nstanford_dir = pos_tagger._stanford_jar.rpartition('/')[0]\nstanford_jars = find_jars_within_path(stanford_dir)\npos_tagger._stanford_jar = ':'.join(stanford_jars)\n\ntext = pos_tagger.tag(word_tokenize(\"Open app and play movie\"))\nprint(text)\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">[('Open', 'VB'), ('app', 'NN'), ('and', 'CC'), ('play', 'VB'), ('movie', 'NN')]\n</code></pre>\n", "abstract": "I am using nltk version 3.2.4. And following code worked for me. Output:"}, {"id": 49345866, "score": 2, "vote": 0, "content": "<h2>Deprecated Answer</h2>\n<p>The answer below is deprecated, please use the solution on <a href=\"https://stackoverflow.com/a/51981566/610569\">https://stackoverflow.com/a/51981566/610569</a> for NLTK v3.3 and above.</p>\n<hr/>\n<h1>EDITED</h1>\n<p>Note: The following answer will only work on:</p>\n<ul>\n<li>NLTK version ==3.2.5</li>\n<li>Stanford Tools compiled since 2016-10-31</li>\n<li>Python 2.7, 3.5 and 3.6</li>\n</ul>\n<p>As both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.</p>\n<p>Always refer to <a href=\"https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software\" rel=\"nofollow noreferrer\">https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software</a> for the latest instruction on how to interface Stanford NLP tools using NLTK!!</p>\n<h1>TL;DR</h1>\n<p>The follow code comes from <a href=\"https://github.com/nltk/nltk/pull/1735#issuecomment-306091826\" rel=\"nofollow noreferrer\">https://github.com/nltk/nltk/pull/1735#issuecomment-306091826</a></p>\n<p>In terminal:</p>\n<pre><code class=\"python\">wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip\nunzip stanford-corenlp-full-2016-10-31.zip &amp;&amp; cd stanford-corenlp-full-2016-10-31\n\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000\n</code></pre>\n<p>In Python:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger\n&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPParser\n\n&gt;&gt;&gt; stpos, stner = CoreNLPPOSTagger(), CoreNLPNERTagger()\n\n&gt;&gt;&gt; stpos.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n&gt;&gt;&gt; stner.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n&gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000')\n\n&gt;&gt;&gt; next(\n...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n... ).pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n&gt;&gt;&gt; (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(\n...     [\n...         'The quick brown fox jumps over the lazy dog.',\n...         'The quick grey wolf jumps over the lazy fox.',\n...     ]\n... )\n\n&gt;&gt;&gt; parse_fox.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n&gt;&gt;&gt; parse_wolf.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|_________      |    |     _______|____    |\n DT   JJ   JJ   NN   VBZ   IN   DT      JJ   NN  .\n |    |    |    |     |    |    |       |    |   |\nThe quick grey wolf jumps over the     lazy fox  .\n\n&gt;&gt;&gt; (parse_dog, ), (parse_friends, ) = parser.parse_sents(\n...     [\n...         \"I 'm a dog\".split(),\n...         \"This is my friends ' cat ( the tabby )\".split(),\n...     ]\n... )\n\n&gt;&gt;&gt; parse_dog.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n        ROOT\n         |\n         S\n  _______|____\n |            VP\n |    ________|___\n NP  |            NP\n |   |         ___|___\nPRP VBP       DT      NN\n |   |        |       |\n I   'm       a      dog\n</code></pre>\n<p>Please take a look at <a href=\"http://www.nltk.org/_modules/nltk/parse/corenlp.html\" rel=\"nofollow noreferrer\">http://www.nltk.org/_modules/nltk/parse/corenlp.html</a>  for more information on of the Stanford API. Take a look at the docstrings!</p>\n", "abstract": "The answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above. Note: The following answer will only work on: As both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix. Always refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!! The follow code comes from https://github.com/nltk/nltk/pull/1735#issuecomment-306091826 In terminal: In Python: Please take a look at http://www.nltk.org/_modules/nltk/parse/corenlp.html  for more information on of the Stanford API. Take a look at the docstrings!"}, {"id": 57767467, "score": 1, "vote": 0, "content": "<p>A new development of the Stanford parser based on a neural model, trained using Tensorflow is very recently made available to be used as a python API. This model is supposed to be far more accurate than the Java-based moel. You can certainly integrate with an NLTK pipeline.</p>\n<p><a href=\"https://stanfordnlp.github.io/stanfordnlp/index.html\" rel=\"nofollow noreferrer\">Link</a> to the parser. Ther repository contains pre-trained parser models for 53 languages.</p>\n", "abstract": "A new development of the Stanford parser based on a neural model, trained using Tensorflow is very recently made available to be used as a python API. This model is supposed to be far more accurate than the Java-based moel. You can certainly integrate with an NLTK pipeline. Link to the parser. Ther repository contains pre-trained parser models for 53 languages."}]}, {"link": "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk", "question": {"id": "4951751", "title": "Creating a new corpus with NLTK", "content": "<p>I reckoned that often the answer to my title is to go and read the documentations, but I ran through the <a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">NLTK book</a> but it doesn't give the answer. I'm kind of new to Python.</p>\n<p>I have a bunch of <code>.txt</code> files and I want to be able to use the corpus functions that NLTK provides for the corpus <code>nltk_data</code>. </p>\n<p>I've tried <code>PlaintextCorpusReader</code> but I couldn't get further than:</p>\n<pre><code class=\"python\">&gt;&gt;&gt;import nltk\n&gt;&gt;&gt;from nltk.corpus import PlaintextCorpusReader\n&gt;&gt;&gt;corpus_root = './'\n&gt;&gt;&gt;newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n&gt;&gt;&gt;newcorpus.words()\n</code></pre>\n<p>How do I segment the <code>newcorpus</code> sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read <code>PlaintextCorpusReader</code> class?</p>\n<p>Can you also lead me to how I can write the segmented data into text files?</p>\n", "abstract": "I reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to Python. I have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data.  I've tried PlaintextCorpusReader but I couldn't get further than: How do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class? Can you also lead me to how I can write the segmented data into text files?"}, "answers": [{"id": 20922201, "score": 74, "vote": 0, "content": "<p>After some years of figuring out how it works, here's the updated tutorial of </p>\n<p><strong>How to create an NLTK corpus with a directory of textfiles?</strong></p>\n<p>The main idea is to make use of the <a href=\"http://nltk.org/api/nltk.corpus.html\"><strong>nltk.corpus.reader</strong></a> package. In the case that you have a directory of textfiles in <strong>English</strong>, it's best to use the <a href=\"http://nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader\"><strong>PlaintextCorpusReader</strong></a>. </p>\n<p>If you have a directory that looks like this:</p>\n<pre><code class=\"python\">newcorpus/\n         file1.txt\n         file2.txt\n         ...\n</code></pre>\n<p>Simply use these lines of code and you can get a corpus:</p>\n<pre><code class=\"python\">import os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\ncorpusdir = 'newcorpus/' # Directory of corpus.\n\nnewcorpus = PlaintextCorpusReader(corpusdir, '.*')\n</code></pre>\n<p><strong>NOTE:</strong> that the <code>PlaintextCorpusReader</code> will use the default <code>nltk.tokenize.sent_tokenize()</code> and <code>nltk.tokenize.word_tokenize()</code> to split your texts into sentences and words and these functions are build for English, it may <strong>NOT</strong> work for all languages.</p>\n<p>Here's the full code with creation of test textfiles and how to create a corpus with NLTK and how to access the corpus at different levels:</p>\n<pre><code class=\"python\">import os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n# Let's create a corpus with 2 texts in different textfile.\ntxt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\ntxt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\ncorpus = [txt1,txt2]\n\n# Make new dir for the corpus.\ncorpusdir = 'newcorpus/'\nif not os.path.isdir(corpusdir):\n    os.mkdir(corpusdir)\n\n# Output the files into the directory.\nfilename = 0\nfor text in corpus:\n    filename+=1\n    with open(corpusdir+str(filename)+'.txt','w') as fout:\n        print&gt;&gt;fout, text\n\n# Check that our corpus do exist and the files are correct.\nassert os.path.isdir(corpusdir)\nfor infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n\n\n# Create a new corpus by specifying the parameters\n# (1) directory of the new corpus\n# (2) the fileids of the corpus\n# NOTE: in this case the fileids are simply the filenames.\nnewcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n\n# Access each file in the corpus.\nfor infile in sorted(newcorpus.fileids()):\n    print infile # The fileids of each file.\n    with newcorpus.open(infile) as fin: # Opens the file.\n        print fin.read().strip() # Prints the content of the file\nprint\n\n# Access the plaintext; outputs pure string/basestring.\nprint newcorpus.raw().strip()\nprint \n\n# Access paragraphs in the corpus. (list of list of list of strings)\n# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n#       nltk.tokenize.word_tokenize.\n#\n# Each element in the outermost list is a paragraph, and\n# Each paragraph contains sentence(s), and\n# Each sentence contains token(s)\nprint newcorpus.paras()\nprint\n\n# To access pargraphs of a specific fileid.\nprint newcorpus.paras(newcorpus.fileids()[0])\n\n# Access sentences in the corpus. (list of list of strings)\n# NOTE: That the texts are flattened into sentences that contains tokens.\nprint newcorpus.sents()\nprint\n\n# To access sentences of a specific fileid.\nprint newcorpus.sents(newcorpus.fileids()[0])\n\n# Access just tokens/words in the corpus. (list of strings)\nprint newcorpus.words()\n\n# To access tokens of a specific fileid.\nprint newcorpus.words(newcorpus.fileids()[0])\n</code></pre>\n<p>Finally, to read a directory of texts and create an NLTK corpus in another languages, you must first ensure that you have a python-callable <strong>word tokenization</strong> and <strong>sentence tokenization</strong> modules that takes string/basestring input and produces such output:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.tokenize import sent_tokenize, word_tokenize\n&gt;&gt;&gt; txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n&gt;&gt;&gt; sent_tokenize(txt1)\n['This is a foo bar sentence.', 'And this is the first txtfile in the corpus.']\n&gt;&gt;&gt; word_tokenize(sent_tokenize(txt1)[0])\n['This', 'is', 'a', 'foo', 'bar', 'sentence', '.']\n</code></pre>\n", "abstract": "After some years of figuring out how it works, here's the updated tutorial of  How to create an NLTK corpus with a directory of textfiles? The main idea is to make use of the nltk.corpus.reader package. In the case that you have a directory of textfiles in English, it's best to use the PlaintextCorpusReader.  If you have a directory that looks like this: Simply use these lines of code and you can get a corpus: NOTE: that the PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() and nltk.tokenize.word_tokenize() to split your texts into sentences and words and these functions are build for English, it may NOT work for all languages. Here's the full code with creation of test textfiles and how to create a corpus with NLTK and how to access the corpus at different levels: Finally, to read a directory of texts and create an NLTK corpus in another languages, you must first ensure that you have a python-callable word tokenization and sentence tokenization modules that takes string/basestring input and produces such output:"}, {"id": 4952238, "score": 41, "vote": 0, "content": "<p>I think the <code>PlaintextCorpusReader</code> already segments the input with a punkt tokenizer, at least if your input language is english.</p>\n<p><a href=\"http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader\" rel=\"nofollow noreferrer\">PlainTextCorpusReader's constructor</a></p>\n<pre><code class=\"python\">def __init__(self, root, fileids,\n             word_tokenizer=WordPunctTokenizer(),\n             sent_tokenizer=nltk.data.LazyLoader(\n                 'tokenizers/punkt/english.pickle'),\n             para_block_reader=read_blankline_block,\n             encoding='utf8'):\n</code></pre>\n<p>You can pass the reader a word and sentence tokenizer, but for the latter the default already is <code>nltk.data.LazyLoader('tokenizers/punkt/english.pickle')</code>.</p>\n<p>For a single string, a tokenizer would be used as follows (explained <a href=\"https://www.nltk.org/api/nltk.tokenize.html\" rel=\"nofollow noreferrer\">here</a>, see section 5 for punkt tokenizer).</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk.data\n&gt;&gt;&gt; text = \"\"\"\n... Punkt knows that the periods in Mr. Smith and Johann S. Bach\n... do not mark sentence boundaries.  And sometimes sentences\n... can start with non-capitalized words.  i is a good variable\n... name.\n... \"\"\"\n&gt;&gt;&gt; tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n&gt;&gt;&gt; tokenizer.tokenize(text.strip())\n</code></pre>\n", "abstract": "I think the PlaintextCorpusReader already segments the input with a punkt tokenizer, at least if your input language is english. PlainTextCorpusReader's constructor You can pass the reader a word and sentence tokenizer, but for the latter the default already is nltk.data.LazyLoader('tokenizers/punkt/english.pickle'). For a single string, a tokenizer would be used as follows (explained here, see section 5 for punkt tokenizer)."}, {"id": 5113509, "score": 15, "vote": 0, "content": "<pre class=\"lang-py prettyprint-override\"><code class=\"python\"> &gt;&gt;&gt; import nltk\n &gt;&gt;&gt; from nltk.corpus import PlaintextCorpusReader\n &gt;&gt;&gt; corpus_root = './'\n &gt;&gt;&gt; newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n \"\"\"\n if the ./ dir contains the file my_corpus.txt, then you \n can view say all the words it by doing this \n \"\"\"\n &gt;&gt;&gt; newcorpus.words('my_corpus.txt')\n</code></pre>\n", "abstract": ""}, {"id": 64299155, "score": 2, "vote": 0, "content": "<pre><code class=\"python\">from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n\nfilecontent1 = \"This is a cow\"\nfilecontent2 = \"This is a Dog\"\n\ncorpusdir = 'nltk_data/'\nwith open(corpusdir + 'content1.txt', 'w') as text_file:\n    text_file.write(filecontent1)\nwith open(corpusdir + 'content2.txt', 'w') as text_file:\n    text_file.write(filecontent2)\n\ntext_corpus = PlaintextCorpusReader(corpusdir, [\"content1.txt\", \"content2.txt\"])\n\nno_of_words_corpus1 = len(text_corpus.words(\"content1.txt\"))\nprint(no_of_words_corpus1)\nno_of_unique_words_corpus1 = len(set(text_corpus.words(\"content1.txt\")))\n\nno_of_words_corpus2 = len(text_corpus.words(\"content2.txt\"))\nno_of_unique_words_corpus2 = len(set(text_corpus.words(\"content2.txt\")))\n\nenter code here\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/573768/sentiment-analysis-for-twitter-in-python", "question": {"id": "573768", "title": "Sentiment analysis for Twitter in Python", "content": "<p>I'm looking for an open source implementation, preferably in python, of <strong>Textual Sentiment Analysis</strong> (<a href=\"http://en.wikipedia.org/wiki/Sentiment_analysis\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Sentiment_analysis</a>). Is anyone familiar with such open source implementation I can use?</p>\n<p>I'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.</p>\n<p>Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.</p>\n<p>BTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.</p>\n<p>Thanks!</p>\n<p>BTW, an early demo is <a href=\"http://twitgraph.appspot.com/?show_inputs=1&amp;duration=30&amp;q=youtube+annotations\" rel=\"noreferrer\">here</a> and the code I have so far is <a href=\"http://code.google.com/p/twitgraph/\" rel=\"noreferrer\">here</a> and I'd love to opensource it with any interested developer.</p>\n", "abstract": "I'm looking for an open source implementation, preferably in python, of Textual Sentiment Analysis (http://en.wikipedia.org/wiki/Sentiment_analysis). Is anyone familiar with such open source implementation I can use? I'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python. Note, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts. BTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself. Thanks! BTW, an early demo is here and the code I have so far is here and I'd love to opensource it with any interested developer."}, "answers": [{"id": 607885, "score": 77, "vote": 0, "content": "<p>Good luck with that.</p>\n<p>Sentiment is enormously contextual, and tweeting culture makes the problem worse because <em>you aren't given the context</em> for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.</p>\n<p>If they say the video is bad, does that mean bad, or <em>bad</em>?</p>\n<blockquote>\n<p>A linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\"</p>\n<p>A voice from the back of the room\n  piped up, \"Yeah . . .right.\"</p>\n</blockquote>\n", "abstract": "Good luck with that. Sentiment is enormously contextual, and tweeting culture makes the problem worse because you aren't given the context for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message. If they say the video is bad, does that mean bad, or bad? A linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\" A voice from the back of the room\n  piped up, \"Yeah . . .right.\""}, {"id": 574015, "score": 44, "vote": 0, "content": "<p>With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.</p>\n<p>You may be interested in <a href=\"http://tadm.sf.net\" rel=\"noreferrer\">The Toolkit for Advanced Discriminative Modeling</a>, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.</p>\n<p>The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for <a href=\"http://comp.ling.utexas.edu/jbaldrid/courses/2006/cl2/\" rel=\"noreferrer\">Computational Linguistics II</a> to get an idea of how to make it work and what previous applications it has served.</p>\n<p>Another great tool which works in the same vein is <a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">Mallet</a>.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  <a href=\"http://www.cs.waikato.ac.nz/ml/weka/\" rel=\"noreferrer\">Weka</a> is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.</p>\n<p>Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.</p>\n", "abstract": "With most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it. You may be interested in The Toolkit for Advanced Discriminative Modeling, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task. The University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for Computational Linguistics II to get an idea of how to make it work and what previous applications it has served. Another great tool which works in the same vein is Mallet.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  Weka is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production. Good luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion."}, {"id": 662118, "score": 19, "vote": 0, "content": "<p>Thanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from <a href=\"http://www.divmod.org/trac/wiki/DivmodReverend\" rel=\"noreferrer\">here</a>. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.</p>\n<p>The full details of my work as in a <a href=\"http://www.gbsheli.com/2009/03/twitgraph-en.html\" rel=\"noreferrer\">blog post</a>.</p>\n<p>Again, your help was very useful, so thank you!</p>\n", "abstract": "Thanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from here. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice. The full details of my work as in a blog post. Again, your help was very useful, so thank you!"}, {"id": 6733983, "score": 14, "vote": 0, "content": "<p>I have constructed a word list labeled with sentiment. You can access it from here:</p>\n<p><a href=\"http://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\" rel=\"nofollow noreferrer\">http://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip</a></p>\n<p>You will find a short Python program on my blog: </p>\n<p><a href=\"http://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/\" rel=\"nofollow noreferrer\">http://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/</a></p>\n<p>This post displays how to use the word list with single sentences as well as with Twitter.</p>\n<p>Word lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage.</p>\n<p>Please note a <code>unicode(s, 'utf-8')</code> is missing from the code (for paedagogic reasons).</p>\n", "abstract": "I have constructed a word list labeled with sentiment. You can access it from here: http://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip You will find a short Python program on my blog:  http://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/ This post displays how to use the word list with single sentences as well as with Twitter. Word lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage. Please note a unicode(s, 'utf-8') is missing from the code (for paedagogic reasons)."}, {"id": 573875, "score": 10, "vote": 0, "content": "<p>A lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives.</p>\n<p>Now, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set.</p>\n<p>Good luck.</p>\n", "abstract": "A lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives. Now, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set. Good luck."}, {"id": 573859, "score": 4, "vote": 0, "content": "<p>I think you may find it difficult to find what you're after. The closest thing that I know of is <a href=\"http://alias-i.com/lingpipe/index.html\" rel=\"nofollow noreferrer\">LingPipe</a>, which has some <a href=\"http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\" rel=\"nofollow noreferrer\">sentiment analysis functionality</a> and is available under a limited kind of open-source licence, but is written in Java.</p>\n<p>Also, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like <a href=\"http://www.cs.pitt.edu/mpqa/\" rel=\"nofollow noreferrer\">the one the University of Pittsburgh provide</a>.</p>\n<p>Check out <a href=\"http://www.wefeelfine.org/\" rel=\"nofollow noreferrer\">We Feel Fine</a> for an implementation of similar idea with a really beautiful interface (and <a href=\"http://twitrratr.com/\" rel=\"nofollow noreferrer\">twitrratr</a>).</p>\n", "abstract": "I think you may find it difficult to find what you're after. The closest thing that I know of is LingPipe, which has some sentiment analysis functionality and is available under a limited kind of open-source licence, but is written in Java. Also, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like the one the University of Pittsburgh provide. Check out We Feel Fine for an implementation of similar idea with a really beautiful interface (and twitrratr)."}, {"id": 6676746, "score": 2, "vote": 0, "content": "<p>Take a look at <a href=\"http://smm.streamcrab.com\" rel=\"nofollow\">Twitter sentiment analysis tool</a>. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found <a href=\"https://github.com/cyhex/smm\" rel=\"nofollow\">here</a>.</p>\n", "abstract": "Take a look at Twitter sentiment analysis tool. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found here."}, {"id": 573805, "score": 1, "vote": 0, "content": "<p>I came across <a href=\"http://www.nltk.org/\" rel=\"nofollow noreferrer\">Natural Language Toolkit</a> a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar.</p>\n", "abstract": "I came across Natural Language Toolkit a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar."}, {"id": 25280946, "score": 1, "vote": 0, "content": "<p>Maybe <a href=\"http://textblob.readthedocs.org/en/dev/\" rel=\"nofollow\">TextBlob</a> (based on NLTK and pattern) is the right sentiment analysis tool for you.</p>\n", "abstract": "Maybe TextBlob (based on NLTK and pattern) is the right sentiment analysis tool for you."}, {"id": 649418, "score": 0, "vote": 0, "content": "<p>Somewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc.</p>\n<p>Once you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines.</p>\n<p>It may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  </p>\n", "abstract": "Somewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc. Once you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines. It may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  "}, {"id": 2436991, "score": 0, "vote": 0, "content": "<p>There's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See <a href=\"http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm\" rel=\"nofollow noreferrer\">http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm</a></p>\n", "abstract": "There's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm"}, {"id": 22452811, "score": 0, "vote": 0, "content": "<p>For those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"<a href=\"https://www.coursera.org/course/datasci\" rel=\"nofollow\">Data Science</a>\" with python code on GitHub (as part of assignment 1 - <a href=\"https://github.com/uwescience/datasci_course_materials/tree/master/assignment1\" rel=\"nofollow\">link</a>). The sentiments are part of the <a href=\"https://github.com/uwescience/datasci_course_materials/blob/master/assignment1/AFINN-README.txt\" rel=\"nofollow\">AFINN-111</a>.</p>\n<p>You can find working solutions, for example <a href=\"https://github.com/calvdee/coursera-data-sci/tree/master/assignment1\" rel=\"nofollow\">here</a>. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see <a href=\"https://github.com/calvdee/coursera-data-sci/blob/master/assignment1/term_sentiment.py\" rel=\"nofollow\">here</a>).</p>\n", "abstract": "For those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"Data Science\" with python code on GitHub (as part of assignment 1 - link). The sentiments are part of the AFINN-111. You can find working solutions, for example here. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see here)."}]}, {"link": "https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings", "question": {"id": "15173225", "title": "Calculate cosine similarity given 2 sentence strings", "content": "<p>From <a href=\"https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity\">Python: tf-idf-cosine: to find document similarity</a> , it is possible to calculate document similarity using tf-idf cosine. Without importing external libraries, are that any ways to calculate cosine similarity between 2 strings?</p>\n<pre><code class=\"python\">s1 = \"This is a foo bar sentence .\"\ns2 = \"This sentence is similar to a foo bar sentence .\"\ns3 = \"What is this string ? Totally not related to the other two lines .\"\n\ncosine_sim(s1, s2) # Should give high cosine similarity\ncosine_sim(s1, s3) # Shouldn't give high cosine similarity value\ncosine_sim(s2, s3) # Shouldn't give high cosine similarity value\n</code></pre>\n", "abstract": "From Python: tf-idf-cosine: to find document similarity , it is possible to calculate document similarity using tf-idf cosine. Without importing external libraries, are that any ways to calculate cosine similarity between 2 strings?"}, "answers": [{"id": 15174569, "score": 180, "vote": 0, "content": "<p>A simple pure-Python implementation would be:</p>\n<pre><code class=\"python\">import math\nimport re\nfrom collections import Counter\n\nWORD = re.compile(r\"\\w+\")\n\n\ndef get_cosine(vec1, vec2):\n    intersection = set(vec1.keys()) &amp; set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\n\ndef text_to_vector(text):\n    words = WORD.findall(text)\n    return Counter(words)\n\n\ntext1 = \"This is a foo bar sentence .\"\ntext2 = \"This sentence is similar to a foo bar sentence .\"\n\nvector1 = text_to_vector(text1)\nvector2 = text_to_vector(text2)\n\ncosine = get_cosine(vector1, vector2)\n\nprint(\"Cosine:\", cosine)\n</code></pre>\n<p>Prints:</p>\n<pre><code class=\"python\">Cosine: 0.861640436855\n</code></pre>\n<p>The cosine formula used here is described <a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">here</a>.</p>\n<p>This does not include weighting of the words by tf-idf, but in order to use tf-idf, you need to have a reasonably large corpus from which to estimate tfidf weights.</p>\n<p>You can also develop it further, by using a more sophisticated way to extract words from a piece of text, stem or lemmatise it, etc.</p>\n", "abstract": "A simple pure-Python implementation would be: Prints: The cosine formula used here is described here. This does not include weighting of the words by tf-idf, but in order to use tf-idf, you need to have a reasonably large corpus from which to estimate tfidf weights. You can also develop it further, by using a more sophisticated way to extract words from a piece of text, stem or lemmatise it, etc."}, {"id": 15173821, "score": 51, "vote": 0, "content": "<p>The short answer is \"no, it is not possible to do that in a principled way that works even remotely well\". It is an unsolved problem in natural language processing research and also happens to be the subject of my doctoral work. I'll very briefly summarize where we are and point you to a few publications:</p>\n<p><strong>Meaning of words</strong></p>\n<p>The most important assumption here is that it is possible to obtain a vector that represents each <em>word</em> in the sentence in quesion. This vector is usually chosen to capture the contexts the word can appear in. For example, if we only consider the three contexts \"eat\", \"red\" and \"fluffy\", the word \"cat\" might be represented as [98, 1, 87], because if you were to read a very very long piece of text (a few billion words is not uncommon by today's standard), the word \"cat\" would appear very often in the context of \"fluffy\" and \"eat\", but not that often in the context of \"red\". In the same way, \"dog\" might be represented as [87,2,34] and \"umbrella\" might be [1,13,0]. Imagening these vectors as points in 3D space, \"cat\" is clearly closer to \"dog\" than it is to \"umbrella\", therefore \"cat\" also means something more similar to \"dog\" than to an \"umbrella\".</p>\n<p>This line of work has been investigated since the early 90s (e.g. <a href=\"http://books.google.co.uk/books/about/Explorations_in_Automatic_Thesaurus_Disc.html?id=ZAxgQRIgzIcC&amp;redir_esc=y\">this</a> work by Greffenstette) and has yielded some surprisingly good results. For example, here is a few random entries in a thesaurus I built recently by having my computer read wikipedia:</p>\n<pre><code class=\"python\">theory -&gt; analysis, concept, approach, idea, method\nvoice -&gt; vocal, tone, sound, melody, singing\njames -&gt; william, john, thomas, robert, george, charles\n</code></pre>\n<p>These lists of similar words were obtained entirely without human intervention- you feed text in and come back a few hours later.</p>\n<p><strong>The problem with phrases</strong></p>\n<p>You might ask why we are not doing the same thing for longer phrases, such as \"ginger foxes love fruit\". It's because we do not have enough text. In order for us to <em>reliably</em> establish what X is similar to, we need to see many examples of X being used in context. When X is a single word like \"voice\", this is not too hard. However, as X gets longer, the chances of finding natural occurrences of X get exponentially slower. For comparison, Google has about 1B pages containing the word \"fox\" and not a single page containing \"ginger foxes love fruit\", despite the fact that it is a perfectly valid English sentence and we all understand what it means.</p>\n<p><strong>Composition</strong></p>\n<p>To tackle the problem of data sparsity, we want to perform composition, i.e. to take vectors for words, which are easy to obtain from real text, and to put the together in a way that captures their meaning. The bad news is nobody has been able to do that well so far.</p>\n<p>The simplest and most obvious way is to add or multiply the individual word vectors together. This leads to undesirable side effect that \"cats chase dogs\" and \"dogs chase cats\" would mean the same to your system. Also, if you are multiplying, you have to be extra careful or every sentences will end up represented by [0,0,0,...,0], which defeats the point.</p>\n<p><strong>Further reading</strong></p>\n<p>I will not discuss the more sophisticated methods for composition that have been proposed so far. I suggest you read Katrin Erk's <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/lnco.362/abstract\">\"Vector space models of word meaning and phrase meaning: a survey\"</a>. This is a very good high-level survey to get you started. Unfortunately, is not freely available on the publisher's website, email the author directly to get a copy. In that paper you will find references to many more concrete methods. The more comprehensible ones are by <a href=\"http://aclweb.org/anthology-new/P/P08/P08-1028.pdf\">Mitchel and Lapata (2008)</a> and <a href=\"http://clic.cimec.unitn.it/marco/publications/bz-adj-com-emnlp10.pdf\">Baroni and Zamparelli (2010)</a>.</p>\n<hr/>\n<p>Edit after comment by @vpekar:\nThe bottom line of this answer is to stress the fact that while <em>naive methods do exist</em> (e.g. addition, multiplication, surface similarity, etc), these are <em>fundamentally flawed</em> and in general one should not expect great performance from them.</p>\n", "abstract": "The short answer is \"no, it is not possible to do that in a principled way that works even remotely well\". It is an unsolved problem in natural language processing research and also happens to be the subject of my doctoral work. I'll very briefly summarize where we are and point you to a few publications: Meaning of words The most important assumption here is that it is possible to obtain a vector that represents each word in the sentence in quesion. This vector is usually chosen to capture the contexts the word can appear in. For example, if we only consider the three contexts \"eat\", \"red\" and \"fluffy\", the word \"cat\" might be represented as [98, 1, 87], because if you were to read a very very long piece of text (a few billion words is not uncommon by today's standard), the word \"cat\" would appear very often in the context of \"fluffy\" and \"eat\", but not that often in the context of \"red\". In the same way, \"dog\" might be represented as [87,2,34] and \"umbrella\" might be [1,13,0]. Imagening these vectors as points in 3D space, \"cat\" is clearly closer to \"dog\" than it is to \"umbrella\", therefore \"cat\" also means something more similar to \"dog\" than to an \"umbrella\". This line of work has been investigated since the early 90s (e.g. this work by Greffenstette) and has yielded some surprisingly good results. For example, here is a few random entries in a thesaurus I built recently by having my computer read wikipedia: These lists of similar words were obtained entirely without human intervention- you feed text in and come back a few hours later. The problem with phrases You might ask why we are not doing the same thing for longer phrases, such as \"ginger foxes love fruit\". It's because we do not have enough text. In order for us to reliably establish what X is similar to, we need to see many examples of X being used in context. When X is a single word like \"voice\", this is not too hard. However, as X gets longer, the chances of finding natural occurrences of X get exponentially slower. For comparison, Google has about 1B pages containing the word \"fox\" and not a single page containing \"ginger foxes love fruit\", despite the fact that it is a perfectly valid English sentence and we all understand what it means. Composition To tackle the problem of data sparsity, we want to perform composition, i.e. to take vectors for words, which are easy to obtain from real text, and to put the together in a way that captures their meaning. The bad news is nobody has been able to do that well so far. The simplest and most obvious way is to add or multiply the individual word vectors together. This leads to undesirable side effect that \"cats chase dogs\" and \"dogs chase cats\" would mean the same to your system. Also, if you are multiplying, you have to be extra careful or every sentences will end up represented by [0,0,0,...,0], which defeats the point. Further reading I will not discuss the more sophisticated methods for composition that have been proposed so far. I suggest you read Katrin Erk's \"Vector space models of word meaning and phrase meaning: a survey\". This is a very good high-level survey to get you started. Unfortunately, is not freely available on the publisher's website, email the author directly to get a copy. In that paper you will find references to many more concrete methods. The more comprehensible ones are by Mitchel and Lapata (2008) and Baroni and Zamparelli (2010). Edit after comment by @vpekar:\nThe bottom line of this answer is to stress the fact that while naive methods do exist (e.g. addition, multiplication, surface similarity, etc), these are fundamentally flawed and in general one should not expect great performance from them."}, {"id": 60895263, "score": 8, "vote": 0, "content": "<p>I have similar solution but might be useful for pandas </p>\n<pre><code class=\"python\">import math\nimport re\nfrom collections import Counter\nimport pandas as pd\n\nWORD = re.compile(r\"\\w+\")\n\n\ndef get_cosine(vec1, vec2):\n    intersection = set(vec1.keys()) &amp; set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\n\ndef text_to_vector(text):\n    words = WORD.findall(text)\n    return Counter(words)\n\ndf=pd.read_csv('/content/drive/article.csv')\ndf['vector1']=df['headline'].apply(lambda x: text_to_vector(x)) \ndf['vector2']=df['snippet'].apply(lambda x: text_to_vector(x)) \ndf['simscore']=df.apply(lambda x: get_cosine(x['vector1'],x['vector2']),axis=1)\n</code></pre>\n", "abstract": "I have similar solution but might be useful for pandas "}, {"id": 53407328, "score": 3, "vote": 0, "content": "<p>Try this. Download the file 'numberbatch-en-17.06.txt' from <a href=\"https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz\" rel=\"nofollow noreferrer\">https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz</a> and extract it. The function 'get_sentence_vector' uses a simple sum of word vectors. However it can be improved by using weighted sum where weights are proportional to Tf-Idf of each word. </p>\n<pre><code class=\"python\">import math\nimport numpy as np\n\nstd_embeddings_index = {}\nwith open('path/to/numberbatch-en-17.06.txt') as f:\n    for line in f:\n        values = line.split(' ')\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        std_embeddings_index[word] = embedding\n\ndef cosineValue(v1,v2):\n    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n    sumxx, sumxy, sumyy = 0, 0, 0\n    for i in range(len(v1)):\n        x = v1[i]; y = v2[i]\n        sumxx += x*x\n        sumyy += y*y\n        sumxy += x*y\n    return sumxy/math.sqrt(sumxx*sumyy)\n\n\ndef get_sentence_vector(sentence, std_embeddings_index = std_embeddings_index ):\n    sent_vector = 0\n    for word in sentence.lower().split():\n        if word not in std_embeddings_index :\n            word_vector = np.array(np.random.uniform(-1.0, 1.0, 300))\n            std_embeddings_index[word] = word_vector\n        else:\n            word_vector = std_embeddings_index[word]\n        sent_vector = sent_vector + word_vector\n\n    return sent_vector\n\ndef cosine_sim(sent1, sent2):\n    return cosineValue(get_sentence_vector(sent1), get_sentence_vector(sent2))\n</code></pre>\n<p>I did run for the given sentences and found the following results</p>\n<pre><code class=\"python\">s1 = \"This is a foo bar sentence .\"\ns2 = \"This sentence is similar to a foo bar sentence .\"\ns3 = \"What is this string ? Totally not related to the other two lines .\"\n\nprint cosine_sim(s1, s2) # Should give high cosine similarity\nprint cosine_sim(s1, s3) # Shouldn't give high cosine similarity value\nprint cosine_sim(s2, s3) # Shouldn't give high cosine similarity value\n\n0.9851735249068168\n0.6570885718962608\n0.6589335425458225\n</code></pre>\n", "abstract": "Try this. Download the file 'numberbatch-en-17.06.txt' from https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz and extract it. The function 'get_sentence_vector' uses a simple sum of word vectors. However it can be improved by using weighted sum where weights are proportional to Tf-Idf of each word.  I did run for the given sentences and found the following results"}, {"id": 51184103, "score": 2, "vote": 0, "content": "<p>Well, if you are aware of <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"nofollow noreferrer\">word embeddings</a> like Glove/Word2Vec/Numberbatch, your job is half done. If not let me explain how this can be tackled. \nConvert each sentence into word tokens, and represent each of these tokens as vectors of high dimension (using the pre-trained word embeddings, or you could <a href=\"https://www.tensorflow.org/tutorials/word2vec\" rel=\"nofollow noreferrer\">train</a> them yourself even!). So, now you just don't capture their surface similarity but rather extract the meaning of each word which comprise the sentence as a whole. After this calculate their cosine similarity and you are set. </p>\n", "abstract": "Well, if you are aware of word embeddings like Glove/Word2Vec/Numberbatch, your job is half done. If not let me explain how this can be tackled. \nConvert each sentence into word tokens, and represent each of these tokens as vectors of high dimension (using the pre-trained word embeddings, or you could train them yourself even!). So, now you just don't capture their surface similarity but rather extract the meaning of each word which comprise the sentence as a whole. After this calculate their cosine similarity and you are set. "}, {"id": 28510163, "score": 1, "vote": 0, "content": "<p>Thanks @vpekar for your implementation. It helped a lot. I just found that it misses the tf-idf weight while calculating the cosine similarity.\nThe Counter(word) returns a dictionary which has the list of words along with their occurence.</p>\n<p>cos(q, d) = sim(q, d) = (q \u00b7 d)/(|q||d|) = (sum(qi, di)/(sqrt(sum(qi2)))*(sqrt(sum(vi2))) where i = 1 to v)</p>\n<ul>\n<li>qi is the tf-idf weight of term i in the query.</li>\n<li>di is the tf-idf</li>\n<li>weight of term i in the document. |q| and |d| are the lengths of q\nand d. </li>\n<li>This is the cosine similarity of q and d . . . . . . or,\nequivalently, the cosine of the angle between q and d.</li>\n</ul>\n<p>Please feel free to view my code <a href=\"https://github.com/puneeth-u-bharadwaj/Data-Mining/blob/master/Cosine-Similarity/src/final/Puneeth_Umesh_Bharadwaj_PA1.py\" rel=\"nofollow\">here</a>. But first you will have to download the anaconda package. It will automatically set you python path in Windows. Add this python interpreter in Eclipse.</p>\n", "abstract": "Thanks @vpekar for your implementation. It helped a lot. I just found that it misses the tf-idf weight while calculating the cosine similarity.\nThe Counter(word) returns a dictionary which has the list of words along with their occurence. cos(q, d) = sim(q, d) = (q \u00b7 d)/(|q||d|) = (sum(qi, di)/(sqrt(sum(qi2)))*(sqrt(sum(vi2))) where i = 1 to v) Please feel free to view my code here. But first you will have to download the anaconda package. It will automatically set you python path in Windows. Add this python interpreter in Eclipse."}, {"id": 69254917, "score": 1, "vote": 0, "content": "<p>Without using external libraries, you can try BLEU or its alternatives. You can refer to its standard implementation: <a href=\"https://github.com/mjpost/sacrebleu\" rel=\"nofollow noreferrer\">SACREBLEU</a>.</p>\n", "abstract": "Without using external libraries, you can try BLEU or its alternatives. You can refer to its standard implementation: SACREBLEU."}, {"id": 71642443, "score": 1, "vote": 0, "content": "<p>The simplest answer I can come up with includes CounterVectorizer.</p>\n<p>Suppose we have 3 pieces of text.</p>\n<pre><code class=\"python\">text_1 = \"\"\" \"\"\"\n\ntext_2 = \"\"\" \"\"\"\n\ntext_3 = \"\"\" \"\"\"\n\ndocuments = [text_1, text_2, text_3]\n</code></pre>\n<ol start=\"2\">\n<li>To compute cosine similarity, we need the count matrix of words from each document</li>\n</ol>\n<pre><code class=\"python\">import pandas as pd\n\n# Create the Document Term Matrix\ncount_vectorizer = CountVectorizer(stop_words='english')\ncount_vectorizer = CountVectorizer()\nsparse_matrix = count_vectorizer.fit_transform(documents)\n\n# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\ndoc_term_matrix = sparse_matrix.todense()\ndf = pd.DataFrame(doc_term_matrix, \n                  columns=count_vectorizer.get_feature_names(), \n                  index=['text_1', 'text_2', 'text_3'])\ndf\n</code></pre>\n<ol start=\"3\">\n<li>And simply cosine similarity function does the job from sklearn.</li>\n</ol>\n<pre><code class=\"python\">from sklearn.metrics.pairwise import cosine_similarity\nprint(cosine_similarity(df, df))\n</code></pre>\n", "abstract": "The simplest answer I can come up with includes CounterVectorizer. Suppose we have 3 pieces of text."}]}, {"link": "https://stackoverflow.com/questions/9647202/ordinal-numbers-replacement", "question": {"id": "9647202", "title": "Ordinal numbers replacement", "content": "<p>I am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).\nI have been googling for the last week and I didn't find any useful standard tool or any function from NLTK.</p>\n<p>So is there any or should I write some regular expressions manually?</p>\n<p>Thanks for any advice</p>\n", "abstract": "I am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).\nI have been googling for the last week and I didn't find any useful standard tool or any function from NLTK. So is there any or should I write some regular expressions manually? Thanks for any advice"}, "answers": [{"id": 20007730, "score": 146, "vote": 0, "content": "<p>The package <a href=\"https://github.com/scrapinghub/number-parser\" rel=\"nofollow noreferrer\">number-parser</a> can parse ordinal words (\"first\", \"second\", etc) to integers.</p>\n<pre><code class=\"python\">from number_parser import parse_ordinal\nn = parse_ordinal(\"first\")\n</code></pre>\n<p>To convert an integer to \"1st\", \"2nd\", etc, you can use the following (taken from <a href=\"https://codegolf.stackexchange.com/questions/4707/outputting-ordinal-numbers-1st-2nd-3rd#answer-4712\">Gareth on codegolf</a>):</p>\n<pre><code class=\"python\">ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10&lt;4)*n%10::4])\n</code></pre>\n<p>This works on any number:</p>\n<pre><code class=\"python\">print([ordinal(n) for n in range(1,32)])\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n</code></pre>\n", "abstract": "The package number-parser can parse ordinal words (\"first\", \"second\", etc) to integers. To convert an integer to \"1st\", \"2nd\", etc, you can use the following (taken from Gareth on codegolf): This works on any number:"}, {"id": 50992575, "score": 49, "vote": 0, "content": "<p>If you don't want to pull in an additional dependency on an external library (as <a href=\"https://stackoverflow.com/a/48104275/857390\">suggested by luckydonald</a>) but also don't want the future maintainer of the code to haunt you down and kill you (because you used <a href=\"https://stackoverflow.com/a/20007730/857390\">golfed code</a> in production) then here's a short-but-maintainable variant:</p>\n<pre><code class=\"python\">def make_ordinal(n):\n    '''\n    Convert an integer into its ordinal representation::\n\n        make_ordinal(0)   =&gt; '0th'\n        make_ordinal(3)   =&gt; '3rd'\n        make_ordinal(122) =&gt; '122nd'\n        make_ordinal(213) =&gt; '213th'\n    '''\n    n = int(n)\n    if 11 &lt;= (n % 100) &lt;= 13:\n        suffix = 'th'\n    else:\n        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n    return str(n) + suffix\n</code></pre>\n", "abstract": "If you don't want to pull in an additional dependency on an external library (as suggested by luckydonald) but also don't want the future maintainer of the code to haunt you down and kill you (because you used golfed code in production) then here's a short-but-maintainable variant:"}, {"id": 36977549, "score": 19, "vote": 0, "content": "<p>How about this:</p>\n<pre><code class=\"python\">suf = lambda n: \"%d%s\"%(n,{1:\"st\",2:\"nd\",3:\"rd\"}.get(n%100 if (n%100)&lt;20 else n%10,\"th\"))\nprint [suf(n) for n in xrange(1,32)]\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n</code></pre>\n", "abstract": "How about this:"}, {"id": 48104275, "score": 11, "vote": 0, "content": "<p>Another solution is the <code>num2words</code> library (<a href=\"https://pypi.python.org/pypi/num2words\" rel=\"nofollow noreferrer\">pip</a> | <a href=\"https://github.com/savoirfairelinux/num2words\" rel=\"nofollow noreferrer\">github</a>).\nIt especially offers <strong>different languages</strong>, so localization/internationalization (aka. l10n/i18n) is a no-brainer.</p>\n<p>Usage is easy after you installed it with <code>pip install num2words</code>:</p>\n<pre><code class=\"python\">from num2words import num2words\n# english is default\nnum2words(4458, to=\"ordinal_num\")\n'4458th'\n\n# examples for other languages\nnum2words(4458, lang=\"en\", to=\"ordinal_num\")\n'4458th'\n\nnum2words(4458, lang=\"es\", to=\"ordinal_num\")\n'4458\u00ba'\n\nnum2words(4458, lang=\"de\", to=\"ordinal_num\")\n'4458.'\n\nnum2words(4458, lang=\"id\", to=\"ordinal_num\")\n'ke-4458'\n</code></pre>\n<p>Bonus:</p>\n<pre><code class=\"python\">num2words(4458, lang=\"en\", to=\"ordinal\")\n'four thousand, four hundred and fifty-eighth'\n</code></pre>\n", "abstract": "Another solution is the num2words library (pip | github).\nIt especially offers different languages, so localization/internationalization (aka. l10n/i18n) is a no-brainer. Usage is easy after you installed it with pip install num2words: Bonus:"}, {"id": 9647336, "score": 8, "vote": 0, "content": "<p>The accepted answer to <a href=\"https://stackoverflow.com/questions/70161/how-to-read-values-from-numbers-written-as-words\">a previous question</a> has an algorithm for half of this: it turns <code>\"first\"</code> into <code>1</code>. To go from there to <code>\"1st\"</code>, do something like:</p>\n<pre><code class=\"python\">suffixes = [\"th\", \"st\", \"nd\", \"rd\", ] + [\"th\"] * 16\nsuffixed_num = str(num) + suffixes[num % 100]\n</code></pre>\n<p>This only works for numbers 0-19.</p>\n", "abstract": "The accepted answer to a previous question has an algorithm for half of this: it turns \"first\" into 1. To go from there to \"1st\", do something like: This only works for numbers 0-19."}, {"id": 21669035, "score": 8, "vote": 0, "content": "<p>I found myself doing something similar, needing to convert addresses with ordinal numbers ('Third St') to a format that a geocoder could comprehend ('3rd St').  While this isn't very elegant, one quick and dirty solution is to use the <a href=\"https://pypi.python.org/pypi/inflect\" rel=\"nofollow noreferrer\">inflect.py</a> to generate a dictionary for translation.</p>\n<p>inflect.py has a <code>number_to_words()</code> function, that will turn a number (e.g. <code>2</code>) to its word form (e.g. <code>'two'</code>).  Additionally, there is an <code>ordinal()</code> function that will take any number (numeral or word form) and turn it into its ordinal form (e.g. <code>4</code> -&gt; <code>fourth</code>, <code>six</code> -&gt; <code>sixth</code>).  Neither of those, on their own, do what you're looking for, but together you can use them to generate a dictionary to translate any supplied ordinal-number-word (within a reasonable range) to its respective numeral ordinal.  Take a look:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import inflect\n&gt;&gt;&gt; p = inflect.engine()\n&gt;&gt;&gt; word_to_number_mapping = {}\n&gt;&gt;&gt;\n&gt;&gt;&gt; for i in range(1, 100):\n...     word_form = p.number_to_words(i)  # 1 -&gt; 'one'\n...     ordinal_word = p.ordinal(word_form)  # 'one' -&gt; 'first'\n...     ordinal_number = p.ordinal(i)  # 1 -&gt; '1st'\n...     word_to_number_mapping[ordinal_word] = ordinal_number  # 'first': '1st'\n...\n&gt;&gt;&gt; print word_to_number_mapping['sixth']\n6th\n&gt;&gt;&gt; print word_to_number_mapping['eleventh']\n11th\n&gt;&gt;&gt; print word_to_number_mapping['forty-third']\n43rd\n</code></pre>\n<p>If you're willing to commit some time, it might be possible to examine inflect.py's inner-workings in both of those functions and build your own code to do this dynamically (I haven't tried to do this).</p>\n", "abstract": "I found myself doing something similar, needing to convert addresses with ordinal numbers ('Third St') to a format that a geocoder could comprehend ('3rd St').  While this isn't very elegant, one quick and dirty solution is to use the inflect.py to generate a dictionary for translation. inflect.py has a number_to_words() function, that will turn a number (e.g. 2) to its word form (e.g. 'two').  Additionally, there is an ordinal() function that will take any number (numeral or word form) and turn it into its ordinal form (e.g. 4 -> fourth, six -> sixth).  Neither of those, on their own, do what you're looking for, but together you can use them to generate a dictionary to translate any supplied ordinal-number-word (within a reasonable range) to its respective numeral ordinal.  Take a look: If you're willing to commit some time, it might be possible to examine inflect.py's inner-workings in both of those functions and build your own code to do this dynamically (I haven't tried to do this)."}, {"id": 18670679, "score": 7, "vote": 0, "content": "<p>I wanted to use ordinals for a project of mine and after a few prototypes I think this method although not small will work for any positive integer, yes <strong>any integer</strong>.</p>\n<p>It works by determiniting if the number is above or below 20, if the number is below 20 it will turn the int 1 into the string 1st , 2 , 2nd; 3, 3rd; and the rest will have \"st\" added to it. </p>\n<p>For numbers over 20 it will take the last and second to last digits, which I have called the tens and unit respectively and test them to see what to add to the number. </p>\n<p>This is in python by the way, so I'm not sure if other languages will be able to find the last or second to last digit on a string if they do it should translate pretty easily.</p>\n<pre><code class=\"python\">def o(numb):\n    if numb &lt; 20: #determining suffix for &lt; 20\n        if numb == 1: \n            suffix = 'st'\n        elif numb == 2:\n            suffix = 'nd'\n        elif numb == 3:\n            suffix = 'rd'\n        else:\n            suffix = 'th'  \n    else:   #determining suffix for &gt; 20\n        tens = str(numb)\n        tens = tens[-2]\n        unit = str(numb)\n        unit = unit[-1]\n        if tens == \"1\":\n           suffix = \"th\"\n        else:\n            if unit == \"1\": \n                suffix = 'st'\n            elif unit == \"2\":\n                suffix = 'nd'\n            elif unit == \"3\":\n                suffix = 'rd'\n            else:\n                suffix = 'th'\n    return str(numb)+ suffix\n</code></pre>\n<p>I called the function \"o\" for ease of use and can be called by importing the file name which I called \"ordinal\" by import ordinal then ordinal.o(number).</p>\n<p>Let me know what you think :D</p>\n", "abstract": "I wanted to use ordinals for a project of mine and after a few prototypes I think this method although not small will work for any positive integer, yes any integer. It works by determiniting if the number is above or below 20, if the number is below 20 it will turn the int 1 into the string 1st , 2 , 2nd; 3, 3rd; and the rest will have \"st\" added to it.  For numbers over 20 it will take the last and second to last digits, which I have called the tens and unit respectively and test them to see what to add to the number.  This is in python by the way, so I'm not sure if other languages will be able to find the last or second to last digit on a string if they do it should translate pretty easily. I called the function \"o\" for ease of use and can be called by importing the file name which I called \"ordinal\" by import ordinal then ordinal.o(number). Let me know what you think :D"}, {"id": 44682178, "score": 6, "vote": 0, "content": "<p>If using django, you could do:</p>\n<pre><code class=\"python\">from django.contrib.humanize.templatetags.humanize import ordinal\nvar = ordinal(number)\n</code></pre>\n<p>(or use ordinal in a django template as the template filter it was intended to be, though calling it like this from python code works as well)</p>\n<p>If not using django you could steal <a href=\"https://github.com/django/django/blob/1.10.7/django/contrib/humanize/templatetags/humanize.py#L20-L34\" rel=\"noreferrer\">their implementation</a> which is very neat.</p>\n", "abstract": "If using django, you could do: (or use ordinal in a django template as the template filter it was intended to be, though calling it like this from python code works as well) If not using django you could steal their implementation which is very neat."}, {"id": 60595608, "score": 5, "vote": 0, "content": "<p>Import <strong>humanize</strong> module and use <strong>ordinal</strong> function.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import humanize\nhumanize.ordinal(4)\n</code></pre>\n<p>Output</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; '4th'\n</code></pre>\n", "abstract": "Import humanize module and use ordinal function. Output"}, {"id": 54531763, "score": 4, "vote": 0, "content": "<p>There's an ordinal function in <a href=\"https://github.com/jmoiron/humanize\" rel=\"nofollow noreferrer\">humanize</a></p>\n<p><code>pip install humanize</code></p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">&gt;&gt;&gt; [(x, humanize.ordinal(x)) for x in (1, 2, 3, 4, 20, 21, 22, 23, 24, 100, 101,\n...                                     102, 103, 113, -1, 0, 1.2, 13.6)]\n[(1, '1st'), (2, '2nd'), (3, '3rd'), (4, '4th'), (20, '20th'), (21, '21st'),\n (22, '22nd'), (23, '23rd'), (24, '24th'), (100, '100th'), (101, '101st'),\n (102, '102nd'), (103, '103rd'), (113, '113th'), (-1, '-1th'), (0, '0th'),\n (1.2, '1st'), (13.6, '13th')]\n\n</code></pre>\n", "abstract": "There's an ordinal function in humanize pip install humanize"}, {"id": 39596504, "score": 3, "vote": 0, "content": "<p>this function works well for each number <em>n</em>.  If <em>n</em> is negative, it is converted to positive. If <em>n</em> is not integer, it is converted to integer. </p>\n<pre><code class=\"python\">def ordinal( n ):\n\n    suffix = ['th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th']\n\n    if n &lt; 0:\n        n *= -1\n\n    n = int(n)\n\n    if n % 100 in (11,12,13):\n        s = 'th'\n    else:\n        s = suffix[n % 10]\n\n    return str(n) + s\n</code></pre>\n", "abstract": "this function works well for each number n.  If n is negative, it is converted to positive. If n is not integer, it is converted to integer. "}, {"id": 50451518, "score": 3, "vote": 0, "content": "<p>This is an alternative options using num2words package.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from num2words import num2words\n&gt;&gt;&gt; num2words(42, to='ordinal_num')\n    '42nd'\n</code></pre>\n", "abstract": "This is an alternative options using num2words package."}, {"id": 45416102, "score": 2, "vote": 0, "content": "<p>If you don't want to import an external module and prefer a one-line solution, then the following is probably (slightly) more readable than the accepted answer:</p>\n<pre><code class=\"python\">def suffix(i):\n    return {1:\"st\", 2:\"nd\", 3:\"rd\"}.get(i%10*(i%100 not in [11,12,13]), \"th\"))\n</code></pre>\n<p>It uses dictionary <code>.get</code>, as suggested by <a href=\"https://codereview.stackexchange.com/a/41300/90593\">https://codereview.stackexchange.com/a/41300/90593</a> and <a href=\"https://stackoverflow.com/a/36977549/5069869\">https://stackoverflow.com/a/36977549/5069869</a>.</p>\n<p>I made use of multiplication with a boolean to handle the special cases (11,12,13) without having to start an if-block. If the condition <code>(i%100 not in [11,12,13])</code> evaluates to <code>False</code>, the whole number is 0 and we get the default 'th' case.</p>\n", "abstract": "If you don't want to import an external module and prefer a one-line solution, then the following is probably (slightly) more readable than the accepted answer: It uses dictionary .get, as suggested by https://codereview.stackexchange.com/a/41300/90593 and https://stackoverflow.com/a/36977549/5069869. I made use of multiplication with a boolean to handle the special cases (11,12,13) without having to start an if-block. If the condition (i%100 not in [11,12,13]) evaluates to False, the whole number is 0 and we get the default 'th' case."}, {"id": 37094466, "score": 1, "vote": 0, "content": "<p>Here is a more complicated solution I just wrote that takes into account compounded ordinals. So it works from <code>first</code> all the way to <code>nine hundred and ninety ninth</code>. I needed it to convert string street names to the number ordinals:</p>\n<pre><code class=\"python\">import re\nfrom collections import OrderedDict\n\nONETHS = {\n    'first': '1ST', 'second': '2ND', 'third': '3RD', 'fourth': '4TH', 'fifth': '5TH', 'sixth': '6TH', 'seventh': '7TH',\n    'eighth': '8TH', 'ninth': '9TH'\n}\n\nTEENTHS = {\n    'tenth': '10TH', 'eleventh': '11TH', 'twelfth': '12TH', 'thirteenth': '13TH',\n    'fourteenth': '14TH', 'fifteenth': '15TH', 'sixteenth': '16TH', 'seventeenth': '17TH', 'eighteenth': '18TH',\n    'nineteenth': '19TH'\n}\n\nTENTHS = {\n    'twentieth': '20TH', 'thirtieth': '30TH', 'fortieth': '40TH', 'fiftieth': '50TH', 'sixtieth': '60TH',\n    'seventieth': '70TH', 'eightieth': '80TH', 'ninetieth': '90TH',\n}\n\nHUNDREDTH = {'hundredth': '100TH'}  # HUNDREDTH not s\n\nONES = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8',\n        'nine': '9'}\n\nTENS = {'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50', 'sixty': '60', 'seventy': '70', 'eighty': '80',\n        'ninety': '90'}\n\nHUNDRED = {'hundred': '100'}\n\n# Used below for ALL_ORDINALS\nALL_THS = {}\nALL_THS.update(ONETHS)\nALL_THS.update(TEENTHS)\nALL_THS.update(TENTHS)\nALL_THS.update(HUNDREDTH)\n\nALL_ORDINALS = OrderedDict()\nALL_ORDINALS.update(ALL_THS)\nALL_ORDINALS.update(TENS)\nALL_ORDINALS.update(HUNDRED)\nALL_ORDINALS.update(ONES)\n\n\ndef split_ordinal_word(word):\n    ordinals = []\n    if not word:\n        return ordinals \n\n    for key, value in ALL_ORDINALS.items():\n        if word.startswith(key):\n            ordinals.append(key)\n            ordinals += split_ordinal_word(word[len(key):])\n            break\n    return ordinals\n\ndef get_ordinals(s):\n    ordinals, start, end = [], [], []\n    s = s.strip().replace('-', ' ').replace('and', '').lower()\n    s = re.sub(' +',' ', s)  # Replace multiple spaces with a single space\n    s = s.split(' ')\n\n    for word in s:\n        found_ordinals = split_ordinal_word(word)\n        if found_ordinals:\n            ordinals += found_ordinals\n        else:  # else if word, for covering blanks\n            if ordinals:  # Already have some ordinals\n                end.append(word)\n            else:\n                start.append(word)\n    return start, ordinals, end\n\n\ndef detect_ordinal_pattern(ordinals):\n    ordinal_length = len(ordinals)\n    ordinal_string = '' # ' '.join(ordinals)\n    if ordinal_length == 1:\n        ordinal_string = ALL_ORDINALS[ordinals[0]]\n    elif ordinal_length == 2:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDREDTH.keys():\n            ordinal_string = ONES[ordinals[0]] + '00TH'\n        elif ordinals[0] in HUNDRED.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][:-1] + ONETHS[ordinals[1]]\n        elif ordinals[0] in TENS.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = TENS[ordinals[0]][0] + ONETHS[ordinals[1]]\n    elif ordinal_length == 3:\n        if ordinals[0] in HUNDRED.keys() and ordinals[1] in TENS.keys() and ordinals[2] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][0] + TENS[ordinals[1]][0] + ONETHS[ordinals[2]]\n        elif ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in ALL_THS.keys():\n            ordinal_string =  ONES[ordinals[0]] + ALL_THS[ordinals[2]]\n    elif ordinal_length == 4:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in TENS.keys() and \\\n           ordinals[3] in ONETHS.keys():\n                ordinal_string = ONES[ordinals[0]] + TENS[ordinals[2]][0] + ONETHS[ordinals[3]]\n\n    return ordinal_string\n</code></pre>\n<p>And here is some sample usage:</p>\n<pre><code class=\"python\"># s = '32 one   hundred and forty-third st toronto, on'\n#s = '32 forty-third st toronto, on'\n#s = '32 one-hundredth st toronto, on'\n#s = '32 hundred and third st toronto, on'\n#s = '32 hundred and thirty first st toronto, on'\n# s = '32 nine hundred and twenty third st toronto, on'\n#s = '32 nine hundred and ninety ninth st toronto, on'\ns = '32 sixty sixth toronto, on'\n\nst, ords, en = get_ordinals(s)\nprint st, detect_ordinal_pattern(ords), en\n</code></pre>\n", "abstract": "Here is a more complicated solution I just wrote that takes into account compounded ordinals. So it works from first all the way to nine hundred and ninety ninth. I needed it to convert string street names to the number ordinals: And here is some sample usage:"}, {"id": 45155421, "score": 1, "vote": 0, "content": "<p>Gareth's code expressed using the modern .format()</p>\n<pre><code class=\"python\">ordinal = lambda n: \"{}{}\".format(n,\"tsnrhtdd\"[(n/10%10!=1)*(n%10&lt;4)*n%10::4])\n</code></pre>\n", "abstract": "Gareth's code expressed using the modern .format()"}, {"id": 50357956, "score": 1, "vote": 0, "content": "<p>This can handle any length number, the exceptions for ...#11 to ...#13 and negative integers.</p>\n<pre><code class=\"python\">def ith(i):return(('th'*(10&lt;(abs(i)%100)&lt;14))+['st','nd','rd',*['th']*7][(abs(i)-1)%10])[0:2]\n</code></pre>\n<p>I suggest using ith() as a name to avoid overriding the builtin ord().</p>\n<pre><code class=\"python\"># test routine\nfor i in range(-200,200):\n    print(i,ith(i))\n</code></pre>\n<p>Note: Tested with Python 3.6; The abs() function was available without explicitly including a math module.</p>\n", "abstract": "This can handle any length number, the exceptions for ...#11 to ...#13 and negative integers. I suggest using ith() as a name to avoid overriding the builtin ord(). Note: Tested with Python 3.6; The abs() function was available without explicitly including a math module."}, {"id": 52354877, "score": 1, "vote": 0, "content": "<p>Try this </p>\n<pre><code class=\"python\">import sys\n\na = int(sys.argv[1])\n\nfor i in range(1,a+1):\n\nj = i\nif(j%100 == 11 or j%100 == 12 or j%100 == 13):\n    print(\"%dth Hello\"%(j))\n    continue            \ni %= 10\nif ((j%10 == 1) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dst Hello\"%(j))\nelif ((j%10 == 2) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dnd Hello\"%(j))\nelif ((j%10 == 3) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%drd Hello\"%(j))\nelse:\n    print(\"%dth Hello\"%(j))\n</code></pre>\n", "abstract": "Try this "}, {"id": 45130861, "score": 0, "vote": 0, "content": "<p>I salute Gareth's lambda code. So elegant. I only half-understand how it works though. So I tried to deconstruct it and came up with this:</p>\n<pre><code class=\"python\">def ordinal(integer):\n\n    int_to_string = str(integer)\n\n    if int_to_string == '1' or int_to_string == '-1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string == '2' or int_to_string == '-2':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string == '3' or int_to_string == '-3':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    elif int_to_string[-1] == '1' and int_to_string[-2] != '1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string[-1] == '2' and int_to_string[-2] != '1':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string[-1] == '3' and int_to_string[-2] != '1':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    else:\n        print int_to_string+'th'\n        return int_to_string+'th';\n\n\n&gt;&gt;&gt; print [ordinal(n) for n in range(1,25)]\n1st\n2nd\n3rd\n4th\n5th\n6th\n7th\n8th\n9th\n10th\n11th\n12th\n13th\n14th\n15th\n16th\n17th\n18th\n19th\n20th\n21st\n22nd\n23rd\n24th\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',             \n'11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', \n'20th', '21st', '22nd', '23rd', '24th']\n</code></pre>\n", "abstract": "I salute Gareth's lambda code. So elegant. I only half-understand how it works though. So I tried to deconstruct it and came up with this:"}]}, {"link": "https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk", "question": {"id": "19130512", "title": "Stopword removal with NLTK", "content": "<p>I am trying to process a user entered text by removing stopwords using nltk toolkit, but with stopword-removal the words like 'and', 'or', 'not' gets removed. I want these words to be present after stopword removal process as they are operators which are required for later processing text as query. I don't know which are the words which can be operators in text query, and I also want to remove unnecessary words from my text.</p>\n", "abstract": "I am trying to process a user entered text by removing stopwords using nltk toolkit, but with stopword-removal the words like 'and', 'or', 'not' gets removed. I want these words to be present after stopword removal process as they are operators which are required for later processing text as query. I don't know which are the words which can be operators in text query, and I also want to remove unnecessary words from my text."}, "answers": [{"id": 19133088, "score": 144, "vote": 0, "content": "<p>There is an in-built stopword list in <code>NLTK</code> made up of 2,400 stopwords for 11 languages (Porter et al), see <a href=\"http://nltk.org/book/ch02.html\" rel=\"noreferrer\">http://nltk.org/book/ch02.html</a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk import word_tokenize\n&gt;&gt;&gt; from nltk.corpus import stopwords\n&gt;&gt;&gt; stop = set(stopwords.words('english'))\n&gt;&gt;&gt; sentence = \"this is a foo bar sentence\"\n&gt;&gt;&gt; print([i for i in sentence.lower().split() if i not in stop])\n['foo', 'bar', 'sentence']\n&gt;&gt;&gt; [i for i in word_tokenize(sentence.lower()) if i not in stop] \n['foo', 'bar', 'sentence']\n</code></pre>\n<p>I recommend looking at using tf-idf to remove stopwords, see <a href=\"https://stackoverflow.com/questions/10464265/effects-of-stemming-on-the-term-frequncy\">Effects of Stemming on the term frequency?</a></p>\n", "abstract": "There is an in-built stopword list in NLTK made up of 2,400 stopwords for 11 languages (Porter et al), see http://nltk.org/book/ch02.html I recommend looking at using tf-idf to remove stopwords, see Effects of Stemming on the term frequency?"}, {"id": 24106778, "score": 71, "vote": 0, "content": "<p>I suggest you create your own list of operator words that you take out of the stopword list. Sets can be conveniently subtracted, so:</p>\n<pre><code class=\"python\">operators = set(('and', 'or', 'not'))\nstop = set(stopwords...) - operators\n</code></pre>\n<p>Then you can simply test if a word is <code>in</code> or <code>not in</code> the set without relying on whether your operators are part of the stopword list. You can then later switch to another stopword list or add an operator.</p>\n<pre><code class=\"python\">if word.lower() not in stop:\n    # use word\n</code></pre>\n", "abstract": "I suggest you create your own list of operator words that you take out of the stopword list. Sets can be conveniently subtracted, so: Then you can simply test if a word is in or not in the set without relying on whether your operators are part of the stopword list. You can then later switch to another stopword list or add an operator."}, {"id": 32469562, "score": 35, "vote": 0, "content": "<p>@alvas's answer does the job but it can be done way faster. Assuming that you have <code>documents</code>: a list of strings.</p>\n<pre><code class=\"python\">from nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n\nfor doc in documents:\n    list_of_words = [i.lower() for i in wordpunct_tokenize(doc) if i.lower() not in stop_words]\n</code></pre>\n<p>Notice that due to the fact that here you are searching in a set (not in a list) the speed would be theoretically <code>len(stop_words)/2</code> times faster, which is significant if you need to operate through many documents.</p>\n<p>For 5000 documents of approximately 300 words each the difference is between 1.8 seconds for my example and 20 seconds for @alvas's.</p>\n<p>P.S. in most of the cases you need to divide the text into words to perform some other classification tasks for which tf-idf is used. So most probably it would be better to use stemmer as well:</p>\n<pre><code class=\"python\">from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n</code></pre>\n<p>and to use <code>[porter.stem(i.lower()) for i in wordpunct_tokenize(doc) if i.lower() not in stop_words]</code> inside of a loop.</p>\n", "abstract": "@alvas's answer does the job but it can be done way faster. Assuming that you have documents: a list of strings. Notice that due to the fact that here you are searching in a set (not in a list) the speed would be theoretically len(stop_words)/2 times faster, which is significant if you need to operate through many documents. For 5000 documents of approximately 300 words each the difference is between 1.8 seconds for my example and 20 seconds for @alvas's. P.S. in most of the cases you need to divide the text into words to perform some other classification tasks for which tf-idf is used. So most probably it would be better to use stemmer as well: and to use [porter.stem(i.lower()) for i in wordpunct_tokenize(doc) if i.lower() not in stop_words] inside of a loop."}, {"id": 24214114, "score": 14, "vote": 0, "content": "<p>@alvas has a good answer. But again it depends on the nature of the task, for example in your application you want to consider all <code>conjunction</code> e.g. <em>and, or, but, if, while</em> and all <code>determiner</code> e.g. <em>the, a, some, most, every, no</em> as stop words considering all others parts of speech as legitimate, then you might want to look into this solution which use Part-of-Speech Tagset to discard words, <a href=\"http://www.nltk.org/book/ch05.html\" rel=\"noreferrer\">Check table 5.1</a>:</p>\n<pre><code class=\"python\">import nltk\n\nSTOP_TYPES = ['DET', 'CNJ']\n\ntext = \"some data here \"\ntokens = nltk.pos_tag(nltk.word_tokenize(text))\ngood_words = [w for w, wtype in tokens if wtype not in STOP_TYPES]\n</code></pre>\n", "abstract": "@alvas has a good answer. But again it depends on the nature of the task, for example in your application you want to consider all conjunction e.g. and, or, but, if, while and all determiner e.g. the, a, some, most, every, no as stop words considering all others parts of speech as legitimate, then you might want to look into this solution which use Part-of-Speech Tagset to discard words, Check table 5.1:"}, {"id": 49035785, "score": 6, "vote": 0, "content": "<p>You can use <a href=\"https://docs.python.org/2/library/string.html#string.punctuation\" rel=\"noreferrer\">string.punctuation</a> with built-in NLTK stopwords list:</p>\n<pre><code class=\"python\">from nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nwords = tokenize(text)\nwordsWOStopwords = removeStopWords(words)\n\ndef tokenize(text):\n        sents = sent_tokenize(text)\n        return [word_tokenize(sent) for sent in sents]\n\ndef removeStopWords(words):\n        customStopWords = set(stopwords.words('english')+list(punctuation))\n        return [word for word in words if word not in customStopWords]\n</code></pre>\n<p>NLTK stopwords complete <a href=\"https://gist.github.com/sebleier/554280\" rel=\"noreferrer\">list</a> </p>\n", "abstract": "You can use string.punctuation with built-in NLTK stopwords list: NLTK stopwords complete list "}, {"id": 67296099, "score": 0, "vote": 0, "content": "<p><strong>STOPWORDS REMOVAL FROM STRING</strong></p>\n<p><em>Here I added Custom stopword list also</em></p>\n<pre><code class=\"python\">nltk.download('stopwords')\nfrom nltk.corpus import stopwords                    # Stop words\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(list(set(['zero'    , 'one'     , 'two'      ,\n               'three'   , 'four'    , 'five'     ,\n               'six'     , 'seven'   , 'eight'    ,\n               'nine'    , 'ten'     ,\n               \n               'may'     , 'also'    , 'across'   ,\n               'among'   , 'beside'  , 'however'  ,\n               'yet'     , 'within'  ,\n               \n               'jan'     ,  'feb'    , 'mar'      ,\n               'apr'     ,  'may'    , 'jun'      ,\n               'jul'     ,  'aug'    , 'sep'      ,\n               'oct'     ,  'nov'    , 'dec'      ,\n               \n               'january' , 'february', 'march'    ,\n               'april'   , 'may'     , 'june'     ,\n               'july'    , 'august'  , 'september',\n               'october' , 'november', 'december' ,\n               \n               'summer'  , 'winter'  , 'fall'     ,\n               'spring'                          \n\n               \"a\"         , \"about\"     ,   \"above\"  , \"after\"   ,\n               \"again\"     , \"against\"   ,   \"ain\"    , \"aren't\"  ,\n               \"all\"       , \"am\"        ,   \"an\"     , \"and\"     ,\n               \"any\"       , \"are\"       ,   \"aren\"   ,  \"as\"     ,\n               \"at\"        ,\n               \n               \"be\"        , \"because\"   ,   \"been\"   , \"before\"  ,\n               \"being\"     , \"below\"     ,   \"between\", \"both\"    ,\n               \"but\"       , \"by\"        ,                  \n               \n               \"can\"       , \"couldn\"    , \"couldn't\" , \"could\"   ,\n               \n               \"d\"         , \"did\"       , \"didn\"     , \"didn't\"  ,\n               \"do\"        , \"does\"      , \"doesn\"    , \"doesn't\" ,\n               \"doing\"     , \"don\"       , \"don't\"    , \"down\"    ,\n               \"during\"    ,\n               \n               \"each\"      ,  \n               \n               \"few\"       , \"for\"      , \"from\"      , \"further\" ,\n               \n               \"had\"       , \"hadn\"     , \"hadn't\"    , \"has\"     ,\n               \"hasn\"      , \"hasn't\"   , \"have\"      , \"haven\"   ,\n               \"haven't\"   , \"having\"   , \"he\"        , \"her\"     ,\n               \"here\"      , \"hers\"     , \"herself\"   , \"him\"     ,\n               \"himself\"   , \"his\"      , \"how\"       ,\n               \"he'd\"      , \"he'll\"    , \"he's\"      , \"here's\"  ,\n               \"how's\"     ,\n               \n               \"i\"         , \"if\"       , \"in\"        , \"into\"    ,\n               \"is\"        , \"isn\"      , \"isn't\"     , \"it\"      ,\n               \"it's\"      , \"its\"      , \"itself\"    , \"i'd\"     ,\n               \"i'll\"      , \"i'm\"      , \"i've\"      ,\n               \n               \"just\"      ,\n               \n               \"ll\"        , \"let's\"    ,\n               \n               \"m\"         , \"ma\"       ,\"me\"         ,\n               \"mightn\"    , \"mightn't\" , \"more\"      , \"most\"    ,\n               \"mustn\"     , \"mustn't\"  , \"my\"        , \"myself\"  ,\n               \"needn\"     , \"needn't\"  , \"no\"        , \"nor\"     ,\n               \"not\"       , \"now\"      ,\n               \n               \"o\"         , \"of\"       , \"off\"       , \"on\"      ,\n               \"once\"      , \"only\"     , \"or\"        , \"other\"   ,\n               \"our\"       , \"ours\"     , \"ourselves\" , \"out\"     ,\n               \"over\"      , \"own\"      , \"ought\"     ,\n               \n               \"re\"        ,\n               \n               \"s\"         , \"same\"     , \"shan\"      , \"shan't\"   ,\n               \"she\"       , \"she's\"    , \"should\"    , \"should've\",\n               \"shouldn\"   , \"shouldn't\", \"so\"        , \"some\"     ,\n               \"such\"      , \"she'd\"    , \"she'll\"    ,\n               \n               \"t\"         , \"than\"     , \"that\"      , \"that'll\"  ,\n               \"the\"       , \"their\"    , \"theirs\"    , \"them\"     ,\n               \"themselves\", \"then\"     , \"there\"     , \"these\"    ,\n               \"they\"      , \"this\"     , \"those\"     , \"through\"  ,\n               \"to\"        , \"too\"      , \"that's\"    , \"there's\"  ,\n               \"they'd\"    , \"they'll\"  , \"they're\"   , \"they've\"  ,\n               \n               \"under\"     , \"until\"    , \"up\"        ,\n               \n               \"ve\"        , \"very\"     ,\n               \n               \"was\"       , \"wasn\"     , \"wasn't\"    , \"we\"       ,\n               \"were\"      , \"weren\"    , \"weren't\"   , \"what\"     ,\n               \"when\"      , \"where\"    , \"which\"     , \"while\"    ,\n               \"who\"       , \"whom\"     , \"why\"       , \"will\"     ,\n               \"with\"      , \"won\"      , \"won't\"     , \"wouldn\"   ,\n               \"wouldn't\"  , \"we'd\"     , \"we'll\"     , \"we're\"    ,\n               \"we've\"     , \"what's\"   , \"when's\"    , \"where's\"  ,\n               \"who's\"     , \"why's\"    , \"would\"     ,\n               \n               \"y\"         , \"you\"      , \"you'd\"     , \"you'll\"   ,\n               \"you're\"    , \"you've\"   , \"your\"      , \"yours\"    , \"yourself\",\n               \"yourselves\",\n               \n               'a',\"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\"          ,\n               \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",      \"almost\"          ,\n               \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"anyone\"        ,  \n               \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",  \"anything\", \"anyway\", \"anyways\"        ,\n               \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\"          ,\n               \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\"   ,\n               \"appear\", \"appreciate\", \"appropriate\", \"associated\"                                            ,\n               \n               \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\"     ,\n               \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\"    ,\n               \"briefly\"                                                                                      ,\n               \n               \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\"   ,\n               \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\"                                ,\n               \n               'd',\"date\", \"different\", \"done\", \"downwards\", \"due\"                                                ,\n               \n               \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\"      ,\n               \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\",\"except\"   ,\n               \"everyone\", \"everything\", \"everywhere\", \"ex\"                                                   ,  \n               \n               \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"four\"  ,\n               \"former\", \"formerly\", \"forth\", \"found\",  \"furthermore\"                                         ,\n               \n               \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\",  \"go\", \"goes\", \"got\",\"gone\"   ,  \n               \"gotten\", \"giving\"                                                                             ,\n               \n               \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"however\"  ,\n               \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",  \"hundred\"                        ,\n               \n               \"id\", \"ie\", \"im\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"itd\", \"index\"    ,\n               'i',\"information\", \"instead\", \"invention\",   \"it'll\", \"inward\", \"immediate\"                        ,\n               \n               \"j\",\n               \n               \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\"                             ,\n               \n               \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"ltd\",    \n               \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\"  ,  \n               \n               'm',\"made\", \"mainly\", \"make\", \"makes\", \"many\", \"maybe\", \"mean\", \"means\", \"meantime\", \"merely\", \"mg\",\n               \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\"     ,\n               \"meanwhile\", \"may\"                                                                             ,\n               \n               \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\" ,\n               \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\"  ,\n               \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"n2\", \"nc\"   ,\n               \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\"                               ,\n               \n               'o',\"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\",\n               \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\",  \"oa\", \"ob\", \"oc\", \"od\"   ,\n               \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\" ,\n               \n               \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\" ,\n               \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\"       ,\n               \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\"       ,\n               \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\" ,\n               \"pr\", \"ps\", \"pt\", \"pu\", \"py\"                                                                   ,\n               \n               \"q\", \"que\", \"quickly\", \"quite\", \"qv\",  \"qj\", \"qu\"                                              ,\n               \n               'r',\"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\" ,\n               \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"run\" ,\n               \"right\",  \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\"   ,\n               \"rs\", \"rt\", \"ru\", \"rv\", \"ry\" \"r\", \"ran\", \"rather\", \"rd\"                                        ,                                                                  \n               \n               's',\"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\"    ,\n               \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\"      ,\n               \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\"           ,\n               \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"soon\"   ,\n               \"somewhat\", \"somewhere\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\" ,\n               \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\"   ,\n               \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\" ,\n               \"sy\", \"sz\",   \"sorry\", \"sometime\", \"somethan\", \"something\", \"sometimes\"                        ,\n               \n               't',\"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"thank\", \"thanx\", \"that've\", \"thence\", \"thereafter\",\n               \"thereby\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"thereto\", \"thereupon\"    ,\n               \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\"      ,\n               \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\"   ,\n               \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"thats\",  \"thanks\",  \"th\",  \"thered\"  ,\n               \"theres\" \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\"    ,\n               \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\"                                                       ,                                                                                        \n               \n               \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\" ,\n               \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ue\", \"ui\", \"uj\", \"uk\" ,\n               \"um\", \"un\", \"uo\", \"ur\", \"ut\",\n               \n               \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"va\", \"vd\", \"vj\", \"vo\", \"vq\",\n               \"vt\", \"vu\"                                                                                     ,\n               \n               \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\",\n               \"whats\", \"whence\", \"whenever\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"wherever\", \"whether\",  \n               \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\" ,\n               \"whereupon\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\",\n               \"wi\", \"wa\", \"wo\",\n               \n               \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\",\n               \n               \"yes\", \"yet\", \"youd\", \"youre\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\",\n               \n               \"z\", \"zero\", \"zi\", \"zz\"\n               \n               \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",                   \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\",\n               \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\",\n               \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\",\n               \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\",\n               \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\",\n               \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\",\n               \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\",\n               \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\",\n               \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n               \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\",\n               \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\",\n               \"jj\", \"jr\", \"js\", \"jt\", \"ju\",\n               \"ke\", \"kg\", \"kj\", \"km\", \"ko\",\n               \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\",\n               \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\",\n               \n               'i',  'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii','ix', 'x',\n               'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx',\n                'xxi', 'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx',\n                'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxv', 'xxxvi', 'xxxvii', 'xxxviii', 'xxxix', 'xl',\n               'xli', 'xlii', 'xliii', 'xliv', 'xlv', 'xlvi', 'xlvii', 'xlviii', 'xlix', 'l',\n               'li', 'lii', 'liii', 'liv', 'lv', 'lvi', 'lvii', 'lviii', 'lix', 'lx',\n               'lxi', 'lxii', 'lxiii', 'lxiv', 'lxv', 'lxvi', 'lxvii', 'lxviii', 'lxix', 'lxx',\n                'lxxi', 'lxxii', 'lxxiii', 'lxxiv', 'lxxv', 'lxxvi', 'lxxvii', 'lxxviii', 'lxxix', 'lxxx',\n                'lxxxi', 'lxxxii', 'lxxxiii', 'lxxxiv', 'lxxxv', 'lxxxvi', 'lxxxvii', 'lxxxviii', 'lxxxix', 'xc',\n                'xci', 'xcii', 'xciii', 'xciv', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xcix', 'c',\n               \n                \"one\", \"first\", \"two\", \"second\", \"three\", \"third\",\n                \"four\", \"fourth\", \"five\", \"fifth\", \"six\",  \"sixth\", \"seven\",\n                \"seventh\", \"eight\", \"eighth\", \"nine\", \"ninth\", \"ten\",\n                \"tenth\", \"eleven\", \"eleventh\", \"twelve\", \"twelfth\", \"thirteen\",\n                \"thirteenth\", \"fourteen\", \"fourteenth\", \"fifteen\", \"fifteenth\",\n                \"sixteen\", \"sixteenth\",  \"seventeen\", \"seventeenth\", \"eighteen\",\n                \"eighteenth\", \"nineteen\", \"nineteenth\", \"twenty\", \"twentieth\",\n                \"one\", \"22nd\", \"second\", \"nd\", \"st\", \"rd\", \"th\",\n               \n                \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10th\",\"11th\",\"12th\",\"13th\",\"14th\",\"15th\",\n                \"16th\",\"17th\",\"18th\",\"19th\",\"20th\",\"21st\",\"22nd\",\"23rd\",\"24th\",\"25th\",\"26th\",\"27th\",\n                \"28th\",\"29th\",\"30th\",\"31st\",\"32nd\",\"33rd\",\"34th\",\"35th\",\"36th\",\"37th\",\"38th\",\"39th\",\n                \"40th\",\"41st\",\"42nd\",\"43rd\",\"44th\",\"45th\",\"46th\",\"47th\",\"48th\",\"49th\",\"50th\",\"51st\",\n                \"52nd\",\"53rd\",\"54th\",\"55th\",\"56th\",\"57th\",\"58th\",\"59th\",\"60th\",\"61st\",\"62nd\",\"63rd\",\n                \"64th\",\"65th\",\"66th\",\"67th\",\"68th\",\"69th\",\"70th\",\"71st\",\"72nd\",\"73rd\",\"74th\",\"75th\",\n                \"76th\",\"77th\",\"78th\",\"79th\",\"80th\",\"81st\",\"82nd\",\"83rd\",\"84th\",\"85th\",\"86th\",\"87th\",\n                \"88th\",\"89th\",\"90th\", \"91st\", \"92nd\", \"93rd\", \"94th\", \"95th\", \"96th\",\"97th\", \"98th\",\n                \"99th\",\"100th\",\"thirty\",\"forty\",\"fifty\",\"thirty\",\"thirtieth\",\"forty\",\"fortieth\",\n                \"fifty\", \"fiftiethiftieth\",\"sixty\",\"sixtieth\",\"seventy\",\"seventieth\", \"eighty\",\n                \"eightieth\", \"ninety\", \"ninetieth\",\"one\", \"hundred\", \"100th\", \"hundredth\",\n                \"order\",\"state\",\"page\",\"file\",\n                \n                \"'d\",\"'ll\",  \"'m\",  \"'re\",  \"'s\",  \"'ve\",  'a',  \n                'about',  'above',  'across',  'after',  'afterwards',  'again',  'against',  'all',  \n                'almost',  'alone',  'along',  'already',  'also',  'although',  'always',  'am',  \n                'among',  'amongst',  'amount',  'an',  'and',  'another',  'any',  'anyhow',  'anyone',  \n                'anything',  'anyway',  'anywhere',  'are',  'around',  'as',  'at',  'back',  'be',\n                'became',  'because',  'become',  'becomes',  'becoming',  'been',  'before',  'beforehand',\n                'behind',  'being',  'below',  'beside',  'besides',  'between',  'beyond',  'both',\n                'bottom',  'but',  'by',  'ca',  'call',  'can',  'cannot',  'could',  'did',  'do',  'does',\n                'doing',  'done',  'down',  'due',  'during',  'each',  'eight',  'either',  'eleven',\n                'else',  'elsewhere',  'empty',  'enough',  'even',  'ever',  'every',  'everyone',\n                'everything',  'everywhere',  'except',  'few',  'fifteen',  'fifty',  'first',\n                'five',  'for',  'former',  'formerly',  'forty',  'four',  'from',  'front',  'full',\n                'further',  'get',  'give',  'go',  'had',  'has',  'have',  'he',  'hence',  'her',\n                'here',  'hereafter',  'hereby',  'herein',  'hereupon',  'hers',  'herself',  'him',  'himself',\n                'his',  'how',  'however',  'hundred',  'i',  'if',  'in',  'indeed',  'into',  'is',  'it',\n                'its',  'itself',  'just',  'keep',  'last',  'latter',  'latterly',  'least',  'less',  'made',\n                'make',  'many',  'may',  'me',  'meanwhile',  'might',  'mine',  'more',  'moreover',  'most',\n                'mostly',  'move',  'much',  'must',  'my',  'myself',  \"n't\",  'name',  'namely',  'neither',\n                'never',  'nevertheless',  'next',  'nine',  'no',  'nobody',  'none',  'noone',  'nor',  'not',\n                'nothing',  'now',  'nowhere',  'n\u2018t',  'n\u2019t',  'of',  'off',  'often',  'on',  'once',  'one',\n                'only',  'onto',  'or',  'other',  'others',  'otherwise',  'our',  'ours',  'ourselves',  'out',\n                'over',  'own',  'part',  'per',  'perhaps',  'please',  'put',  'quite',  'rather',  're',  'really',\n                'regarding',  'same',  'say',  'see',  'seem',  'seemed',  'seeming',  'seems',  'serious',  'several',\n                'she',  'should',  'show',  'side',  'since',  'six',  'sixty',  'so',  'some',  'somehow',  'someone',\n                'something',  'sometime',  'sometimes',  'somewhere',  'still',  'such',  'take',  'ten',  'than',\n                'that',  'the',  'their',  'them',  'themselves',  'then',  'thence',  'there',  'thereafter',\n                'thereby',  'therefore',  'therein',  'thereupon',  'these',  'they',  'third',  'this',  'those',\n                'though',  'three',  'through',  'throughout',  'thru',  'thus',  'to',  'together',  'too',  'top',\n                'toward',  'towards',  'twelve',  'twenty',  'two',  'under',  'unless',  'until',  'up',  'upon',  'us',\n                'used',  'using',  'various',  'very',  'via',  'was',  'we',  'well',  'were',  'what',  'whatever',  'when',\n                'whence',  'whenever',  'where',  'whereafter',  'whereas',  'whereby',  'wherein',  'whereupon',  'wherever',\n                'whether',  'which',  'while',  'whither',  'who',  'whoever',  'whole',  'whom',  'whose',  'why',  'will',\n                'with',  'within',  'without',  'would',  'yet',  'you',  'your',  'yours',  'yourself',  'yourselves',  '\u2018d',\n                '\u2018ll',  '\u2018m',  '\u2018re',  '\u2018s',  '\u2018ve',  '\u2019d',  '\u2019ll',  '\u2019m',  '\u2019re',  '\u2019s',  '\u2019ve'\n\n                       \n                       ])))\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\n\nstop_words = stopwords.words(\"english\")\n\nsentence = \"PDF.co is a website that contains different tools to read, write and process PDF documents\"\nwords = word_tokenize(sentence)\n\nsentence_wo_stopwords = [word for word in words if not word in stop_words]\n\nprint(\" \".join(sentence_wo_stopwords))\n</code></pre>\n", "abstract": "STOPWORDS REMOVAL FROM STRING Here I added Custom stopword list also"}]}, {"link": "https://stackoverflow.com/questions/10383044/fuzzy-string-comparison", "question": {"id": "10383044", "title": "Fuzzy String Comparison", "content": "<p>What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. </p>\n<p>I am unsure which operation to use to allow me to complete this in Python 3. </p>\n<p>I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  </p>\n<h2>Text: Sample</h2>\n<p>Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.</p>\n<p>Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n// Should score high point but not 1</p>\n<p>Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n// Should score lower than text 20</p>\n<p>Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n// Should score lower than text 21 but NOT 0</p>\n<p>Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\n// Should score a 0!</p>\n", "abstract": "What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0.  I am unsure which operation to use to allow me to complete this in Python 3.  I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.   Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats. Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n// Should score high point but not 1 Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n// Should score lower than text 20 Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n// Should score lower than text 21 but NOT 0 Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\n// Should score a 0!"}, "answers": [{"id": 28467760, "score": 119, "vote": 0, "content": "<p>There is a package called <a href=\"https://github.com/seatgeek/fuzzywuzzy\" rel=\"noreferrer\"><code>fuzzywuzzy</code></a>. Install via pip:</p>\n<pre><code class=\"python\">pip install fuzzywuzzy\n</code></pre>\n<p>Simple usage:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from fuzzywuzzy import fuzz\n&gt;&gt;&gt; fuzz.ratio(\"this is a test\", \"this is a test!\")\n    96\n</code></pre>\n<p>The package is built on top of <code>difflib</code>. Why not just use that, you ask? Apart from being a bit simpler, it has a number of different matching methods (like token order insensitivity, partial string matching) which make it more powerful in practice. The <code>process.extract</code> functions are especially useful: find the best matching strings and ratios from a set. From their readme:</p>\n<blockquote>\n<p>Partial Ratio</p>\n</blockquote>\n<pre><code class=\"python\">&gt;&gt;&gt; fuzz.partial_ratio(\"this is a test\", \"this is a test!\")\n    100\n</code></pre>\n<blockquote>\n<p>Token Sort Ratio</p>\n</blockquote>\n<pre><code class=\"python\">&gt;&gt;&gt; fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n    90\n&gt;&gt;&gt; fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n    100\n</code></pre>\n<blockquote>\n<p>Token Set Ratio</p>\n</blockquote>\n<pre><code class=\"python\">&gt;&gt;&gt; fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n    84\n&gt;&gt;&gt; fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n    100\n</code></pre>\n<blockquote>\n<p>Process</p>\n</blockquote>\n<pre><code class=\"python\">&gt;&gt;&gt; choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n&gt;&gt;&gt; process.extract(\"new york jets\", choices, limit=2)\n    [('New York Jets', 100), ('New York Giants', 78)]\n&gt;&gt;&gt; process.extractOne(\"cowboys\", choices)\n    (\"Dallas Cowboys\", 90)\n</code></pre>\n", "abstract": "There is a package called fuzzywuzzy. Install via pip: Simple usage: The package is built on top of difflib. Why not just use that, you ask? Apart from being a bit simpler, it has a number of different matching methods (like token order insensitivity, partial string matching) which make it more powerful in practice. The process.extract functions are especially useful: find the best matching strings and ratios from a set. From their readme: Partial Ratio Token Sort Ratio Token Set Ratio Process"}, {"id": 10383524, "score": 101, "vote": 0, "content": "<p>There is a module in the standard library (called <a href=\"http://docs.python.org/library/difflib.html\" rel=\"noreferrer\"><code>difflib</code></a>) that can compare strings and return a score based on their similarity. The <a href=\"http://docs.python.org/library/difflib.html#sequencematcher-objects\" rel=\"noreferrer\"><code>SequenceMatcher</code></a> class should do what you want.</p>\n<p>Small example from Python prompt:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from difflib import SequenceMatcher as SM\n&gt;&gt;&gt; s1 = ' It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.'\n&gt;&gt;&gt; s2 = ' It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines.'\n&gt;&gt;&gt; SM(None, s1, s2).ratio()\n0.9112903225806451\n</code></pre>\n", "abstract": "There is a module in the standard library (called difflib) that can compare strings and return a score based on their similarity. The SequenceMatcher class should do what you want. Small example from Python prompt:"}, {"id": 38316398, "score": 17, "vote": 0, "content": "<p><a href=\"https://pypi.python.org/pypi/fuzzyset\" rel=\"noreferrer\"><code>fuzzyset</code></a> is much faster than <code>fuzzywuzzy</code> (<code>difflib</code>) for both indexing and searching.</p>\n<pre><code class=\"python\">from fuzzyset import FuzzySet\ncorpus = \"\"\"It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n    It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n    I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n    It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\"\"\"\ncorpus = [line.lstrip() for line in corpus.split(\"\\n\")]\nfs = FuzzySet(corpus)\nquery = \"It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.\"\nfs.get(query)\n# [(0.873015873015873, 'It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines')]\n</code></pre>\n<p>Warning: Be careful not to mix <code>unicode</code> and <code>bytes</code> in your fuzzyset.</p>\n", "abstract": "fuzzyset is much faster than fuzzywuzzy (difflib) for both indexing and searching. Warning: Be careful not to mix unicode and bytes in your fuzzyset."}, {"id": 40755119, "score": 9, "vote": 0, "content": "<p>The task is called <a href=\"https://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)\" rel=\"noreferrer\">Paraphrase Identification</a> which is an active area of research in Natural Language Processing. I have linked several state of the art papers many of which you can find open source code on GitHub for.</p>\n<p>Note that all the answered question assume that there is some string/surface similarity between the two sentences while in reality two sentences with little string similarity can be semantically similar.</p>\n<p>If you're interested in that kind of similarity you can use <a href=\"https://github.com/ryankiros/skip-thoughts\" rel=\"noreferrer\">Skip-Thoughts</a>.\nInstall the software according to the GitHub guides and go to paraphrase detection section in readme:</p>\n<pre><code class=\"python\">import skipthoughts\nmodel = skipthoughts.load_model()\nvectors = skipthoughts.encode(model, X_sentences)\n</code></pre>\n<p>This converts your sentences (X_sentences) to vectors. Later you can find the similarity of two vectors by:</p>\n<pre><code class=\"python\">similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n</code></pre>\n<p>where we are assuming vector[0] and vector<a href=\"https://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)\" rel=\"noreferrer\">1</a> are the corresponding vector to X_sentences[0], X_sentences<a href=\"https://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)\" rel=\"noreferrer\">1</a> which you wanted to find their scores.</p>\n<p>There are other models to convert a sentence to a vector which you can find <a href=\"https://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)\" rel=\"noreferrer\">here</a>.</p>\n<p>Once you convert your sentences into vectors the similarity is just a matter of finding the Cosine similarity between those vectors.</p>\n<p><strong>Update in 2020</strong>\nThere is this new model called <a href=\"https://github.com/google-research/bert\" rel=\"noreferrer\">BERT</a> released by Google based on a deep learning framework called Tensorflow. There is also an implementation that many people find easier to use called <a href=\"https://github.com/huggingface/transformers\" rel=\"noreferrer\">Transformers</a>. What these programs do, is that they accept two phrases or sentences, and they are able to be trained to say if these two phrases/sentences are the same or not. To train them, you need a number of sentences with labels 1 or 0 (if they have the same meaning or not). You train these models using your training data (already labelled data), and then you'll be able to use the trained model to make prediction for a new pair of phrases/sentences. You can find how to train (they call it fine-tune) these models on their corresponding github pages or in many other places such as <a href=\"https://www.kaggle.com/deepaksinghrawat/finetuning-bert-on-mrpc-corpus-using-fastai\" rel=\"noreferrer\">this</a>.</p>\n<p>There are also already labelled training data available in English called MRPC (microsoft paraphrase identification corpus). Note that there  multilingual or language-specific versions of BERT also exists so this model can be extended (e.g. trained) in other languages as well.  </p>\n", "abstract": "The task is called Paraphrase Identification which is an active area of research in Natural Language Processing. I have linked several state of the art papers many of which you can find open source code on GitHub for. Note that all the answered question assume that there is some string/surface similarity between the two sentences while in reality two sentences with little string similarity can be semantically similar. If you're interested in that kind of similarity you can use Skip-Thoughts.\nInstall the software according to the GitHub guides and go to paraphrase detection section in readme: This converts your sentences (X_sentences) to vectors. Later you can find the similarity of two vectors by: where we are assuming vector[0] and vector1 are the corresponding vector to X_sentences[0], X_sentences1 which you wanted to find their scores. There are other models to convert a sentence to a vector which you can find here. Once you convert your sentences into vectors the similarity is just a matter of finding the Cosine similarity between those vectors. Update in 2020\nThere is this new model called BERT released by Google based on a deep learning framework called Tensorflow. There is also an implementation that many people find easier to use called Transformers. What these programs do, is that they accept two phrases or sentences, and they are able to be trained to say if these two phrases/sentences are the same or not. To train them, you need a number of sentences with labels 1 or 0 (if they have the same meaning or not). You train these models using your training data (already labelled data), and then you'll be able to use the trained model to make prediction for a new pair of phrases/sentences. You can find how to train (they call it fine-tune) these models on their corresponding github pages or in many other places such as this. There are also already labelled training data available in English called MRPC (microsoft paraphrase identification corpus). Note that there  multilingual or language-specific versions of BERT also exists so this model can be extended (e.g. trained) in other languages as well.  "}, {"id": 70934364, "score": 1, "vote": 0, "content": "<p>There is also this fast and accurate fuzzy comparison library licensed by MIT:\n<a href=\"https://github.com/maxbachmann/RapidFuzz\" rel=\"nofollow noreferrer\">https://github.com/maxbachmann/RapidFuzz</a></p>\n", "abstract": "There is also this fast and accurate fuzzy comparison library licensed by MIT:\nhttps://github.com/maxbachmann/RapidFuzz"}]}, {"link": "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use", "question": {"id": "526469", "title": "Practical examples of NLTK use", "content": "<p>I'm playing around with the <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Natural Language Toolkit</a> (NLTK).</p>\n<p>Its documentation (<a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">Book</a> and <a href=\"http://nltk.googlecode.com/svn/trunk/doc/howto/index.html\" rel=\"noreferrer\">HOWTO</a>) are quite bulky and the examples are sometimes slightly advanced. </p>\n<p>Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the <a href=\"http://streamhacker.wordpress.com/tag/nltk/\" rel=\"noreferrer\">NTLK articles</a> on the <em>Stream Hacker</em> blog. </p>\n", "abstract": "I'm playing around with the Natural Language Toolkit (NLTK). Its documentation (Book and HOWTO) are quite bulky and the examples are sometimes slightly advanced.  Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the NTLK articles on the Stream Hacker blog. "}, "answers": [{"id": 526485, "score": 28, "vote": 0, "content": "<p>Here's my own practical example for the benefit of anyone else looking this question up (excuse the sample text, it was the first thing I found on <a href=\"http://en.wikipedia.org/wiki/Mr_Blobby\" rel=\"noreferrer\">Wikipedia</a>):</p>\n<pre><code class=\"python\">import nltk\nimport pprint\n\ntokenizer = None\ntagger = None\n\ndef init_nltk():\n    global tokenizer\n    global tagger\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n    tagger = nltk.UnigramTagger(nltk.corpus.brown.tagged_sents())\n\ndef tag(text):\n    global tokenizer\n    global tagger\n    if not tokenizer:\n        init_nltk()\n    tokenized = tokenizer.tokenize(text)\n    tagged = tagger.tag(tokenized)\n    tagged.sort(lambda x,y:cmp(x[1],y[1]))\n    return tagged\n\ndef main():\n    text = \"\"\"Mr Blobby is a fictional character who featured on Noel\n    Edmonds' Saturday night entertainment show Noel's House Party,\n    which was often a ratings winner in the 1990s. Mr Blobby also\n    appeared on the Jamie Rose show of 1997. He was designed as an\n    outrageously over the top parody of a one-dimensional, mute novelty\n    character, which ironically made him distinctive, absurd and popular.\n    He was a large pink humanoid, covered with yellow spots, sporting a\n    permanent toothy grin and jiggling eyes. He communicated by saying\n    the word \"blobby\" in an electronically-altered voice, expressing\n    his moods through tone of voice and repetition.\n\n    There was a Mrs. Blobby, seen briefly in the video, and sold as a\n    doll.\n\n    However Mr Blobby actually started out as part of the 'Gotcha'\n    feature during the show's second series (originally called 'Gotcha\n    Oscars' until the threat of legal action from the Academy of Motion\n    Picture Arts and Sciences[citation needed]), in which celebrities\n    were caught out in a Candid Camera style prank. Celebrities such as\n    dancer Wayne Sleep and rugby union player Will Carling would be\n    enticed to take part in a fictitious children's programme based around\n    their profession. Mr Blobby would clumsily take part in the activity,\n    knocking over the set, causing mayhem and saying \"blobby blobby\n    blobby\", until finally when the prank was revealed, the Blobby\n    costume would be opened - revealing Noel inside. This was all the more\n    surprising for the \"victim\" as during rehearsals Blobby would be\n    played by an actor wearing only the arms and legs of the costume and\n    speaking in a normal manner.[citation needed]\"\"\"\n    tagged = tag(text)    \n    l = list(set(tagged))\n    l.sort(lambda x,y:cmp(x[1],y[1]))\n    pprint.pprint(l)\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">[('rugby', None),\n ('Oscars', None),\n ('1990s', None),\n ('\",', None),\n ('Candid', None),\n ('\"', None),\n ('blobby', None),\n ('Edmonds', None),\n ('Mr', None),\n ('outrageously', None),\n ('.[', None),\n ('toothy', None),\n ('Celebrities', None),\n ('Gotcha', None),\n (']),', None),\n ('Jamie', None),\n ('humanoid', None),\n ('Blobby', None),\n ('Carling', None),\n ('enticed', None),\n ('programme', None),\n ('1997', None),\n ('s', None),\n (\"'\", \"'\"),\n ('[', '('),\n ('(', '('),\n (']', ')'),\n (',', ','),\n ('.', '.'),\n ('all', 'ABN'),\n ('the', 'AT'),\n ('an', 'AT'),\n ('a', 'AT'),\n ('be', 'BE'),\n ('were', 'BED'),\n ('was', 'BEDZ'),\n ('is', 'BEZ'),\n ('and', 'CC'),\n ('one', 'CD'),\n ('until', 'CS'),\n ('as', 'CS'),\n ('This', 'DT'),\n ('There', 'EX'),\n ('of', 'IN'),\n ('inside', 'IN'),\n ('from', 'IN'),\n ('around', 'IN'),\n ('with', 'IN'),\n ('through', 'IN'),\n ('-', 'IN'),\n ('on', 'IN'),\n ('in', 'IN'),\n ('by', 'IN'),\n ('during', 'IN'),\n ('over', 'IN'),\n ('for', 'IN'),\n ('distinctive', 'JJ'),\n ('permanent', 'JJ'),\n ('mute', 'JJ'),\n ('popular', 'JJ'),\n ('such', 'JJ'),\n ('fictional', 'JJ'),\n ('yellow', 'JJ'),\n ('pink', 'JJ'),\n ('fictitious', 'JJ'),\n ('normal', 'JJ'),\n ('dimensional', 'JJ'),\n ('legal', 'JJ'),\n ('large', 'JJ'),\n ('surprising', 'JJ'),\n ('absurd', 'JJ'),\n ('Will', 'MD'),\n ('would', 'MD'),\n ('style', 'NN'),\n ('threat', 'NN'),\n ('novelty', 'NN'),\n ('union', 'NN'),\n ('prank', 'NN'),\n ('winner', 'NN'),\n ('parody', 'NN'),\n ('player', 'NN'),\n ('actor', 'NN'),\n ('character', 'NN'),\n ('victim', 'NN'),\n ('costume', 'NN'),\n ('action', 'NN'),\n ('activity', 'NN'),\n ('dancer', 'NN'),\n ('grin', 'NN'),\n ('doll', 'NN'),\n ('top', 'NN'),\n ('mayhem', 'NN'),\n ('citation', 'NN'),\n ('part', 'NN'),\n ('repetition', 'NN'),\n ('manner', 'NN'),\n ('tone', 'NN'),\n ('Picture', 'NN'),\n ('entertainment', 'NN'),\n ('night', 'NN'),\n ('series', 'NN'),\n ('voice', 'NN'),\n ('Mrs', 'NN'),\n ('video', 'NN'),\n ('Motion', 'NN'),\n ('profession', 'NN'),\n ('feature', 'NN'),\n ('word', 'NN'),\n ('Academy', 'NN-TL'),\n ('Camera', 'NN-TL'),\n ('Party', 'NN-TL'),\n ('House', 'NN-TL'),\n ('eyes', 'NNS'),\n ('spots', 'NNS'),\n ('rehearsals', 'NNS'),\n ('ratings', 'NNS'),\n ('arms', 'NNS'),\n ('celebrities', 'NNS'),\n ('children', 'NNS'),\n ('moods', 'NNS'),\n ('legs', 'NNS'),\n ('Sciences', 'NNS-TL'),\n ('Arts', 'NNS-TL'),\n ('Wayne', 'NP'),\n ('Rose', 'NP'),\n ('Noel', 'NP'),\n ('Saturday', 'NR'),\n ('second', 'OD'),\n ('his', 'PP$'),\n ('their', 'PP$'),\n ('him', 'PPO'),\n ('He', 'PPS'),\n ('more', 'QL'),\n ('However', 'RB'),\n ('actually', 'RB'),\n ('also', 'RB'),\n ('clumsily', 'RB'),\n ('originally', 'RB'),\n ('only', 'RB'),\n ('often', 'RB'),\n ('ironically', 'RB'),\n ('briefly', 'RB'),\n ('finally', 'RB'),\n ('electronically', 'RB-HL'),\n ('out', 'RP'),\n ('to', 'TO'),\n ('show', 'VB'),\n ('Sleep', 'VB'),\n ('take', 'VB'),\n ('opened', 'VBD'),\n ('played', 'VBD'),\n ('caught', 'VBD'),\n ('appeared', 'VBD'),\n ('revealed', 'VBD'),\n ('started', 'VBD'),\n ('saying', 'VBG'),\n ('causing', 'VBG'),\n ('expressing', 'VBG'),\n ('knocking', 'VBG'),\n ('wearing', 'VBG'),\n ('speaking', 'VBG'),\n ('sporting', 'VBG'),\n ('revealing', 'VBG'),\n ('jiggling', 'VBG'),\n ('sold', 'VBN'),\n ('called', 'VBN'),\n ('made', 'VBN'),\n ('altered', 'VBN'),\n ('based', 'VBN'),\n ('designed', 'VBN'),\n ('covered', 'VBN'),\n ('communicated', 'VBN'),\n ('needed', 'VBN'),\n ('seen', 'VBN'),\n ('set', 'VBN'),\n ('featured', 'VBN'),\n ('which', 'WDT'),\n ('who', 'WPS'),\n ('when', 'WRB')]\n</code></pre>\n", "abstract": "Here's my own practical example for the benefit of anyone else looking this question up (excuse the sample text, it was the first thing I found on Wikipedia): Output:"}, {"id": 7734737, "score": 18, "vote": 0, "content": "<p>NLP in general is very useful so you might want to broaden your search to general application of text analytics. I used NLTK to aid MOSS 2010 by generating file taxonomy by extracting concept maps. It worked really well. It doesn't take long before files start to cluster in useful ways.</p>\n<p>Often times to understand text analytics you have to think in tangents to the ways you are used to thinking. For example, text analytics is extremely useful to discovery. Most people, though, don't even know what the difference is between search and discovery. If you read up on those subjects you will likely \"discover\" ways in which you might want to put NLTK to work.</p>\n<p>Also, consider your world view of text files without NLTK. You have a bunch of random length strings separated by whitespace and punctuation. Some of the punctuation changes how it is used such as the period (which is also a decimal point and a postfix marker for an abbreviation.) With NLTK you get words and more to the point you get parts of speech. Now you have a handle on the content. Use NLTK to discover the concepts and actions in the document. Use NLTK to get at the \"meaning\" of the document. Meaning in this case refers to the essencial relationships in the document.</p>\n<p>It is a good thing to be curious about NLTK. Text Analytics is set to breakout in a big way in the next few years. Those who understand it will be better suited to take advantage of the new opportunities better.</p>\n", "abstract": "NLP in general is very useful so you might want to broaden your search to general application of text analytics. I used NLTK to aid MOSS 2010 by generating file taxonomy by extracting concept maps. It worked really well. It doesn't take long before files start to cluster in useful ways. Often times to understand text analytics you have to think in tangents to the ways you are used to thinking. For example, text analytics is extremely useful to discovery. Most people, though, don't even know what the difference is between search and discovery. If you read up on those subjects you will likely \"discover\" ways in which you might want to put NLTK to work. Also, consider your world view of text files without NLTK. You have a bunch of random length strings separated by whitespace and punctuation. Some of the punctuation changes how it is used such as the period (which is also a decimal point and a postfix marker for an abbreviation.) With NLTK you get words and more to the point you get parts of speech. Now you have a handle on the content. Use NLTK to discover the concepts and actions in the document. Use NLTK to get at the \"meaning\" of the document. Meaning in this case refers to the essencial relationships in the document. It is a good thing to be curious about NLTK. Text Analytics is set to breakout in a big way in the next few years. Those who understand it will be better suited to take advantage of the new opportunities better."}, {"id": 1249895, "score": 14, "vote": 0, "content": "<p>I'm the author of <a href=\"http://streamhacker.com/\" rel=\"noreferrer\">streamhacker.com</a> (and thanks for the mention, I get a fair amount of click traffic from this particular question). What specifically are you trying to do? NLTK has a lot of tools for doing various things, but is somewhat lacking clear information on what to use the tools for, and how best to use them. It's also oriented towards academic problems, and so it can be heavy going to translate the <a href=\"http://en.wikipedia.org/wiki/Pedagogy\" rel=\"noreferrer\">pedagogical</a> examples to practical solutions.</p>\n", "abstract": "I'm the author of streamhacker.com (and thanks for the mention, I get a fair amount of click traffic from this particular question). What specifically are you trying to do? NLTK has a lot of tools for doing various things, but is somewhat lacking clear information on what to use the tools for, and how best to use them. It's also oriented towards academic problems, and so it can be heavy going to translate the pedagogical examples to practical solutions."}]}, {"link": "https://stackoverflow.com/questions/26899235/python-nltk-syntaxerror-non-ascii-character-xc3-in-file-sentiment-analysis", "question": {"id": "26899235", "title": "Python NLTK: SyntaxError: Non-ASCII character &#39;\\xc3&#39; in file (Sentiment Analysis -NLP)", "content": "<p>I am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NumPy1.9.1 version. </p>\n<p>This is the code :</p>\n<pre><code class=\"python\">__author__ = 'karan'\nimport nltk\nimport re\nimport sys\n\n\n\ndef main():\n    print(\"Start\");\n    # getting the stop words\n    stopWords = open(\"english.txt\",\"r\");\n    stop_word = stopWords.read().split();\n    AllStopWrd = []\n    for wd in stop_word:\n        AllStopWrd.append(wd);\n    print(\"stop words-&gt; \",AllStopWrd);\n\n    # sample and also cleaning it\n    tweet1= 'Love, my new toy\u00ed\u00a0\u00bd\u00ed\u00b8\u00ed\u00a0\u00bd\u00ed\u00b8#iPhone6. Its good https://twitter.com/Sandra_Ortega/status/513807261769424897/photo/1'\n    print(\"old tweet-&gt; \",tweet1)\n    tweet1 = tweet1.lower()\n    tweet1 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet1).split())\n    print(tweet1);\n    tw = tweet1.split()\n    print(tw)\n\n\n    #tokenize\n    sentences = nltk.word_tokenize(tweet1)\n    print(\"tokenized -&gt;\", sentences)\n\n\n    #remove stop words\n    Otweet =[]\n    for w in tw:\n        if w not in AllStopWrd:\n            Otweet.append(w);\n    print(\"sans stop word-&gt; \",Otweet)\n\n\n    # get taggers for neg/pos/inc/dec/inv words\n    taggers ={}\n    negWords = open(\"neg.txt\",\"r\");\n    neg_word = negWords.read().split();\n    print(\"ned words-&gt; \",neg_word)\n    posWords = open(\"pos.txt\",\"r\");\n    pos_word = posWords.read().split();\n    print(\"pos words-&gt; \",pos_word)\n    incrWords = open(\"incr.txt\",\"r\");\n    inc_word = incrWords.read().split();\n    print(\"incr words-&gt; \",inc_word)\n    decrWords = open(\"decr.txt\",\"r\");\n    dec_word = decrWords.read().split();\n    print(\"dec wrds-&gt; \",dec_word)\n    invWords = open(\"inverse.txt\",\"r\");\n    inv_word = invWords.read().split();\n    print(\"inverse words-&gt; \",inv_word)\n    for nw in neg_word:\n        taggers.update({nw:'negative'});\n    for pw in pos_word:\n        taggers.update({pw:'positive'});\n    for iw in inc_word:\n        taggers.update({iw:'inc'});\n    for dw in dec_word:\n        taggers.update({dw:'dec'});\n    for ivw in inv_word:\n        taggers.update({ivw:'inv'});\n    print(\"tagger-&gt; \",taggers)\n    print(taggers.get('little'))\n\n    # get parts of speech\n    posTagger = [nltk.pos_tag(tw)]\n    print(\"posTagger-&gt; \",posTagger)\n\nmain();\n</code></pre>\n<p>This is the error that I am getting when running my code:</p>\n<pre><code class=\"python\">SyntaxError: Non-ASCII character '\\xc3' in file C:/Users/karan/PycharmProjects/mainProject/sentiment.py on line 19, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n</code></pre>\n<p>How do I fix this error?</p>\n<p>I also tried the code using Python 3.4.2 and with NLTK 3.0 and NumPy 1.9.1 but then I get the error:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 80, in &lt;module&gt;\n    main();\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 72, in main\n    posTagger = [nltk.pos_tag(tw)]\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 100, in pos_tag\n    tagger = load(_POS_TAGGER)\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\data.py\", line 779, in load\n    resource_val = pickle.load(opened_resource)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)\n</code></pre>\n", "abstract": "I am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NumPy1.9.1 version.  This is the code : This is the error that I am getting when running my code: How do I fix this error? I also tried the code using Python 3.4.2 and with NLTK 3.0 and NumPy 1.9.1 but then I get the error:"}, "answers": [{"id": 26899264, "score": 197, "vote": 0, "content": "<p>Add the following to the top of your file   <code># coding=utf-8</code></p>\n<p>If you go to the link in the error you can seen the reason why:</p>\n<p><strong>Defining the Encoding</strong></p>\n<p><em>Python will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding=</em></p>\n", "abstract": "Add the following to the top of your file   # coding=utf-8 If you go to the link in the error you can seen the reason why: Defining the Encoding Python will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding="}]}, {"link": "https://stackoverflow.com/questions/9706769/any-tutorials-for-developing-chatbots", "question": {"id": "9706769", "title": "Any tutorials for developing chatbots?", "content": "<p>As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.</p>\n<p>I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).</p>\n<p>So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.</p>\n", "abstract": "As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot. I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later). So, I am looking for a tutorial/ any other information which would certainly help me to get my project done."}, "answers": [{"id": 9707402, "score": 126, "vote": 0, "content": "<p>You can read a nice introduction to various techniques used to design chatbots here: <a href=\"http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.php\" rel=\"noreferrer\">http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.php</a></p>\n<p>Also, here are a few useful links:</p>\n<ul>\n<li><a href=\"http://web.archive.org/web/20120320060043/\" rel=\"noreferrer\">http://web.archive.org/web/20120320060043/</a></li>\n<li><a href=\"http://ai-programming.com/bot_tutorial.htm\" rel=\"noreferrer\">http://ai-programming.com/bot_tutorial.htm</a></li>\n<li><a href=\"http://www.alicebot.org/be.html\" rel=\"noreferrer\">http://www.alicebot.org/be.html</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/List_of_chatterbots\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/List_of_chatterbots</a></li>\n<li><a href=\"http://www.codeproject.com/Articles/36106/Chatbot-Tutorial\" rel=\"noreferrer\">http://www.codeproject.com/Articles/36106/Chatbot-Tutorial</a></li>\n<li><a href=\"http://www.slideshare.net/amyiris/ai-and-python-developing-a-conversational-interface-using-python\" rel=\"noreferrer\">http://www.slideshare.net/amyiris/ai-and-python-developing-a-conversational-interface-using-python</a></li>\n</ul>\n<p>The <a href=\"http://www.nltk.org\" rel=\"noreferrer\">Natural Language Toolkit (python)</a> implements a few chatbots: <a href=\"http://nltk.github.com/api/nltk.chat.html\" rel=\"noreferrer\">http://nltk.github.com/api/nltk.chat.html</a></p>\n<p>Simple pipeline architecture for a spoken dialogue system from the book <a href=\"http://shop.oreilly.com/product/9780596516499.do\" rel=\"noreferrer\">Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit</a> By\u00a0Steven Bird, Ewan Klein, Edward Loper:</p>\n<p><a href=\"https://i.stack.imgur.com/aivDD.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/aivDD.png\"/></a></p>\n", "abstract": "You can read a nice introduction to various techniques used to design chatbots here: http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.php Also, here are a few useful links: The Natural Language Toolkit (python) implements a few chatbots: http://nltk.github.com/api/nltk.chat.html Simple pipeline architecture for a spoken dialogue system from the book Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit By\u00a0Steven Bird, Ewan Klein, Edward Loper: "}, {"id": 9706899, "score": 16, "vote": 0, "content": "<p>The two places I would start with are <a href=\"http://www.radiolab.org/2011/may/31/\" rel=\"noreferrer\">how cleverbot works [part of a podcast]</a> and then go through the <a href=\"http://www.nltk.org/book\" rel=\"noreferrer\">Natural Language Toolkit Book</a> to learn about the algorithms to use. (NLTK uses python, but the book is also a python tutorial)</p>\n", "abstract": "The two places I would start with are how cleverbot works [part of a podcast] and then go through the Natural Language Toolkit Book to learn about the algorithms to use. (NLTK uses python, but the book is also a python tutorial)"}]}, {"link": "https://stackoverflow.com/questions/6115677/english-grammar-for-parsing-in-nltk", "question": {"id": "6115677", "title": "English grammar for parsing in NLTK", "content": "<p>Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. </p>\n<p>Thanks a lot!</p>\n", "abstract": "Is there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence.  Thanks a lot!"}, "answers": [{"id": 17935542, "score": 33, "vote": 0, "content": "<p>You can take a look at <a href=\"https://github.com/emilmont/pyStatParser\">pyStatParser</a>, a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from stat_parser import Parser\n&gt;&gt;&gt; parser = Parser()\n&gt;&gt;&gt; print parser.parse(\"How can the net amount of entropy of the universe be massively decreased?\")\n(SBARQ\n  (WHADVP (WRB how))\n  (SQ\n    (MD can)\n    (NP\n      (NP (DT the) (JJ net) (NN amount))\n      (PP\n        (IN of)\n        (NP\n          (NP (NNS entropy))\n          (PP (IN of) (NP (DT the) (NN universe))))))\n    (VP (VB be) (ADJP (RB massively) (VBN decreased))))\n  (. ?))\n</code></pre>\n", "abstract": "You can take a look at pyStatParser, a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second."}, {"id": 32466634, "score": 26, "vote": 0, "content": "<p>My library, <a href=\"http://spacy.io\">spaCy</a>, provides a high performance dependency parser.</p>\n<p>Installation:</p>\n<pre><code class=\"python\">pip install spacy\npython -m spacy.en.download all\n</code></pre>\n<p>Usage:</p>\n<pre><code class=\"python\">from spacy.en import English\nnlp = English()\ndoc = nlp(u'A whole document.\\nNo preprocessing require.   Robust to arbitrary formating.')\nfor sent in doc:\n    for token in sent:\n        if token.is_alpha:\n            print token.orth_, token.tag_, token.head.lemma_\n</code></pre>\n<p><a href=\"http://aclweb.org/anthology/P/P15/P15-1038.pdf\">Choi et al. (2015)</a> found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models.</p>\n", "abstract": "My library, spaCy, provides a high performance dependency parser. Installation: Usage: Choi et al. (2015) found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models."}, {"id": 6115756, "score": 6, "vote": 0, "content": "<p>There are a few grammars in the <a href=\"http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml\" rel=\"noreferrer\"><code>nltk_data</code></a> distribution. In your Python interpreter, issue <code>nltk.download()</code>.</p>\n", "abstract": "There are a few grammars in the nltk_data distribution. In your Python interpreter, issue nltk.download()."}, {"id": 24964944, "score": 6, "vote": 0, "content": "<p>There is a Library called <a href=\"http://www.clips.ua.ac.be/pattern\">Pattern</a>. It is quite fast and easy to use.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from pattern.en import parse\n&gt;&gt;&gt;  \n&gt;&gt;&gt; s = 'The mobile web is more important than mobile apps.'\n&gt;&gt;&gt; s = parse(s, relations=True, lemmata=True)\n&gt;&gt;&gt; print s\n\n'The/DT/B-NP/O/NP-SBJ-1/the mobile/JJ/I-NP/O/NP-SBJ-1/mobile' ... \n</code></pre>\n", "abstract": "There is a Library called Pattern. It is quite fast and easy to use."}, {"id": 11859070, "score": 5, "vote": 0, "content": "<p>Use the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser.</p>\n<p>Just download the MaltParser from <a href=\"http://www.maltparser.org/index.html\" rel=\"noreferrer\">http://www.maltparser.org/index.html</a> and use the NLTK like this:</p>\n<pre><code class=\"python\">import nltk\nparser = nltk.parse.malt.MaltParser()\n</code></pre>\n", "abstract": "Use the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser. Just download the MaltParser from http://www.maltparser.org/index.html and use the NLTK like this:"}, {"id": 26854669, "score": 4, "vote": 0, "content": "<p>I've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (<a href=\"http://www.clips.ua.ac.be/pages/pattern-en\" rel=\"nofollow\">http://www.clips.ua.ac.be/pages/pattern-en</a>). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  </p>\n", "abstract": "I've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (http://www.clips.ua.ac.be/pages/pattern-en). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  "}, {"id": 46917366, "score": 4, "vote": 0, "content": "<p>Did you try POS tagging in NLTK?</p>\n<pre><code class=\"python\">text = word_tokenize(\"And now for something completely different\")\nnltk.pos_tag(text)\n</code></pre>\n<p>The answer is something like this</p>\n<pre><code class=\"python\">[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),('completely', 'RB'), ('different', 'JJ')]\n</code></pre>\n<p>Got this example from here <a href=\"http://www.nltk.org/book/ch05.html\" rel=\"nofollow noreferrer\">NLTK_chapter03</a></p>\n", "abstract": "Did you try POS tagging in NLTK? The answer is something like this Got this example from here NLTK_chapter03"}, {"id": 60269058, "score": 1, "vote": 0, "content": "<p>I'm found out that nltk working good with parser grammar developed by Stanford.</p>\n<p><a href=\"https://bbengfort.github.io/snippets/2018/06/22/corenlp-nltk-parses.html\" rel=\"nofollow noreferrer\">Syntax Parsing with Stanford CoreNLP and NLTK</a></p>\n<p>It is very easy to start to use Stanford CoreNLP and NLTK. All you need is small preparation, after that you can parse sentences with following code:</p>\n<pre><code class=\"python\">from nltk.parse.corenlp import CoreNLPParser\nparser = CoreNLPParser()\nparse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n</code></pre>\n<p>Preparation:</p>\n<ol>\n<li>Download <a href=\"https://stanfordnlp.github.io/CoreNLP/index.html#download\" rel=\"nofollow noreferrer\">Java Stanford model</a></li>\n<li>Run CoreNLPServer</li>\n</ol>\n<p>You can use following code to run CoreNLPServer:</p>\n<pre><code class=\"python\">import os\nfrom nltk.parse.corenlp import CoreNLPServer\n# The server needs to know the location of the following files:\n#   - stanford-corenlp-X.X.X.jar\n#   - stanford-corenlp-X.X.X-models.jar\nSTANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-02-27\")\n# Create the server\nserver = CoreNLPServer(\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1.jar\"),\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1-models.jar\"),    \n)\n# Start the server in the background\nserver.start()\n</code></pre>\n<blockquote>\n<p>Do not forget stop server with executing server.stop() </p>\n</blockquote>\n", "abstract": "I'm found out that nltk working good with parser grammar developed by Stanford. Syntax Parsing with Stanford CoreNLP and NLTK It is very easy to start to use Stanford CoreNLP and NLTK. All you need is small preparation, after that you can parse sentences with following code: Preparation: You can use following code to run CoreNLPServer: Do not forget stop server with executing server.stop() "}]}, {"link": "https://stackoverflow.com/questions/36952763/how-to-return-history-of-validation-loss-in-keras", "question": {"id": "36952763", "title": "How to return history of validation loss in Keras", "content": "<p>Using Anaconda Python 2.7 Windows 10.</p>\n<p>I am training a language model using the Keras exmaple:</p>\n<pre><code class=\"python\">print('Build model...')\nmodel = Sequential()\nmodel.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(512, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\ndef sample(a, temperature=1.0):\n    # helper function to sample an index from a probability array\n    a = np.log(a) / temperature\n    a = np.exp(a) / np.sum(np.exp(a))\n    return np.argmax(np.random.multinomial(1, a, 1))\n\n\n# train the model, output generated text after each iteration\nfor iteration in range(1, 3):\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    model.fit(X, y, batch_size=128, nb_epoch=1)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print()\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n</code></pre>\n<p>According to Keras documentation, the <code>model.fit</code> method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.</p>\n<pre><code class=\"python\">hist = model.fit(X, y, validation_split=0.2)\nprint(hist.history)\n</code></pre>\n<p>After training my model, if I run <code>print(model.history)</code> I get the error:</p>\n<pre><code class=\"python\"> AttributeError: 'Sequential' object has no attribute 'history'\n</code></pre>\n<p>How do I return my model history after training my model with the above code?</p>\n<p><strong>UPDATE</strong></p>\n<p>The issue was that:</p>\n<p>The following had to first be defined:</p>\n<pre><code class=\"python\">from keras.callbacks import History \nhistory = History()\n</code></pre>\n<p>The callbacks option had to be called</p>\n<pre><code class=\"python\">model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])\n</code></pre>\n<p>But now if I print</p>\n<pre><code class=\"python\">print(history.History)\n</code></pre>\n<p>it returns</p>\n<pre><code class=\"python\">{}\n</code></pre>\n<p>even though I ran an iteration. </p>\n", "abstract": "Using Anaconda Python 2.7 Windows 10. I am training a language model using the Keras exmaple: According to Keras documentation, the model.fit method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics. After training my model, if I run print(model.history) I get the error: How do I return my model history after training my model with the above code? UPDATE The issue was that: The following had to first be defined: The callbacks option had to be called But now if I print it returns even though I ran an iteration. "}, "answers": [{"id": 46421980, "score": 51, "vote": 0, "content": "<p>Just an example started from</p>\n<pre><code class=\"python\">history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0)\n</code></pre>\n<p>You can use</p>\n<pre><code class=\"python\">print(history.history.keys())\n</code></pre>\n<p>to list all data in history.</p>\n<p>Then, you can print the history of validation loss like this: </p>\n<pre><code class=\"python\">print(history.history['val_loss'])\n</code></pre>\n", "abstract": "Just an example started from You can use to list all data in history. Then, you can print the history of validation loss like this: "}, {"id": 36956440, "score": 45, "vote": 0, "content": "<p>It's been solved.</p>\n<p>The losses only save to the History over the epochs. I was running iterations instead of using the Keras built in epochs option.</p>\n<p>so instead of doing 4 iterations I now have</p>\n<pre><code class=\"python\">model.fit(......, nb_epoch = 4)\n</code></pre>\n<p>Now it returns the loss for each epoch run:</p>\n<pre><code class=\"python\">print(hist.history)\n{'loss': [1.4358016599558268, 1.399221191623641, 1.381293383180471, 1.3758836857303727]}\n</code></pre>\n", "abstract": "It's been solved. The losses only save to the History over the epochs. I was running iterations instead of using the Keras built in epochs option. so instead of doing 4 iterations I now have Now it returns the loss for each epoch run:"}, {"id": 50137577, "score": 12, "vote": 0, "content": "<p>The following simple code works great for me:</p>\n<pre><code class=\"python\">    seqModel =model.fit(x_train, y_train,\n          batch_size      = batch_size,\n          epochs          = num_epochs,\n          validation_data = (x_test, y_test),\n          shuffle         = True,\n          verbose=0, callbacks=[TQDMNotebookCallback()]) #for visualization\n</code></pre>\n<p>Make sure you assign the fit function to an output variable. Then you can access that variable very easily</p>\n<pre><code class=\"python\"># visualizing losses and accuracy\ntrain_loss = seqModel.history['loss']\nval_loss   = seqModel.history['val_loss']\ntrain_acc  = seqModel.history['acc']\nval_acc    = seqModel.history['val_acc']\nxc         = range(num_epochs)\n\nplt.figure()\nplt.plot(xc, train_loss)\nplt.plot(xc, val_loss)\n</code></pre>\n<p>Hope this helps.\nsource: <a href=\"https://keras.io/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch\" rel=\"noreferrer\">https://keras.io/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch</a></p>\n", "abstract": "The following simple code works great for me: Make sure you assign the fit function to an output variable. Then you can access that variable very easily Hope this helps.\nsource: https://keras.io/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch"}, {"id": 36953175, "score": 7, "vote": 0, "content": "<p>The dictionary with histories of \"acc\", \"loss\", etc. is available and saved in <code>hist.history</code> variable.</p>\n", "abstract": "The dictionary with histories of \"acc\", \"loss\", etc. is available and saved in hist.history variable."}, {"id": 49333520, "score": 5, "vote": 0, "content": "<p>I have also found that you can use <code>verbose=2</code> to make keras print out the Losses:</p>\n<pre><code class=\"python\">history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=2)\n</code></pre>\n<p>And that would print nice lines like this:</p>\n<pre><code class=\"python\">Epoch 1/1\n - 5s - loss: 0.6046 - acc: 0.9999 - val_loss: 0.4403 - val_acc: 0.9999\n</code></pre>\n<p>According to their <a href=\"https://keras.io/models/sequential/\" rel=\"noreferrer\">documentation</a>:</p>\n<pre><code class=\"python\">verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n</code></pre>\n", "abstract": "I have also found that you can use verbose=2 to make keras print out the Losses: And that would print nice lines like this: According to their documentation:"}, {"id": 56902717, "score": 4, "vote": 0, "content": "<p>For plotting the loss directly the following works:</p>\n<pre><code class=\"python\">import matplotlib.pyplot as plt\n...    \nmodel_ = model.fit(X, Y, epochs= ..., verbose=1 )\nplt.plot(list(model_.history.values())[0],'k-o')\n</code></pre>\n", "abstract": "For plotting the loss directly the following works:"}, {"id": 47965709, "score": 2, "vote": 0, "content": "<p>Another option is CSVLogger: <a href=\"https://keras.io/callbacks/#csvlogger\" rel=\"nofollow noreferrer\">https://keras.io/callbacks/#csvlogger</a>.\nIt creates a csv file appending the result of each epoch. Even if you interrupt training, you get to see how it evolved.</p>\n", "abstract": "Another option is CSVLogger: https://keras.io/callbacks/#csvlogger.\nIt creates a csv file appending the result of each epoch. Even if you interrupt training, you get to see how it evolved."}, {"id": 50383658, "score": 2, "vote": 0, "content": "<p>Actually, you can also do it with the iteration method. Because sometimes we might need to use the iteration method instead of the built-in epochs method to visualize the training results after each iteration.</p>\n<pre><code class=\"python\">history = [] #Creating a empty list for holding the loss later\nfor iteration in range(1, 3):\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    result = model.fit(X, y, batch_size=128, nb_epoch=1) #Obtaining the loss after each training\n    history.append(result.history['loss']) #Now append the loss after the training to the list.\n    start_index = random.randint(0, len(text) - maxlen - 1)\nprint(history)\n</code></pre>\n<p>This way allows you to get the loss you want while maintaining your iteration method.</p>\n", "abstract": "Actually, you can also do it with the iteration method. Because sometimes we might need to use the iteration method instead of the built-in epochs method to visualize the training results after each iteration. This way allows you to get the loss you want while maintaining your iteration method."}, {"id": 64400949, "score": 2, "vote": 0, "content": "<p>Thanks to Alloush,</p>\n<p>Following parameter must be included in <code>model.fit()</code>:</p>\n<pre><code class=\"python\">validation_data = (x_test, y_test)\n</code></pre>\n<p>If it is not defined, <code>val_acc</code> and <code>val_loss</code> will not\nbe exist at output.</p>\n", "abstract": "Thanks to Alloush, Following parameter must be included in model.fit(): If it is not defined, val_acc and val_loss will not\nbe exist at output."}, {"id": 61203462, "score": 1, "vote": 0, "content": "<p>Those who got still error like me:</p>\n<p>Convert <code>model.fit_generator()</code> to <code>model.fit()</code></p>\n", "abstract": "Those who got still error like me: Convert model.fit_generator() to model.fit()"}, {"id": 71343782, "score": 0, "vote": 0, "content": "<p>you can get loss and metrics like below:\nreturned history object is dictionary and you can access model loss( val_loss) or accuracy(val_accuracy) like below:</p>\n<pre><code class=\"python\">model_hist=model.fit(train_data,train_lbl,epochs=my_epoch,batch_size=sel_batch_size,validation_data=val_data)\n\nacc=model_hist.history['accuracy']\n\nval_acc=model_hist.history['val_accuracy']\n\nloss=model_hist.history['loss']\n\nval_loss=model_hist.history['val_loss']\n</code></pre>\n<p>dont forget that for getting val_loss or val_accuracy you should specify validation data in the \"fit\" function.</p>\n", "abstract": "you can get loss and metrics like below:\nreturned history object is dictionary and you can access model loss( val_loss) or accuracy(val_accuracy) like below: dont forget that for getting val_loss or val_accuracy you should specify validation data in the \"fit\" function."}, {"id": 73694405, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">history = model.fit(partial_train_data, partial_train_targets,\nvalidation_data=(val_data, val_targets),\nepochs=num_epochs, batch_size=1, verbose=0)\nmae_history = history.history['val_mean_absolute_error']\n\nI had the same problem. The following code worked  for me.\n\nmae_history = history.history['val_mae']\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/40288323/what-do-spacys-part-of-speech-and-dependency-tags-mean", "question": {"id": "40288323", "title": "What do spaCy&#39;s part-of-speech and dependency tags mean?", "content": "<p>spaCy tags up each of the <code>Token</code>s in a <code>Document</code> with a part of speech (in two different formats, one stored in the <code>pos</code> and <code>pos_</code> properties of the <code>Token</code> and the other stored in the <code>tag</code> and <code>tag_</code> properties) and a syntactic dependency to its <code>.head</code> token (stored in the <code>dep</code> and <code>dep_</code> properties).</p>\n<p>Some of these tags are self-explanatory, even to somebody like me without a linguistics background:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; en_nlp = spacy.load('en')\n&gt;&gt;&gt; document = en_nlp(\"I shot a man in Reno just to watch him die.\")\n&gt;&gt;&gt; document[1]\nshot\n&gt;&gt;&gt; document[1].pos_\n'VERB'\n</code></pre>\n<p>Others... are not:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; document[1].tag_\n'VBD'\n&gt;&gt;&gt; document[2].pos_\n'DET'\n&gt;&gt;&gt; document[3].dep_\n'dobj'\n</code></pre>\n<p>Worse, the <a href=\"https://spacy.io/docs/\" rel=\"noreferrer\">official docs</a> don't contain even a list of the possible tags for most of these properties, nor the meanings of any of them. They sometimes mention what tokenization standard they use, but these claims aren't currently entirely accurate and on top of that the standards are tricky to track down.</p>\n<p>What are the possible values of the <code>tag_</code>, <code>pos_</code>, and <code>dep_</code> properties, and what do they mean?</p>\n", "abstract": "spaCy tags up each of the Tokens in a Document with a part of speech (in two different formats, one stored in the pos and pos_ properties of the Token and the other stored in the tag and tag_ properties) and a syntactic dependency to its .head token (stored in the dep and dep_ properties). Some of these tags are self-explanatory, even to somebody like me without a linguistics background: Others... are not: Worse, the official docs don't contain even a list of the possible tags for most of these properties, nor the meanings of any of them. They sometimes mention what tokenization standard they use, but these claims aren't currently entirely accurate and on top of that the standards are tricky to track down. What are the possible values of the tag_, pos_, and dep_ properties, and what do they mean?"}, "answers": [{"id": 40288324, "score": 116, "vote": 0, "content": "<h1>tl;dr answer</h1>\n<p>Just expand the lists at:</p>\n<ul>\n<li><a href=\"https://spacy.io/api/annotation#pos-tagging\" rel=\"noreferrer\">https://spacy.io/api/annotation#pos-tagging</a> (POS tags) and</li>\n<li><a href=\"https://spacy.io/api/annotation#dependency-parsing\" rel=\"noreferrer\">https://spacy.io/api/annotation#dependency-parsing</a> (dependency tags)</li>\n</ul>\n<h1>Longer answer</h1>\n<p>The docs have greatly improved since I first asked this question, and spaCy now documents this much better.</p>\n<h2>Part-of-speech tags</h2>\n<p>The <code>pos</code> and <code>tag</code> attributes are tabulated at <a href=\"https://spacy.io/api/annotation#pos-tagging\" rel=\"noreferrer\">https://spacy.io/api/annotation#pos-tagging</a>, and the origin of those lists of values is described. At the time of this (January 2020) edit, the docs say of the <code>pos</code> attribute that:</p>\n<blockquote>\n<p>spaCy maps all language-specific part-of-speech tags to a small, fixed set of word type tags following the <a href=\"http://universaldependencies.org/u/pos/\" rel=\"noreferrer\">Universal Dependencies scheme</a>. The universal tags don\u2019t code for any morphological features and only cover the word type. They\u2019re available as the <a href=\"https://spacy.io/api/token#attributes\" rel=\"noreferrer\"><code>Token.pos</code></a> and <a href=\"https://spacy.io/api/token#attributes\" rel=\"noreferrer\"><code>Token.pos_</code></a> attributes.</p>\n</blockquote>\n<p>As for the <code>tag</code> attribute, the docs say:</p>\n<blockquote>\n<p>The English part-of-speech tagger uses the <a href=\"https://catalog.ldc.upenn.edu/LDC2013T19\" rel=\"noreferrer\">OntoNotes 5</a> version of the Penn Treebank tag set. We also map the tags to the simpler Universal Dependencies v2 POS tag set.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>The German part-of-speech tagger uses the <a href=\"http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/index.html\" rel=\"noreferrer\">TIGER Treebank</a> annotation scheme. We also map the tags to the simpler Universal Dependencies v2 POS tag set.</p>\n</blockquote>\n<p>You thus have a choice between using a coarse-grained tag set that is consistent across languages (<code>.pos</code>), or a fine-grained tag set (<code>.tag</code>) that is specific to a particular treebank, and hence a particular language.</p>\n<h3><code>.pos_</code> tag list</h3>\n<p>The docs list the following coarse-grained tags used for the <code>pos</code> and <code>pos_</code> attributes:</p>\n<ul>\n<li><code>ADJ</code>: adjective, e.g. big, old, green, incomprehensible, first</li>\n<li><code>ADP</code>: adposition, e.g. in, to, during</li>\n<li><code>ADV</code>: adverb, e.g. very, tomorrow, down, where, there</li>\n<li><code>AUX</code>: auxiliary, e.g. is, has (done), will (do), should (do)</li>\n<li><code>CONJ</code>: conjunction, e.g. and, or, but</li>\n<li><code>CCONJ</code>: coordinating conjunction, e.g. and, or, but</li>\n<li><code>DET</code>: determiner, e.g. a, an, the</li>\n<li><code>INTJ</code>: interjection, e.g. psst, ouch, bravo, hello</li>\n<li><code>NOUN</code>: noun, e.g. girl, cat, tree, air, beauty</li>\n<li><code>NUM</code>: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV</li>\n<li><code>PART</code>: particle, e.g. \u2019s, not,</li>\n<li><code>PRON</code>: pronoun, e.g I, you, he, she, myself, themselves, somebody</li>\n<li><code>PROPN</code>: proper noun, e.g. Mary, John, London, NATO, HBO</li>\n<li><code>PUNCT</code>: punctuation, e.g. ., (, ), ?</li>\n<li><code>SCONJ</code>: subordinating conjunction, e.g. if, while, that</li>\n<li><code>SYM</code>: symbol, e.g. $, %, \u00a7, \u00a9, +, \u2212, \u00d7, \u00f7, =, :), \ud83d\ude1d</li>\n<li><code>VERB</code>: verb, e.g. run, runs, running, eat, ate, eating</li>\n<li><code>X</code>: other, e.g. sfpksdpsxmsa</li>\n<li><code>SPACE</code>: space, e.g.    </li>\n</ul>\n<p>Note that the docs are lying slightly when they say that this list follows the Universal Dependencies Scheme; there are two tags listed above that aren't part of that scheme.</p>\n<p>One of those is <code>CONJ</code>, which used to exist in the Universal POS Tags scheme but has been split into <code>CCONJ</code> and <code>SCONJ</code> since spaCy was first written. Based on the mappings of tag-&gt;pos in the docs, it would seem that spaCy's current models don't actually use <code>CONJ</code>, but it still exists in spaCy's code and docs for some reason - perhaps backwards compatibility with old models.</p>\n<p>The second is <code>SPACE</code>, which isn't part of the Universal POS Tags scheme (and never has been, as far as I know) and is used by spaCy for any spacing besides single normal ASCII spaces (which don't get their own token):</p>\n<pre><code class=\"python\">&gt;&gt;&gt; document = en_nlp(\"This\\nsentence\\thas      some weird spaces in\\n\\n\\n\\n\\t\\t   it.\")\n&gt;&gt;&gt; for token in document:\n...   print('%r (%s)' % (str(token), token.pos_))\n... \n'This' (DET)\n'\\n' (SPACE)\n'sentence' (NOUN)\n'\\t' (SPACE)\n'has' (VERB)\n'     ' (SPACE)\n'some' (DET)\n'weird' (ADJ)\n'spaces' (NOUN)\n'in' (ADP)\n'\\n\\n\\n\\n\\t\\t   ' (SPACE)\n'it' (PRON)\n'.' (PUNCT)\n</code></pre>\n<p>I'll omit the full list of <code>.tag_</code> tags (the finer-grained ones) from this answer, since they're numerous, well-documented now, different for English and German, and probably more likely to change between releases. Instead, look at the list in the docs (e.g. <a href=\"https://spacy.io/api/annotation#pos-en\" rel=\"noreferrer\">https://spacy.io/api/annotation#pos-en</a> for English) which lists every possible tag, the <code>.pos_</code> value it maps to, and a description of what it means.</p>\n<h3>Dependency tokens</h3>\n<p>There are now <em>three</em> different schemes that spaCy uses for dependency tagging: <a href=\"https://spacy.io/api/annotation#dependency-parsing-english\" rel=\"noreferrer\">one for English</a>, <a href=\"https://spacy.io/api/annotation#dependency-parsing-german\" rel=\"noreferrer\">one for German</a>, and <a href=\"https://spacy.io/api/annotation#dependency-parsing-universal\" rel=\"noreferrer\">one for everything else</a>. Once again, the list of values is huge and I won't reproduce it in full here. Every dependency has a brief definition next to it, but unfortunately, many of them - like \"appositional modifier\" or \"clausal complement\" - are terms of art that are rather alien to an everyday programmer like me. If you're not a linguist, you'll simply have to research the meanings of those terms of art to make sense of them.</p>\n<p>I can at least provide a starting point for that research for people working with English text, though. If you'd like to see some <em>examples</em> of the CLEAR dependencies (used by the English model) in real sentences, check out the 2012 work of Jinho D. Choi: either his <a href=\"https://scholar.colorado.edu/cgi/viewcontent.cgi?referer=https://www.google.co.uk/&amp;httpsredir=1&amp;article=1046&amp;context=csci_gradetds\" rel=\"noreferrer\"><em>Optimization of Natural Language Processing Components for Robustness and Scalability</em></a> or his <a href=\"https://web.archive.org/web/20170809024928/http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf\" rel=\"noreferrer\"><em>Guidelines for the CLEAR Style\nConstituent to Dependency Conversion</em></a> (which seems to just be a subsection of the former paper). Both list all the CLEAR dependency labels that existed in 2012 along with definitions and example sentences. (Unfortunately, the set of CLEAR dependency labels has changed a little since 2012, so some of the modern labels are not listed or exemplified in Choi's work - but it remains a useful resource despite being slightly outdated.)</p>\n", "abstract": "Just expand the lists at: The docs have greatly improved since I first asked this question, and spaCy now documents this much better. The pos and tag attributes are tabulated at https://spacy.io/api/annotation#pos-tagging, and the origin of those lists of values is described. At the time of this (January 2020) edit, the docs say of the pos attribute that: spaCy maps all language-specific part-of-speech tags to a small, fixed set of word type tags following the Universal Dependencies scheme. The universal tags don\u2019t code for any morphological features and only cover the word type. They\u2019re available as the Token.pos and Token.pos_ attributes. As for the tag attribute, the docs say: The English part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. We also map the tags to the simpler Universal Dependencies v2 POS tag set. and The German part-of-speech tagger uses the TIGER Treebank annotation scheme. We also map the tags to the simpler Universal Dependencies v2 POS tag set. You thus have a choice between using a coarse-grained tag set that is consistent across languages (.pos), or a fine-grained tag set (.tag) that is specific to a particular treebank, and hence a particular language. The docs list the following coarse-grained tags used for the pos and pos_ attributes: Note that the docs are lying slightly when they say that this list follows the Universal Dependencies Scheme; there are two tags listed above that aren't part of that scheme. One of those is CONJ, which used to exist in the Universal POS Tags scheme but has been split into CCONJ and SCONJ since spaCy was first written. Based on the mappings of tag->pos in the docs, it would seem that spaCy's current models don't actually use CONJ, but it still exists in spaCy's code and docs for some reason - perhaps backwards compatibility with old models. The second is SPACE, which isn't part of the Universal POS Tags scheme (and never has been, as far as I know) and is used by spaCy for any spacing besides single normal ASCII spaces (which don't get their own token): I'll omit the full list of .tag_ tags (the finer-grained ones) from this answer, since they're numerous, well-documented now, different for English and German, and probably more likely to change between releases. Instead, look at the list in the docs (e.g. https://spacy.io/api/annotation#pos-en for English) which lists every possible tag, the .pos_ value it maps to, and a description of what it means. There are now three different schemes that spaCy uses for dependency tagging: one for English, one for German, and one for everything else. Once again, the list of values is huge and I won't reproduce it in full here. Every dependency has a brief definition next to it, but unfortunately, many of them - like \"appositional modifier\" or \"clausal complement\" - are terms of art that are rather alien to an everyday programmer like me. If you're not a linguist, you'll simply have to research the meanings of those terms of art to make sense of them. I can at least provide a starting point for that research for people working with English text, though. If you'd like to see some examples of the CLEAR dependencies (used by the English model) in real sentences, check out the 2012 work of Jinho D. Choi: either his Optimization of Natural Language Processing Components for Robustness and Scalability or his Guidelines for the CLEAR Style\nConstituent to Dependency Conversion (which seems to just be a subsection of the former paper). Both list all the CLEAR dependency labels that existed in 2012 along with definitions and example sentences. (Unfortunately, the set of CLEAR dependency labels has changed a little since 2012, so some of the modern labels are not listed or exemplified in Choi's work - but it remains a useful resource despite being slightly outdated.)"}, {"id": 48143824, "score": 48, "vote": 0, "content": "<p>Just a quick tip about getting the detail meaning of the short forms. You can use <code>explain</code> method like following:</p>\n<pre><code class=\"python\">spacy.explain('pobj')\n</code></pre>\n<p>which will give you output like:</p>\n<pre><code class=\"python\">'object of preposition'\n</code></pre>\n", "abstract": "Just a quick tip about getting the detail meaning of the short forms. You can use explain method like following: which will give you output like:"}, {"id": 52057531, "score": 9, "vote": 0, "content": "<p>The official documentation now provides much more details for all those annotations at <a href=\"https://spacy.io/api/annotation\" rel=\"noreferrer\">https://spacy.io/api/annotation</a> (and the list of other attributes for tokens can be found at <a href=\"https://spacy.io/api/token\" rel=\"noreferrer\">https://spacy.io/api/token</a>).</p>\n<p>As the documentation shows, their parts-of-speech (POS) and dependency tags have both Universal and specific variations for different languages and the <code>explain()</code> function is a very useful shortcut to get a better description of a tag's meaning without the documentation, e.g.</p>\n<pre><code class=\"python\">spacy.explain(\"VBD\")\n</code></pre>\n<p>which gives \"verb, past tense\".</p>\n", "abstract": "The official documentation now provides much more details for all those annotations at https://spacy.io/api/annotation (and the list of other attributes for tokens can be found at https://spacy.io/api/token). As the documentation shows, their parts-of-speech (POS) and dependency tags have both Universal and specific variations for different languages and the explain() function is a very useful shortcut to get a better description of a tag's meaning without the documentation, e.g. which gives \"verb, past tense\"."}, {"id": 66369254, "score": 7, "vote": 0, "content": "<p>Direct links (if you don't feel like going through endless spacy documentation to get the full tables):</p>\n<ul>\n<li><p>for .pos_ (parts of speech, English): <a href=\"https://universaldependencies.org/docs/en/pos/\" rel=\"noreferrer\">https://universaldependencies.org/docs/en/pos/</a></p>\n</li>\n<li><p>for .dep_ (dependency relations, English): <a href=\"https://universaldependencies.org/docs/en/dep/\" rel=\"noreferrer\">https://universaldependencies.org/docs/en/dep/</a></p>\n</li>\n</ul>\n", "abstract": "Direct links (if you don't feel like going through endless spacy documentation to get the full tables): for .pos_ (parts of speech, English): https://universaldependencies.org/docs/en/pos/ for .dep_ (dependency relations, English): https://universaldependencies.org/docs/en/dep/"}, {"id": 68365822, "score": 5, "vote": 0, "content": "<p>After the recent update of Spacy to v3. The above links do not work. You may visit these links to get the complete list\n<a href=\"https://v2.spacy.io/api/annotation\" rel=\"noreferrer\">https://v2.spacy.io/api/annotation</a></p>\n<p><strong>Universal POS Tags</strong>\n<a href=\"https://i.stack.imgur.com/r57Dr.jpg\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/r57Dr.jpg\"/></a></p>\n<p><strong>English POS Tags</strong>\n<a href=\"https://i.stack.imgur.com/xCRih.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/xCRih.png\"/></a></p>\n", "abstract": "After the recent update of Spacy to v3. The above links do not work. You may visit these links to get the complete list\nhttps://v2.spacy.io/api/annotation Universal POS Tags\n English POS Tags\n"}, {"id": 47700964, "score": 2, "vote": 0, "content": "<p>At present, dependency parsing and tagging in SpaCy appears to be implemented only at the word level, and not at the phrase (other than noun phrase) or clause level. This means SpaCy can be used to identify things like nouns (NN, NNS), adjectives (JJ, JJR, JJS), and verbs (VB, VBD, VBG, etc.), but not adjective phrases (ADJP), adverbial phrases (ADVP), or questions (SBARQ, SQ). </p>\n<p>For illustration, when you use SpaCy to parse the sentence \"Which way is the bus going?\", we get <a href=\"https://i.stack.imgur.com/uRGma.png\" rel=\"nofollow noreferrer\">the following tree.</a></p>\n<p>By contrast, if you use the Stanford parser you get <a href=\"https://i.stack.imgur.com/tPTox.png\" rel=\"nofollow noreferrer\">a much more deeply structured syntax tree.</a></p>\n", "abstract": "At present, dependency parsing and tagging in SpaCy appears to be implemented only at the word level, and not at the phrase (other than noun phrase) or clause level. This means SpaCy can be used to identify things like nouns (NN, NNS), adjectives (JJ, JJR, JJS), and verbs (VB, VBD, VBG, etc.), but not adjective phrases (ADJP), adverbial phrases (ADVP), or questions (SBARQ, SQ).  For illustration, when you use SpaCy to parse the sentence \"Which way is the bus going?\", we get the following tree. By contrast, if you use the Stanford parser you get a much more deeply structured syntax tree."}]}, {"link": "https://stackoverflow.com/questions/15057945/how-do-i-tokenize-a-string-sentence-in-nltk", "question": {"id": "15057945", "title": "How do I tokenize a string sentence in NLTK?", "content": "<p>I am using nltk, so I want to create my own custom texts just like the default ones on nltk.books. However, I've just got up to the method like</p>\n<pre><code class=\"python\">my_text = ['This', 'is', 'my', 'text']\n</code></pre>\n<p>I'd like to discover any way to input my \"text\" as:</p>\n<pre><code class=\"python\">my_text = \"This is my text, this is a nice way to input text.\"\n</code></pre>\n<p>Which method, python's or from nltk allows me to do this. And more important, how can I dismiss punctuation symbols?</p>\n", "abstract": "I am using nltk, so I want to create my own custom texts just like the default ones on nltk.books. However, I've just got up to the method like I'd like to discover any way to input my \"text\" as: Which method, python's or from nltk allows me to do this. And more important, how can I dismiss punctuation symbols?"}, "answers": [{"id": 15057966, "score": 169, "vote": 0, "content": "<p>This is actually on the <a href=\"http://nltk.org/\">main page of nltk.org</a>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; sentence = \"\"\"At eight o'clock on Thursday morning\n... Arthur didn't feel very good.\"\"\"\n&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)\n&gt;&gt;&gt; tokens\n['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n</code></pre>\n", "abstract": "This is actually on the main page of nltk.org:"}, {"id": 15152945, "score": -7, "vote": 0, "content": "<p>As @PavelAnossov answered, the canonical answer, use the <code>word_tokenize</code> function in nltk:</p>\n<pre><code class=\"python\">from nltk import word_tokenize\nsent = \"This is my text, this is a nice way to input text.\"\nword_tokenize(sent)\n</code></pre>\n<hr/>\n<p><strong>If your sentence is truly simple enough:</strong></p>\n<p>Using the <code>string.punctuation</code> set, remove punctuation then split using the whitespace delimiter:</p>\n<pre><code class=\"python\">import string\nx = \"This is my text, this is a nice way to input text.\"\ny = \"\".join([i for i in x if not in string.punctuation]).split(\" \")\nprint y\n</code></pre>\n", "abstract": "As @PavelAnossov answered, the canonical answer, use the word_tokenize function in nltk: If your sentence is truly simple enough: Using the string.punctuation set, remove punctuation then split using the whitespace delimiter:"}]}, {"link": "https://stackoverflow.com/questions/10252448/how-to-check-whether-a-sentence-is-correct-simple-grammar-check-in-python", "question": {"id": "10252448", "title": "How to check whether a sentence is correct (simple grammar check in Python)?", "content": "<p>How to check whether a sentence is valid in Python?</p>\n<p>Examples:</p>\n<pre><code class=\"python\">I love Stackoverflow - Correct\nI Stackoverflow love - Incorrect\n</code></pre>\n", "abstract": "How to check whether a sentence is valid in Python? Examples:"}, "answers": [{"id": 25142528, "score": 49, "vote": 0, "content": "<p>There are various Web Services providing automated proofreading and grammar checking. Some have a Python library to simplify querying.</p>\n<p>As far as I can tell, most of those tools (certainly After the Deadline and LanguageTool) are rule based. The checked text is compared with a large set of rules describing common errors. If a rule matches, the software calls it an error. If a rule does not match, the software does nothing (it cannot detect errors it does not have rules for).</p>\n<h3><a href=\"http://www.afterthedeadline.com/development.slp\" rel=\"noreferrer\">After the Deadline</a></h3>\n<pre><code class=\"python\">import ATD\nATD.setDefaultKey(\"your API key\")\nerrors = ATD.checkDocument(\"Looking too the water. Fixing your writing typoss.\")\nfor error in errors:\n print \"%s error for: %s **%s**\" % (error.type, error.precontext, error.string)\n print \"some suggestions: %s\" % (\", \".join(error.suggestions),)\n</code></pre>\n<p>Expected output:</p>\n<pre><code class=\"python\">grammar error for: Looking **too the**\nsome suggestions: to the\nspelling error for: writing **typoss**\nsome suggestions: typos\n</code></pre>\n<p>It is possible to run the server application on your own machine, 4 GB RAM is recommended.</p>\n<h3><a href=\"https://www.languagetool.org/development/\" rel=\"noreferrer\">LanguageTool</a></h3>\n<p><a href=\"https://pypi.python.org/pypi/language-check\" rel=\"noreferrer\">https://pypi.python.org/pypi/language-check</a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; import language_check\n&gt;&gt;&gt; tool = language_check.LanguageTool('en-US')\n&gt;&gt;&gt; text = 'A sentence with a error in the Hitchhiker\u2019s Guide tot he Galaxy'\n&gt;&gt;&gt; matches = tool.check(text)\n\n&gt;&gt;&gt; matches[0].fromy, matches[0].fromx\n(0, 16)\n&gt;&gt;&gt; matches[0].ruleId, matches[0].replacements\n('EN_A_VS_AN', ['an'])\n&gt;&gt;&gt; matches[1].fromy, matches[1].fromx\n(0, 50)\n&gt;&gt;&gt; matches[1].ruleId, matches[1].replacements\n('TOT_HE', ['to the'])\n\n&gt;&gt;&gt; print(matches[1])\nLine 1, column 51, Rule ID: TOT_HE[1]\nMessage: Did you mean 'to the'?\nSuggestion: to the\n...\n\n&gt;&gt;&gt; language_check.correct(text, matches)\n'A sentence with an error in the Hitchhiker\u2019s Guide to the Galaxy'\n</code></pre>\n<p>It is also possible to run the server side privately.</p>\n<h3><a href=\"http://www.gingersoftware.com/grammarcheck\" rel=\"noreferrer\">Ginger</a></h3>\n<p>Additionally, <a href=\"https://github.com/zoncoen/python-ginger\" rel=\"noreferrer\">this</a> is a hacky (screen scraping) library for Ginger, arguably one of the most polished free-to-use grammar checking options out there.</p>\n<h3>Microsoft Word</h3>\n<p>It should be possible to script Microsoft Word and use its grammar checking functionality.</p>\n<h3>More</h3>\n<p>There is a <a href=\"https://www.openoffice.org/lingucomponent/grammar.html\" rel=\"noreferrer\">curated list of grammar checkers on Open Office website</a>. Noted in comments by Patrick.</p>\n", "abstract": "There are various Web Services providing automated proofreading and grammar checking. Some have a Python library to simplify querying. As far as I can tell, most of those tools (certainly After the Deadline and LanguageTool) are rule based. The checked text is compared with a large set of rules describing common errors. If a rule matches, the software calls it an error. If a rule does not match, the software does nothing (it cannot detect errors it does not have rules for). Expected output: It is possible to run the server application on your own machine, 4 GB RAM is recommended. https://pypi.python.org/pypi/language-check It is also possible to run the server side privately. Additionally, this is a hacky (screen scraping) library for Ginger, arguably one of the most polished free-to-use grammar checking options out there. It should be possible to script Microsoft Word and use its grammar checking functionality. There is a curated list of grammar checkers on Open Office website. Noted in comments by Patrick."}, {"id": 10252472, "score": 25, "vote": 0, "content": "<p>Check out <a href=\"http://www.nltk.org\" rel=\"noreferrer\">NLTK</a>.  They have support for grammars that you can use to parse your sentence.  You can define a grammar, or use one that is provided, along with a context-free parser.  If the sentence parses, then it has valid grammar; if not, then it doesn't.  These grammars may not have the widest coverage (eg, it might not know how to handle a word like StackOverflow), but this approach will allow you to say specifically what is valid or invalid in the grammar.  <a href=\"http://www.nltk.org/book/ch08.html\" rel=\"noreferrer\">Chapter 8</a> of the NLTK book covers parsing and should explain what you need to know.</p>\n<p>An alternative would be to write a python interface to a wide-coverage parser (like the <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">Stanford parser</a> or <a href=\"http://svn.ask.it.usyd.edu.au/trac/candc\" rel=\"noreferrer\">C&amp;C</a>).  These are statistical parsers that will be able to understand sentences even if they haven't seen all the words or all the grammatical constructions before.  The downside is that sometimes the parser will still return a parse for a sentence with bad grammar because it will use the statistics to make the best guess possible.</p>\n<p>So, it really depends on exactly what your goal is.  If you want very precise control over what is considered grammatical, use a context-free parser with NLTK.  If you want robustness and wide-coverage, use a statistical parser.</p>\n", "abstract": "Check out NLTK.  They have support for grammars that you can use to parse your sentence.  You can define a grammar, or use one that is provided, along with a context-free parser.  If the sentence parses, then it has valid grammar; if not, then it doesn't.  These grammars may not have the widest coverage (eg, it might not know how to handle a word like StackOverflow), but this approach will allow you to say specifically what is valid or invalid in the grammar.  Chapter 8 of the NLTK book covers parsing and should explain what you need to know. An alternative would be to write a python interface to a wide-coverage parser (like the Stanford parser or C&C).  These are statistical parsers that will be able to understand sentences even if they haven't seen all the words or all the grammatical constructions before.  The downside is that sometimes the parser will still return a parse for a sentence with bad grammar because it will use the statistics to make the best guess possible. So, it really depends on exactly what your goal is.  If you want very precise control over what is considered grammatical, use a context-free parser with NLTK.  If you want robustness and wide-coverage, use a statistical parser."}, {"id": 61666450, "score": 8, "vote": 0, "content": "<p>Some other answers have mentioned <a href=\"http://languagetool.org\" rel=\"noreferrer\">LanguageTool</a>, the largest open-source grammar checker. It didn't have a reliable, up-to-date Python port until now. </p>\n<p>I recommend <a href=\"https://github.com/jxmorris12/language_tool_python\" rel=\"noreferrer\">language_tool_python</a>, a grammar checker that supports Python 3 and the latest versions of Java and LanguageTool. It's the only up-to-date, free Python grammar checker. (full disclosure, I made this library)</p>\n", "abstract": "Some other answers have mentioned LanguageTool, the largest open-source grammar checker. It didn't have a reliable, up-to-date Python port until now.  I recommend language_tool_python, a grammar checker that supports Python 3 and the latest versions of Java and LanguageTool. It's the only up-to-date, free Python grammar checker. (full disclosure, I made this library)"}, {"id": 64314471, "score": 6, "vote": 0, "content": "<p>I would suggest the <a href=\"https://pypi.org/project/language-tool-python/\" rel=\"noreferrer\">language-tool-python</a>. For example:</p>\n<pre><code class=\"python\">import language_tool_python\ntool = language_tool_python.LanguageTool('en-US')\n\ntext = \"Your the best but their are allso  good !\"\nmatches = tool.check(text)\nlen(matches)\n</code></pre>\n<p>and we get:</p>\n<pre><code class=\"python\">4\n</code></pre>\n<p>We can have a look at the 4 issues that it found:</p>\n<p><strong>1st Issue:</strong></p>\n<p><code>matches[0]</code></p>\n<p>And we get:</p>\n<pre><code class=\"python\">Match({'ruleId': 'YOUR_YOU_RE', 'message': 'Did you mean \"You\\'re\"?', 'replacements': [\"You're\"], 'context': 'Your the best but their are allso  good !', 'offset': 0, 'errorLength': 4, 'category': 'TYPOS', 'ruleIssueType': 'misspelling'})\n</code></pre>\n<p><strong>2nd Issue:</strong></p>\n<p><code>matches[1]</code></p>\n<p>and we get:</p>\n<pre><code class=\"python\">Match({'ruleId': 'THEIR_IS', 'message': 'Did you mean \"there\"?', 'replacements': ['there'], 'context': 'Your the best but their are allso  good !', 'offset': 18, 'errorLength': 5, 'category': 'CONFUSED_WORDS', 'ruleIssueType': 'misspelling'})\n</code></pre>\n<p><strong>3rd Issue:</strong>\n<code>matches[2]</code>\nand we get:</p>\n<pre><code class=\"python\">Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['also', 'all so'], 'context': 'Your the best but their are allso  good !', 'offset': 28, 'errorLength': 5, 'category': 'TYPOS', 'ruleIssueType': 'misspelling'})\n</code></pre>\n<p><strong>4th Issue:</strong></p>\n<p><code>matches[3]</code></p>\n<p>and we get:</p>\n<pre><code class=\"python\">Match({'ruleId': 'WHITESPACE_RULE', 'message': 'Possible typo: you repeated a whitespace', 'replacements': [' '], 'context': 'Your the best but their are allso  good!', 'offset': 33, 'errorLength': 2, 'category': 'TYPOGRAPHY', 'ruleIssueType': 'whitespace'})\n</code></pre>\n<p>If you are looking for a more detailed example you can have a look at the related post of <a href=\"https://predictivehacks.com/languagetool-grammar-and-spell-checker-in-python/\" rel=\"noreferrer\">Predictive Hacks</a></p>\n", "abstract": "I would suggest the language-tool-python. For example: and we get: We can have a look at the 4 issues that it found: 1st Issue: matches[0] And we get: 2nd Issue: matches[1] and we get: 3rd Issue:\nmatches[2]\nand we get: 4th Issue: matches[3] and we get: If you are looking for a more detailed example you can have a look at the related post of Predictive Hacks"}, {"id": 73516256, "score": 2, "vote": 0, "content": "<p>Step 1</p>\n<pre><code class=\"python\">pip install Caribe\n</code></pre>\n<p>Step 2</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import Caribe as cb\nsentence=\"I is playing football\"\noutput=cb.caribe_corrector(sentence)\nprint(output)\n</code></pre>\n", "abstract": "Step 1 Step 2"}]}, {"link": "https://stackoverflow.com/questions/41170726/add-remove-custom-stop-words-with-spacy", "question": {"id": "41170726", "title": "Add/remove custom stop words with spacy", "content": "<p>What is the best way to add/remove stop words with spacy? I am using <a href=\"https://spacy.io/docs/api/token\" rel=\"noreferrer\"><code>token.is_stop</code></a> function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!</p>\n", "abstract": "What is the best way to add/remove stop words with spacy? I am using token.is_stop function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!"}, "answers": [{"id": 51627002, "score": 64, "vote": 0, "content": "<p>Using Spacy 2.0.11, you can update its stopwords set using one of the following:</p>\n<p>To add a single stopword:</p>\n<pre><code class=\"python\">import spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.add(\"my_new_stopword\")\n</code></pre>\n<p>To add several stopwords at once:</p>\n<pre><code class=\"python\">import spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",}\n</code></pre>\n<p>To remove a single stopword:</p>\n<pre><code class=\"python\">import spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.remove(\"whatever\")\n</code></pre>\n<p>To remove several stopwords at once:</p>\n<pre><code class=\"python\">import spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words -= {\"whatever\", \"whenever\"}\n</code></pre>\n<p>Note: To see the current set of stopwords, use:</p>\n<pre><code class=\"python\">print(nlp.Defaults.stop_words)\n</code></pre>\n<p>Update : It was noted in the comments that this fix only affects the current execution. To update the model, you can use the methods <code>nlp.to_disk(\"/path\")</code> and <code>nlp.from_disk(\"/path\")</code> (further described at <a href=\"https://spacy.io/usage/saving-loading\" rel=\"noreferrer\">https://spacy.io/usage/saving-loading</a>).</p>\n", "abstract": "Using Spacy 2.0.11, you can update its stopwords set using one of the following: To add a single stopword: To add several stopwords at once: To remove a single stopword: To remove several stopwords at once: Note: To see the current set of stopwords, use: Update : It was noted in the comments that this fix only affects the current execution. To update the model, you can use the methods nlp.to_disk(\"/path\") and nlp.from_disk(\"/path\") (further described at https://spacy.io/usage/saving-loading)."}, {"id": 41172279, "score": 48, "vote": 0, "content": "<p>You can edit them before processing your text like this (see <a href=\"https://github.com/explosion/spaCy/issues/364\" rel=\"noreferrer\">this post</a>):</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import spacy\n&gt;&gt;&gt; nlp = spacy.load(\"en\")\n&gt;&gt;&gt; nlp.vocab[\"the\"].is_stop = False\n&gt;&gt;&gt; nlp.vocab[\"definitelynotastopword\"].is_stop = True\n&gt;&gt;&gt; sentence = nlp(\"the word is definitelynotastopword\")\n&gt;&gt;&gt; sentence[0].is_stop\nFalse\n&gt;&gt;&gt; sentence[3].is_stop\nTrue\n</code></pre>\n<p>Note: This seems to work &lt;=v1.8. For newer versions, see other answers.</p>\n", "abstract": "You can edit them before processing your text like this (see this post): Note: This seems to work <=v1.8. For newer versions, see other answers."}, {"id": 46380305, "score": 20, "vote": 0, "content": "<p>For version 2.0 I used this:</p>\n<pre><code class=\"python\">from spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS) # &lt;- set of Spacy's default stop words\n\nSTOP_WORDS.add(\"your_additional_stop_word_here\")\n\nfor word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    lexeme.is_stop = True\n</code></pre>\n<p>This loads all stop words into a set.</p>\n<p>You can amend your stop words to <code>STOP_WORDS</code> or use your own list in the first place.</p>\n", "abstract": "For version 2.0 I used this: This loads all stop words into a set. You can amend your stop words to STOP_WORDS or use your own list in the first place."}, {"id": 49474553, "score": 5, "vote": 0, "content": "<p>For 2.0 use the following:</p>\n<pre><code class=\"python\">for word in nlp.Defaults.stop_words:\n    lex = nlp.vocab[word]\n    lex.is_stop = True\n</code></pre>\n", "abstract": "For 2.0 use the following:"}, {"id": 57626079, "score": 4, "vote": 0, "content": "<p>This collects the stop words too :) </p>\n<p><code>spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS</code></p>\n", "abstract": "This collects the stop words too :)  spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"}, {"id": 58027768, "score": 0, "vote": 0, "content": "<p>In latest version following would remove the word out of the list:</p>\n<pre><code class=\"python\">spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nspacy_stopwords.remove('not')\n</code></pre>\n", "abstract": "In latest version following would remove the word out of the list:"}, {"id": 66483358, "score": 0, "vote": 0, "content": "<p>For version 2.3.0\nIf you want to replace the entire list instead of adding or removing a few stop words, you can do this:</p>\n<pre><code class=\"python\">custom_stop_words = set(['the','and','a'])\n\n# First override the stop words set for the language\ncls = spacy.util.get_lang_class('en')\ncls.Defaults.stop_words = custom_stop_words\n\n# Now load your model\nnlp = spacy.load('en_core_web_md')\n\n</code></pre>\n<p>The trick is to assign the stop word set for the language before loading the model. It also ensures that any upper/lower case variation of the stop words are considered stop words.</p>\n", "abstract": "For version 2.3.0\nIf you want to replace the entire list instead of adding or removing a few stop words, you can do this: The trick is to assign the stop word set for the language before loading the model. It also ensures that any upper/lower case variation of the stop words are considered stop words."}]}, {"link": "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python", "question": {"id": "19790188", "title": "Expanding English language contractions in Python", "content": "<p>The English language has <a href=\"http://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\">a couple of contractions</a>. For instance:</p>\n<pre><code class=\"python\">you've -&gt; you have\nhe's -&gt; he is\n</code></pre>\n<p>These can sometimes cause headache when you are doing natural language processing. Is there a Python library, which can expand these contractions?</p>\n", "abstract": "The English language has a couple of contractions. For instance: These can sometimes cause headache when you are doing natural language processing. Is there a Python library, which can expand these contractions?"}, "answers": [{"id": 19794953, "score": 65, "vote": 0, "content": "<p>I made that wikipedia contraction-to-expansion page into a python dictionary (see below)</p>\n<p>Note, as you might expect, that you definitely want to use double quotes when querying the dictionary:</p>\n<p><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/1BeI3.png\"/></p>\n<p>Also, I've left multiple options in as in the wikipedia page. Feel free to modify it as you wish. Note that disambiguation to the right expansion would be a tricky problem!</p>\n<pre><code class=\"python\">contractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n</code></pre>\n", "abstract": "I made that wikipedia contraction-to-expansion page into a python dictionary (see below) Note, as you might expect, that you definitely want to use double quotes when querying the dictionary:  Also, I've left multiple options in as in the wikipedia page. Feel free to modify it as you wish. Note that disambiguation to the right expansion would be a tricky problem!"}, {"id": 47091490, "score": 29, "vote": 0, "content": "<p>The answers above will work perfectly well and could be better for ambiguous contraction (although I would argue that there aren't that many ambiguous cases). I would use something more readable and easier to maintain:</p>\n<pre><code class=\"python\">import re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ntest = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\nprint(decontracted(test))\n# Hey I am Yann, how are you and how is it going ? That is interesting: I would love to hear more about it.\n</code></pre>\n<p>It might have some flaws I didn't think about though.</p>\n<p>Reposted from <a href=\"https://stackoverflow.com/questions/43018030/replace-appostrophe-short-words-in-python/47091370#47091370\">my other answer</a></p>\n", "abstract": "The answers above will work perfectly well and could be better for ambiguous contraction (although I would argue that there aren't that many ambiguous cases). I would use something more readable and easier to maintain: It might have some flaws I didn't think about though. Reposted from my other answer"}, {"id": 54664630, "score": 21, "vote": 0, "content": "<p>I have found a library for this, <code>contractions</code> Its very simple.</p>\n<pre><code class=\"python\">import contractions\nprint(contractions.fix(\"you've\"))\nprint(contractions.fix(\"he's\"))\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code class=\"python\">you have\nhe is\n</code></pre>\n", "abstract": "I have found a library for this, contractions Its very simple. Output:"}, {"id": 19790352, "score": 18, "vote": 0, "content": "<p>You don't need a library, it is possible to do with reg exp for example.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import re\n&gt;&gt;&gt; contractions_dict = {\n...     'didn\\'t': 'did not',\n...     'don\\'t': 'do not',\n... }\n&gt;&gt;&gt; contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n&gt;&gt;&gt; def expand_contractions(s, contractions_dict=contractions_dict):\n...     def replace(match):\n...         return contractions_dict[match.group(0)]\n...     return contractions_re.sub(replace, s)\n...\n&gt;&gt;&gt; expand_contractions('You don\\'t need a library')\n'You do not need a library'\n</code></pre>\n", "abstract": "You don't need a library, it is possible to do with reg exp for example."}, {"id": 49688078, "score": 11, "vote": 0, "content": "<p>This is a very cool and easy to use library for the purpose \n<a href=\"https://pypi.python.org/pypi/pycontractions/1.0.1\" rel=\"noreferrer\">https://pypi.python.org/pypi/pycontractions/1.0.1</a>.</p>\n<p>Example of use (detailed in link):</p>\n<pre><code class=\"python\">from pycontractions import Contractions\n\n# Load your favorite word2vec model\ncont = Contractions('GoogleNews-vectors-negative300.bin')\n\n# optional, prevents loading on first expand_texts call\ncont.load_models()\n\nout = list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n                            \"Theyre going to the zoo and she'll be home for dinner.\"], precise=True))\nprint(out)\n</code></pre>\n<p>You will also need GoogleNews-vectors-negative300.bin, link to download in the pycontractions link above.\n*Example code in python3. </p>\n", "abstract": "This is a very cool and easy to use library for the purpose \nhttps://pypi.python.org/pypi/pycontractions/1.0.1. Example of use (detailed in link): You will also need GoogleNews-vectors-negative300.bin, link to download in the pycontractions link above.\n*Example code in python3. "}, {"id": 19790512, "score": 4, "vote": 0, "content": "<p>I would like to add little to alko's answer here. If you check wikipedia, the number of English Language contractions as mentioned there are less than 100. Granted, in real scenario this number could be more than that. But still, I am pretty sure that 200-300 words are all you will have for English contraction words. Now, do you want to get a separate library for those (I don't think what you are looking for actually exists, though)?. However, you can easily solve this problem with dictionary and using regex. I would recommend using a nice tokenizer as<a href=\"http://nltk.org/\" rel=\"nofollow\">Natural Language Toolkit</a> and the rest you should have no problem in implementing yourself.</p>\n", "abstract": "I would like to add little to alko's answer here. If you check wikipedia, the number of English Language contractions as mentioned there are less than 100. Granted, in real scenario this number could be more than that. But still, I am pretty sure that 200-300 words are all you will have for English contraction words. Now, do you want to get a separate library for those (I don't think what you are looking for actually exists, though)?. However, you can easily solve this problem with dictionary and using regex. I would recommend using a nice tokenizer asNatural Language Toolkit and the rest you should have no problem in implementing yourself."}, {"id": 62541145, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    # contraction_mapping is a dictionary of words having the compact form\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match) \\\n                                   if contraction_mapping.get(match) \\\n                                    else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n</code></pre>\n", "abstract": ""}, {"id": 46807947, "score": 0, "vote": 0, "content": "<p>Even though this is an old question, I figured I might as well answer since there is still no real solution to this as far as I can see.</p>\n<p>I have had to work on this on a related NLP project and I decided to tackle the problem since there didn't seem to be anything here. You can check my <a href=\"https://github.com/yannick-c/expander\" rel=\"nofollow noreferrer\">expander github repository</a> if you are interested.</p>\n<p>It's a fairly badly optimized (I think) program based on NLTK, the Stanford Core NLP models, which you will have to download separately, and <a href=\"https://stackoverflow.com/a/19794953\">the dictionary in the previous answer</a>. All the necessary information should be in the README and the lavishly commented code. I know commented code is dead code, but this is just how I write to keep things clear for myself.</p>\n<p>The example input in <code>expander.py</code> are the following sentences:</p>\n<pre><code class=\"python\">    [\"I won't let you get away with that\",  # won't -&gt;  will not\n    \"I'm a bad person\",  # 'm -&gt; am\n    \"It's his cat anyway\",  # 's -&gt; is\n    \"It's not what you think\",  # 's -&gt; is\n    \"It's a man's world\",  # 's -&gt; is and 's possessive\n    \"Catherine's been thinking about it\",  # 's -&gt; has\n    \"It'll be done\",  # 'll -&gt; will\n    \"Who'd've thought!\",  # 'd -&gt; would, 've -&gt; have\n    \"She said she'd go.\",  # she'd -&gt; she would\n    \"She said she'd gone.\",  # she'd -&gt; had\n    \"Y'all'd've a great time, wouldn't it be so cold!\", # Y'all'd've -&gt; You all would have, wouldn't -&gt; would not\n    \" My name is Jack.\",   # No replacements.\n    \"'Tis questionable whether Ma'am should be going.\", # 'Tis -&gt; it is, Ma'am -&gt; madam\n    \"As history tells, 'twas the night before Christmas.\", # 'Twas -&gt; It was\n    \"Martha, Peter and Christine've been indulging in a menage-\u00e0-trois.\"] # 've -&gt; have\n</code></pre>\n<p>To which the output is</p>\n<pre><code class=\"python\">    [\"I will not let you get away with that\",\n    \"I am a bad person\",\n    \"It is his cat anyway\",\n    \"It is not what you think\",\n    \"It is a man's world\",\n    \"Catherine has been thinking about it\",\n    \"It will be done\",\n    \"Who would have thought!\",\n    \"She said she would go.\",\n    \"She said she had gone.\",\n    \"You all would have a great time, would not it be so cold!\",\n    \"My name is Jack.\",\n    \"It is questionable whether Madam should be going.\",\n    \"As history tells, it was the night before Christmas.\",\n    \"Martha, Peter and Christine have been indulging in a menage-\u00e0-trois.\"]\n</code></pre>\n<p>So for this small set of test sentences, I came up with to test some edge-cases, it works well.</p>\n<p>Since this project has lost importance right now, I am not actively developing this anymore. Any help on this project would be appreciated. Things to be done are written in the TODO list. Or if you have any tips on how to improve my python I would also be very thankful.</p>\n", "abstract": "Even though this is an old question, I figured I might as well answer since there is still no real solution to this as far as I can see. I have had to work on this on a related NLP project and I decided to tackle the problem since there didn't seem to be anything here. You can check my expander github repository if you are interested. It's a fairly badly optimized (I think) program based on NLTK, the Stanford Core NLP models, which you will have to download separately, and the dictionary in the previous answer. All the necessary information should be in the README and the lavishly commented code. I know commented code is dead code, but this is just how I write to keep things clear for myself. The example input in expander.py are the following sentences: To which the output is So for this small set of test sentences, I came up with to test some edge-cases, it works well. Since this project has lost importance right now, I am not actively developing this anymore. Any help on this project would be appreciated. Things to be done are written in the TODO list. Or if you have any tips on how to improve my python I would also be very thankful."}]}, {"link": "https://stackoverflow.com/questions/19994396/best-way-to-identify-and-extract-dates-from-text-python", "question": {"id": "19994396", "title": "Best way to identify and extract dates from text Python?", "content": "<p>As part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources.</p>\n<p>For example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms:</p>\n<blockquote>\n<p>Central design committee session Tuesday 10/22 6:30 pm</p>\n<p>Th 9/19 LAB: Serial encoding (Section 2.2)</p>\n<p>There will be another one on December 15th for those who are unable to make it today.</p>\n<p>Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm</p>\n<p>He will be flying in Sept. 15th.</p>\n</blockquote>\n<p>While these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no \"The meeting will be two weeks from tomorrow\"\u2014it's all explicit).  </p>\n<p>As someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the <code>dateutil.parser</code> module and <a href=\"https://github.com/bear/parsedatetime\">parsedatetime</a>, but those seem to be for <em>after</em> you've isolated the date.</p>\n<p>Because of this, is there any good way to extract the date and the extraneous text </p>\n<pre><code class=\"python\">input:  Th 9/19 LAB: Serial encoding (Section 2.2)\noutput: ['Th 9/19', 'LAB: Serial encoding (Section 2.2)']\n</code></pre>\n<p>or something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?</p>\n", "abstract": "As part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources. For example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms: Central design committee session Tuesday 10/22 6:30 pm Th 9/19 LAB: Serial encoding (Section 2.2) There will be another one on December 15th for those who are unable to make it today. Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm He will be flying in Sept. 15th. While these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no \"The meeting will be two weeks from tomorrow\"\u2014it's all explicit).   As someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the dateutil.parser module and parsedatetime, but those seem to be for after you've isolated the date. Because of this, is there any good way to extract the date and the extraneous text  or something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?"}, "answers": [{"id": 35069076, "score": 74, "vote": 0, "content": "<p>I was also looking for a solution to this and couldn't find any, so a friend and I built a tool to do this. I thought I would come back and share incase others found it helpful.</p>\n<p><a href=\"https://github.com/akoumjian/datefinder\" rel=\"noreferrer\">datefinder -- find and extract dates inside text</a></p>\n<p>Here's an example:</p>\n<pre><code class=\"python\">import datefinder\n\nstring_with_dates = '''\n    Central design committee session Tuesday 10/22 6:30 pm\n    Th 9/19 LAB: Serial encoding (Section 2.2)\n    There will be another one on December 15th for those who are unable to make it today.\n    Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm\n    He will be flying in Sept. 15th.\n    We expect to deliver this between late 2021 and early 2022.\n'''\n\nmatches = datefinder.find_dates(string_with_dates)\nfor match in matches:\n    print(match)\n</code></pre>\n", "abstract": "I was also looking for a solution to this and couldn't find any, so a friend and I built a tool to do this. I thought I would come back and share incase others found it helpful. datefinder -- find and extract dates inside text Here's an example:"}, {"id": 52636491, "score": 16, "vote": 0, "content": "<p>I am surprised that there is no mention of <a href=\"https://github.com/FraBle/python-sutime\" rel=\"noreferrer\">SUTime</a> and <a href=\"https://github.com/scrapinghub/dateparser\" rel=\"noreferrer\">dateparser's search_dates</a> method.  </p>\n<pre><code class=\"python\">from sutime import SUTime\nimport os\nimport json\nfrom dateparser.search import search_dates\n\nstr1 = \"Let's meet sometime next Thursday\" \n\n# You'll get more information about these jar files from SUTime's github page\njar_files = os.path.join(os.path.dirname(__file__), 'jars')\nsutime = SUTime(jars=jar_files, mark_time_ranges=True)\n\nprint(json.dumps(sutime.parse(str1), sort_keys=True, indent=4))\n\"\"\"output: \n[\n    {\n        \"end\": 33,\n        \"start\": 20,\n        \"text\": \"next Thursday\",\n        \"type\": \"DATE\",\n        \"value\": \"2018-10-11\"\n    }\n]\n\"\"\"\n\nprint(search_dates(str1))\n#output:\n#[('Thursday', datetime.datetime(2018, 9, 27, 0, 0))]\n</code></pre>\n<p>Although I have tried other modules like dateutil, datefinder and natty (couldn't get duckling to work with python), this two seem to give the most promising results. </p>\n<p>The results from SUTime are more reliable and it's clear from the above code snippet. However, the SUTime fails in some basic scenarios like parsing a text </p>\n<blockquote>\n<p>\"I won't be available until 9/19\"</p>\n</blockquote>\n<p>or </p>\n<blockquote>\n<p>\"I won't be available between (September 18-September 20).</p>\n</blockquote>\n<p>It gives no result for the first text and only gives month and year for the second text. \nThis is however handled quite well in the search_dates method. \nsearch_dates method is more aggressive and will give all possible dates related to any words in the input text. </p>\n<p>I haven't yet found a way to parse the text strictly for dates in search_methods. If I could find a way to do that, it'll be my first choice over SUTime and I would also make sure to update this answer if I find it. </p>\n", "abstract": "I am surprised that there is no mention of SUTime and dateparser's search_dates method.   Although I have tried other modules like dateutil, datefinder and natty (couldn't get duckling to work with python), this two seem to give the most promising results.  The results from SUTime are more reliable and it's clear from the above code snippet. However, the SUTime fails in some basic scenarios like parsing a text  \"I won't be available until 9/19\" or  \"I won't be available between (September 18-September 20). It gives no result for the first text and only gives month and year for the second text. \nThis is however handled quite well in the search_dates method. \nsearch_dates method is more aggressive and will give all possible dates related to any words in the input text.  I haven't yet found a way to parse the text strictly for dates in search_methods. If I could find a way to do that, it'll be my first choice over SUTime and I would also make sure to update this answer if I find it. "}, {"id": 51324040, "score": 8, "vote": 0, "content": "<p>You can use the <a href=\"https://github.com/dateutil/dateutil\" rel=\"noreferrer\">dateutil module</a>'s <code>parse</code> method with the <code>fuzzy</code> option.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from dateutil.parser import parse\n&gt;&gt;&gt; parse(\"Central design committee session Tuesday 10/22 6:30 pm\", fuzzy=True)\ndatetime.datetime(2018, 10, 22, 18, 30)\n&gt;&gt;&gt; parse(\"There will be another one on December 15th for those who are unable to make it today.\", fuzzy=True)\ndatetime.datetime(2018, 12, 15, 0, 0)\n&gt;&gt;&gt; parse(\"Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm\", fuzzy=True)\ndatetime.datetime(2018, 3, 9, 23, 59)\n&gt;&gt;&gt; parse(\"He will be flying in Sept. 15th.\", fuzzy=True)\ndatetime.datetime(2018, 9, 15, 0, 0)\n&gt;&gt;&gt; parse(\"Th 9/19 LAB: Serial encoding (Section 2.2)\", fuzzy=True)\ndatetime.datetime(2002, 9, 19, 0, 0)\n</code></pre>\n", "abstract": "You can use the dateutil module's parse method with the fuzzy option."}, {"id": 19994689, "score": 7, "vote": 0, "content": "<p>If you can identify the segments that actually contain the date information, parsing them can be fairly simple with <a href=\"https://pypi.python.org/pypi/parsedatetime\" rel=\"noreferrer\">parsedatetime</a>. There are a few things to consider though namely that your dates don't have years and you should pick a locale.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import parsedatetime\n&gt;&gt;&gt; p = parsedatetime.Calendar()\n&gt;&gt;&gt; p.parse(\"December 15th\")\n((2013, 12, 15, 0, 13, 30, 4, 319, 0), 1)\n&gt;&gt;&gt; p.parse(\"9/18 11:59 pm\")\n((2014, 9, 18, 23, 59, 0, 4, 319, 0), 3)\n&gt;&gt;&gt; # It chooses 2014 since that's the *next* occurence of 9/18\n</code></pre>\n<p>It doesn't always work perfectly when you have extraneous text.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; p.parse(\"9/19 LAB: Serial encoding\")\n((2014, 9, 19, 0, 15, 30, 4, 319, 0), 1)\n&gt;&gt;&gt; p.parse(\"9/19 LAB: Serial encoding (Section 2.2)\")\n((2014, 2, 2, 0, 15, 32, 4, 319, 0), 1)\n</code></pre>\n<p>Honestly, this seems like the kind of problem that would be simple enough to parse for particular formats and pick the most likely out of each sentence. Beyond that, it would be a decent machine learning problem.</p>\n", "abstract": "If you can identify the segments that actually contain the date information, parsing them can be fairly simple with parsedatetime. There are a few things to consider though namely that your dates don't have years and you should pick a locale. It doesn't always work perfectly when you have extraneous text. Honestly, this seems like the kind of problem that would be simple enough to parse for particular formats and pick the most likely out of each sentence. Beyond that, it would be a decent machine learning problem."}, {"id": 57878377, "score": 2, "vote": 0, "content": "<p>Newer versions of <code>parsedatetime</code> lib provide search functionality. </p>\n<p>Example</p>\n<pre><code class=\"python\">from dateparser.search import search_dates\n\ndates = search_dates('Central design committee session Tuesday 10/22 6:30 pm')\n</code></pre>\n", "abstract": "Newer versions of parsedatetime lib provide search functionality.  Example"}, {"id": 19996931, "score": 1, "vote": 0, "content": "<p>Hi I'm not sure bellow approach is machine learning but you may try it:</p>\n<ul>\n<li>add some context from outside text, e.g publishing time of text message, posting, now etc. (your text doesn't tell anything about year) </li>\n<li><p>extract all tokens with separator white-space and should get something like this:</p>\n<pre><code class=\"python\">['Th','Wednesday','9:34pm','7:34','pm','am','9/18','9/','/18', '19','12']\n</code></pre></li>\n<li><p>process them with rule-sets e.g subsisting from weekdays and/or variations of components forming time and mark them e.g. '%d:%dpm', '%d am', '%d/%d', '%d/ %d' etc.  may means time. \n Note that it may have compositions e.g. \"12 / 31\" is 3gram ('12','/','31') should be one token \"12/31\" of interest.</p></li>\n<li><p>\"see\" what tokens are around marked tokens like \"9:45pm\" e.g ('Th\",'9/19','9:45pm') is 3gram formed from \"interesting\" tokens and apply rules about it that may determine meaning. </p></li>\n<li><p>process for more specific analysis for example if have 31/12 so 31 &gt; 12 means d/m, or vice verse, but if have 12/12 m,d will be available only in context build from text and/or outside.</p></li>\n</ul>\n<p>Cheers</p>\n", "abstract": "Hi I'm not sure bellow approach is machine learning but you may try it: extract all tokens with separator white-space and should get something like this: process them with rule-sets e.g subsisting from weekdays and/or variations of components forming time and mark them e.g. '%d:%dpm', '%d am', '%d/%d', '%d/ %d' etc.  may means time. \n Note that it may have compositions e.g. \"12 / 31\" is 3gram ('12','/','31') should be one token \"12/31\" of interest. \"see\" what tokens are around marked tokens like \"9:45pm\" e.g ('Th\",'9/19','9:45pm') is 3gram formed from \"interesting\" tokens and apply rules about it that may determine meaning.  process for more specific analysis for example if have 31/12 so 31 > 12 means d/m, or vice verse, but if have 12/12 m,d will be available only in context build from text and/or outside. Cheers"}, {"id": 67433549, "score": 0, "vote": 0, "content": "<p>There is no any perfact solution. IT's completely depend on which type of data u are suppose to work. Quickly review and analyze data by going through certain set of data manually and prepare regex pattern and test it wheather it is working or not.</p>\n<p>Predefined all packages solve a date extraction problem up to some extent and it is limited one. if one will approximately find out pattern by looking to data then user can prepare regex. It will help them to prevent to iterate and loop over all rules written in packages.</p>\n", "abstract": "There is no any perfact solution. IT's completely depend on which type of data u are suppose to work. Quickly review and analyze data by going through certain set of data manually and prepare regex pattern and test it wheather it is working or not. Predefined all packages solve a date extraction problem up to some extent and it is limited one. if one will approximately find out pattern by looking to data then user can prepare regex. It will help them to prevent to iterate and loop over all rules written in packages."}]}, {"link": "https://stackoverflow.com/questions/39843584/gensim-doc2vec-vs-tensorflow-doc2vec", "question": {"id": "39843584", "title": "gensim Doc2Vec vs tensorflow Doc2Vec", "content": "<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>\n<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>\n<ol>\n<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>\n<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>\n<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>\n<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>\n</ol>\n<h3>Gensim</h3>\n<pre><code class=\"python\">model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)\nmodel.build_vocab(corpus)\nepochs = 100\nfor i in range(epochs):\n    model.train(corpus)\n</code></pre>\n<h3>TF</h3>\n<pre><code class=\"python\">batch_size = 512\nembedding_size = 100 # Dimension of the embedding vector.\nnum_sampled = 10 # Number of negative examples to sample.\n\n\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device('/cpu:0'):\n    # Input data.\n    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])\n\n    # The variables   \n    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],\n                             stddev=1.0 / np.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    ###########################\n    # Model.\n    ###########################\n    # Look up embeddings for inputs and stack words side by side\n    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),\n                            shape=[int(batch_size/context_window),-1])\n    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n    embed = tf.concat(1,[embed_words, embed_docs])\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n                                   train_labels, num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n</code></pre>\n<h2>Update:</h2>\n<p>Check out the jupyter notebook <a href=\"https://github.com/sachinruk/doc2vec_tf\" rel=\"noreferrer\">here</a> (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis.</p>\n", "abstract": "I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better. I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows: Check out the jupyter notebook here (I have both models working and tested in here). It still feels like the gensim model is performing better in this initial analysis."}, "answers": [{"id": 45767145, "score": 21, "vote": 0, "content": "<p>Old question, but an answer would be useful for future visitors. So here are some of my thoughts.</p>\n<p>There are some problems in the <code>tensorflow</code> implementation:</p>\n<ul>\n<li><code>window</code> is 1-side size, so <code>window=5</code> would be <code>5*2+1</code> = <code>11</code> words. </li>\n<li>Note that with PV-DM version of doc2vec, the <code>batch_size</code> would be the number of documents. So <code>train_word_dataset</code> shape would be <code>batch_size * context_window</code>, while <code>train_doc_dataset</code> and <code>train_labels</code> shapes would be <code>batch_size</code>.</li>\n<li>More importantly, <code>sampled_softmax_loss</code> is not <code>negative_sampling_loss</code>. They are two different approximations of <code>softmax_loss</code>.</li>\n</ul>\n<p>So for the OP's listed questions:</p>\n<ol>\n<li>This implementation of <code>doc2vec</code> in <code>tensorflow</code> is working and correct in its own way, but it is different from both the <code>gensim</code> implementation and the paper.</li>\n<li><code>window</code> is 1-side size as said above. If document size is less than context size, then the smaller one would be use.</li>\n<li>There are many reasons why <code>gensim</code> implementation is faster. First, <code>gensim</code> was optimized heavily, all operations are faster than naive python operations, especially data I/O. Second, some preprocessing steps such as <code>min_count</code> filtering in <code>gensim</code> would reduce the dataset size. More importantly, <code>gensim</code> uses <code>negative_sampling_loss</code>, which is much faster than <code>sampled_softmax_loss</code>, I guess this is the main reason.</li>\n<li>Is it easier to find somethings when there are many of them? Just kidding ;-)<br/>\nIt's true that there are many solutions in this non-convex optimization problem, so the model would just find a local optimum. Interestingly, in neural network, most local optima are \"good enough\". It has been observed that stochastic gradient descent seems to find better local optima than larger batch gradient descent, although this is still a riddle in current research.</li>\n</ol>\n", "abstract": "Old question, but an answer would be useful for future visitors. So here are some of my thoughts. There are some problems in the tensorflow implementation: So for the OP's listed questions:"}]}, {"link": "https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary", "question": {"id": "35596031", "title": "gensim word2vec: Find number of words in vocabulary", "content": "<p>After training a word2vec model using python <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\" rel=\"noreferrer\">gensim</a>, how do you find the number of words in the model's vocabulary?</p>\n", "abstract": "After training a word2vec model using python gensim, how do you find the number of words in the model's vocabulary?"}, "answers": [{"id": 35641434, "score": 107, "vote": 0, "content": "<p>In recent versions, the <code>model.wv</code> property holds the words-and-vectors, and can itself can report a length \u2013 the number of words it contains. So if <code>w2v_model</code> is your <code>Word2Vec</code> (or <code>Doc2Vec</code> or <code>FastText</code>) model, it's enough to just do:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">vocab_len = len(w2v_model.wv)\n</code></pre>\n<p>If your model is just a raw set of word-vectors, like a <code>KeyedVectors</code> instance rather than a full <code>Word2Vec</code>/etc model, it's just:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">vocab_len = len(kv_model)\n</code></pre>\n<p>Other useful internals in Gensim 4.0+ include <code>model.wv.index_to_key</code>, a plain list of the key (word) in each index position, and <code>model.wv.key_to_index</code>, a plain dict mapping keys (words) to their index positions.</p>\n<p>In pre-4.0 versions, the vocabulary was in the <code>vocab</code> field of the Word2Vec model's <code>wv</code> property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length:</p>\n<pre><code class=\"python\">len(w2v_model.wv.vocab)\n</code></pre>\n<p>In very-old gensim versions before 0.13 <code>vocab</code> appeared directly on the model. So way back then you would use <code>w2v_model.vocab</code> instead of <code>w2v_model.wv.vocab</code>.</p>\n<p>But if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory &amp; performance improvements, and the changes required in calling code are relatively small \u2013 some renamings &amp; moves, covered in the <a href=\"https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes\" rel=\"noreferrer\">4.0 Migration Notes</a>.</p>\n", "abstract": "In recent versions, the model.wv property holds the words-and-vectors, and can itself can report a length \u2013 the number of words it contains. So if w2v_model is your Word2Vec (or Doc2Vec or FastText) model, it's enough to just do: If your model is just a raw set of word-vectors, like a KeyedVectors instance rather than a full Word2Vec/etc model, it's just: Other useful internals in Gensim 4.0+ include model.wv.index_to_key, a plain list of the key (word) in each index position, and model.wv.key_to_index, a plain dict mapping keys (words) to their index positions. In pre-4.0 versions, the vocabulary was in the vocab field of the Word2Vec model's wv property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length: In very-old gensim versions before 0.13 vocab appeared directly on the model. So way back then you would use w2v_model.vocab instead of w2v_model.wv.vocab. But if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory & performance improvements, and the changes required in calling code are relatively small \u2013 some renamings & moves, covered in the 4.0 Migration Notes."}, {"id": 54891912, "score": 7, "vote": 0, "content": "<p>One more way to get the vocabulary size is from the embedding matrix itself as in:</p>\n<pre><code class=\"python\">In [33]: from gensim.models import Word2Vec\n\n# load the pretrained model\nIn [34]: model = Word2Vec.load(pretrained_model)\n\n# get the shape of embedding matrix    \nIn [35]: model.wv.vectors.shape\nOut[35]: (662109, 300)\n\n# `vocabulary_size` is just the number of rows (i.e. axis 0)\nIn [36]: model.wv.vectors.shape[0]\nOut[36]: 662109\n</code></pre>\n", "abstract": "One more way to get the vocabulary size is from the embedding matrix itself as in:"}, {"id": 68083472, "score": 4, "vote": 0, "content": "<p>Gojomo's answer raises an <code>AttributeError</code> for Gensim 4.0.0+.</p>\n<p>For these versions, you can get the length of the vocabulary as follows:</p>\n<p><code>len(w2v_model.wv.index_to_key)</code></p>\n<p>(which is slightly faster than: <code>len(w2v_model.wv.key_to_index)</code>)</p>\n", "abstract": "Gojomo's answer raises an AttributeError for Gensim 4.0.0+. For these versions, you can get the length of the vocabulary as follows: len(w2v_model.wv.index_to_key) (which is slightly faster than: len(w2v_model.wv.key_to_index))"}, {"id": 73127054, "score": 0, "vote": 0, "content": "<p><strong>Latest:</strong></p>\n<p>Use model.wv.key_to_index, after creating gensim model</p>\n<p>vocab dict became key_to_index for looking up a key's integer index, or get_vecattr() and set_vecattr() for other per-key attributes:<a href=\"https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes\" rel=\"nofollow noreferrer\">https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes</a></p>\n", "abstract": "Latest: Use model.wv.key_to_index, after creating gensim model vocab dict became key_to_index for looking up a key's integer index, or get_vecattr() and set_vecattr() for other per-key attributes:https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"}]}, {"link": "https://stackoverflow.com/questions/20290870/improving-the-extraction-of-human-names-with-nltk", "question": {"id": "20290870", "title": "Improving the extraction of human names with nltk", "content": "<p>I am trying to extract human names from text. </p>\n<p>Does anyone have a method that they would recommend?</p>\n<p>This is what I tried (code is below):\nI am using <code>nltk</code> to find everything marked as a person and then generating a list of all the NNP parts of that person. I am skipping persons where there is only one NNP which avoids grabbing a lone surname.</p>\n<p>I am getting decent results but was wondering if there are better ways to go about solving this problem.</p>\n<p>Code:</p>\n<pre><code class=\"python\">import nltk\nfrom nameparser.parser import HumanName\n\ndef get_human_names(text):\n    tokens = nltk.tokenize.word_tokenize(text)\n    pos = nltk.pos_tag(tokens)\n    sentt = nltk.ne_chunk(pos, binary = False)\n    person_list = []\n    person = []\n    name = \"\"\n    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n        for leaf in subtree.leaves():\n            person.append(leaf[0])\n        if len(person) &gt; 1: #avoid grabbing lone surnames\n            for part in person:\n                name += part + ' '\n            if name[:-1] not in person_list:\n                person_list.append(name[:-1])\n            name = ''\n        person = []\n\n    return (person_list)\n\ntext = \"\"\"\nSome economists have responded positively to Bitcoin, including \nFrancois R. Velde, senior economist of the Federal Reserve in Chicago \nwho described it as \"an elegant solution to the problem of creating a \ndigital currency.\" In November 2013 Richard Branson announced that \nVirgin Galactic would accept Bitcoin as payment, saying that he had invested \nin Bitcoin and found it \"fascinating how a whole new global currency \nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical. \nEconomist Paul Krugman has suggested that the structure of the currency \nincentivizes hoarding and that its value derives from the expectation that \nothers will accept it as payment. Economist Larry Summers has expressed \na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \nstrategist for ConvergEx Group, has remarked on the effect of increasing \nuse of Bitcoin and its restricted supply, noting, \"When incremental \nadoption meets relatively fixed supply, it should be no surprise that \nprices go up. And that\u2019s exactly what is happening to BTC prices.\"\n\"\"\"\n\nnames = get_human_names(text)\nprint \"LAST, FIRST\"\nfor name in names: \n    last_first = HumanName(name).last + ', ' + HumanName(name).first\n        print last_first\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">LAST, FIRST\nVelde, Francois\nBranson, Richard\nGalactic, Virgin\nKrugman, Paul\nSummers, Larry\nColas, Nick\n</code></pre>\n<p>Apart from Virgin Galactic, this is all valid output. Of course, knowing that Virgin Galactic isn't a human name in the context of this article is the hard (maybe impossible) part.</p>\n", "abstract": "I am trying to extract human names from text.  Does anyone have a method that they would recommend? This is what I tried (code is below):\nI am using nltk to find everything marked as a person and then generating a list of all the NNP parts of that person. I am skipping persons where there is only one NNP which avoids grabbing a lone surname. I am getting decent results but was wondering if there are better ways to go about solving this problem. Code: Output: Apart from Virgin Galactic, this is all valid output. Of course, knowing that Virgin Galactic isn't a human name in the context of this article is the hard (maybe impossible) part."}, "answers": [{"id": 24119115, "score": 33, "vote": 0, "content": "<p>Must agree with the suggestion that \"make my code better\" isn't well suited for this site, but I can give you some way where you can <strong>try to dig in</strong>.</p>\n<p><strong>Disclaimer:</strong> This answer is ~7 years old. Definitely, it needs to be updated to newer Python and NLTK versions. Please, try to do it yourself, and if it works, share your know-how with us.</p>\n<p>Take a look at <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"nofollow noreferrer\">Stanford Named Entity Recognizer (NER)</a>. Its binding has been included in NLTK v 2.0, but you must download some core files. Here is <a href=\"https://gist.github.com/troyane/c9355a3103ea08679baf\" rel=\"nofollow noreferrer\">script</a> which can do all of that for you.</p>\n<p>I wrote this script:</p>\n<pre><code class=\"python\">import nltk\nfrom nltk.tag.stanford import NERTagger\nst = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\ntext = \"\"\"YOUR TEXT GOES HERE\"\"\"\n\nfor sent in nltk.sent_tokenize(text):\n    tokens = nltk.tokenize.word_tokenize(sent)\n    tags = st.tag(tokens)\n    for tag in tags:\n        if tag[1]=='PERSON': print tag\n</code></pre>\n<p>and got not so bad output:</p>\n<blockquote>\n<p>('Francois', 'PERSON')\n('R.', 'PERSON')\n('Velde', 'PERSON')\n('Richard', 'PERSON')\n('Branson', 'PERSON')\n('Virgin', 'PERSON')\n('Galactic', 'PERSON')\n('Bitcoin', 'PERSON')\n('Bitcoin', 'PERSON')\n('Paul', 'PERSON')\n('Krugman', 'PERSON')\n('Larry', 'PERSON')\n('Summers', 'PERSON')\n('Bitcoin', 'PERSON')\n('Nick', 'PERSON')\n('Colas', 'PERSON')</p>\n</blockquote>\n<p>Hope this is helpful.</p>\n", "abstract": "Must agree with the suggestion that \"make my code better\" isn't well suited for this site, but I can give you some way where you can try to dig in. Disclaimer: This answer is ~7 years old. Definitely, it needs to be updated to newer Python and NLTK versions. Please, try to do it yourself, and if it works, share your know-how with us. Take a look at Stanford Named Entity Recognizer (NER). Its binding has been included in NLTK v 2.0, but you must download some core files. Here is script which can do all of that for you. I wrote this script: and got not so bad output: ('Francois', 'PERSON')\n('R.', 'PERSON')\n('Velde', 'PERSON')\n('Richard', 'PERSON')\n('Branson', 'PERSON')\n('Virgin', 'PERSON')\n('Galactic', 'PERSON')\n('Bitcoin', 'PERSON')\n('Bitcoin', 'PERSON')\n('Paul', 'PERSON')\n('Krugman', 'PERSON')\n('Larry', 'PERSON')\n('Summers', 'PERSON')\n('Bitcoin', 'PERSON')\n('Nick', 'PERSON')\n('Colas', 'PERSON') Hope this is helpful."}, {"id": 28728926, "score": 12, "vote": 0, "content": "<p>For anyone else looking, I found this article to be useful: <a href=\"http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code\" rel=\"noreferrer\">http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code</a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; def extract_entities(text):\n...     for sent in nltk.sent_tokenize(text):\n...         for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n...             if hasattr(chunk, 'node'):\n...                 print chunk.node, ' '.join(c[0] for c in chunk.leaves())\n...\n</code></pre>\n", "abstract": "For anyone else looking, I found this article to be useful: http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code"}, {"id": 45062402, "score": 6, "vote": 0, "content": "<p>The answer of @trojane didn't quite work for me, but helped a lot for this one.</p>\n<h2>Prerequesites</h2>\n<p>Create a folder <code>stanford-ner</code> and download the following two files to it:</p>\n<ul>\n<li><a href=\"https://github.com/wolfgangmm/exist-stanford-ner/blob/master/resources/classifiers/english.all.3class.distsim.crf.ser.gz\" rel=\"nofollow noreferrer\">english.all.3class.distsim.crf.ser.gz</a></li>\n<li><a href=\"https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\" rel=\"nofollow noreferrer\">stanford-ner.jar</a> (Look for <a href=\"https://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"nofollow noreferrer\">download</a> and extract the archive)</li>\n</ul>\n<h2>Script</h2>\n<pre><code class=\"python\">#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport nltk\nfrom nltk.tag.stanford import StanfordNERTagger\n\ntext = u\"\"\"\nSome economists have responded positively to Bitcoin, including\nFrancois R. Velde, senior economist of the Federal Reserve in Chicago\nwho described it as \"an elegant solution to the problem of creating a\ndigital currency.\" In November 2013 Richard Branson announced that\nVirgin Galactic would accept Bitcoin as payment, saying that he had invested\nin Bitcoin and found it \"fascinating how a whole new global currency\nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical.\nEconomist Paul Krugman has suggested that the structure of the currency\nincentivizes hoarding and that its value derives from the expectation that\nothers will accept it as payment. Economist Larry Summers has expressed\na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market\nstrategist for ConvergEx Group, has remarked on the effect of increasing\nuse of Bitcoin and its restricted supply, noting, \"When incremental\nadoption meets relatively fixed supply, it should be no surprise that\nprices go up. And that\u2019s exactly what is happening to BTC prices.\n\"\"\"\n\nst = StanfordNERTagger('stanford-ner/english.all.3class.distsim.crf.ser.gz',\n                       'stanford-ner/stanford-ner.jar')\n\nfor sent in nltk.sent_tokenize(text):\n    tokens = nltk.tokenize.word_tokenize(sent)\n    tags = st.tag(tokens)\n    for tag in tags:\n        if tag[1] in [\"PERSON\", \"LOCATION\", \"ORGANIZATION\"]:\n            print(tag)\n</code></pre>\n<h2>Results</h2>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">('Bitcoin', 'LOCATION')       # wrong\n('Francois', 'PERSON')\n('R.', 'PERSON')\n('Velde', 'PERSON')\n('Federal', 'ORGANIZATION')\n('Reserve', 'ORGANIZATION')\n('Chicago', 'LOCATION')\n('Richard', 'PERSON')\n('Branson', 'PERSON')\n('Virgin', 'PERSON')         # Wrong\n('Galactic', 'PERSON')       # Wrong\n('Bitcoin', 'PERSON')        # Wrong\n('Bitcoin', 'LOCATION')      # Wrong\n('Bitcoin', 'LOCATION')      # Wrong\n('Paul', 'PERSON')\n('Krugman', 'PERSON')\n('Larry', 'PERSON')\n('Summers', 'PERSON')\n('Bitcoin', 'PERSON')        # Wrong\n('Nick', 'PERSON')\n('Colas', 'PERSON')\n('ConvergEx', 'ORGANIZATION')\n('Group', 'ORGANIZATION')     \n('Bitcoin', 'LOCATION')       # Wrong\n('BTC', 'ORGANIZATION')       # Wrong\n</code></pre>\n", "abstract": "The answer of @trojane didn't quite work for me, but helped a lot for this one. Create a folder stanford-ner and download the following two files to it:"}, {"id": 49500219, "score": 6, "vote": 0, "content": "<p>I actually wanted to extract only the person name, so, thought to check all the names that come as an output against wordnet( A large lexical database of English).\nMore Information on Wordnet can be found here: <a href=\"http://www.nltk.org/howto/wordnet.html\" rel=\"nofollow noreferrer\">http://www.nltk.org/howto/wordnet.html</a></p>\n<pre><code class=\"python\">import nltk\nfrom nameparser.parser import HumanName\nfrom nltk.corpus import wordnet\n\n\nperson_list = []\nperson_names=person_list\ndef get_human_names(text):\n    tokens = nltk.tokenize.word_tokenize(text)\n    pos = nltk.pos_tag(tokens)\n    sentt = nltk.ne_chunk(pos, binary = False)\n\n    person = []\n    name = \"\"\n    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n        for leaf in subtree.leaves():\n            person.append(leaf[0])\n        if len(person) &gt; 1: #avoid grabbing lone surnames\n            for part in person:\n                name += part + ' '\n            if name[:-1] not in person_list:\n                person_list.append(name[:-1])\n            name = ''\n        person = []\n#     print (person_list)\n\ntext = \"\"\"\n\nSome economists have responded positively to Bitcoin, including \nFrancois R. Velde, senior economist of the Federal Reserve in Chicago \nwho described it as \"an elegant solution to the problem of creating a \ndigital currency.\" In November 2013 Richard Branson announced that \nVirgin Galactic would accept Bitcoin as payment, saying that he had invested \nin Bitcoin and found it \"fascinating how a whole new global currency \nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical. \nEconomist Paul Krugman has suggested that the structure of the currency \nincentivizes hoarding and that its value derives from the expectation that \nothers will accept it as payment. Economist Larry Summers has expressed \na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \nstrategist for ConvergEx Group, has remarked on the effect of increasing \nuse of Bitcoin and its restricted supply, noting, \"When incremental \nadoption meets relatively fixed supply, it should be no surprise that \nprices go up. And that\u2019s exactly what is happening to BTC prices.\"\n\"\"\"\n\nnames = get_human_names(text)\nfor person in person_list:\n    person_split = person.split(\" \")\n    for name in person_split:\n        if wordnet.synsets(name):\n            if(name in person):\n                person_names.remove(person)\n                break\n\nprint(person_names)\n</code></pre>\n<p>OUTPUT</p>\n<pre><code class=\"python\">['Francois R. Velde', 'Richard Branson', 'Economist Paul Krugman', 'Nick Colas']\n</code></pre>\n<p>Apart from Larry Summers all the names are correct and that is because of the last name \"Summers\". </p>\n", "abstract": "I actually wanted to extract only the person name, so, thought to check all the names that come as an output against wordnet( A large lexical database of English).\nMore Information on Wordnet can be found here: http://www.nltk.org/howto/wordnet.html OUTPUT Apart from Larry Summers all the names are correct and that is because of the last name \"Summers\". "}, {"id": 20460835, "score": 5, "vote": 0, "content": "<p>You can try to do resolution of the found names, and check if you can find them in a database such as freebase.com. Get the data locally and query it (it's in RDF), or use google's api: <a href=\"https://developers.google.com/freebase/v1/getting-started\" rel=\"nofollow noreferrer\">https://developers.google.com/freebase/v1/getting-started</a>. Most big companies, geographical locations, etc. (that would be caught by your snippet) could be then discarded based on the freebase data. </p>\n", "abstract": "You can try to do resolution of the found names, and check if you can find them in a database such as freebase.com. Get the data locally and query it (it's in RDF), or use google's api: https://developers.google.com/freebase/v1/getting-started. Most big companies, geographical locations, etc. (that would be caught by your snippet) could be then discarded based on the freebase data. "}, {"id": 56905694, "score": 2, "vote": 0, "content": "<p>I would like to post a brutal and greedy solution here to solve the problem cast by @Enthusiast: get the full name of a person if possible.</p>\n<p>The capitalization of the first character in each name is used as a criterion for recognizing PERSON in <code>Spacy</code>. For example, 'jim hoffman' itself won't be recognized as a named entity, while 'Jim Hoffman' will be. </p>\n<p>Therefore, if our task is simply picking out persons from a script, we may simply first capitalize the first letter of each word, and then dump it to <code>spacy</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import spacy\n\ndef capitalizeWords(text):\n\n  newText = ''\n\n  for sentence in text.split('.'):\n    newSentence = ''\n    for word in sentence.split():\n      newSentence += word+' '\n    newText += newSentence+'\\n'\n\n  return newText\n\nnlp = spacy.load('en_core_web_md')\n\ndoc = nlp(capitalizeWords(rawText))\n\n#......\n\n</code></pre>\n<p>Note that this approach covers full names at the cost of the increasing of false positives.</p>\n", "abstract": "I would like to post a brutal and greedy solution here to solve the problem cast by @Enthusiast: get the full name of a person if possible. The capitalization of the first character in each name is used as a criterion for recognizing PERSON in Spacy. For example, 'jim hoffman' itself won't be recognized as a named entity, while 'Jim Hoffman' will be.  Therefore, if our task is simply picking out persons from a script, we may simply first capitalize the first letter of each word, and then dump it to spacy. Note that this approach covers full names at the cost of the increasing of false positives."}, {"id": 38613988, "score": 1, "vote": 0, "content": "<p>This worked pretty well for me. I just had to change one line in order for it to run. </p>\n<pre><code class=\"python\">    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n</code></pre>\n<p>needs to be</p>\n<pre><code class=\"python\">    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n</code></pre>\n<p>There were imperfections in the output (for example it identified \"Money Laundering\" as a person), but with my data a name database may not be dependable. </p>\n", "abstract": "This worked pretty well for me. I just had to change one line in order for it to run.  needs to be There were imperfections in the output (for example it identified \"Money Laundering\" as a person), but with my data a name database may not be dependable. "}]}, {"link": "https://stackoverflow.com/questions/13603882/feature-selection-and-reduction-for-text-classification", "question": {"id": "13603882", "title": "Feature Selection and Reduction for Text Classification", "content": "<p>I am currently working on a project, a <strong>simple sentiment analyzer</strong> such that there will be <strong>2 and 3 classes</strong> in <strong>separate cases</strong>. I am using a <strong>corpus</strong> that is pretty <strong>rich</strong> in the means of <strong>unique words</strong> (around 200.000). I used <strong>bag-of-words</strong> method for <strong>feature selection</strong> and to reduce the number of <strong>unique features</strong>, an elimination is done due to a <strong>threshold value</strong> of <strong>frequency of occurrence</strong>. The <strong>final set of features</strong> includes around 20.000 features, which is actually a <strong>90% decrease</strong>, but <strong>not enough</strong> for intended <strong>accuracy</strong> of test-prediction. I am using <strong>LibSVM</strong> and <strong>SVM-light</strong> in turn for training and prediction (both <strong>linear</strong> and <strong>RBF kernel</strong>) and also <strong>Python</strong> and <strong>Bash</strong> in general.</p>\n<p>The <strong>highest accuracy</strong> observed so far <strong>is around 75%</strong> and I <strong>need at least 90%</strong>. This is the case for <strong>binary classification</strong>. For <strong>multi-class training</strong>, the accuracy falls to <strong>~60%</strong>. I <strong>need at least 90%</strong> at both cases and can not figure how to increase it: via <strong>optimizing training parameters</strong> or <strong>via optimizing feature selection</strong>?</p>\n<p>I have read articles about <strong>feature selection</strong> in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:</p>\n<ul>\n<li>Frequency approach of <strong>bag-of-words</strong> (BOW)</li>\n<li><strong>Information Gain</strong> (IG)</li>\n<li><strong>X^2 Statistic</strong> (CHI)</li>\n</ul>\n<p>The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of <strong>IG</strong> and <strong>CHI</strong> and looking for any help to guide me in that way.</p>\n<p>Thanks a lot, and if you need any additional info for help, just let me know.</p>\n<hr/>\n<ul>\n<li><p>@larsmans: <strong>Frequency Threshold</strong>: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   </p></li>\n<li><p>@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found <strong>Proportional Difference</strong> (PD) method the best for feature selection, where features are uni-grams and <strong>Term Presence</strong> (TP) for the weighting (I didn't understand why you tagged <strong>Term-Frequency-Inverse-Document-Frequency</strong> (TF-IDF) as an indexing method, I rather consider it as a <strong>feature weighting</strong> approach).  <strong>Pre-processing</strong> is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as <strong>morphological parsing</strong> and <strong>stemming</strong>. Also note that I am working on <strong>Turkish</strong>, which has <strong>different characteristics</strong> compared to English. Finally, I managed to reach <strong>~88% accuracy</strong> (f-measure) for <strong>binary</strong> classification and <strong>~84%</strong> for <strong>multi-class</strong>. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried <strong>LDA</strong> and <strong>LSI</strong> and moving on to <strong>moVMF</strong> and maybe <strong>spherical models</strong> (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between <strong>feature space dimension reduction</strong> methods (LDA, LSI, moVMF etc.) and <strong>clustering methods</strong> (k-means, hierarchical etc.).</p></li>\n</ul>\n", "abstract": "I am currently working on a project, a simple sentiment analyzer such that there will be 2 and 3 classes in separate cases. I am using a corpus that is pretty rich in the means of unique words (around 200.000). I used bag-of-words method for feature selection and to reduce the number of unique features, an elimination is done due to a threshold value of frequency of occurrence. The final set of features includes around 20.000 features, which is actually a 90% decrease, but not enough for intended accuracy of test-prediction. I am using LibSVM and SVM-light in turn for training and prediction (both linear and RBF kernel) and also Python and Bash in general. The highest accuracy observed so far is around 75% and I need at least 90%. This is the case for binary classification. For multi-class training, the accuracy falls to ~60%. I need at least 90% at both cases and can not figure how to increase it: via optimizing training parameters or via optimizing feature selection? I have read articles about feature selection in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows: The first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of IG and CHI and looking for any help to guide me in that way. Thanks a lot, and if you need any additional info for help, just let me know. @larsmans: Frequency Threshold: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.    @TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found Proportional Difference (PD) method the best for feature selection, where features are uni-grams and Term Presence (TP) for the weighting (I didn't understand why you tagged Term-Frequency-Inverse-Document-Frequency (TF-IDF) as an indexing method, I rather consider it as a feature weighting approach).  Pre-processing is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as morphological parsing and stemming. Also note that I am working on Turkish, which has different characteristics compared to English. Finally, I managed to reach ~88% accuracy (f-measure) for binary classification and ~84% for multi-class. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried LDA and LSI and moving on to moVMF and maybe spherical models (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between feature space dimension reduction methods (LDA, LSI, moVMF etc.) and clustering methods (k-means, hierarchical etc.)."}, "answers": [{"id": 15461587, "score": 39, "vote": 0, "content": "<p>This is probably a bit late to the table, but...</p>\n<p>As Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption.</p>\n<p>Do you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably.</p>\n<p>Have you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information.</p>\n<p>Its also worth noting that dimension reduction <strong>is</strong> feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space.</p>\n<p>Regarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set.</p>\n<p>Also how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset.</p>\n<p>Personally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost.</p>\n<hr/>\n<p><strong>Additional</strong></p>\n<p>Based on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned.</p>\n<p>Having said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here:</p>\n<p><a href=\"http://www.cs.technion.ac.il/~gabr/papers/fs-svm.pdf\" rel=\"noreferrer\" title=\"Text Categorization with Many Redundant Features: Using Aggressive Feature Selection to Make SVMs Competitive with C4.5\">Paper with Outlier Count information</a></p>\n<p>With regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation.</p>\n<p>In respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. </p>\n", "abstract": "This is probably a bit late to the table, but... As Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption. Do you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably. Have you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information. Its also worth noting that dimension reduction is feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space. Regarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set. Also how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset. Personally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost. Additional Based on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned. Having said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here: Paper with Outlier Count information With regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation. In respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. "}, {"id": 13644397, "score": 5, "vote": 0, "content": "<p>I would recommend dimensionality reduction instead of feature selection. Consider either <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\" rel=\"nofollow\">singular value decomposition</a>, <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\" rel=\"nofollow\">principal component analysis</a>, or even better considering it's tailored for bag-of-words representations, <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"nofollow\">Latent Dirichlet Allocation</a>. This will allow you to notionally retain representations that include all words, but to collapse them to fewer dimensions by exploiting similarity (or even synonymy-type) relations between them.</p>\n<p>All these methods have fairly standard implementations that you can get access to and run---if you let us know which language you're using, I or someone else will be able to point you in the right direction.</p>\n", "abstract": "I would recommend dimensionality reduction instead of feature selection. Consider either singular value decomposition, principal component analysis, or even better considering it's tailored for bag-of-words representations, Latent Dirichlet Allocation. This will allow you to notionally retain representations that include all words, but to collapse them to fewer dimensions by exploiting similarity (or even synonymy-type) relations between them. All these methods have fairly standard implementations that you can get access to and run---if you let us know which language you're using, I or someone else will be able to point you in the right direction."}, {"id": 62043479, "score": 4, "vote": 0, "content": "<p>There's a <code>python</code> library for feature selection\n<code>TextFeatureSelection</code>.  This library provides discriminatory power in the form of score for each word token, bigram, trigram etc.</p>\n<p>Those who are aware of feature selection methods in machine learning, it is based on <em>filter method</em> and provides ML engineers required tools to improve the classification accuracy in their NLP and deep learning models. It has 4 methods namely <strong>Chi-square</strong>, <strong>Mutual information</strong>, <strong>Proportional difference</strong>\u00a0and <strong>Information gain</strong> to help select words as features before being fed into machine learning classifiers.</p>\n<pre><code class=\"python\">from TextFeatureSelection import TextFeatureSelection\n\n#Multiclass classification problem\ninput_doc_list=['i am very happy','i just had an awesome weekend','this is a very difficult terrain to trek. i wish i stayed back at home.','i just had lunch','Do you want chips?']\ntarget=['Positive','Positive','Negative','Neutral','Neutral']\nfsOBJ=TextFeatureSelection(target=target,input_doc_list=input_doc_list)\nresult_df=fsOBJ.getScore()\nprint(result_df)\n\n#Binary classification\ninput_doc_list=['i am content with this location','i am having the time of my life','you cannot learn machine learning without linear algebra','i want to go to mars']\ntarget=[1,1,0,1]\nfsOBJ=TextFeatureSelection(target=target,input_doc_list=input_doc_list)\nresult_df=fsOBJ.getScore()\nprint(result_df)\n</code></pre>\n<p><strong>Edit:</strong></p>\n<p>It now has genetic algorithm for feature selection as well.</p>\n<pre><code class=\"python\">from TextFeatureSelection import TextFeatureSelectionGA\n#Input documents: doc_list\n#Input labels: label_list\ngetGAobj=TextFeatureSelectionGA(percentage_of_token=60)\nbest_vocabulary=getGAobj.getGeneticFeatures(doc_list=doc_list,label_list=label_list)\n</code></pre>\n<p><strong>Edit2</strong></p>\n<p>There is another method now<code>TextFeatureSelectionEnsemble</code>, which combines feature selection while ensembling. It does feature selection for base models through document frequency thresholds. At ensemble layer, it uses genetic algorithm to identify best combination of base models and keeps only those.</p>\n<pre><code class=\"python\">from TextFeatureSelection import TextFeatureSelectionEnsemble \n\nimdb_data=pd.read_csv('../input/IMDB Dataset.csv')\nle = LabelEncoder()\nimdb_data['labels'] = le.fit_transform(imdb_data['sentiment'].values)\n\n#convert raw text and labels to python list\ndoc_list=imdb_data['review'].tolist()\nlabel_list=imdb_data['labels'].tolist()\n\n#Initialize parameter for TextFeatureSelectionEnsemble and start training\ngaObj=TextFeatureSelectionEnsemble(doc_list,label_list,n_crossvalidation=2,pickle_path='/home/user/folder/',average='micro',base_model_list=['LogisticRegression','RandomForestClassifier','ExtraTreesClassifier','KNeighborsClassifier'])\nbest_columns=gaObj.doTFSE()`\n</code></pre>\n<p>Check the project for details: <a href=\"https://pypi.org/project/TextFeatureSelection/\" rel=\"nofollow noreferrer\">https://pypi.org/project/TextFeatureSelection/</a></p>\n", "abstract": "There's a python library for feature selection\nTextFeatureSelection.  This library provides discriminatory power in the form of score for each word token, bigram, trigram etc. Those who are aware of feature selection methods in machine learning, it is based on filter method and provides ML engineers required tools to improve the classification accuracy in their NLP and deep learning models. It has 4 methods namely Chi-square, Mutual information, Proportional difference\u00a0and Information gain to help select words as features before being fed into machine learning classifiers. Edit: It now has genetic algorithm for feature selection as well. Edit2 There is another method nowTextFeatureSelectionEnsemble, which combines feature selection while ensembling. It does feature selection for base models through document frequency thresholds. At ensemble layer, it uses genetic algorithm to identify best combination of base models and keeps only those. Check the project for details: https://pypi.org/project/TextFeatureSelection/"}, {"id": 13615685, "score": 1, "vote": 0, "content": "<p>Linear svm is recommended for high dimensional features. Based on my experience the ultimate limitation of SVM accuracy depends on the positive and negative \"features\". You can do a grid search (or in the case of linear svm you can just search for the best cost value) to find the optimal parameters for maximum accuracy, but in the end you are limited by the separability of your feature-sets. The fact that you are not getting 90% means that you still have some work to do finding better features to describe your members of the classes.</p>\n", "abstract": "Linear svm is recommended for high dimensional features. Based on my experience the ultimate limitation of SVM accuracy depends on the positive and negative \"features\". You can do a grid search (or in the case of linear svm you can just search for the best cost value) to find the optimal parameters for maximum accuracy, but in the end you are limited by the separability of your feature-sets. The fact that you are not getting 90% means that you still have some work to do finding better features to describe your members of the classes."}, {"id": 24150769, "score": 1, "vote": 0, "content": "<p>I'm sure this is way too late to be of use to the poster, but perhaps it will be useful to someone else. The chi-squared approach to feature reduction is pretty simple to implement. Assuming BoW binary classification into classes C1 and C2, for each feature f in candidate_features calculate the freq of f in C1; calculate total words C1; repeat calculations for C2; Calculate a chi-sqaure determine filter candidate_features based on whether p-value is below a certain threshold (e.g. p &lt; 0.05). A tutorial using Python and nltk can been seen here: <a href=\"http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/\" rel=\"nofollow\">http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/</a> (though if I remember correctly, I believe the author incorrectly applies this technique to his test data, which biases the reported results).</p>\n", "abstract": "I'm sure this is way too late to be of use to the poster, but perhaps it will be useful to someone else. The chi-squared approach to feature reduction is pretty simple to implement. Assuming BoW binary classification into classes C1 and C2, for each feature f in candidate_features calculate the freq of f in C1; calculate total words C1; repeat calculations for C2; Calculate a chi-sqaure determine filter candidate_features based on whether p-value is below a certain threshold (e.g. p < 0.05). A tutorial using Python and nltk can been seen here: http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/ (though if I remember correctly, I believe the author incorrectly applies this technique to his test data, which biases the reported results)."}]}, {"link": "https://stackoverflow.com/questions/2661778/tag-generation-from-a-text-content", "question": {"id": "2661778", "title": "tag generation from a text content", "content": "<p>I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.</p>\n<p>Additionally, I will be grateful if you point any Python based solution / library for this.</p>\n<p>Thanks</p>\n", "abstract": "I am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools. Additionally, I will be grateful if you point any Python based solution / library for this. Thanks"}, "answers": [{"id": 2664351, "score": 59, "vote": 0, "content": "<p>One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document.</p>\n<p>To identify keywords like this, you could use the <a href=\"http://en.wikipedia.org/wiki/Pointwise_mutual_information\" rel=\"noreferrer\">point-wise mutual information</a> of the keyword and the document. This is given by <code>PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]</code>. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection.</p>\n<p>To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score. </p>\n<p>If you want to extract <strong>multiword tags</strong>, see the StackOverflow question <a href=\"https://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries\">How to extract common / significant phrases from a series of text entries</a>. </p>\n<p>Borrowing from my answer to that question, the <a href=\"http://www.nltk.org/howto/collocations.html\" rel=\"noreferrer\">NLTK collocations how-to</a> covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:</p>\n<pre><code class=\"python\">import nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n</code></pre>\n", "abstract": "One way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document. To identify keywords like this, you could use the point-wise mutual information of the keyword and the document. This is given by PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection. To identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score.  If you want to extract multiword tags, see the StackOverflow question How to extract common / significant phrases from a series of text entries.  Borrowing from my answer to that question, the NLTK collocations how-to covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:"}, {"id": 2663346, "score": 10, "vote": 0, "content": "<p>First, the key python library for computational linguistics is <a href=\"http://nltk.sourceforge.net/index.php/Main_Page\" rel=\"noreferrer\">NLTK</a> (\"<strong>Natural Language Toolkit</strong>\"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">collection</a> of tutorials, FAQs, etc. I recommend it highly.</p>\n<p>Below is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic.</p>\n<p>Keywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a <em>priori</em> knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency.</p>\n<pre><code class=\"python\">text = \"\"\" The intensity of the feeling makes up for the disproportion of the objects.  Things are equal to the imagination, which have the power of affecting the mind with an equal degree of terror, admiration, delight, or love.  When Lear calls upon the heavens to avenge his cause, \"for they are old like him,\" there is nothing extravagant or impious in this sublime identification of his age with theirs; for there is no other image which could do justice to the agonising sense of his wrongs and his despair! \"\"\"\n\nBAD_CHARS = \".!?,\\'\\\"\"\n\n# transform text into a list words--removing punctuation and filtering small words\nwords = [ word.strip(BAD_CHARS) for word in text.strip().split() if len(word) &gt; 4 ]\n\nword_freq = {}\n\n# generate a 'word histogram' for the text--ie, a list of the frequencies of each word\nfor word in words :\n  word_freq[word] = word_freq.get(word, 0) + 1\n\n# sort the word list by frequency \n# (just a DSU sort, there's a python built-in for this, but i can't remember it)\ntx = [ (v, k) for (k, v) in word_freq.items()]\ntx.sort(reverse=True)\nword_freq_sorted = [ (k, v) for (v, k) in tx ]\n\n# eg, what are the most common words in that text?\nprint(word_freq_sorted)\n# returns: [('which', 4), ('other', 4), ('like', 4), ('what', 3), ('upon', 3)]\n# obviously using a text larger than 50 or so words will give you more meaningful results\n\nterm_importance = lambda word : 1.0/word_freq[word]\n\n# select document keywords from the words at/near the top of this list:\nmap(term_importance, word_freq.keys())\n</code></pre>\n", "abstract": "First, the key python library for computational linguistics is NLTK (\"Natural Language Toolkit\"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive collection of tutorials, FAQs, etc. I recommend it highly. Below is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic. Keywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a priori knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency."}, {"id": 2664255, "score": 5, "vote": 0, "content": "<p><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a> tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.  </p>\n<p>I had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.  </p>\n<p>I recommend you try playing with <a href=\"http://mallet.cs.umass.edu/\" rel=\"noreferrer\">http://mallet.cs.umass.edu/</a> and seeing if this model fits your needs..  </p>\n<p>LDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give.</p>\n", "abstract": "http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.   I had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.   I recommend you try playing with http://mallet.cs.umass.edu/ and seeing if this model fits your needs..   LDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give."}, {"id": 2661789, "score": 2, "vote": 0, "content": "<p>A very simple solution to the problem would be:</p>\n<ul>\n<li>count the occurences of each word in the text</li>\n<li>consider the most frequent terms as the key phrases</li>\n<li>have a black-list of 'stop words' to remove common words like the, and, it, is etc</li>\n</ul>\n<p>I'm sure there are cleverer, stats based solutions though.</p>\n<p>If you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method.</p>\n", "abstract": "A very simple solution to the problem would be: I'm sure there are cleverer, stats based solutions though. If you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method."}, {"id": 64285526, "score": 0, "vote": 0, "content": "<p><a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"nofollow noreferrer\">Latent Dirichlet allocation</a> or <a href=\"https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process\" rel=\"nofollow noreferrer\">Hierarchical Dirichlet Process</a> can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics.</p>\n<p>A basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts.</p>\n<p>With a general reference for preprocessing needed for these codes being found <a href=\"https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#20topicdistributionacrossdocuments\" rel=\"nofollow noreferrer\">here</a>, we can find tags through the following process using <a href=\"https://github.com/RaRe-Technologies/gensim\" rel=\"nofollow noreferrer\">gensim</a>.</p>\n<p>A heuristic way of deriving the optimal number of topics for LDA is found in <a href=\"https://stackoverflow.com/a/64370066/12446118\">this answer</a>. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from gensim.models import LdaModel, HdpModel\nfrom gensim import corpora\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">num_topics = 10\nnum_tags = 5\n</code></pre>\n<p>Assume further that we have a variable <code>corpus</code>, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words):</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">dirichlet_dict = corpora.Dictionary(corpus)\nbow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n</code></pre>\n<p>Create an LDA or HDP model:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">dirichlet_model = LdaModel(corpus=bow_corpus,\n                           id2word=dirichlet_dict,\n                           num_topics=num_topics,\n                           update_every=1,\n                           chunksize=len(bow_corpus),\n                           passes=20,\n                           alpha='auto')\n\n# dirichlet_model = HdpModel(corpus=bow_corpus, \n#                            id2word=dirichlet_dict,\n#                            chunksize=len(bow_corpus))\n</code></pre>\n<p>The following code produces ordered lists for the most important words per topic (note that here is where <code>num_tags</code> defines the desired tags per text):</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">shown_topics = dirichlet_model.show_topics(num_topics=num_topics, \n                                           num_words=num_tags,\n                                           formatted=False)\nmodel_topics = [[word[0] for word in topic[1]] for topic in shown_topics]\n</code></pre>\n<p>Then find the coherence of the topics across the texts:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">topic_corpus = dirichlet_model.__getitem__(bow=bow_corpus, eps=0) # cutoff probability to 0 \ntopics_per_text = [text for text in topic_corpus]\n</code></pre>\n<p>From here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">corpus_tags = []\n\nfor i in range(len(bow_corpus)):\n    # The complexity here is to make sure that it works with HDP\n    significant_topics = list(set([t[0] for t in topics_per_text[i]]))\n    topic_indexes_by_coherence = [tup[0] for tup in sorted(enumerate(topics_per_text[i]), key=lambda x:x[1])]\n    significant_topics_by_coherence = [significant_topics[i] for i in topic_indexes_by_coherence]\n\n    ordered_topics = [model_topics[i] for i in significant_topics_by_coherence][:num_topics] # subset for HDP\n    ordered_topic_coherences = [topics_per_text[i] for i in topic_indexes_by_coherence][:num_topics] # subset for HDP\n\n    text_tags = []\n    for i in range(num_topics):\n        # Find the number of indexes to select, which can later be extended if the word has already been selected\n        selection_indexes = list(range(int(round(num_tags * ordered_topic_coherences[i]))))\n        if selection_indexes == [] and len(text_tags) &lt; num_tags: \n            # Fix potential rounding error by giving this topic one selection\n            selection_indexes = [0]\n              \n        for s_i in selection_indexes:\n            # ignore_words is a list of words should not be included\n            if ordered_topics[i][s_i] not in text_tags and ordered_topics[i][s_i] not in ignore_words:\n                text_tags.append(ordered_topics[i][s_i])\n            else:\n                selection_indexes.append(selection_indexes[-1] + 1)\n\n    # Fix for if too many were selected\n    text_tags = text_tags[:num_tags]\n\n    corpus_tags.append(text_tags)\n</code></pre>\n<p><code>corpus_tags</code> will be a list of tags for each text based on how coherent the text is to the derived topics.</p>\n<p>See <a href=\"https://stackoverflow.com/a/64284694/12446118\">this answer</a> for a similar version of this that generates tags for a whole text corpus.</p>\n", "abstract": "Latent Dirichlet allocation or Hierarchical Dirichlet Process can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics. A basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts. With a general reference for preprocessing needed for these codes being found here, we can find tags through the following process using gensim. A heuristic way of deriving the optimal number of topics for LDA is found in this answer. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text: Assume further that we have a variable corpus, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words): Create an LDA or HDP model: The following code produces ordered lists for the most important words per topic (note that here is where num_tags defines the desired tags per text): Then find the coherence of the topics across the texts: From here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following: corpus_tags will be a list of tags for each text based on how coherent the text is to the derived topics. See this answer for a similar version of this that generates tags for a whole text corpus."}]}, {"link": "https://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data", "question": {"id": "11333903", "title": "NLTK Named Entity Recognition with Custom Data", "content": "<p>I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-</p>\n<ol>\n<li>Can I use my own data to train an Named Entity Recognizer in NLTK?</li>\n<li>If I can train using my own data, is the named_entity.py the file to be modified?</li>\n<li>Does the input file format have to be in IOB eg. Eric NNP B-PERSON ?</li>\n<li>Are there any resources - apart from the nltk cookbook and nlp with python that I can use?</li>\n</ol>\n<p>I would really appreciate help in this regard</p>\n", "abstract": "I'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK- I would really appreciate help in this regard"}, "answers": [{"id": 11399077, "score": 24, "vote": 0, "content": "<p>Are you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: <a href=\"http://nlp.stanford.edu/software/CRF-NER.shtml\" rel=\"noreferrer\">http://nlp.stanford.edu/software/CRF-NER.shtml</a>.  The process for training the classifier using your own data is very well-documented in the FAQ.  </p>\n<p>If you really need to use NLTK, I'd hit up the mailing list for some advice from other users: <a href=\"http://groups.google.com/group/nltk-users\" rel=\"noreferrer\">http://groups.google.com/group/nltk-users</a>.  </p>\n<p>Hope this helps!</p>\n", "abstract": "Are you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: http://nlp.stanford.edu/software/CRF-NER.shtml.  The process for training the classifier using your own data is very well-documented in the FAQ.   If you really need to use NLTK, I'd hit up the mailing list for some advice from other users: http://groups.google.com/group/nltk-users.   Hope this helps!"}, {"id": 33909254, "score": 14, "vote": 0, "content": "<p>You can easily use the Stanford NER alongwith nltk.\nThe python script is like</p>\n<pre><code class=\"python\">from nltk.tag.stanford import NERTagger\nimport os\njava_path = \"/Java/jdk1.8.0_45/bin/java.exe\"\nos.environ['JAVAHOME'] = java_path\nst = NERTagger('../ner-model.ser.gz','../stanford-ner.jar')\ntagging = st.tag(text.split())   \n</code></pre>\n<p>To train your own data and to create a model you can refer to the first question on Stanford NER FAQ.</p>\n<p>The link is <a href=\"http://nlp.stanford.edu/software/crf-faq.shtml\">http://nlp.stanford.edu/software/crf-faq.shtml</a></p>\n", "abstract": "You can easily use the Stanford NER alongwith nltk.\nThe python script is like To train your own data and to create a model you can refer to the first question on Stanford NER FAQ. The link is http://nlp.stanford.edu/software/crf-faq.shtml"}, {"id": 46529010, "score": 1, "vote": 0, "content": "<p>I also had this issue, but I managed to work it out. \nYou can use your own training data. I documented the main requirements/steps for this in my <a href=\"https://github.com/arop/ner-re-pt/wiki/NLTK\" rel=\"nofollow noreferrer\">github repository</a>.</p>\n<p>I used <a href=\"https://github.com/japerk/nltk-trainer\" rel=\"nofollow noreferrer\">NLTK-trainer</a>, so basicly you have to get the training data in the right format (token NNP B-tag), and run the training script. Check my repository for more info.</p>\n", "abstract": "I also had this issue, but I managed to work it out. \nYou can use your own training data. I documented the main requirements/steps for this in my github repository. I used NLTK-trainer, so basicly you have to get the training data in the right format (token NNP B-tag), and run the training script. Check my repository for more info."}, {"id": 51585044, "score": 1, "vote": 0, "content": "<p>There are some functions in the <a href=\"http://www.nltk.org/_modules/nltk/chunk/named_entity.html\" rel=\"nofollow noreferrer\">nltk.chunk.named_entity</a> module that train a NER tagger. However, they were specifically written for ACE corpus and not totally cleaned up, so one will need to write their own training procedures with those as a reference.</p>\n<p>There are also two relatively recent guides (<a href=\"https://nlpforhackers.io/named-entity-extraction/\" rel=\"nofollow noreferrer\">1</a> <a href=\"https://nlpforhackers.io/training-ner-large-dataset/\" rel=\"nofollow noreferrer\">2</a>) online detailing the process of using NLTK to train the GMB corpus.</p>\n<p>However, as mentioned in answers above, now that many tools are available, one really should not need to resort to NLTK if streamlined training process is desired. Toolkits such as CoreNLP and spaCy do a much better job. As using NLTK is not that much different to writing your own training code from scratch, there is not that much value in doing so. NLTK and OpenNLP can be regarded as somehow belonging to a past era before the explosion of recent progress in NLP.</p>\n", "abstract": "There are some functions in the nltk.chunk.named_entity module that train a NER tagger. However, they were specifically written for ACE corpus and not totally cleaned up, so one will need to write their own training procedures with those as a reference. There are also two relatively recent guides (1 2) online detailing the process of using NLTK to train the GMB corpus. However, as mentioned in answers above, now that many tools are available, one really should not need to resort to NLTK if streamlined training process is desired. Toolkits such as CoreNLP and spaCy do a much better job. As using NLTK is not that much different to writing your own training code from scratch, there is not that much value in doing so. NLTK and OpenNLP can be regarded as somehow belonging to a past era before the explosion of recent progress in NLP."}, {"id": 66377050, "score": 0, "vote": 0, "content": "<blockquote>\n<ol start=\"4\">\n<li>Are there any resources - apart from the nltk cookbook and nlp with python that I can use?</li>\n</ol>\n</blockquote>\n<p>You can consider using <code>spaCy</code> to train your own custom data for NER task. Here is an example from this <a href=\"https://stackoverflow.com/questions/66311315/custom-ners-training-with-spacy-3-throws-valueerror/66356372#66356372\">thread</a> to train a model on a custom training set to detect a new entity <code>ANIMAL</code>. The code was fixed and updated for easier reading.</p>\n<pre><code class=\"python\">import random\nimport spacy\nfrom spacy.training import Example\n\nLABEL = 'ANIMAL'\nTRAIN_DATA = [\n    (\"Horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"Do they bite?\", {'entities': []}),\n    (\"horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"horses pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"they pretend to care about your feelings, those horses\", {'entities': [(48, 54, LABEL)]}),\n    (\"horses?\", {'entities': [(0, 6, LABEL)]})\n]\nnlp = spacy.load('en_core_web_sm')  # load existing spaCy model\nner = nlp.get_pipe('ner')\nner.add_label(LABEL)\n\noptimizer = nlp.create_optimizer()\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    for itn in range(20):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in TRAIN_DATA:\n            doc = nlp.make_doc(text)\n            example = Example.from_dict(doc, annotations)\n            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)\n        print(losses)\n\n# test the trained model\ntest_text = 'Do you like horses?'\ndoc = nlp(test_text)\nprint(\"Entities in '%s'\" % test_text)\nfor ent in doc.ents:\n    print(ent.label_, \" -- \", ent.text)\n</code></pre>\n<p>Here are the outputs:</p>\n<pre><code class=\"python\">{'ner': 9.60289144264557}\n{'ner': 8.875474230820478}\n{'ner': 6.370401408220459}\n{'ner': 6.687456469517201}\n... \n{'ner': 1.3796682589133492e-05}\n{'ner': 1.7709562613218738e-05}\n\nEntities in 'Do you like horses?'\nANIMAL  --  horses\n</code></pre>\n", "abstract": "You can consider using spaCy to train your own custom data for NER task. Here is an example from this thread to train a model on a custom training set to detect a new entity ANIMAL. The code was fixed and updated for easier reading. Here are the outputs:"}, {"id": 66507786, "score": 0, "vote": 0, "content": "<p>To complete the answer by @Thang M. Pham, you need to label your data before training. To do so, you can use the <a href=\"https://github.com/ieriii/spacy-annotator\" rel=\"nofollow noreferrer\">spacy-annotator</a>.</p>\n<p>Here is an example taken from another answer:\n<a href=\"https://stackoverflow.com/questions/49483971/train-spacy-ner-on-indian-names/64063515#64063515\">Train Spacy NER on Indian Names</a></p>\n", "abstract": "To complete the answer by @Thang M. Pham, you need to label your data before training. To do so, you can use the spacy-annotator. Here is an example taken from another answer:\nTrain Spacy NER on Indian Names"}]}, {"link": "https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score", "question": {"id": "34232190", "title": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "content": "<p>I am working on keyword extraction problem. Consider the very general case</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n\nt = \"\"\"Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n\n\"How useless is the Plane!\" said one of them. \"It bears no fruit whatever, and only serves to litter the ground with leaves.\"\n\n\"Ungrateful creatures!\" said a voice from the Plane Tree. \"You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!\"\n\nOur best blessings are often the least appreciated.\"\"\"\n\ntfs = tfidf.fit_transform(t.split(\" \"))\nstr = 'tree cat travellers fruit jupiter'\nresponse = tfidf.transform([str])\nfeature_names = tfidf.get_feature_names()\n\nfor col in response.nonzero()[1]:\n    print(feature_names[col], ' - ', response[0, col])\n</code></pre>\n<p>and this gives me</p>\n<pre><code class=\"python\">  (0, 28)   0.443509712811\n  (0, 27)   0.517461475101\n  (0, 8)    0.517461475101\n  (0, 6)    0.517461475101\ntree  -  0.443509712811\ntravellers  -  0.517461475101\njupiter  -  0.517461475101\nfruit  -  0.517461475101\n</code></pre>\n<p>which is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?</p>\n", "abstract": "I am working on keyword extraction problem. Consider the very general case and this gives me which is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?"}, "answers": [{"id": 34236002, "score": 43, "vote": 0, "content": "<p>You have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for:</p>\n<pre><code class=\"python\">feature_array = np.array(tfidf.get_feature_names())\ntfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n\nn = 3\ntop_n = feature_array[tfidf_sorting][:n]\n</code></pre>\n<p>This gives me:</p>\n<pre><code class=\"python\">array([u'fruit', u'travellers', u'jupiter'], \n  dtype='&lt;U13')\n</code></pre>\n<p>The <code>argsort</code> call is really the useful one, <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\" rel=\"noreferrer\">here are the docs for it</a>. We have to do <code>[::-1]</code> because <code>argsort</code> only supports sorting small to large. We call <code>flatten</code> to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. Note that including the call to <code>flatten</code> will only work if you're testing one document at at time.</p>\n<p>Also, on another note, did you mean something like <code>tfs = tfidf.fit_transform(t.split(\"\\n\\n\"))</code>? Otherwise, each term in the multiline string is being treated as a \"document\". Using <code>\\n\\n</code> instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf.</p>\n", "abstract": "You have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for: This gives me: The argsort call is really the useful one, here are the docs for it. We have to do [::-1] because argsort only supports sorting small to large. We call flatten to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. Note that including the call to flatten will only work if you're testing one document at at time. Also, on another note, did you mean something like tfs = tfidf.fit_transform(t.split(\"\\n\\n\"))? Otherwise, each term in the multiline string is being treated as a \"document\". Using \\n\\n instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf."}, {"id": 56713677, "score": 12, "vote": 0, "content": "<p>Solution using sparse matrix itself (without <code>.toarray()</code>)!</p>\n<pre><code class=\"python\">import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ncorpus = [\n    'I would like to check this document',\n    'How about one more document',\n    'Aim is to capture the key words from the corpus',\n    'frequency of words in a document is called term frequency'\n]\n\nX = tfidf.fit_transform(corpus)\nfeature_names = np.array(tfidf.get_feature_names())\n\n\nnew_doc = ['can key words in this new document be identified?',\n           'idf is the inverse document frequency caculcated for each of the words']\nresponses = tfidf.transform(new_doc)\n\n\ndef get_top_tf_idf_words(response, top_n=2):\n    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n    return feature_names[response.indices[sorted_nzs]]\n  \nprint([get_top_tf_idf_words(response,2) for response in responses])\n\n#[array(['key', 'words'], dtype='&lt;U9'),\n array(['frequency', 'words'], dtype='&lt;U9')]\n</code></pre>\n", "abstract": "Solution using sparse matrix itself (without .toarray())!"}, {"id": 67247677, "score": 5, "vote": 0, "content": "<p>Here is a <strong>quick code</strong> for that:\n(<code>documents</code> is a list)</p>\n<pre><code class=\"python\">def get_tfidf_top_features(documents,n_top=10):\n  fidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n  tfidf = tfidf_vectorizer.fit_transform(documents)\n  importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n  tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names())\n  return tfidf_feature_names[importance[:n_top]]\n</code></pre>\n", "abstract": "Here is a quick code for that:\n(documents is a list)"}]}, {"link": "https://stackoverflow.com/questions/3753021/using-nltk-and-wordnet-how-do-i-convert-simple-tense-verb-into-its-present-pas", "question": {"id": "3753021", "title": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "content": "<p>Using NLTK and <a href=\"https://en.wikipedia.org/wiki/WordNet\" rel=\"noreferrer\">WordNet</a>, how do I convert simple tense verb into its present, past or past participle form?</p>\n<p><strong>For example:</strong></p>\n<p>I want to write a function which would give me verb in expected form as follows.</p>\n<pre><code class=\"python\">v = 'go'\npresent = present_tense(v)\nprint present # prints \"going\"\n\npast = past_tense(v)\nprint past # prints \"went\"\n</code></pre>\n", "abstract": "Using NLTK and WordNet, how do I convert simple tense verb into its present, past or past participle form? For example: I want to write a function which would give me verb in expected form as follows."}, "answers": [{"id": 26802243, "score": 27, "vote": 0, "content": "<p>With the help of NLTK this can also be done. It can give the base form of the verb. But not the exact tense, but it still can be useful. Try the following code.</p>\n<pre><code class=\"python\">from nltk.stem.wordnet import WordNetLemmatizer\nwords = ['gave','went','going','dating']\nfor word in words:\n    print word+\"--&gt;\"+WordNetLemmatizer().lemmatize(word,'v')\n</code></pre>\n<p>The output is:</p>\n<pre><code class=\"python\">gave--&gt;give\nwent--&gt;go\ngoing--&gt;go\ndating--&gt;date\n</code></pre>\n<p>Have a look at Stack Overflow question <em><a href=\"https://stackoverflow.com/questions/25534214\">NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?</a></em>.</p>\n", "abstract": "With the help of NLTK this can also be done. It can give the base form of the verb. But not the exact tense, but it still can be useful. Try the following code. The output is: Have a look at Stack Overflow question NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?."}, {"id": 3761041, "score": 23, "vote": 0, "content": "<p>I think what you're looking for is the <a href=\"http://nodebox.net/code/index.php/Linguistics#verb_conjugation\" rel=\"noreferrer\">NodeBox::Linguistics</a> library. It does exactly that:</p>\n<pre><code class=\"python\">print en.verb.present(\"gave\")\n&gt;&gt;&gt; give\n</code></pre>\n", "abstract": "I think what you're looking for is the NodeBox::Linguistics library. It does exactly that:"}, {"id": 48300948, "score": 20, "vote": 0, "content": "<p>For Python3:</p>\n<pre><code class=\"python\">pip install pattern\n</code></pre>\n<p>then</p>\n<pre><code class=\"python\">from pattern.en import conjugate, lemma, lexeme, PRESENT, SG\nprint (lemma('gave'))\nprint (lexeme('gave'))\nprint (conjugate(verb='give',tense=PRESENT,number=SG)) # he / she / it\n</code></pre>\n<p>yields</p>\n<pre><code class=\"python\">give \n['give', 'gives', 'giving', 'gave', 'given'] \ngives\n</code></pre>\n<p>thnks to @Agargara for pointing &amp; authors of Pattern for their beautiful work, go support them ;-)</p>\n<p>PS. To use most of pattern's functionality in python 3.7+, you might want to use the trick described <a href=\"https://github.com/RaRe-Technologies/gensim/issues/2438#issuecomment-644753776\" rel=\"nofollow noreferrer\">here</a></p>\n", "abstract": "For Python3: then yields thnks to @Agargara for pointing & authors of Pattern for their beautiful work, go support them ;-) PS. To use most of pattern's functionality in python 3.7+, you might want to use the trick described here"}, {"id": 6461693, "score": 0, "vote": 0, "content": "<p>JWI (the WordNet library by MIT) also has a stemmer (WordNetStemmer) which converts different morphological forms of a word like (\"written\", \"writes\", \"wrote\") to their base form. It seems it works only for nouns (like plurals) and verbs though. </p>\n<p><em><a href=\"http://tipsandtricks.runicsoft.com/Other/JavaStemmer.html\" rel=\"nofollow\">Word Stemming in Java with WordNet and JWNL</a></em> also shows how to do this kind of stemming using JWNL, another Java-based Wordnet library:</p>\n", "abstract": "JWI (the WordNet library by MIT) also has a stemmer (WordNetStemmer) which converts different morphological forms of a word like (\"written\", \"writes\", \"wrote\") to their base form. It seems it works only for nouns (like plurals) and verbs though.  Word Stemming in Java with WordNet and JWNL also shows how to do this kind of stemming using JWNL, another Java-based Wordnet library:"}]}, {"link": "https://stackoverflow.com/questions/27470670/how-to-use-gensim-doc2vec-with-pre-trained-word-vectors", "question": {"id": "27470670", "title": "How to use Gensim doc2vec with pre-trained word vectors?", "content": "<p>I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?</p>\n<p>Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?</p>\n<p>Thanks.</p>\n", "abstract": "I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec? Or is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training? Thanks."}, "answers": [{"id": 30337118, "score": 25, "vote": 0, "content": "<p>Note that the \"DBOW\" (<code>dm=0</code>) training mode doesn't require or even create word-vectors as part of the training. It merely learns document vectors that are good at predicting each word in turn (much like the word2vec skip-gram training mode). </p>\n<p>(Before gensim 0.12.0, there was the parameter <code>train_words</code> mentioned in another comment, which some documentation suggested will co-train words. However, I don't believe this ever actually worked. Starting in gensim 0.12.0, there is the parameter <code>dbow_words</code>, which works to skip-gram train words simultaneous with DBOW doc-vectors. Note that this makes training take longer \u2013 by a factor related to <code>window</code>. So if you don't need word-vectors, you may still leave this off.)</p>\n<p>In the \"DM\" training method (<code>dm=1</code>), word-vectors are inherently trained during the process along with doc-vectors, and are likely to also affect the quality of the doc-vectors. It's theoretically possible to pre-initialize the word-vectors from prior data. But I don't know any strong theoretical or experimental reason to be confident this would improve the doc-vectors. </p>\n<p>One fragmentary experiment I ran along these lines suggested the doc-vector training got off to a faster start \u2013 better predictive qualities after the first few passes \u2013 but this advantage faded with more passes. Whether you hold the word vectors constant or let them continue to adjust with the new training is also likely an important consideration... but which choice is better may depend on your goals, data set, and the quality/relevance of the pre-existing word-vectors. </p>\n<p>(You could repeat my experiment with the <code>intersect_word2vec_format()</code> method available in gensim 0.12.0, and try different levels of making pre-loaded vectors resistant-to-new-training via the <code>syn0_lockf</code> values. But remember this is experimental territory: the basic doc2vec results don't rely on, or even necessarily improve with, reused word vectors.) </p>\n", "abstract": "Note that the \"DBOW\" (dm=0) training mode doesn't require or even create word-vectors as part of the training. It merely learns document vectors that are good at predicting each word in turn (much like the word2vec skip-gram training mode).  (Before gensim 0.12.0, there was the parameter train_words mentioned in another comment, which some documentation suggested will co-train words. However, I don't believe this ever actually worked. Starting in gensim 0.12.0, there is the parameter dbow_words, which works to skip-gram train words simultaneous with DBOW doc-vectors. Note that this makes training take longer \u2013 by a factor related to window. So if you don't need word-vectors, you may still leave this off.) In the \"DM\" training method (dm=1), word-vectors are inherently trained during the process along with doc-vectors, and are likely to also affect the quality of the doc-vectors. It's theoretically possible to pre-initialize the word-vectors from prior data. But I don't know any strong theoretical or experimental reason to be confident this would improve the doc-vectors.  One fragmentary experiment I ran along these lines suggested the doc-vector training got off to a faster start \u2013 better predictive qualities after the first few passes \u2013 but this advantage faded with more passes. Whether you hold the word vectors constant or let them continue to adjust with the new training is also likely an important consideration... but which choice is better may depend on your goals, data set, and the quality/relevance of the pre-existing word-vectors.  (You could repeat my experiment with the intersect_word2vec_format() method available in gensim 0.12.0, and try different levels of making pre-loaded vectors resistant-to-new-training via the syn0_lockf values. But remember this is experimental territory: the basic doc2vec results don't rely on, or even necessarily improve with, reused word vectors.) "}, {"id": 27699833, "score": 12, "vote": 0, "content": "<p>Well, I am recently using Doc2Vec too. And I was thinking of using LDA result as word vector and fix those word vectors to get a document vector. The result isn't very interesting though. Maybe it's just my data set isn't that good. \nThe code is below. Doc2Vec saves word vectors and document vectors together in dictionary doc2vecmodel.syn0. You can direct change the vector values. The only problem may be that you need to find out which position in syn0 represents which word or document. The vectors are stored in random order in dictionary syn0.</p>\n<pre><code class=\"python\">import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nfrom gensim import corpora, models, similarities\nimport gensim\nfrom sklearn import svm, metrics\nimport numpy\n\n#Read in texts into div_texts(for LDA and Doc2Vec)\ndiv_texts = []\nf = open(\"clean_ad_nonad.txt\")\nlines = f.readlines()\nf.close()\nfor line in lines:\n    div_texts.append(line.strip().split(\" \"))\n\n#Set up dictionary and MMcorpus\ndictionary = corpora.Dictionary(div_texts)\ndictionary.save(\"ad_nonad_lda_deeplearning.dict\")\n#dictionary = corpora.Dictionary.load(\"ad_nonad_lda_deeplearning.dict\")\nprint dictionary.token2id[\"junk\"]\ncorpus = [dictionary.doc2bow(text) for text in div_texts]\ncorpora.MmCorpus.serialize(\"ad_nonad_lda_deeplearning.mm\", corpus)\n\n#LDA training\nid2token = {}\ntoken2id = dictionary.token2id\nfor onemap in dictionary.token2id:\n    id2token[token2id[onemap]] = onemap\n#ldamodel = models.LdaModel(corpus, num_topics = 100, passes = 1000, id2word = id2token)\n#ldamodel.save(\"ldamodel1000pass.lda\")\n#ldamodel = models.LdaModel(corpus, num_topics = 100, id2word = id2token)\nldamodel = models.LdaModel.load(\"ldamodel1000pass.lda\")\nldatopics = ldamodel.show_topics(num_topics = 100, num_words = len(dictionary), formatted = False)\nprint ldatopics[10][1]\nprint ldatopics[10][1][1]\nldawordindex = {}\nfor i in range(len(dictionary)):\n    ldawordindex[ldatopics[0][i][1]] = i\n\n#Doc2Vec initialize\nsentences = []\nfor i in range(len(div_texts)):\n    string = \"SENT_\" + str(i)\n    sentence = models.doc2vec.LabeledSentence(div_texts[i], labels = [string])\n    sentences.append(sentence)\ndoc2vecmodel = models.Doc2Vec(sentences, size = 100, window = 5, min_count = 0, dm = 1)\nprint \"Initial word vector for word junk:\"\nprint doc2vecmodel[\"junk\"]\n\n#Replace the word vector with word vectors from LDA\nprint len(doc2vecmodel.syn0)\nindex2wordcollection = doc2vecmodel.index2word\nprint index2wordcollection\nfor i in range(len(doc2vecmodel.syn0)):\n    if index2wordcollection[i].startswith(\"SENT_\"):\n        continue\n    wordindex = ldawordindex[index2wordcollection[i]]\n    wordvectorfromlda = [ldatopics[j][wordindex][0] for j in range(100)]\n    doc2vecmodel.syn0[i] = wordvectorfromlda\n#print doc2vecmodel.index2word[26841]\n#doc2vecmodel.syn0[0] = [0 for i in range(100)]\nprint \"Changed word vector for word junk:\"\nprint doc2vecmodel[\"junk\"]\n\n#Train Doc2Vec\ndoc2vecmodel.train_words = False \nprint \"Initial doc vector for 1st document\"\nprint doc2vecmodel[\"SENT_0\"]\nfor i in range(50):\n    print \"Round: \" + str(i)\n    doc2vecmodel.train(sentences)\nprint \"Trained doc vector for 1st document\"\nprint doc2vecmodel[\"SENT_0\"]\n\n#Using SVM to do classification\nresultlist = []\nfor i in range(4143):\n    string = \"SENT_\" + str(i)\n    resultlist.append(doc2vecmodel[string])\nsvm_x_train = []\nfor i in range(1000):\n    svm_x_train.append(resultlist[i])\nfor i in range(2210,3210):\n    svm_x_train.append(resultlist[i])\nprint len(svm_x_train)\n\nsvm_x_test = []\nfor i in range(1000,2210):\n    svm_x_test.append(resultlist[i])\nfor i in range(3210,4143):\n    svm_x_test.append(resultlist[i])\nprint len(svm_x_test)\n\nsvm_y_train = numpy.array([0 for i in range(2000)])\nfor i in range(1000,2000):\n    svm_y_train[i] = 1\nprint svm_y_train\n\nsvm_y_test = numpy.array([0 for i in range(2143)])\nfor i in range(1210,2143):\n    svm_y_test[i] = 1\nprint svm_y_test\n\n\nsvc = svm.SVC(kernel='linear')\nsvc.fit(svm_x_train, svm_y_train)\n\nexpected = svm_y_test\npredicted = svc.predict(svm_x_test)\n\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (svc, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n\nprint doc2vecmodel[\"junk\"]\n</code></pre>\n", "abstract": "Well, I am recently using Doc2Vec too. And I was thinking of using LDA result as word vector and fix those word vectors to get a document vector. The result isn't very interesting though. Maybe it's just my data set isn't that good. \nThe code is below. Doc2Vec saves word vectors and document vectors together in dictionary doc2vecmodel.syn0. You can direct change the vector values. The only problem may be that you need to find out which position in syn0 represents which word or document. The vectors are stored in random order in dictionary syn0."}, {"id": 39337595, "score": 12, "vote": 0, "content": "<p><a href=\"https://github.com/jhlau/gensim\" rel=\"noreferrer\">This forked version of gensim</a> allows loading pre-trained word vectors for training doc2vec. <a href=\"https://github.com/jhlau/doc2vec/blob/master/train_model.py\" rel=\"noreferrer\">Here</a> you have an example on how to use it. The word vectors must be in the C-word2vec tool text format: one line per word vector where first comes a string representing the word and then space-separated float values, one for each dimension of the embedding.</p>\n<p>This work belongs to a <a href=\"https://arxiv.org/abs/1607.05368\" rel=\"noreferrer\">paper</a> in which they claim that using pre-trained word embeddings actually helps building the document vectors. However I am getting <em>almost</em> the same results no matter I load the pre-trained embeddings or not.</p>\n<p><strong>Edit:</strong> actually there is one remarkable difference in my experiments. When I loaded the pretrained embeddings I trained doc2vec for half of the iterations to get <em>almost</em> the same results (training longer than that produced worse results in my task).</p>\n", "abstract": "This forked version of gensim allows loading pre-trained word vectors for training doc2vec. Here you have an example on how to use it. The word vectors must be in the C-word2vec tool text format: one line per word vector where first comes a string representing the word and then space-separated float values, one for each dimension of the embedding. This work belongs to a paper in which they claim that using pre-trained word embeddings actually helps building the document vectors. However I am getting almost the same results no matter I load the pre-trained embeddings or not. Edit: actually there is one remarkable difference in my experiments. When I loaded the pretrained embeddings I trained doc2vec for half of the iterations to get almost the same results (training longer than that produced worse results in my task)."}, {"id": 27512681, "score": 2, "vote": 0, "content": "<p>Radim just posted a <a href=\"http://radimrehurek.com/2014/12/doc2vec-tutorial/\" rel=\"nofollow\">tutorial</a> on the doc2vec features of gensim (yesterday, I believe - your question is timely!).</p>\n<p>Gensim supports loading pre-trained vectors from <a href=\"https://code.google.com/p/word2vec/\" rel=\"nofollow\">the C implementation</a>, as described in <a href=\"http://radimrehurek.com/gensim/models/word2vec.html\" rel=\"nofollow\">the gensim models.word2vec API documentation</a>.</p>\n", "abstract": "Radim just posted a tutorial on the doc2vec features of gensim (yesterday, I believe - your question is timely!). Gensim supports loading pre-trained vectors from the C implementation, as described in the gensim models.word2vec API documentation."}]}, {"link": "https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents", "question": {"id": "42781292", "title": "Doc2Vec Get most similar documents", "content": "<p>I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the <code>Doc2Vec</code> model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far</p>\n<pre><code class=\"python\">import gensim, re\nimport pandas as pd\n\n# TOKENIZER\ndef tokenizer(input_string):\n    return re.findall(r\"[\\w']+\", input_string)\n\n# IMPORT DATA\ndata = pd.read_csv('mp_1002_prepd.txt')\ndata.columns = ['merged']\ndata.loc[:, 'tokens'] = data.merged.apply(tokenizer)\nsentences= []\nfor item_no, line in enumerate(data['tokens'].values.tolist()):\n    sentences.append(LabeledSentence(line,[item_no]))\n\n# MODEL PARAMETERS\ndm = 1 # 1 for distributed memory(default); 0 for dbow \ncores = multiprocessing.cpu_count()\nsize = 300\ncontext_window = 50\nseed = 42\nmin_count = 1\nalpha = 0.5\nmax_iter = 200\n\n# BUILD MODEL\nmodel = gensim.models.doc2vec.Doc2Vec(documents = sentences,\ndm = dm,\nalpha = alpha, # initial learning rate\nseed = seed,\nmin_count = min_count, # ignore words with freq less than min_count\nmax_vocab_size = None, # \nwindow = context_window, # the number of words before and after to be used as context\nsize = size, # is the dimensionality of the feature vector\nsample = 1e-4, # ?\nnegative = 5, # ?\nworkers = cores, # number of cores\niter = max_iter # number of iterations (epochs) over the corpus)\n\n# QUERY BASED DOC RANKING ??\n</code></pre>\n<p>The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the <code>infer_vector</code> but then realised that it considers the query as a document, updates the model and returns the results. I tried using the <code>most_similar</code> and <code>most_similar_cosmul</code> methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?</p>\n", "abstract": "I am trying to build a document retrieval model that returns most documents ordered by their relevancy with respect to a query or a search string. For this I trained a doc2vec model using the Doc2Vec model in gensim. My dataset is in the form of a pandas dataset which has each document stored as a string on each line. This is the code I have so far The part where I am struggling is in finding documents that are most similar/relevant to the query. I used the infer_vector but then realised that it considers the query as a document, updates the model and returns the results. I tried using the most_similar and most_similar_cosmul methods but I get words along with a similarity score(I guess) in return. What I want to do is when I enter a search string(a query), I should get the documents (ids) that are most relevant along with a similarity score(cosine etc). How do I get this part done?"}, "answers": [{"id": 42817402, "score": 55, "vote": 0, "content": "<p>You need to use <code>infer_vector</code> to get a document vector of the new text - which does not alter the underlying model.</p>\n<p>Here is how you do it:</p>\n<pre><code class=\"python\">tokens = \"a new sentence to match\".split()\n\nnew_vector = model.infer_vector(tokens)\nsims = model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity\n</code></pre>\n<p>Edit: </p>\n<p>Here is an example of how the underlying model does not change after <code>infer_vec</code> is called.</p>\n<pre><code class=\"python\">import numpy as np\n\nwords = \"king queen man\".split()\n\nlen_before =  len(model.docvecs) #number of docs\n\n#word vectors for king, queen, man\nw_vec0 = model[words[0]]\nw_vec1 = model[words[1]]\nw_vec2 = model[words[2]]\n\nnew_vec = model.infer_vector(words)\n\nlen_after =  len(model.docvecs)\n\nprint np.array_equal(model[words[0]], w_vec0) # True\nprint np.array_equal(model[words[1]], w_vec1) # True\nprint np.array_equal(model[words[2]], w_vec2) # True\n\nprint len_before == len_after #True\n</code></pre>\n", "abstract": "You need to use infer_vector to get a document vector of the new text - which does not alter the underlying model. Here is how you do it: Edit:  Here is an example of how the underlying model does not change after infer_vec is called."}]}, {"link": "https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim", "question": {"id": "35716121", "title": "How to extract phrases from corpus using gensim", "content": "<p>For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using <strong>Phrases</strong> model in gensim, I tried below code but it's not giving me desired output.</p>\n<p><strong>My code</strong></p>\n<pre><code class=\"python\">from gensim.models import Phrases\ndocuments = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\"]\n\nsentence_stream = [doc.split(\" \") for doc in documents]\nbigram = Phrases(sentence_stream)\nsent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\nprint(bigram[sent])\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code class=\"python\">[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\n</code></pre>\n<p><strong>But it should come as</strong> </p>\n<pre><code class=\"python\">[u'the', u'mayor', u'of', u'new_york', u'was', u'there']\n</code></pre>\n<p>But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?</p>\n<pre><code class=\"python\">print bigram.vocab\n\ndefaultdict(&lt;type 'int'&gt;, {'useful': 1, 'was_there': 1, 'learning_can': 1, 'learning': 1, 'of_new': 1, 'can_be': 1, 'mayor': 1, 'there': 1, 'machine': 1, 'new': 1, 'was': 1, 'useful_sometimes': 1, 'be': 1, 'mayor_of': 1, 'york_was': 1, 'york': 1, 'machine_learning': 1, 'the_mayor': 1, 'new_york': 1, 'of': 1, 'sometimes': 1, 'can': 1, 'be_useful': 1, 'the': 1}) \n</code></pre>\n", "abstract": "For preprocessing the corpus I was planing to extarct common phrases from the corpus, for this I tried using Phrases model in gensim, I tried below code but it's not giving me desired output. My code Output But it should come as  But when I tried to print vocab of train data, I can see bigram, but its not working with test data, where I am going wrong?"}, "answers": [{"id": 35748858, "score": 70, "vote": 0, "content": "<p>I got the solution for the problem , There was two parameters I didn't take care of it which should be passed to <strong>Phrases() model</strong>, those are</p>\n<ol>\n<li><p><strong>min_count</strong> ignore all words and bigrams with total collected count lower than this. <strong>Bydefault it value is 5</strong></p></li>\n<li><p><strong>threshold</strong> represents a threshold for forming the phrases (higher means fewer phrases). A phrase of words a and b is accepted if (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) &gt; threshold, where N is the total vocabulary size. <strong>Bydefault it value is 10.0</strong></p></li>\n</ol>\n<p>With my above train data with two statements, threshold value was <strong>0</strong>, so I change train datasets and add those two parameters.</p>\n<p><strong>My New code</strong></p>\n<pre><code class=\"python\">from gensim.models import Phrases\ndocuments = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n\nsentence_stream = [doc.split(\" \") for doc in documents]\nbigram = Phrases(sentence_stream, min_count=1, threshold=2)\nsent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\nprint(bigram[sent])\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code class=\"python\">[u'the', u'mayor', u'of', u'new_york', u'was', u'there']\n</code></pre>\n<p>Gensim is really awesome :)</p>\n", "abstract": "I got the solution for the problem , There was two parameters I didn't take care of it which should be passed to Phrases() model, those are min_count ignore all words and bigrams with total collected count lower than this. Bydefault it value is 5 threshold represents a threshold for forming the phrases (higher means fewer phrases). A phrase of words a and b is accepted if (cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold, where N is the total vocabulary size. Bydefault it value is 10.0 With my above train data with two statements, threshold value was 0, so I change train datasets and add those two parameters. My New code Output Gensim is really awesome :)"}]}, {"link": "https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk", "question": {"id": "35275001", "title": "Use of PunktSentenceTokenizer in NLTK", "content": "<p>I am learning Natural Language Processing using NLTK.\nI came across the code using <code>PunktSentenceTokenizer</code> whose actual use I cannot understand in the given code. The code is given :</p>\n<pre><code class=\"python\">import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n\ntrain_text = state_union.raw(\"2005-GWBush.txt\")\nsample_text = state_union.raw(\"2006-GWBush.txt\")\n\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text) #A\n\ntokenized = custom_sent_tokenizer.tokenize(sample_text)   #B\n\ndef process_content():\ntry:\n    for i in tokenized[:5]:\n        words = nltk.word_tokenize(i)\n        tagged = nltk.pos_tag(words)\n        print(tagged)\n\nexcept Exception as e:\n    print(str(e))\n\n\nprocess_content()\n</code></pre>\n<p>So, why do we use PunktSentenceTokenizer.  And what is going on in the line marked A and B. I mean there is a training text and the other a sample text, but what is the need for two data sets to get the Part of Speech tagging.</p>\n<p>Line marked as <code>A</code> and <code>B</code> is which I am not able to understand.</p>\n<p>PS : I did try to look in the NLTK book but could not understand what is the real use of PunktSentenceTokenizer</p>\n", "abstract": "I am learning Natural Language Processing using NLTK.\nI came across the code using PunktSentenceTokenizer whose actual use I cannot understand in the given code. The code is given : So, why do we use PunktSentenceTokenizer.  And what is going on in the line marked A and B. I mean there is a training text and the other a sample text, but what is the need for two data sets to get the Part of Speech tagging. Line marked as A and B is which I am not able to understand. PS : I did try to look in the NLTK book but could not understand what is the real use of PunktSentenceTokenizer"}, "answers": [{"id": 35279885, "score": 37, "vote": 0, "content": "<p><code>PunktSentenceTokenizer</code> is the abstract class for the default sentence tokenizer, i.e. <code>sent_tokenize()</code>, provided in NLTK. It is an implmentation of <a href=\"http://old.linguistics.rub.de/~strunk/ks2005FINAL.pdf\" rel=\"noreferrer\">Unsupervised Multilingual Sentence\nBoundary Detection (Kiss and Strunk (2005)</a>. See <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tokenize/__init__.py#L79\" rel=\"noreferrer\">https://github.com/nltk/nltk/blob/develop/nltk/tokenize/<strong>init</strong>.py#L79</a></p>\n<p>Given a paragraph with multiple sentence, e.g:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.corpus import state_union\n&gt;&gt;&gt; train_text = state_union.raw(\"2005-GWBush.txt\").split('\\n')\n&gt;&gt;&gt; train_text[11]\nu'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. This evening I will set forth policies to advance that ideal at home and around the world. '\n</code></pre>\n<p>You can use the <code>sent_tokenize()</code>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; sent_tokenize(train_text[11])\n[u'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.', u'This evening I will set forth policies to advance that ideal at home and around the world. ']\n&gt;&gt;&gt; for sent in sent_tokenize(train_text[11]):\n...     print sent\n...     print '--------'\n... \nTwo weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.\n--------\nThis evening I will set forth policies to advance that ideal at home and around the world. \n--------\n</code></pre>\n<p>The <code>sent_tokenize()</code> uses a pre-trained model from <code>nltk_data/tokenizers/punkt/english.pickle</code>. You can also specify other languages, the list of available languages with pre-trained models in NLTK are:</p>\n<pre><code class=\"python\">alvas@ubi:~/nltk_data/tokenizers/punkt$ ls\nczech.pickle     finnish.pickle  norwegian.pickle   slovene.pickle\ndanish.pickle    french.pickle   polish.pickle      spanish.pickle\ndutch.pickle     german.pickle   portuguese.pickle  swedish.pickle\nenglish.pickle   greek.pickle    PY3                turkish.pickle\nestonian.pickle  italian.pickle  README\n</code></pre>\n<p>Given a text in another language, do this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; german_text = u\"Die Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter. \u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \"\n\n&gt;&gt;&gt; for sent in sent_tokenize(german_text, language='german'):\n...     print sent\n...     print '---------'\n... \nDie Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter.\n---------\n\u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \n---------\n</code></pre>\n<p>To train your own punkt model, see <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py\" rel=\"noreferrer\">https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py</a> and <a href=\"https://stackoverflow.com/questions/21160310/training-data-format-for-nltk-punkt\">training data format for nltk punkt</a></p>\n", "abstract": "PunktSentenceTokenizer is the abstract class for the default sentence tokenizer, i.e. sent_tokenize(), provided in NLTK. It is an implmentation of Unsupervised Multilingual Sentence\nBoundary Detection (Kiss and Strunk (2005). See https://github.com/nltk/nltk/blob/develop/nltk/tokenize/init.py#L79 Given a paragraph with multiple sentence, e.g: You can use the sent_tokenize(): The sent_tokenize() uses a pre-trained model from nltk_data/tokenizers/punkt/english.pickle. You can also specify other languages, the list of available languages with pre-trained models in NLTK are: Given a text in another language, do this: To train your own punkt model, see https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py and training data format for nltk punkt"}, {"id": 35279795, "score": 23, "vote": 0, "content": "<p><code>PunktSentenceTokenizer</code> is an sentence boundary detection algorithm that must be trained to be used [1]. NLTK already includes a pre-trained version of the PunktSentenceTokenizer. </p>\n<p>So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:</p>\n<pre><code class=\"python\">In [1]: import nltk\nIn [2]: tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\nIn [3]: txt = \"\"\" This is one sentence. This is another sentence.\"\"\"\nIn [4]: tokenizer.tokenize(txt)\nOut[4]: [' This is one sentence.', 'This is another sentence.']\n</code></pre>\n<p>You can also provide your own training data to train the tokenizer before using it. Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text.</p>\n<p><code>custom_sent_tokenizer = PunktSentenceTokenizer(train_text)</code></p>\n<p>For most of the cases, it is totally fine to use the pre-trained version. So you can simply initialize the tokenizer without providing any arguments.</p>\n<p>So \"what all this has to do with POS tagging\"? The NLTK POS tagger works with tokenized sentences, so you need to break your text into sentences and word tokens before you can POS tag.</p>\n<p><a href=\"http://www.nltk.org/api/nltk.tokenize.html\" rel=\"noreferrer\">NLTK's documentation.</a></p>\n<p>[1] Kiss and Strunk, \"\n<a href=\"http://dl.acm.org/citation.cfm?id=1245122\" rel=\"noreferrer\">Unsupervised Multilingual Sentence Boundary Detection</a>\"  </p>\n", "abstract": "PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used [1]. NLTK already includes a pre-trained version of the PunktSentenceTokenizer.  So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version: You can also provide your own training data to train the tokenizer before using it. Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text. custom_sent_tokenizer = PunktSentenceTokenizer(train_text) For most of the cases, it is totally fine to use the pre-trained version. So you can simply initialize the tokenizer without providing any arguments. So \"what all this has to do with POS tagging\"? The NLTK POS tagger works with tokenized sentences, so you need to break your text into sentences and word tokens before you can POS tag. NLTK's documentation. [1] Kiss and Strunk, \"\nUnsupervised Multilingual Sentence Boundary Detection\"  "}, {"id": 49485299, "score": 1, "vote": 0, "content": "<p>You can refer below link to get more insight on usage of PunktSentenceTokenizer.\nIt vividly explains why PunktSentenceTokenizer is used instead of sent-tokenize() with regard to your case.</p>\n<p><a href=\"http://nlpforhackers.io/splitting-text-into-sentences/\" rel=\"nofollow noreferrer\">http://nlpforhackers.io/splitting-text-into-sentences/</a></p>\n", "abstract": "You can refer below link to get more insight on usage of PunktSentenceTokenizer.\nIt vividly explains why PunktSentenceTokenizer is used instead of sent-tokenize() with regard to your case. http://nlpforhackers.io/splitting-text-into-sentences/"}, {"id": 56281590, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">def process_content(corpus):\n\n    tokenized = PunktSentenceTokenizer().tokenize(corpus)\n\n    try:\n        for sent in tokenized:\n            words = nltk.word_tokenize(sent)\n            tagged = nltk.pos_tag(words)\n            print(tagged)\n    except Exception as e:\n        print(str(e))\n\nprocess_content(train_text)\n</code></pre>\n<p>Without even training it on other text data it works the same as it is pre-trained.</p>\n", "abstract": "Without even training it on other text data it works the same as it is pre-trained."}]}, {"link": "https://stackoverflow.com/questions/25145552/tfidf-for-large-dataset", "question": {"id": "25145552", "title": "TFIDF for Large Dataset", "content": "<p>I have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process.</p>\n<p>Does anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?</p>\n", "abstract": "I have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process. Does anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?"}, "answers": [{"id": 25168689, "score": 31, "vote": 0, "content": "<p>Gensim has an efficient <a href=\"http://radimrehurek.com/gensim/intro.html\" rel=\"noreferrer\">tf-idf model</a> and does not need to have everything in memory at once.</p>\n<p>Your corpus simply needs to be an iterable, so it does not need to have the whole corpus in memory at a time.</p>\n<p>The <a href=\"https://github.com/piskvorky/gensim/blob/develop/gensim/scripts/make_wikicorpus.py\" rel=\"noreferrer\">make_wiki script</a> runs over Wikipedia in about 50m on a laptop according to the comments.</p>\n", "abstract": "Gensim has an efficient tf-idf model and does not need to have everything in memory at once. Your corpus simply needs to be an iterable, so it does not need to have the whole corpus in memory at a time. The make_wiki script runs over Wikipedia in about 50m on a laptop according to the comments."}, {"id": 25158670, "score": 15, "vote": 0, "content": "<p>I believe you can use a <code>HashingVectorizer</code> to get a smallish <code>csr_matrix</code> out of your text data and then use a <code>TfidfTransformer</code> on that. Storing a sparse matrix of 8M rows and several tens of thousands of columns isn't such a big deal. Another option would be not to use TF-IDF at all- it could be the case that your system works reasonably well without it. </p>\n<p>In practice you may have to subsample your data set- sometimes a system will do just as well by just learning from 10% of all available data. This is an empirical question, there is not way to tell in advance what strategy would be best for your task. I wouldn't worry about scaling to 8M document until I am convinced I need them (i.e. until I have seen a learning curve showing a clear upwards trend). </p>\n<p>Below is something I was working on this morning as an example. You can see the performance of the system tends to improve as I add more documents, but it is already at a stage where it seems to make little difference. Given how long it takes to train, I don't think training it on 500 files is worth my time.</p>\n<p><img alt=\"\" src=\"https://i.stack.imgur.com/RKy3r.png\"/></p>\n", "abstract": "I believe you can use a HashingVectorizer to get a smallish csr_matrix out of your text data and then use a TfidfTransformer on that. Storing a sparse matrix of 8M rows and several tens of thousands of columns isn't such a big deal. Another option would be not to use TF-IDF at all- it could be the case that your system works reasonably well without it.  In practice you may have to subsample your data set- sometimes a system will do just as well by just learning from 10% of all available data. This is an empirical question, there is not way to tell in advance what strategy would be best for your task. I wouldn't worry about scaling to 8M document until I am convinced I need them (i.e. until I have seen a learning curve showing a clear upwards trend).  Below is something I was working on this morning as an example. You can see the performance of the system tends to improve as I add more documents, but it is already at a stage where it seems to make little difference. Given how long it takes to train, I don't think training it on 500 files is worth my time. "}, {"id": 59176012, "score": 5, "vote": 0, "content": "<p>I solve that problem using sklearn and pandas. </p>\n<p>Iterate in your dataset once using pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iteration\" rel=\"noreferrer\">iterator</a> and create a set of all words, after that use it in CountVectorizer vocabulary. With that the Count Vectorizer will generate a list of sparse matrix all of them with the same shape. Now is just use <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.vstack.html#scipy.sparse.vstack\" rel=\"noreferrer\">vstack</a> to group them. The sparse matrix resulted have the same information (but the words in another order) as CountVectorizer object and fitted with all your data.</p>\n<p>That solution is not the best if you consider the time complexity but is good for memory complexity. I use that in a dataset with 20GB +,</p>\n<p>I wrote a python code (NOT THE COMPLETE SOLUTION) that show the properties, write a generator or use pandas chunks for iterate in your dataset.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.sparse import vstack\n\n\n# each string is a sample\ntext_test = [\n    'good people beauty wrong',\n    'wrong smile people wrong',\n    'idea beauty good good',\n]\n\n# scikit-learn basic usage\n\nvectorizer = CountVectorizer()\n\nresult1 = vectorizer.fit_transform(text_test)\nprint(vectorizer.inverse_transform(result1))\nprint(f\"First approach:\\n {result1}\")\n\n# Another solution is\n\nvocabulary = set()\n\nfor text in text_test:\n    for word in text.split():\n        vocabulary.add(word)\n\nvectorizer = CountVectorizer(vocabulary=vocabulary)\n\noutputs = [] \nfor text in text_test: # use a generator\n    outputs.append(vectorizer.fit_transform([text]))\n\n\nresult2 = vstack(outputs)\nprint(vectorizer.inverse_transform(result2))\n\nprint(f\"Second approach:\\n {result2}\")\n</code></pre>\n<p>Finally, use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\" rel=\"noreferrer\">TfidfTransformer</a>.</p>\n", "abstract": "I solve that problem using sklearn and pandas.  Iterate in your dataset once using pandas iterator and create a set of all words, after that use it in CountVectorizer vocabulary. With that the Count Vectorizer will generate a list of sparse matrix all of them with the same shape. Now is just use vstack to group them. The sparse matrix resulted have the same information (but the words in another order) as CountVectorizer object and fitted with all your data. That solution is not the best if you consider the time complexity but is good for memory complexity. I use that in a dataset with 20GB +, I wrote a python code (NOT THE COMPLETE SOLUTION) that show the properties, write a generator or use pandas chunks for iterate in your dataset. Finally, use TfidfTransformer."}, {"id": 67782295, "score": -2, "vote": 0, "content": "<p>The lengths of the documents The number of terms in common Whether the terms are common or unusual How many times each term appears</p>\n", "abstract": "The lengths of the documents The number of terms in common Whether the terms are common or unusual How many times each term appears"}]}, {"link": "https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word", "question": {"id": "25534214", "title": "NLTK WordNet Lemmatizer: Shouldn&#39;t it lemmatize all inflections of a word?", "content": "<p>I'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to.</p>\n<p>For example, the word <code>loves</code> is lemmatized to <code>love</code> which is correct, but the word <code>loving</code> remains <code>loving</code> even after lemmatization. Here <code>loving</code> is as in the sentence \"I'm loving it\".</p>\n<p>Isn't <code>love</code> the stem of the inflected word <code>loving</code>? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior?</p>\n<p>What are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word <code>killing</code> should have <code>kill</code> as the stem if <code>killing</code> is used as a verb, but it should have <code>killing</code> as the stem if it is used as a noun (as in <code>the killing was done by xyz</code>).</p>\n", "abstract": "I'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to. For example, the word loves is lemmatized to love which is correct, but the word loving remains loving even after lemmatization. Here loving is as in the sentence \"I'm loving it\". Isn't love the stem of the inflected word loving? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior? What are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word killing should have kill as the stem if killing is used as a verb, but it should have killing as the stem if it is used as a noun (as in the killing was done by xyz)."}, "answers": [{"id": 25535348, "score": 76, "vote": 0, "content": "<p>The WordNet lemmatizer <em>does</em> take the POS tag into account, but it doesn't magically determine it:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; nltk.stem.WordNetLemmatizer().lemmatize('loving')\n'loving'\n&gt;&gt;&gt; nltk.stem.WordNetLemmatizer().lemmatize('loving', 'v')\nu'love'\n</code></pre>\n<p>Without a POS tag, it assumes everything you feed it is a noun. So here it thinks you're passing it the noun \"loving\" (as in \"sweet loving\").</p>\n", "abstract": "The WordNet lemmatizer does take the POS tag into account, but it doesn't magically determine it: Without a POS tag, it assumes everything you feed it is a noun. So here it thinks you're passing it the noun \"loving\" (as in \"sweet loving\")."}, {"id": 25544239, "score": 33, "vote": 0, "content": "<p>The best way to troubleshoot this is to actually look in Wordnet. Take a look here: <a href=\"http://wordnetweb.princeton.edu/perl/webwn?s=loving&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=\">Loving in wordnet</a>. As you can see, there is actually an adjective \"loving\" present in Wordnet. As a matter of fact, there is even the adverb \"lovingly\": <a href=\"http://wordnetweb.princeton.edu/perl/webwn?s=lovingly&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=00000\">lovingly in Wordnet</a>. Because wordnet doesn't actually know what part of speech you actually want, it defaults to noun ('n' in Wordnet). If you are using Penn Treebank tag set, here's some handy function for transforming Penn to WN tags:</p>\n<pre><code class=\"python\">from nltk.corpus import wordnet as wn\n\ndef is_noun(tag):\n    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n\n\ndef is_verb(tag):\n    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\n\ndef is_adverb(tag):\n    return tag in ['RB', 'RBR', 'RBS']\n\n\ndef is_adjective(tag):\n    return tag in ['JJ', 'JJR', 'JJS']\n\n\ndef penn_to_wn(tag):\n    if is_adjective(tag):\n        return wn.ADJ\n    elif is_noun(tag):\n        return wn.NOUN\n    elif is_adverb(tag):\n        return wn.ADV\n    elif is_verb(tag):\n        return wn.VERB\n    return None\n</code></pre>\n<p>Hope this helps.</p>\n", "abstract": "The best way to troubleshoot this is to actually look in Wordnet. Take a look here: Loving in wordnet. As you can see, there is actually an adjective \"loving\" present in Wordnet. As a matter of fact, there is even the adverb \"lovingly\": lovingly in Wordnet. Because wordnet doesn't actually know what part of speech you actually want, it defaults to noun ('n' in Wordnet). If you are using Penn Treebank tag set, here's some handy function for transforming Penn to WN tags: Hope this helps."}, {"id": 48355033, "score": 6, "vote": 0, "content": "<p>it's clearer and more effective than enumeration\uff1a</p>\n<pre><code class=\"python\">from nltk.corpus import wordnet\n\ndef get_wordnet_pos(self, treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\ndef penn_to_wn(tag):\n    return get_wordnet_pos(tag)\n</code></pre>\n", "abstract": "it's clearer and more effective than enumeration\uff1a"}, {"id": 57686805, "score": 3, "vote": 0, "content": "<p>As an extension to the accepted answer from <code>@Fred Foo</code> above;</p>\n<pre><code class=\"python\">from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n\nlem = WordNetLemmatizer()\nword = input(\"Enter word:\\t\")\n\n# Get the single character pos constant from pos_tag like this:\npos_label = (pos_tag(word_tokenize(word))[0][1][0]).lower()\n\n# pos_refs = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n#            'v': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n#            'r': ['RB', 'RBR', 'RBS'],\n#            'a': ['JJ', 'JJR', 'JJS']}\n\nif pos_label == 'j': pos_label = 'a'    # 'j' &lt;--&gt; 'a' reassignment\n\nif pos_label in ['r']:  # For adverbs it's a bit different\n    print(wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name())\nelif pos_label in ['a', 's', 'v']: # For adjectives and verbs\n    print(lem.lemmatize(word, pos=pos_label))\nelse:   # For nouns and everything else as it is the default kwarg\n    print(lem.lemmatize(word))\n</code></pre>\n", "abstract": "As an extension to the accepted answer from @Fred Foo above;"}]}, {"link": "https://stackoverflow.com/questions/3182268/nltk-and-language-detection", "question": {"id": "3182268", "title": "NLTK and language detection", "content": "<p>How do I detect what language a text is written in using NLTK?</p>\n<p>The examples I've seen use <code>nltk.detect</code>, but when I've installed it on my mac, I cannot find this package.</p>\n", "abstract": "How do I detect what language a text is written in using NLTK? The examples I've seen use nltk.detect, but when I've installed it on my mac, I cannot find this package."}, "answers": [{"id": 3384659, "score": 41, "vote": 0, "content": "<p>Have you come across the following code snippet?</p>\n<pre><code class=\"python\">english_vocab = set(w.lower() for w in nltk.corpus.words.words())\ntext_vocab = set(w.lower() for w in text if w.lower().isalpha())\nunusual = text_vocab.difference(english_vocab) \n</code></pre>\n<p>from <a href=\"http://groups.google.com/group/nltk-users/browse_thread/thread/a5f52af2cbc4cfeb?pli=1&amp;safe=active\" rel=\"noreferrer\">http://groups.google.com/group/nltk-users/browse_thread/thread/a5f52af2cbc4cfeb?pli=1&amp;safe=active</a></p>\n<p>Or the following demo file?</p>\n<p><a href=\"https://web.archive.org/web/20120202055535/http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/misc/langid.py\" rel=\"noreferrer\">https://web.archive.org/web/20120202055535/http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/misc/langid.py</a></p>\n", "abstract": "Have you come across the following code snippet? from http://groups.google.com/group/nltk-users/browse_thread/thread/a5f52af2cbc4cfeb?pli=1&safe=active Or the following demo file? https://web.archive.org/web/20120202055535/http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/misc/langid.py"}, {"id": 38752290, "score": 30, "vote": 0, "content": "<p>This library is not from NLTK either but certainly helps. </p>\n<blockquote>\n<p>$ sudo pip install langdetect </p>\n</blockquote>\n<p><em>Supported Python versions 2.6, 2.7, 3.x.</em></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from langdetect import detect\n\n&gt;&gt;&gt; detect(\"War doesn't show who's right, just who's left.\")\n'en'\n&gt;&gt;&gt; detect(\"Ein, zwei, drei, vier\")\n'de'\n</code></pre>\n<p><a href=\"https://pypi.python.org/pypi/langdetect?\" rel=\"noreferrer\">https://pypi.python.org/pypi/langdetect?</a></p>\n<p>P.S.: Don't expect this to work correctly always:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; detect(\"today is a good day\")\n'so'\n&gt;&gt;&gt; detect(\"today is a good day.\")\n'so'\n&gt;&gt;&gt; detect(\"la vita e bella!\")\n'it'\n&gt;&gt;&gt; detect(\"khoobi? khoshi?\")\n'so'\n&gt;&gt;&gt; detect(\"wow\")\n'pl'\n&gt;&gt;&gt; detect(\"what a day\")\n'en'\n&gt;&gt;&gt; detect(\"yay!\")\n'so'\n</code></pre>\n", "abstract": "This library is not from NLTK either but certainly helps.  $ sudo pip install langdetect  Supported Python versions 2.6, 2.7, 3.x. https://pypi.python.org/pypi/langdetect? P.S.: Don't expect this to work correctly always:"}, {"id": 17386925, "score": 20, "vote": 0, "content": "<p>Although this is not in the NLTK, I have had great results with another Python-based library : </p>\n<p><a href=\"https://github.com/saffsd/langid.py\">https://github.com/saffsd/langid.py</a></p>\n<p>This is very simple to import and includes a large number of languages in its model.</p>\n", "abstract": "Although this is not in the NLTK, I have had great results with another Python-based library :  https://github.com/saffsd/langid.py This is very simple to import and includes a large number of languages in its model."}, {"id": 58432286, "score": 8, "vote": 0, "content": "<p>Super late but, you could use <code>textcat</code> classifier in <code>nltk</code>, <a href=\"https://www.nltk.org/api/nltk.classify.html#nltk.classify.textcat.TextCat\" rel=\"noreferrer\">here</a>. This <a href=\"http://www.let.rug.nl/~vannoord/TextCat/textcat.pdf\" rel=\"noreferrer\">paper</a> discusses the algorithm. </p>\n<p>It returns a country code in <em>ISO 639-3</em>, so I would use <code>pycountry</code> to get the  full name.</p>\n<p>For example, load the libraries</p>\n<pre><code class=\"python\">import nltk\nimport pycountry\nfrom nltk.stem import SnowballStemmer\n</code></pre>\n<p><strong>Now let's look at two phrases, and <code>guess</code> their language:</strong></p>\n<pre><code class=\"python\">phrase_one = \"good morning\"\nphrase_two = \"goeie more\"\n\ntc = nltk.classify.textcat.TextCat() \nguess_one = tc.guess_language(phrase_one)\nguess_two = tc.guess_language(phrase_two)\n\nguess_one_name = pycountry.languages.get(alpha_3=guess_one).name\nguess_two_name = pycountry.languages.get(alpha_3=guess_two).name\nprint(guess_one_name)\nprint(guess_two_name)\n\nEnglish\nAfrikaans\n</code></pre>\n<p>You could then pass them into other <code>nltk</code> functions, for example:</p>\n<pre><code class=\"python\">stemmer = SnowballStemmer(guess_one_name.lower())\ns1 = \"walking\"\nprint(stemmer.stem(s1))\nwalk\n</code></pre>\n<p><em>Disclaimer obviously this will not always work, especially for sparse data</em></p>\n<p>Extreme example</p>\n<pre><code class=\"python\">guess_example = tc.guess_language(\"hello\")\nprint(pycountry.languages.get(alpha_3=guess_example).name)\nKonkani (individual language)\n</code></pre>\n", "abstract": "Super late but, you could use textcat classifier in nltk, here. This paper discusses the algorithm.  It returns a country code in ISO 639-3, so I would use pycountry to get the  full name. For example, load the libraries Now let's look at two phrases, and guess their language: You could then pass them into other nltk functions, for example: Disclaimer obviously this will not always work, especially for sparse data Extreme example"}]}, {"link": "https://stackoverflow.com/questions/14095971/how-to-tweak-the-nltk-sentence-tokenizer", "question": {"id": "14095971", "title": "How to tweak the NLTK sentence tokenizer", "content": "<p>I'm using NLTK to analyze a few classic texts and I'm running in to trouble tokenizing the text by sentence. For example, here's what I get for a snippet from <em><a href=\"http://www.gutenberg.org/cache/epub/2701/pg2701.txt\">Moby Dick</a></em>:</p>\n<pre><code class=\"python\">import nltk\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n\n'''\n(Chapter 16)\nA clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\nthat's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"\n'''\nsample = 'A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs. Hussey?\"'\n\nprint \"\\n-----\\n\".join(sent_tokenize.tokenize(sample))\n'''\nOUTPUT\n\"A clam for supper?\n-----\na cold clam; is THAT what you mean, Mrs.\n-----\nHussey?\n-----\n\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs.\n-----\nHussey?\n-----\n\"\n'''\n</code></pre>\n<p>I don't expect perfection here, considering that Melville's syntax is a bit dated, but NLTK ought to be able to handle terminal double quotes and titles like \"Mrs.\" Since the tokenizer is the result of an unsupervised training algo, however, I can't figure out how to tinker with it.</p>\n<p>Anyone have recommendations for a better sentence tokenizer? I'd prefer a simple heuristic that I can hack rather than having to train my own parser. </p>\n", "abstract": "I'm using NLTK to analyze a few classic texts and I'm running in to trouble tokenizing the text by sentence. For example, here's what I get for a snippet from Moby Dick: I don't expect perfection here, considering that Melville's syntax is a bit dated, but NLTK ought to be able to handle terminal double quotes and titles like \"Mrs.\" Since the tokenizer is the result of an unsupervised training algo, however, I can't figure out how to tinker with it. Anyone have recommendations for a better sentence tokenizer? I'd prefer a simple heuristic that I can hack rather than having to train my own parser. "}, "answers": [{"id": 14103163, "score": 51, "vote": 0, "content": "<p>You need to supply a list of abbreviations to the tokenizer, like so:</p>\n<pre><code class=\"python\">from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\npunkt_param = PunktParameters()\npunkt_param.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'prof', 'inc'])\nsentence_splitter = PunktSentenceTokenizer(punkt_param)\ntext = \"is THAT what you mean, Mrs. Hussey?\"\nsentences = sentence_splitter.tokenize(text)\n</code></pre>\n<p>sentences is now:</p>\n<pre><code class=\"python\">['is THAT what you mean, Mrs. Hussey?']\n</code></pre>\n<p>Update: This does not work if the last word of the sentence has an apostrophe or a quotation mark attached to it (like <strong>Hussey?'</strong>). So a quick-and-dirty way around this is to put spaces in front of apostrophes and quotes that follow sentence-end symbols (.!?):</p>\n<pre><code class=\"python\">text = text.replace('?\"', '? \"').replace('!\"', '! \"').replace('.\"', '. \"')\n</code></pre>\n", "abstract": "You need to supply a list of abbreviations to the tokenizer, like so: sentences is now: Update: This does not work if the last word of the sentence has an apostrophe or a quotation mark attached to it (like Hussey?'). So a quick-and-dirty way around this is to put spaces in front of apostrophes and quotes that follow sentence-end symbols (.!?):"}, {"id": 25375857, "score": 41, "vote": 0, "content": "<p>You can modify the NLTK's pre-trained English sentence tokenizer to recognize more abbreviations by adding them to the set <code>_params.abbrev_types</code>. For example: </p>\n<pre><code class=\"python\">extra_abbreviations = ['dr', 'vs', 'mr', 'mrs', 'prof', 'inc', 'i.e']\nsentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nsentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n</code></pre>\n<p>Note that the abbreviations must be specified without the final period, but do include any internal periods, as in <code>'i.e'</code> above. For details about the other tokenizer parameters, refer to <a href=\"http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktParameters\" rel=\"noreferrer\">the relevant documentation.</a></p>\n", "abstract": "You can modify the NLTK's pre-trained English sentence tokenizer to recognize more abbreviations by adding them to the set _params.abbrev_types. For example:  Note that the abbreviations must be specified without the final period, but do include any internal periods, as in 'i.e' above. For details about the other tokenizer parameters, refer to the relevant documentation."}, {"id": 14101885, "score": 9, "vote": 0, "content": "<p>You can tell the <code>PunktSentenceTokenizer.tokenize</code> method to include \"terminal\" double quotes with the rest of the sentence by setting the <code>realign_boundaries</code> parameter to <code>True</code>. See the code below for an example.</p>\n<p>I do not know a clean way to prevent text like <code>Mrs. Hussey</code> from being split into two sentences. However, here is a hack which </p>\n<ul>\n<li>mangles all occurrences of <code>Mrs. Hussey</code> to <code>Mrs._Hussey</code>,</li>\n<li>then splits the text into sentences with <code>sent_tokenize.tokenize</code>,</li>\n<li>then for each sentence, unmangles <code>Mrs._Hussey</code> back to <code>Mrs. Hussey</code></li>\n</ul>\n<p>I wish I knew a better way, but this might work in a pinch.</p>\n<hr/>\n<pre><code class=\"python\">import nltk\nimport re\nimport functools\n\nmangle = functools.partial(re.sub, r'([MD]rs?[.]) ([A-Z])', r'\\1_\\2')\nunmangle = functools.partial(re.sub, r'([MD]rs?[.])_([A-Z])', r'\\1 \\2')\n\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n\nsample = '''\"A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs. Hussey?\"'''    \n\nsample = mangle(sample)\nsentences = [unmangle(sent) for sent in sent_tokenize.tokenize(\n    sample, realign_boundaries = True)]    \n\nprint u\"\\n-----\\n\".join(sentences)\n</code></pre>\n<p>yields</p>\n<pre><code class=\"python\">\"A clam for supper?\n-----\na cold clam; is THAT what you mean, Mrs. Hussey?\"\n-----\nsays I, \"but that's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"\n</code></pre>\n", "abstract": "You can tell the PunktSentenceTokenizer.tokenize method to include \"terminal\" double quotes with the rest of the sentence by setting the realign_boundaries parameter to True. See the code below for an example. I do not know a clean way to prevent text like Mrs. Hussey from being split into two sentences. However, here is a hack which  I wish I knew a better way, but this might work in a pinch. yields"}, {"id": 24049094, "score": 3, "vote": 0, "content": "<p>So I had a similar issue and tried out vpekar's solution above.</p>\n<p>Perhaps mine is some sort of edge case but I observed the same behavior after applying the replacements, however, when I tried replacing the punctuation with the quotations placed before them, I got the output I was looking for. Presumably lack of adherence to MLA is less important than retaining the original quote as a single sentence.</p>\n<p>To be more clear:</p>\n<pre><code class=\"python\">text = text.replace('?\"', '\"?').replace('!\"', '\"!').replace('.\"', '\".')\n</code></pre>\n<p>If MLA is important though you could always go back and reverse these changes wherever it counts.</p>\n", "abstract": "So I had a similar issue and tried out vpekar's solution above. Perhaps mine is some sort of edge case but I observed the same behavior after applying the replacements, however, when I tried replacing the punctuation with the quotations placed before them, I got the output I was looking for. Presumably lack of adherence to MLA is less important than retaining the original quote as a single sentence. To be more clear: If MLA is important though you could always go back and reverse these changes wherever it counts."}]}, {"link": "https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset", "question": {"id": "42821330", "title": "Restore original text from Keras\u2019s imdb dataset", "content": "<p>Restore original text from Keras\u2019s imdb dataset</p>\n<p>I want to restore imdb\u2019s original text from Keras\u2019s imdb dataset.</p>\n<p>First, when I load Keras\u2019s imdb dataset, it returned sequence of word index.</p>\n<p>\n<pre><code class=\"python\">&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()\n&gt;&gt;&gt; X_train[0]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n</code></pre>\n<p>I found imdb.get_word_index method(), it returns word index dictionary like {\u2018create\u2019: 984, \u2018make\u2019: 94,\u2026}. For converting, I create index word dictionary.\n\n\n<pre><code class=\"python\">&gt;&gt;&gt; word_index = imdb.get_word_index()\n&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}\n</code></pre>\n<p>Then, I tried to restore original text like following.</p>\n<p>\n<pre><code class=\"python\">&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n</code></pre>\n<p>I\u2019m not good at English, but I know this sentence is something strange.</p>\n<p>Why is this happened? How can I restore original text?</p>\n</p></p></p>", "abstract": "Restore original text from Keras\u2019s imdb dataset I want to restore imdb\u2019s original text from Keras\u2019s imdb dataset. First, when I load Keras\u2019s imdb dataset, it returned sequence of word index. \n>>> (X_train, y_train), (X_test, y_test) = imdb.load_data()\n>>> X_train[0]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n\nI found imdb.get_word_index method(), it returns word index dictionary like {\u2018create\u2019: 984, \u2018make\u2019: 94,\u2026}. For converting, I create index word dictionary.\n\n\n>>> word_index = imdb.get_word_index()\n>>> index_word = {v:k for k,v in word_index.items()}\n\nThen, I tried to restore original text like following.\n\n>>> ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n\nI\u2019m not good at English, but I know this sentence is something strange.\nWhy is this happened? How can I restore original text?\n I found imdb.get_word_index method(), it returns word index dictionary like {\u2018create\u2019: 984, \u2018make\u2019: 94,\u2026}. For converting, I create index word dictionary.\n\n\n>>> word_index = imdb.get_word_index()\n>>> index_word = {v:k for k,v in word_index.items()}\n\nThen, I tried to restore original text like following.\n\n>>> ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n\nI\u2019m not good at English, but I know this sentence is something strange.\nWhy is this happened? How can I restore original text?\n Then, I tried to restore original text like following. \n>>> ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n\nI\u2019m not good at English, but I know this sentence is something strange.\nWhy is this happened? How can I restore original text?\n I\u2019m not good at English, but I know this sentence is something strange. Why is this happened? How can I restore original text?"}, "answers": [{"id": 44891281, "score": 49, "vote": 0, "content": "<p>Your example is coming out as gibberish, it's much worse than just some missing stop words.</p>\n<p>If you re-read the docs for the <code>start_char</code>, <code>oov_char</code>, and <code>index_from</code> parameters of the [<code>keras.datasets.imdb.load_data</code>](<a href=\"https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\" rel=\"noreferrer\">https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification</a>\n) method they explain what is happening:</p>\n<p><code>start_char</code>: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.</p>\n<p><code>oov_char</code>: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character.</p>\n<p><code>index_from</code>: int. Index actual words with this index and higher.</p>\n<p>That dictionary you inverted assumes the word indices start from <code>1</code>.</p>\n<p>But the indices returned my keras have <code>&lt;START&gt;</code> and <code>&lt;UNKNOWN&gt;</code> as indexes <code>1</code> and <code>2</code>. (And it assumes you will use <code>0</code> for <code>&lt;PADDING&gt;</code>).</p>\n<p>This works for me:</p>\n<pre><code class=\"python\">import keras\nNUM_WORDS=1000 # only use top 1000 words\nINDEX_FROM=3   # word index offset\n\ntrain,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\ntrain_x,train_y = train\ntest_x,test_y = test\n\nword_to_id = keras.datasets.imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id[\"&lt;PAD&gt;\"] = 0\nword_to_id[\"&lt;START&gt;\"] = 1\nword_to_id[\"&lt;UNK&gt;\"] = 2\nword_to_id[\"&lt;UNUSED&gt;\"] = 3\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in train_x[0] ))\n</code></pre>\n<p>The punctuation is missing, but that's all:</p>\n<pre><code class=\"python\">\"&lt;START&gt; this film was just brilliant casting &lt;UNK&gt; &lt;UNK&gt; story\n direction &lt;UNK&gt; really &lt;UNK&gt; the part they played and you could just\n imagine being there robert &lt;UNK&gt; is an amazing actor ...\"\n</code></pre>\n", "abstract": "Your example is coming out as gibberish, it's much worse than just some missing stop words. If you re-read the docs for the start_char, oov_char, and index_from parameters of the [keras.datasets.imdb.load_data](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n) method they explain what is happening: start_char: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character. oov_char: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character. index_from: int. Index actual words with this index and higher. That dictionary you inverted assumes the word indices start from 1. But the indices returned my keras have <START> and <UNKNOWN> as indexes 1 and 2. (And it assumes you will use 0 for <PADDING>). This works for me: The punctuation is missing, but that's all:"}, {"id": 44635045, "score": 8, "vote": 0, "content": "<p>You can get the original dataset without stop words removed using get_file from  keras.utils.data_utils:</p>\n<pre><code class=\"python\">path = get_file('imdb_full.pkl',\n               origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n                md5_hash='d091312047c43cf9e4e38fef92437263')\nf = open(path, 'rb')\n(training_data, training_labels), (test_data, test_labels) = pickle.load(f)\n</code></pre>\n<p>Credit - Jeremy Howards <a href=\"http://course.fast.ai/lessons/lesson5.html\" rel=\"noreferrer\">fast.ai course lesson 5</a></p>\n", "abstract": "You can get the original dataset without stop words removed using get_file from  keras.utils.data_utils: Credit - Jeremy Howards fast.ai course lesson 5"}, {"id": 42821426, "score": 2, "vote": 0, "content": "<p>This happened because of a basic <code>NLP</code> data preparation. Loads of the so called <em>stop words</em> were removed from text in order to make learning feasible. Usually - also the most of puntuation and less frequent words are removed from text during preprocessing. I think that the only way to restore original text is to find the most matching texts at IMDB using e.g. a Google's browser API.</p>\n", "abstract": "This happened because of a basic NLP data preparation. Loads of the so called stop words were removed from text in order to make learning feasible. Usually - also the most of puntuation and less frequent words are removed from text during preprocessing. I think that the only way to restore original text is to find the most matching texts at IMDB using e.g. a Google's browser API."}, {"id": 59884709, "score": 2, "vote": 0, "content": "<p>This encoding will work along with the labels:</p>\n<pre><code class=\"python\">from keras.datasets import imdb\n(x_train,y_train),(x_test,y_test) = imdb.load_data()\nword_index = imdb.get_word_index() # get {word : index}\nindex_word = {v : k for k,v in word_index.items()} # get {index : word}\n\nindex = 1\nprint(\" \".join([index_word[idx] for idx in x_train[index]]))\nprint(\"positve\" if y_train[index]==1 else \"negetive\")\n</code></pre>\n<p>Upvote if helps. :)</p>\n", "abstract": "This encoding will work along with the labels: Upvote if helps. :)"}, {"id": 58048264, "score": 0, "vote": 0, "content": "<p>The indices are offset by 3 because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\" and \"unknown\". The following should work.</p>\n<pre><code class=\"python\">imdb = tf.keras.datasets.imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\nreview = [reverse_word_index.get(i-3, \"?\") for i in train_data[0]]\n</code></pre>\n", "abstract": "The indices are offset by 3 because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\" and \"unknown\". The following should work."}, {"id": 59341922, "score": 0, "vote": 0, "content": "<p>This works for me:</p>\n<pre><code class=\"python\">word_index = imdb.get_word_index()                                    \nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])            \ndecoded_review = ' '.join([reverse_word_index.get(i - 3, \"\") for i in train_data[0]])\n</code></pre>\n", "abstract": "This works for me:"}, {"id": 61549942, "score": 0, "vote": 0, "content": "<p>To get an equivalent array of all the reviews:</p>\n<pre><code class=\"python\">def decode_imdb_reviews(text_data):\n    result = [0 for x in range(len(text_data))]\n    word_index = imdb.get_word_index()\n    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n    for review in range(0,len(text_data)):\n        for index in enumerate(text_data[review]):\n            decoded_review = ' '.join([reverse_word_index.get(index - 3, '#') for index in text_data[review]])\n        result[review] = decoded_review\n    return result\n\ntext_data = []\ntext_data = decode_imdb_reviews(train_data)\n</code></pre>\n", "abstract": "To get an equivalent array of all the reviews:"}, {"id": 64272290, "score": 0, "vote": 0, "content": "<p>Try below code. this code worked.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\"># load dataset \nfrom tensorflow.keras.datasets import imdb \n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=500)\n\n\n# vec 2 num \nimdb_vec2num = imdb.get_word_index(path=\"imdb_word_index.json\")\n# num 2 vec \nimdb_num2vec = {value+3:key for key, value in imdb_vec2num.items()}\nimdb_num2vec[0] = \"&lt;PAD&gt;\"\nimdb_num2vec[1] = \"&lt;START&gt;\"\nimdb_num2vec[2] = \"&lt;UNK&gt;\"\nimdb_num2vec[3] = \"&lt;UNUSED&gt;\"\n\n# index-word table\nimdb_num2vec = {value:key for key, value in imdb_index.items()}\n\n# change encoded sentences to sentences\ndef restore_sentences(num2word, encoded_sentences): \n    sentences = []\n    for encoded_sentence in encoded_sentences:\n        sentences.append([num2word[ele] for ele in encoded_sentence])\n    return sentences\n\n# example \nsentences = restore_sentences(imdb_num2vec, x_train)\n</code></pre>\n", "abstract": "Try below code. this code worked."}, {"id": 61974682, "score": -1, "vote": 0, "content": "<pre><code class=\"python\">from keras.datasets import imdb\nNUM_WORDS=1000 # only use top 1000 words\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n</code></pre>\n<p>Get the index details:</p>\n<pre><code class=\"python\">word_to_id = keras.datasets.imdb.get_word_index()\n</code></pre>\n<p>Build the Key-Value pair:</p>\n<pre><code class=\"python\">id_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in x_train[0] ))\n</code></pre>\n", "abstract": "Get the index details: Build the Key-Value pair:"}]}, {"link": "https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk", "question": {"id": "7443330", "title": "How do I do dependency parsing in NLTK?", "content": "<p>Going through the NLTK book, it's not clear how to generate a dependency tree from a given sentence.</p>\n<p>The relevant section of the book: <a href=\"https://www.nltk.org/book/ch08.html#dependencies-and-dependency-grammar\" rel=\"noreferrer\">sub-chapter on dependency grammar</a> gives an <a href=\"https://www.nltk.org/book/ch08.html#fig-depgraph0\" rel=\"noreferrer\">example figure</a> but it doesn't show how to parse a sentence to come up with those relationships - or maybe I'm missing something fundamental in NLP?</p>\n<p><strong>EDIT:</strong>\nI want something similar to what the <a href=\"http://nlp.stanford.edu:8080/parser/\" rel=\"noreferrer\">stanford parser</a> does:\nGiven a sentence \"I shot an elephant in my sleep\", it should return something like:</p>\n<pre><code class=\"python\">nsubj(shot-2, I-1)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n</code></pre>\n", "abstract": "Going through the NLTK book, it's not clear how to generate a dependency tree from a given sentence. The relevant section of the book: sub-chapter on dependency grammar gives an example figure but it doesn't show how to parse a sentence to come up with those relationships - or maybe I'm missing something fundamental in NLP? EDIT:\nI want something similar to what the stanford parser does:\nGiven a sentence \"I shot an elephant in my sleep\", it should return something like:"}, "answers": [{"id": 33808164, "score": 80, "vote": 0, "content": "<p>We can use Stanford Parser from NLTK.</p>\n<h3>Requirements</h3>\n<p>You need to download two things from their website: </p>\n<ol>\n<li>The <a href=\"https://nlp.stanford.edu/software/lex-parser.shtml#Download\" rel=\"noreferrer\">Stanford CoreNLP parser</a>.</li>\n<li><a href=\"http://nlp.stanford.edu/software/corenlp.shtml\" rel=\"noreferrer\">Language model</a> for your desired language (e.g. <a href=\"http://nlp.stanford.edu/software/stanford-english-corenlp-2018-02-27-models.jar\" rel=\"noreferrer\">english language model</a>)</li>\n</ol>\n<h3>Warning!</h3>\n<p>Make sure that your language model version matches your Stanford CoreNLP parser version!</p>\n<p>The current CoreNLP version as of May 22, 2018 is 3.9.1. </p>\n<p>After downloading the two files, extract the zip file anywhere you like.</p>\n<h3>Python Code</h3>\n<p>Next, load the model and use it through NLTK</p>\n<pre><code class=\"python\">from nltk.parse.stanford import StanfordDependencyParser\n\npath_to_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser.jar'\npath_to_models_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n\ndependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n\nresult = dependency_parser.raw_parse('I shot an elephant in my sleep')\ndep = result.next()\n\nlist(dep.triples())\n</code></pre>\n<h3>Output</h3>\n<p>The output of the last line is:</p>\n<pre><code class=\"python\">[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')),\n ((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')),\n ((u'elephant', u'NN'), u'det', (u'an', u'DT')),\n ((u'shot', u'VBD'), u'prep', (u'in', u'IN')),\n ((u'in', u'IN'), u'pobj', (u'sleep', u'NN')),\n ((u'sleep', u'NN'), u'poss', (u'my', u'PRP$'))]\n</code></pre>\n<p>I think this is what you want.</p>\n", "abstract": "We can use Stanford Parser from NLTK. You need to download two things from their website:  Make sure that your language model version matches your Stanford CoreNLP parser version! The current CoreNLP version as of May 22, 2018 is 3.9.1.  After downloading the two files, extract the zip file anywhere you like. Next, load the model and use it through NLTK The output of the last line is: I think this is what you want."}, {"id": 7446611, "score": 8, "vote": 0, "content": "<p>I think you could use a corpus-based dependency parser instead of the grammar-based one NLTK provides.</p>\n<p>Doing corpus-based dependency parsing on a even a small amount of text in Python is not ideal performance-wise. So in NLTK they do provide a <a href=\"http://nltk.googlecode.com/svn/trunk/doc/api/nltk.parse.malt.MaltParser-class.html\" rel=\"nofollow noreferrer\">wrapper</a> to <a href=\"http://maltparser.org/\" rel=\"nofollow noreferrer\">MaltParser</a>, a corpus based dependency parser.</p>\n<p>You might find this other question about <a href=\"https://stackoverflow.com/questions/2705888/rdf-of-sentences/2706567#2706567\">RDF representation of sentences</a> relevant.</p>\n", "abstract": "I think you could use a corpus-based dependency parser instead of the grammar-based one NLTK provides. Doing corpus-based dependency parsing on a even a small amount of text in Python is not ideal performance-wise. So in NLTK they do provide a wrapper to MaltParser, a corpus based dependency parser. You might find this other question about RDF representation of sentences relevant."}, {"id": 42949939, "score": 7, "vote": 0, "content": "<p>If you need better performance, then spacy (<a href=\"https://spacy.io/\" rel=\"nofollow noreferrer\">https://spacy.io/</a>) is the best choice. Usage is very simple:</p>\n<pre><code class=\"python\">import spacy\n\nnlp = spacy.load('en')\nsents = nlp(u'A woman is walking through the door.')\n</code></pre>\n<p>You'll get a dependency tree as output, and you can dig out very easily every information you need. You can also define your own custom pipelines. See more on their website.</p>\n<p><a href=\"https://spacy.io/docs/usage/\" rel=\"nofollow noreferrer\">https://spacy.io/docs/usage/</a></p>\n", "abstract": "If you need better performance, then spacy (https://spacy.io/) is the best choice. Usage is very simple: You'll get a dependency tree as output, and you can dig out very easily every information you need. You can also define your own custom pipelines. See more on their website. https://spacy.io/docs/usage/"}, {"id": 28891566, "score": 3, "vote": 0, "content": "<p>If you want to be serious about dependance parsing don't use the NLTK, all the algorithms are dated, and slow. Try something like this: <a href=\"https://spacy.io/\" rel=\"nofollow\">https://spacy.io/</a></p>\n", "abstract": "If you want to be serious about dependance parsing don't use the NLTK, all the algorithms are dated, and slow. Try something like this: https://spacy.io/"}, {"id": 50832517, "score": 3, "vote": 0, "content": "<p>To use Stanford Parser from NLTK</p>\n<p><strong>1) Run CoreNLP Server at localhost</strong><br/>\nDownload <a href=\"https://stanfordnlp.github.io/CoreNLP/download.html\" rel=\"nofollow noreferrer\">Stanford CoreNLP here</a> (and also model file for your language).\nThe server can be started by running the following command (more details <a href=\"https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\" rel=\"nofollow noreferrer\">here</a>)</p>\n<pre><code class=\"python\"># Run the server using all jars in the current directory (e.g., the CoreNLP home directory)\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n</code></pre>\n<p>or by NLTK API (need to configure the <code>CORENLP_HOME</code> environment variable first)  </p>\n<pre><code class=\"python\">os.environ[\"CORENLP_HOME\"] = \"dir\"\nclient = corenlp.CoreNLPClient()\n# do something\nclient.stop()\n</code></pre>\n<p><strong>2) Call the dependency parser from NLTK</strong></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPDependencyParser\n&gt;&gt;&gt; dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n&gt;&gt;&gt; parse, = dep_parser.raw_parse(\n...     'The quick brown fox jumps over the lazy dog.'\n... )\n&gt;&gt;&gt; print(parse.to_conll(4))  \nThe     DT      4       det\nquick   JJ      4       amod\nbrown   JJ      4       amod\nfox     NN      5       nsubj\njumps   VBZ     0       ROOT\nover    IN      9       case\nthe     DT      9       det\nlazy    JJ      9       amod\ndog     NN      5       nmod\n.       .       5       punct\n</code></pre>\n<p>See detail <a href=\"https://www.nltk.org/api/nltk.parse.html#nltk.parse.corenlp.CoreNLPDependencyParser\" rel=\"nofollow noreferrer\">documentation here</a>, also this question <a href=\"https://stackoverflow.com/questions/47584738/nltk-corenlpdependencyparser-failed-to-establish-connection\">NLTK CoreNLPDependencyParser: Failed to establish connection</a>.</p>\n", "abstract": "To use Stanford Parser from NLTK 1) Run CoreNLP Server at localhost\nDownload Stanford CoreNLP here (and also model file for your language).\nThe server can be started by running the following command (more details here) or by NLTK API (need to configure the CORENLP_HOME environment variable first)   2) Call the dependency parser from NLTK See detail documentation here, also this question NLTK CoreNLPDependencyParser: Failed to establish connection."}, {"id": 17846159, "score": 2, "vote": 0, "content": "<p>From the Stanford Parser documentation: \"the dependencies can be obtained using our software [...] on phrase-structure trees using the EnglishGrammaticalStructure class available in the parser package.\" <a href=\"http://nlp.stanford.edu/software/stanford-dependencies.shtml\" rel=\"nofollow\">http://nlp.stanford.edu/software/stanford-dependencies.shtml</a></p>\n<p>The dependencies manual also mentions: \"Or our conversion tool can convert the\noutput of other constituency parsers to the Stanford Dependencies representation.\" <a href=\"http://nlp.stanford.edu/software/dependencies_manual.pdf\" rel=\"nofollow\">http://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>\n<p>Neither functionality seem to be implemented in NLTK currently.</p>\n", "abstract": "From the Stanford Parser documentation: \"the dependencies can be obtained using our software [...] on phrase-structure trees using the EnglishGrammaticalStructure class available in the parser package.\" http://nlp.stanford.edu/software/stanford-dependencies.shtml The dependencies manual also mentions: \"Or our conversion tool can convert the\noutput of other constituency parsers to the Stanford Dependencies representation.\" http://nlp.stanford.edu/software/dependencies_manual.pdf Neither functionality seem to be implemented in NLTK currently."}, {"id": 55912913, "score": 1, "vote": 0, "content": "<p>A little late to the party, but I wanted to add some example code with SpaCy that gets you your desired output:</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I shot an elephant in my sleep\")\nfor token in doc:\n    print(\"{2}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n</code></pre>\n<p>And here's the output, very similar to your desired output:</p>\n<pre><code class=\"python\">nsubj(shot-2, I-1)\nROOT(shot-2, shot-2)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n</code></pre>\n<p>Hope that helps!</p>\n", "abstract": "A little late to the party, but I wanted to add some example code with SpaCy that gets you your desired output: And here's the output, very similar to your desired output: Hope that helps!"}]}, {"link": "https://stackoverflow.com/questions/35857519/efficiently-count-word-frequencies-in-python", "question": {"id": "35857519", "title": "Efficiently count word frequencies in python", "content": "<p>I'd like to count frequencies of all words in a text file.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; countInFile('test.txt')\n</code></pre>\n<p>should return <code>{'aaa':1, 'bbb': 2, 'ccc':1}</code> if the target text file is like:</p>\n<pre><code class=\"python\"># test.txt\naaa bbb ccc\nbbb\n</code></pre>\n<p>I've implemented it with pure python following <a href=\"https://stackoverflow.com/questions/12117576/how-to-count-word-frequencies-within-a-file-in-python\">some posts</a>. However, I've found out pure-python ways are insufficient due to huge file size (&gt; 1GB).</p>\n<p>I think borrowing sklearn's power is a candidate.</p>\n<p>If you let CountVectorizer count frequencies for each line, I guess you will get word frequencies by summing up each column. But, it sounds a bit indirect way.</p>\n<p>What is the most efficient and straightforward way to count words in a file with python?</p>\n<h3>Update</h3>\n<p>My (very slow) code is here:</p>\n<pre><code class=\"python\">from collections import Counter\n\ndef get_term_frequency_in_file(source_file_path):\n    wordcount = {}\n    with open(source_file_path) as f:\n        for line in f:\n            line = line.lower().translate(None, string.punctuation)\n            this_wordcount = Counter(line.split())\n            wordcount = add_merge_two_dict(wordcount, this_wordcount)\n    return wordcount\n\ndef add_merge_two_dict(x, y):\n    return { k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y) }\n</code></pre>\n", "abstract": "I'd like to count frequencies of all words in a text file. should return {'aaa':1, 'bbb': 2, 'ccc':1} if the target text file is like: I've implemented it with pure python following some posts. However, I've found out pure-python ways are insufficient due to huge file size (> 1GB). I think borrowing sklearn's power is a candidate. If you let CountVectorizer count frequencies for each line, I guess you will get word frequencies by summing up each column. But, it sounds a bit indirect way. What is the most efficient and straightforward way to count words in a file with python? My (very slow) code is here:"}, "answers": [{"id": 35857833, "score": 49, "vote": 0, "content": "<p>The most succinct approach is to use the tools Python gives you.</p>\n<pre><code class=\"python\">from future_builtins import map  # Only on Python 2\n\nfrom collections import Counter\nfrom itertools import chain\n\ndef countInFile(filename):\n    with open(filename) as f:\n        return Counter(chain.from_iterable(map(str.split, f)))\n</code></pre>\n<p>That's it. <code>map(str.split, f)</code> is making a generator that returns <code>list</code>s of words from each line. Wrapping in <code>chain.from_iterable</code> converts that to a single generator that produces a word at a time. <code>Counter</code> takes an input iterable and counts all unique values in it. At the end, you <code>return</code> a  <code>dict</code>-like object (a <code>Counter</code>) that stores all unique words and their counts, and during creation, you only store a line of data at a time and the total counts, not the whole file at once.</p>\n<p>In theory, on Python 2.7 and 3.1, you might do slightly better looping over the chained results yourself and using a <code>dict</code> or <code>collections.defaultdict(int)</code> to count (because <code>Counter</code> is implemented in Python, which can make it slower in some cases), but letting <code>Counter</code> do the work is simpler and more self-documenting (I mean, the whole goal is counting, so use a <code>Counter</code>). Beyond that, on CPython (the reference interpreter) 3.2 and higher <code>Counter</code> has a C level accelerator for counting iterable inputs that will run faster than anything you could write in pure Python.</p>\n<p><strong>Update:</strong> You seem to want punctuation stripped and case-insensitivity, so here's a variant of my earlier code that does that:</p>\n<pre><code class=\"python\">from string import punctuation\n\ndef countInFile(filename):\n    with open(filename) as f:\n        linewords = (line.translate(None, punctuation).lower().split() for line in f)\n        return Counter(chain.from_iterable(linewords))\n</code></pre>\n<p>Your code runs much more slowly because it's creating and destroying many small <code>Counter</code> and <code>set</code> objects, rather than <code>.update</code>-ing a single <code>Counter</code> once per line (which, while slightly slower than what I gave in the updated code block, would be at least algorithmically similar in scaling factor).</p>\n", "abstract": "The most succinct approach is to use the tools Python gives you. That's it. map(str.split, f) is making a generator that returns lists of words from each line. Wrapping in chain.from_iterable converts that to a single generator that produces a word at a time. Counter takes an input iterable and counts all unique values in it. At the end, you return a  dict-like object (a Counter) that stores all unique words and their counts, and during creation, you only store a line of data at a time and the total counts, not the whole file at once. In theory, on Python 2.7 and 3.1, you might do slightly better looping over the chained results yourself and using a dict or collections.defaultdict(int) to count (because Counter is implemented in Python, which can make it slower in some cases), but letting Counter do the work is simpler and more self-documenting (I mean, the whole goal is counting, so use a Counter). Beyond that, on CPython (the reference interpreter) 3.2 and higher Counter has a C level accelerator for counting iterable inputs that will run faster than anything you could write in pure Python. Update: You seem to want punctuation stripped and case-insensitivity, so here's a variant of my earlier code that does that: Your code runs much more slowly because it's creating and destroying many small Counter and set objects, rather than .update-ing a single Counter once per line (which, while slightly slower than what I gave in the updated code block, would be at least algorithmically similar in scaling factor)."}, {"id": 35917089, "score": 16, "vote": 0, "content": "<p>A memory efficient and accurate way is to make use of </p>\n<ul>\n<li>CountVectorizer in <code>scikit</code> (for ngram extraction)</li>\n<li>NLTK for <code>word_tokenize</code></li>\n<li><code>numpy</code> matrix sum to collect the counts</li>\n<li><code>collections.Counter</code> for collecting the counts and vocabulary</li>\n</ul>\n<p>An example:</p>\n<pre><code class=\"python\">import urllib.request\nfrom collections import Counter\n\nimport numpy as np \n\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Our sample textfile.\nurl = 'https://raw.githubusercontent.com/Simdiva/DSL-Task/master/data/DSLCC-v2.0/test/test.txt'\nresponse = urllib.request.urlopen(url)\ndata = response.read().decode('utf8')\n\n\n# Note that `ngram_range=(1, 1)` means we want to extract Unigrams, i.e. tokens.\nngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 1), min_df=1)\n# X matrix where the row represents sentences and column is our one-hot vector for each token in our vocabulary\nX = ngram_vectorizer.fit_transform(data.split('\\n'))\n\n# Vocabulary\nvocab = list(ngram_vectorizer.get_feature_names())\n\n# Column-wise sum of the X matrix.\n# It's some crazy numpy syntax that looks horribly unpythonic\n# For details, see http://stackoverflow.com/questions/3337301/numpy-matrix-to-array\n# and http://stackoverflow.com/questions/13567345/how-to-calculate-the-sum-of-all-columns-of-a-2d-numpy-array-efficiently\ncounts = X.sum(axis=0).A1\n\nfreq_distribution = Counter(dict(zip(vocab, counts)))\nprint (freq_distribution.most_common(10))\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">[(',', 32000),\n ('.', 17783),\n ('de', 11225),\n ('a', 7197),\n ('que', 5710),\n ('la', 4732),\n ('je', 4304),\n ('se', 4013),\n ('\u043d\u0430', 3978),\n ('na', 3834)]\n</code></pre>\n<p>Essentially, you can also do this:</p>\n<pre><code class=\"python\">from collections import Counter\nimport numpy as np \nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef freq_dist(data):\n    \"\"\"\n    :param data: A string with sentences separated by '\\n'\n    :type data: str\n    \"\"\"\n    ngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 1), min_df=1)\n    X = ngram_vectorizer.fit_transform(data.split('\\n'))\n    vocab = list(ngram_vectorizer.get_feature_names())\n    counts = X.sum(axis=0).A1\n    return Counter(dict(zip(vocab, counts)))\n</code></pre>\n<p>Let's <code>timeit</code>:</p>\n<pre><code class=\"python\">import time\n\nstart = time.time()\nword_distribution = freq_dist(data)\nprint (time.time() - start)\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">5.257147789001465\n</code></pre>\n<p>Note that <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" rel=\"noreferrer\"><code>CountVectorizer</code></a> can also take a file instead of a string and t<strong>here's no need to read the whole file into memory</strong>. In code:</p>\n<pre><code class=\"python\">import io\nfrom collections import Counter\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ninfile = '/path/to/input.txt'\n\nngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), min_df=1)\n\nwith io.open(infile, 'r', encoding='utf8') as fin:\n    X = ngram_vectorizer.fit_transform(fin)\n    vocab = ngram_vectorizer.get_feature_names()\n    counts = X.sum(axis=0).A1\n    freq_distribution = Counter(dict(zip(vocab, counts)))\n    print (freq_distribution.most_common(10))\n</code></pre>\n", "abstract": "A memory efficient and accurate way is to make use of  An example: [out]: Essentially, you can also do this: Let's timeit: [out]: Note that CountVectorizer can also take a file instead of a string and there's no need to read the whole file into memory. In code:"}, {"id": 39891791, "score": 4, "vote": 0, "content": "<p>Here's some benchmark. It'll look strange but the crudest code wins.</p>\n<p>[code]:</p>\n<pre><code class=\"python\">from collections import Counter, defaultdict\nimport io, time\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ninfile = '/path/to/file'\n\ndef extract_dictionary_sklearn(file_path):\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        ngram_vectorizer = CountVectorizer(analyzer='word')\n        X = ngram_vectorizer.fit_transform(fin)\n        vocab = ngram_vectorizer.get_feature_names()\n        counts = X.sum(axis=0).A1\n    return Counter(dict(zip(vocab, counts)))\n\ndef extract_dictionary_native(file_path):\n    dictionary = Counter()\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        for line in fin:\n            dictionary.update(line.split())\n    return dictionary\n\ndef extract_dictionary_paddle(file_path):\n    dictionary = defaultdict(int)\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        for line in fin:\n            for words in line.split():\n                dictionary[word] +=1\n    return dictionary\n\nstart = time.time()\nextract_dictionary_sklearn(infile)\nprint time.time() - start\n\nstart = time.time()\nextract_dictionary_native(infile)\nprint time.time() - start\n\nstart = time.time()\nextract_dictionary_paddle(infile)\nprint time.time() - start\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">38.306814909\n24.8241138458\n12.1182529926\n</code></pre>\n<p>Data size (154MB) used in the benchmark above:</p>\n<pre><code class=\"python\">$ wc -c /path/to/file\n161680851\n\n$ wc -l /path/to/file\n2176141\n</code></pre>\n<p>Some things to note:</p>\n<ul>\n<li>With the <code>sklearn</code> version, there's an overhead of vectorizer creation + numpy manipulation and conversion into a <code>Counter</code> object</li>\n<li>Then native <code>Counter</code> update version, it seems like <code>Counter.update()</code> is an expensive operation</li>\n</ul>\n", "abstract": "Here's some benchmark. It'll look strange but the crudest code wins. [code]: [out]: Data size (154MB) used in the benchmark above: Some things to note:"}, {"id": 35857691, "score": 3, "vote": 0, "content": "<p>This should suffice.</p>\n<pre><code class=\"python\">def countinfile(filename):\n    d = {}\n    with open(filename, \"r\") as fin:\n        for line in fin:\n            words = line.strip().split()\n            for word in words:\n                try:\n                    d[word] += 1\n                except KeyError:\n                    d[word] = 1\n    return d\n</code></pre>\n", "abstract": "This should suffice."}, {"id": 35944402, "score": 2, "vote": 0, "content": "<p>Instead of decoding the whole bytes read from the url, I process the binary data. Because <code>bytes.translate</code> expects its second argument to be a byte string, I utf-8 encode <code>punctuation</code>. After removing punctuations, I utf-8 decode the byte string.  </p>\n<p>The function <code>freq_dist</code> expects an iterable. That's why I've passed <code>data.splitlines()</code>.  </p>\n<pre><code class=\"python\">from urllib2 import urlopen\nfrom collections import Counter\nfrom string import punctuation\nfrom time import time\nimport sys\nfrom pprint import pprint\n\nurl = 'https://raw.githubusercontent.com/Simdiva/DSL-Task/master/data/DSLCC-v2.0/test/test.txt'\n\ndata = urlopen(url).read()\n\ndef freq_dist(data):\n    \"\"\"\n    :param data: file-like object opened in binary mode or\n                 sequence of byte strings separated by '\\n'\n    :type data: an iterable sequence\n    \"\"\"\n    #For readability   \n    #return Counter(word for line in data\n    #    for word in line.translate(\n    #    None,bytes(punctuation.encode('utf-8'))).decode('utf-8').split())\n\n    punc = punctuation.encode('utf-8')\n    words = (word for line in data for word in line.translate(None, punc).decode('utf-8').split())\n    return Counter(words)\n\n\nstart = time()\nword_dist = freq_dist(data.splitlines())\nprint('elapsed: {}'.format(time() - start))\npprint(word_dist.most_common(10))\n</code></pre>\n<p>Output;</p>\n<pre><code class=\"python\">elapsed: 0.806480884552\n\n[(u'de', 11106),\n (u'a', 6742),\n (u'que', 5701),\n (u'la', 4319),\n (u'je', 4260),\n (u'se', 3938),\n (u'\\u043d\\u0430', 3929),\n (u'na', 3623),\n (u'da', 3534),\n (u'i', 3487)]\n</code></pre>\n<p>It seems <code>dict</code> is more efficient than <code>Counter</code> object.  </p>\n<pre><code class=\"python\">def freq_dist(data):\n    \"\"\"\n    :param data: A string with sentences separated by '\\n'\n    :type data: str\n    \"\"\"\n    d = {}\n    punc = punctuation.encode('utf-8')\n    words = (word for line in data for word in line.translate(None, punc).decode('utf-8').split())\n    for word in words:\n        d[word] = d.get(word, 0) + 1\n    return d\n\nstart = time()\nword_dist = freq_dist(data.splitlines())\nprint('elapsed: {}'.format(time() - start))\npprint(sorted(word_dist.items(), key=lambda x: (x[1], x[0]), reverse=True)[:10])\n</code></pre>\n<p>Output;</p>\n<pre><code class=\"python\">elapsed: 0.642680168152\n\n[(u'de', 11106),\n (u'a', 6742),\n (u'que', 5701),\n (u'la', 4319),\n (u'je', 4260),\n (u'se', 3938),\n (u'\\u043d\\u0430', 3929),\n (u'na', 3623),\n (u'da', 3534),\n (u'i', 3487)]\n</code></pre>\n<p>To be more memory efficient when opening huge file, you have to pass just the opened url. But the timing will include file download time too.</p>\n<pre><code class=\"python\">data = urlopen(url)\nword_dist = freq_dist(data)\n</code></pre>\n", "abstract": "Instead of decoding the whole bytes read from the url, I process the binary data. Because bytes.translate expects its second argument to be a byte string, I utf-8 encode punctuation. After removing punctuations, I utf-8 decode the byte string.   The function freq_dist expects an iterable. That's why I've passed data.splitlines().   Output; It seems dict is more efficient than Counter object.   Output; To be more memory efficient when opening huge file, you have to pass just the opened url. But the timing will include file download time too."}, {"id": 35857655, "score": 0, "vote": 0, "content": "<p>Skip CountVectorizer and scikit-learn.</p>\n<p>The file may be too large to load into memory but I doubt the python dictionary gets too large. The easiest option for you may be to split the large file into 10-20 smaller files and extend your code to loop over the smaller files.</p>\n", "abstract": "Skip CountVectorizer and scikit-learn. The file may be too large to load into memory but I doubt the python dictionary gets too large. The easiest option for you may be to split the large file into 10-20 smaller files and extend your code to loop over the smaller files."}, {"id": 54939567, "score": 0, "vote": 0, "content": "<p>you can try with sklearn</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer()\n\n    data=['i am student','the student suffers a lot']\n    transformed_data =vectorizer.fit_transform(data)\n    vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n    print (vocab)\n</code></pre>\n", "abstract": "you can try with sklearn"}, {"id": 59916683, "score": 0, "vote": 0, "content": "<p>Combining every ones else's views and some of my own :)\nHere is what I have for you</p>\n<pre><code class=\"python\">from collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntext='''Note that if you use RegexpTokenizer option, you lose \nnatural language features special to word_tokenize \nlike splitting apart contractions. You can naively \nsplit on the regex \\w+ without any need for the NLTK.\n'''\n\n# tokenize\nraw = ' '.join(word_tokenize(text.lower()))\n\ntokenizer = RegexpTokenizer(r'[A-Za-z]{2,}')\nwords = tokenizer.tokenize(raw)\n\n# remove stopwords\nstop_words = set(stopwords.words('english'))\nwords = [word for word in words if word not in stop_words]\n\n# count word frequency, sort and return just 20\ncounter = Counter()\ncounter.update(words)\nmost_common = counter.most_common(20)\nmost_common\n</code></pre>\n<h1>Output</h1>\n<p>(All ones)</p>\n<pre>\n[('note', 1),\n ('use', 1),\n ('regexptokenizer', 1),\n ('option', 1),\n ('lose', 1),\n ('natural', 1),\n ('language', 1),\n ('features', 1),\n ('special', 1),\n ('word', 1),\n ('tokenize', 1),\n ('like', 1),\n ('splitting', 1),\n ('apart', 1),\n ('contractions', 1),\n ('naively', 1),\n ('split', 1),\n ('regex', 1),\n ('without', 1),\n ('need', 1)]\n</pre>\n<p>One can do better than this in terms of efficiency but if you are not worried about it too much, this code is the best.</p>\n", "abstract": "Combining every ones else's views and some of my own :)\nHere is what I have for you (All ones) One can do better than this in terms of efficiency but if you are not worried about it too much, this code is the best."}]}, {"link": "https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag", "question": {"id": "30821188", "title": "Python NLTK pos_tag not returning the correct part-of-speech tag", "content": "<p>Having this:</p>\n<pre><code class=\"python\">text = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n</code></pre>\n<p>And running:</p>\n<pre><code class=\"python\">nltk.pos_tag(text)\n</code></pre>\n<p>I get:</p>\n<pre><code class=\"python\">[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n</code></pre>\n<p>This is incorrect. The tags for <code>quick brown lazy</code> in the sentence should be:</p>\n<pre><code class=\"python\">('quick', 'JJ'), ('brown', 'JJ') , ('lazy', 'JJ')\n</code></pre>\n<p>Testing this through their <a href=\"http://nlp.stanford.edu:8080/corenlp/process\">online tool</a> gives the same result; <code>quick</code>, <code>brown</code> and <code>fox</code> should be adjectives not nouns.</p>\n", "abstract": "Having this: And running: I get: This is incorrect. The tags for quick brown lazy in the sentence should be: Testing this through their online tool gives the same result; quick, brown and fox should be adjectives not nouns."}, "answers": [{"id": 30823202, "score": 69, "vote": 0, "content": "<p><strong>In short</strong>:</p>\n<blockquote>\n<p>NLTK is not perfect. In fact, no model is perfect.</p>\n</blockquote>\n<p><strong>Note:</strong></p>\n<p>As of NLTK version 3.1, default <code>pos_tag</code> function is no longer the <a href=\"https://stackoverflow.com/questions/31386224/what-created-maxent-treebank-pos-tagger-english-pickle\">old MaxEnt English pickle</a>. </p>\n<p>It is now the <strong>perceptron tagger</strong> from <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/perceptron.py\" rel=\"noreferrer\">@Honnibal's implementation</a>, see <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L87\" rel=\"noreferrer\"><code>nltk.tag.pos_tag</code></a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; print inspect.getsource(pos_tag)\ndef pos_tag(tokens, tagset=None):\n    tagger = PerceptronTagger()\n    return _pos_tag(tokens, tagset, tagger) \n</code></pre>\n<p>Still it's better but not perfect:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk import pos_tag\n&gt;&gt;&gt; pos_tag(\"The quick brown fox jumps over the lazy dog\".split())\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n</code></pre>\n<p>At some point, if someone wants <code>TL;DR</code> solutions, see <a href=\"https://github.com/alvations/nltk_cli\" rel=\"noreferrer\">https://github.com/alvations/nltk_cli</a></p>\n<hr/>\n<p><strong>In long</strong>:</p>\n<p><strong>Try using other tagger (see <a href=\"https://github.com/nltk/nltk/tree/develop/nltk/tag\" rel=\"noreferrer\">https://github.com/nltk/nltk/tree/develop/nltk/tag</a>) , e.g.</strong>:</p>\n<ul>\n<li>HunPos</li>\n<li>Stanford POS</li>\n<li>Senna</li>\n</ul>\n<p><strong>Using default MaxEnt POS tagger from NLTK, i.e. <code>nltk.pos_tag</code></strong>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk import word_tokenize, pos_tag\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; pos_tag(word_tokenize(text))\n[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n</code></pre>\n<p><strong>Using Stanford POS tagger</strong>:</p>\n<pre><code class=\"python\">$ cd ~\n$ wget http://nlp.stanford.edu/software/stanford-postagger-2015-04-20.zip\n$ unzip stanford-postagger-2015-04-20.zip\n$ mv stanford-postagger-2015-04-20 stanford-postagger\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.stanford import POSTagger\n&gt;&gt;&gt; _path_to_model = home + '/stanford-postagger/models/english-bidirectional-distsim.tagger'\n&gt;&gt;&gt; _path_to_jar = home + '/stanford-postagger/stanford-postagger.jar'\n&gt;&gt;&gt; st = POSTagger(path_to_model=_path_to_model, path_to_jar=_path_to_jar)\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; st.tag(text.split())\n[(u'The', u'DT'), (u'quick', u'JJ'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'jumps', u'VBZ'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n</code></pre>\n<p><strong>Using HunPOS</strong> (NOTE: the default encoding is ISO-8859-1 not UTF8):</p>\n<pre><code class=\"python\">$ cd ~\n$ wget https://hunpos.googlecode.com/files/hunpos-1.0-linux.tgz\n$ tar zxvf hunpos-1.0-linux.tgz\n$ wget https://hunpos.googlecode.com/files/en_wsj.model.gz\n$ gzip -d en_wsj.model.gz \n$ mv en_wsj.model hunpos-1.0-linux/\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.hunpos import HunposTagger\n&gt;&gt;&gt; _path_to_bin = home + '/hunpos-1.0-linux/hunpos-tag'\n&gt;&gt;&gt; _path_to_model = home + '/hunpos-1.0-linux/en_wsj.model'\n&gt;&gt;&gt; ht = HunposTagger(path_to_model=_path_to_model, path_to_bin=_path_to_bin)\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; ht.tag(text.split())\n[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n</code></pre>\n<p><strong>Using Senna</strong> (Make sure you've the latest version of NLTK, there were some changes made to the API):</p>\n<pre><code class=\"python\">$ cd ~\n$ wget http://ronan.collobert.com/senna/senna-v3.0.tgz\n$ tar zxvf senna-v3.0.tgz\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser(\"~\")\n&gt;&gt;&gt; from nltk.tag.senna import SennaTagger\n&gt;&gt;&gt; st = SennaTagger(home+'/senna')\n&gt;&gt;&gt; text = \"The quick brown fox jumps over the lazy dog\"\n&gt;&gt;&gt; st.tag(text.split())\n[('The', u'DT'), ('quick', u'JJ'), ('brown', u'JJ'), ('fox', u'NN'), ('jumps', u'VBZ'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'NN')]\n</code></pre>\n<hr/>\n<p><strong>Or try building a better POS tagger</strong>:</p>\n<ul>\n<li>Ngram Tagger: <a href=\"http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/\" rel=\"noreferrer\">http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/</a></li>\n<li>Affix/Regex Tagger: <a href=\"http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/\" rel=\"noreferrer\">http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/</a> </li>\n<li>Build Your Own Brill (Read the code it's a pretty fun tagger, <a href=\"http://www.nltk.org/_modules/nltk/tag/brill.html\" rel=\"noreferrer\">http://www.nltk.org/_modules/nltk/tag/brill.html</a>), see <a href=\"http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/\" rel=\"noreferrer\">http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/</a></li>\n<li>Perceptron Tagger: <a href=\"https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\" rel=\"noreferrer\">https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/</a></li>\n<li>LDA Tagger: <a href=\"http://scm.io/blog/hack/2015/02/lda-intentions/\" rel=\"noreferrer\">http://scm.io/blog/hack/2015/02/lda-intentions/</a></li>\n</ul>\n<hr/>\n<p><strong>Complains about <code>pos_tag</code> accuracy on stackoverflow include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/13529945/pos-tagging-nltk-thinks-noun-is-adjective\">POS tagging - NLTK thinks noun is adjective</a></li>\n<li><a href=\"https://stackoverflow.com/questions/21786257/python-nltk-pos-tagger-not-behaving-as-expected\">python NLTK POS tagger not behaving as expected</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8146748/how-to-obtain-better-results-using-nltk-pos-tag\">How to obtain better results using NLTK pos tag</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8365557/pos-tag-in-nltk-does-not-tag-sentences-correctly\">pos_tag in NLTK does not tag sentences correctly</a></li>\n</ul>\n<p><strong>Issues about NLTK HunPos include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/5088448/how-to-i-tag-textfiles-with-hunpos-in-nltk\">How do I tag textfiles with hunpos in nltk?</a> </li>\n<li><a href=\"https://stackoverflow.com/questions/5091389/does-anyone-know-how-to-configure-the-hunpos-wrapper-class-on-nltk\">Does anyone know how to configure the hunpos wrapper class on nltk?</a></li>\n</ul>\n<p><strong>Issues with NLTK and Stanford POS tagger include</strong>:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk\">trouble importing stanford pos tagger into nltk</a></li>\n<li><a href=\"https://stackoverflow.com/questions/27116495/java-command-fails-in-nltk-stanford-pos-tagger\">Java Command Fails in NLTK Stanford POS Tagger</a></li>\n<li><a href=\"https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python\">Error using Stanford POS Tagger in NLTK Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk\">How to improve speed with Stanford NLP Tagger and NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/27171298/nltk-stanford-pos-tagger-error-java-command-failed\">Nltk stanford pos tagger error : Java command failed</a></li>\n<li><a href=\"https://stackoverflow.com/questions/8555312/instantiating-and-using-stanfordtagger-within-nltk\">Instantiating and using StanfordTagger within NLTK</a></li>\n<li><a href=\"https://stackoverflow.com/questions/26647253/running-stanford-pos-tagger-in-nltk-leads-to-not-a-valid-win32-application-on\">Running Stanford POS tagger in NLTK leads to \"not a valid Win32 application\" on Windows</a></li>\n</ul>\n", "abstract": "In short: NLTK is not perfect. In fact, no model is perfect. Note: As of NLTK version 3.1, default pos_tag function is no longer the old MaxEnt English pickle.  It is now the perceptron tagger from @Honnibal's implementation, see nltk.tag.pos_tag Still it's better but not perfect: At some point, if someone wants TL;DR solutions, see https://github.com/alvations/nltk_cli In long: Try using other tagger (see https://github.com/nltk/nltk/tree/develop/nltk/tag) , e.g.: Using default MaxEnt POS tagger from NLTK, i.e. nltk.pos_tag: Using Stanford POS tagger: Using HunPOS (NOTE: the default encoding is ISO-8859-1 not UTF8): Using Senna (Make sure you've the latest version of NLTK, there were some changes made to the API): Or try building a better POS tagger: Complains about pos_tag accuracy on stackoverflow include: Issues about NLTK HunPos include: Issues with NLTK and Stanford POS tagger include:"}, {"id": 58586549, "score": 2, "vote": 0, "content": "<p>Solutions such as changing to the Stanford or Senna or HunPOS tagger will definitely yield results, but here is a much simpler way to experiment with different taggers that are also included within NLTK.</p>\n<p>The default POS tagger in NTLK right now is the averaged perceptron tagger.  Here's a function that will opt to use the Maxent Treebank Tagger instead:</p>\n<pre><code class=\"python\">def treebankTag(text)\n    words = nltk.word_tokenize(text)\n    treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n    return treebankTagger.tag(words)\n</code></pre>\n<p>I have found that the averaged perceptron pre-trained tagger in NLTK is biased to treating some adjectives as nouns, as in your example.  The treebank tagger has gotten more adjectives correct for me.</p>\n", "abstract": "Solutions such as changing to the Stanford or Senna or HunPOS tagger will definitely yield results, but here is a much simpler way to experiment with different taggers that are also included within NLTK. The default POS tagger in NTLK right now is the averaged perceptron tagger.  Here's a function that will opt to use the Maxent Treebank Tagger instead: I have found that the averaged perceptron pre-trained tagger in NLTK is biased to treating some adjectives as nouns, as in your example.  The treebank tagger has gotten more adjectives correct for me."}, {"id": 67396794, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">def tagPOS(textcontent, taggedtextcontent, defined_tags):\n    # Write your code here\n    token = nltk.word_tokenize(textcontent)\n    nltk_pos_tags = nltk.pos_tag(token)\n    \n    unigram_pos_tag = nltk.UnigramTagger(model=defined_tags).tag(token)\n    \n    tagged_pos_tag = [ nltk.tag.str2tuple(word) for word in taggedtextcontent.split() ]\n    \n    return (nltk_pos_tags,tagged_pos_tag,unigram_pos_tag)\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/2945357/how-best-to-parse-a-simple-grammar", "question": {"id": "2945357", "title": "How best to parse a simple grammar?", "content": "<p>Ok, so I've asked a bunch of smaller questions about this project, but I still don't have much confidence in the designs I'm coming up with, so I'm going to ask a question on a broader scale.</p>\n<p>I am parsing pre-requisite descriptions for a course catalog. The descriptions almost always follow a certain form, which makes me think I can parse most of them.</p>\n<p>From the text, I would like to generate a graph of course pre-requisite relationships. (That part will be easy, after I have parsed the data.)</p>\n<p>Some sample inputs and outputs:</p>\n<pre><code class=\"python\">\"CS 2110\" =&gt; (\"CS\", 2110) # 0\n\n\"CS 2110 and INFO 3300\" =&gt; [(\"CS\", 2110), (\"INFO\", 3300)] # 1\n\"CS 2110, INFO 3300\" =&gt; [(\"CS\", 2110), (\"INFO\", 3300)] # 1\n\"CS 2110, 3300, 3140\" =&gt; [(\"CS\", 2110), (\"CS\", 3300), (\"CS\", 3140)] # 1\n\n\"CS 2110 or INFO 3300\" =&gt; [[(\"CS\", 2110)], [(\"INFO\", 3300)]] # 2\n\n\"MATH 2210, 2230, 2310, or 2940\" =&gt; [[(\"MATH\", 2210), (\"MATH\", 2230), (\"MATH\", 2310)], [(\"MATH\", 2940)]] # 3  \n</code></pre>\n<ol>\n<li><p>If the entire description is just a course, it is output directly.</p></li>\n<li><p>If the courses are conjoined (\"and\"), they are all output in the same list</p></li>\n<li><p>If the course are disjoined (\"or\"), they are in separate lists</p></li>\n<li><p>Here, we have both \"and\" and \"or\".</p></li>\n</ol>\n<p>One caveat that makes it easier: it appears that the nesting of \"and\"/\"or\" phrases is never greater than as shown in example 3.</p>\n<p>What is the best way to do this? I started with PLY, but I couldn't figure out how to resolve the reduce/reduce conflicts. The advantage of PLY is that it's easy to manipulate what each parse rule generates:</p>\n<pre><code class=\"python\">def p_course(p):\n 'course : DEPT_CODE COURSE_NUMBER'\n p[0] = (p[1], int(p[2]))\n</code></pre>\n<p>With PyParse, it's less clear how to modify the output of <code>parseString()</code>. I was considering building upon @Alex Martelli's idea of keeping state in an object and building up the output from that, but I'm not sure exactly how that is best done.</p>\n<pre><code class=\"python\"> def addCourse(self, str, location, tokens):\n  self.result.append((tokens[0][0], tokens[0][1]))\n\n def makeCourseList(self, str, location, tokens):\n\n  dept = tokens[0][0]\n  new_tokens = [(dept, tokens[0][1])]\n  new_tokens.extend((dept, tok) for tok in tokens[1:])\n\n  self.result.append(new_tokens)\n</code></pre>\n<p>For instance, to handle \"or\" cases:</p>\n<pre><code class=\"python\">    def __init__(self):\n            self.result = []\n            # ...\n  self.statement = (course_data + Optional(OR_CONJ + course_data)).setParseAction(self.disjunctionCourses)\n\n\n\n def disjunctionCourses(self, str, location, tokens):\n  if len(tokens) == 1:\n   return tokens\n\n  print \"disjunction tokens: %s\" % tokens\n</code></pre>\n<p>How does <code>disjunctionCourses()</code> know which smaller phrases to disjoin? All it gets is tokens, but what's been parsed so far is stored in <code>result</code>, so how can the function tell which data in <code>result</code> corresponds to which elements of <code>token</code>? I guess I could search through the tokens, then find an element of <code>result</code> with the same data, but that feel convoluted...</p>\n<p>Also, there are many descriptions that include misc text, like:</p>\n<pre><code class=\"python\">\"CS 2110 or permission of instructor\"\n\"INFO 3140 or equivalent experience\"\n\"PYSCH 2210 and sophomore standing\"\n</code></pre>\n<p>But it isn't critical that I parse that text.</p>\n<p><strong>What's a better way to approach this problem?</strong></p>\n", "abstract": "Ok, so I've asked a bunch of smaller questions about this project, but I still don't have much confidence in the designs I'm coming up with, so I'm going to ask a question on a broader scale. I am parsing pre-requisite descriptions for a course catalog. The descriptions almost always follow a certain form, which makes me think I can parse most of them. From the text, I would like to generate a graph of course pre-requisite relationships. (That part will be easy, after I have parsed the data.) Some sample inputs and outputs: If the entire description is just a course, it is output directly. If the courses are conjoined (\"and\"), they are all output in the same list If the course are disjoined (\"or\"), they are in separate lists Here, we have both \"and\" and \"or\". One caveat that makes it easier: it appears that the nesting of \"and\"/\"or\" phrases is never greater than as shown in example 3. What is the best way to do this? I started with PLY, but I couldn't figure out how to resolve the reduce/reduce conflicts. The advantage of PLY is that it's easy to manipulate what each parse rule generates: With PyParse, it's less clear how to modify the output of parseString(). I was considering building upon @Alex Martelli's idea of keeping state in an object and building up the output from that, but I'm not sure exactly how that is best done. For instance, to handle \"or\" cases: How does disjunctionCourses() know which smaller phrases to disjoin? All it gets is tokens, but what's been parsed so far is stored in result, so how can the function tell which data in result corresponds to which elements of token? I guess I could search through the tokens, then find an element of result with the same data, but that feel convoluted... Also, there are many descriptions that include misc text, like: But it isn't critical that I parse that text. What's a better way to approach this problem?"}, "answers": [{"id": 2945415, "score": 28, "vote": 0, "content": "<pre><code class=\"python\">def parse(astr):\n    astr=astr.replace(',','')\n    astr=astr.replace('and','')    \n    tokens=astr.split()\n    dept=None\n    number=None\n    result=[]\n    option=[]\n    for tok in tokens:\n        if tok=='or':\n            result.append(option)\n            option=[]\n            continue\n        if tok.isalpha():\n            dept=tok\n            number=None\n        else:\n            number=int(tok)\n        if dept and number:\n            option.append((dept,number))\n    else:\n        if option:\n            result.append(option)\n    return result\n\nif __name__=='__main__':\n    tests=[ (\"CS 2110\" , [[(\"CS\", 2110)]]),\n            (\"CS 2110 and INFO 3300\" , [[(\"CS\", 2110), (\"INFO\", 3300)]]),\n            (\"CS 2110, INFO 3300\" , [[(\"CS\", 2110), (\"INFO\", 3300)]]),\n            (\"CS 2110, 3300, 3140\", [[(\"CS\", 2110), (\"CS\", 3300), (\"CS\", 3140)]]),\n            (\"CS 2110 or INFO 3300\", [[(\"CS\", 2110)], [(\"INFO\", 3300)]]),\n            (\"MATH 2210, 2230, 2310, or 2940\", [[(\"MATH\", 2210), (\"MATH\", 2230), (\"MATH\", 2310)], [(\"MATH\", 2940)]])]\n\n    for test,answer in tests:\n        result=parse(test)\n        if result==answer:\n            print('GOOD: {0} =&gt; {1}'.format(test,answer))\n        else:\n            print('ERROR: {0} =&gt; {1} != {2}'.format(test,result,answer))\n            break\n</code></pre>\n<p>yields</p>\n<pre><code class=\"python\">GOOD: CS 2110 =&gt; [[('CS', 2110)]]\nGOOD: CS 2110 and INFO 3300 =&gt; [[('CS', 2110), ('INFO', 3300)]]\nGOOD: CS 2110, INFO 3300 =&gt; [[('CS', 2110), ('INFO', 3300)]]\nGOOD: CS 2110, 3300, 3140 =&gt; [[('CS', 2110), ('CS', 3300), ('CS', 3140)]]\nGOOD: CS 2110 or INFO 3300 =&gt; [[('CS', 2110)], [('INFO', 3300)]]\nGOOD: MATH 2210, 2230, 2310, or 2940 =&gt; [[('MATH', 2210), ('MATH', 2230), ('MATH', 2310)], [('MATH', 2940)]]\n</code></pre>\n", "abstract": "yields"}, {"id": 2945940, "score": 7, "vote": 0, "content": "<p>For simple grammars I really like Parsing Expression Grammars (PEGs), which amount to a disciplined, structured way of writing a recursive-descent parser.  In a dynamically typed language like Python you can do useful things without having a separate \"parser generator\".  That means no nonsense with reduce-reduce conflicts or other arcana of LR parsing.</p>\n<p>I did a little searching and <a href=\"http://fdik.org/pyPEG/\" rel=\"noreferrer\">pyPEG</a> appears to be a nice library for Python.</p>\n", "abstract": "For simple grammars I really like Parsing Expression Grammars (PEGs), which amount to a disciplined, structured way of writing a recursive-descent parser.  In a dynamically typed language like Python you can do useful things without having a separate \"parser generator\".  That means no nonsense with reduce-reduce conflicts or other arcana of LR parsing. I did a little searching and pyPEG appears to be a nice library for Python."}, {"id": 54966388, "score": 4, "vote": 0, "content": "<p>I know that this question is about a decade old and has certainly been answered now. I am mainly posting this answer to prove myself that I have understood  <code>PEG</code> parsers at last. I'm using the fantastic <a href=\"https://pypi.org/project/parsimonious/\" rel=\"nofollow noreferrer\"><strong><code>parsimonious</code> module</strong></a> here.<br/>\nThat being said, you could come up with a parsing grammar, build an ast and visit this one to obtain the desired structure:</p>\n<pre><code class=\"python\">from parsimonious.nodes import NodeVisitor\nfrom parsimonious.grammar import Grammar\nfrom itertools import groupby\n\ngrammar = Grammar(\n    r\"\"\"\n    term            = course (operator course)*\n    course          = coursename? ws coursenumber\n    coursename      = ~\"[A-Z]+\"\n    coursenumber    = ~\"\\d+\"\n    operator        = ws (and / or / comma) ws\n    and             = \"and\"\n    or              = (comma ws)? \"or\"\n    comma           = \",\"\n    ws              = ~\"\\s*\"\n    \"\"\"\n)\n\nclass CourseVisitor(NodeVisitor):\n    def __init__(self):\n        self.current = None\n        self.courses = []\n        self.listnum = 1\n\n    def generic_visit(self, node, children):\n        pass\n\n    def visit_coursename(self, node, children):\n        if node.text:\n            self.current = node.text\n\n    def visit_coursenumber(self, node, children):\n        course = (self.current, int(node.text), self.listnum)\n        self.courses.append(course)\n\n    def visit_or(self, node, children):\n        self.listnum += 1\n\ncourses = [\"CS 2110\", \"CS 2110 and INFO 3300\",\n           \"CS 2110, INFO 3300\", \"CS 2110, 3300, 3140\",\n           \"CS 2110 or INFO 3300\", \"MATH 2210, 2230, 2310, or 2940\"]\n\nfor course in courses:\n    tree = grammar.parse(course)\n    cv = CourseVisitor()\n    cv.visit(tree)\n    courses = [list(v) for _, v in groupby(cv.courses, lambda x: x[2])]\n    print(courses)\n</code></pre>\n<p>Here, we walk our way from bottom to top, starting with brickets like whitespace, the operators <code>or</code>, <code>and</code> and <code>,</code> which will eventually lead to the course and finally the <code>term</code>. The visitor class builds the desired (well, kind of, one needs to get rid of the last tuple element) structure.</p>\n", "abstract": "I know that this question is about a decade old and has certainly been answered now. I am mainly posting this answer to prove myself that I have understood  PEG parsers at last. I'm using the fantastic parsimonious module here.\nThat being said, you could come up with a parsing grammar, build an ast and visit this one to obtain the desired structure: Here, we walk our way from bottom to top, starting with brickets like whitespace, the operators or, and and , which will eventually lead to the course and finally the term. The visitor class builds the desired (well, kind of, one needs to get rid of the last tuple element) structure."}, {"id": 2945398, "score": 1, "vote": 0, "content": "<p>If you get reduce/reduce conflicts you need to specify the precedence of \"or\" and \"and\". Im guessing \"and\" binds tightest, meaning \"CS 101 and CS 102 or CS 201\" means [[CS 101, CS 102] [CS 201]]. </p>\n<p>If you can find examples of both then the grammar is ambigous and you are out of luck. However you might be able to let this ambiguity be left underspecified, all depending on what you are going to do with the results.</p>\n<p>PS, Looks like the language is regular, you could consider a DFA.</p>\n", "abstract": "If you get reduce/reduce conflicts you need to specify the precedence of \"or\" and \"and\". Im guessing \"and\" binds tightest, meaning \"CS 101 and CS 102 or CS 201\" means [[CS 101, CS 102] [CS 201]].  If you can find examples of both then the grammar is ambigous and you are out of luck. However you might be able to let this ambiguity be left underspecified, all depending on what you are going to do with the results. PS, Looks like the language is regular, you could consider a DFA."}, {"id": 70104231, "score": 1, "vote": 0, "content": "<p>Just in the name of completness there is <a href=\"https://sly.readthedocs.io/en/latest/sly.html#sly-overview\" rel=\"nofollow noreferrer\">SLY</a>. The creator David Beazley has a great <a href=\"https://www.youtube.com/watch?v=zJ9z6Ge-vXs&amp;ab_channel=PyCon2018\" rel=\"nofollow noreferrer\">talk about it at PyCon 2018</a> which is fun.</p>\n", "abstract": "Just in the name of completness there is SLY. The creator David Beazley has a great talk about it at PyCon 2018 which is fun."}, {"id": 2946028, "score": 0, "vote": 0, "content": "<p>I don't pretend to know much about parsing a grammar, and for your case the solution by unutbu is all you'll need. But I learnt a fair bit about parsing from Eric Lippert in his recent series of blog posts.</p>\n<p><a href=\"https://web.archive.org/web/20150905100950/http://blogs.msdn.com/b/ericlippert/archive/2010/04/26/every-program-there-is-part-one.aspx\" rel=\"nofollow noreferrer\">Link</a></p>\n<p>It's a 7 part series that goes through creating and parsing a grammar, then optimizing the grammar to make parsing easier and more performant. He produces C# code to generate all combinations of particular grammars, but it shouldn't be too much of a stretch to convert that into python to parse a fairly simple grammar of your own.</p>\n", "abstract": "I don't pretend to know much about parsing a grammar, and for your case the solution by unutbu is all you'll need. But I learnt a fair bit about parsing from Eric Lippert in his recent series of blog posts. Link It's a 7 part series that goes through creating and parsing a grammar, then optimizing the grammar to make parsing easier and more performant. He produces C# code to generate all combinations of particular grammars, but it shouldn't be too much of a stretch to convert that into python to parse a fairly simple grammar of your own."}]}, {"link": "https://stackoverflow.com/questions/45126071/how-to-extract-numbers-along-with-comparison-adjectives-or-ranges", "question": {"id": "45126071", "title": "How to extract numbers (along with comparison adjectives or ranges)", "content": "<p>I am working on two NLP projects in Python, and both have a similar task to <strong>extract numerical values and comparison operators</strong> from sentences, like the following:</p>\n<pre><code class=\"python\">\"... greater than $10 ... \",\n\"... weight not more than 200lbs ...\",\n\"... height in 5-7 feets ...\",\n\"... faster than 30 seconds ... \"\n</code></pre>\n<p>I found two different approaches to solve this problem:</p>\n<ul>\n<li>using very complex regular expressions.</li>\n<li>using <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\" rel=\"nofollow noreferrer\">Named Entity Recognition</a> (and some regexes, too).</li>\n</ul>\n<p>How can I parse numerical values out of such sentences? I assume this is a common task in NLP.</p>\n<hr/>\n<p>The desired output would be something like:</p>\n<p><strong>Input:</strong></p>\n<blockquote>\n<p>\"greater than $10\"</p>\n</blockquote>\n<p><strong>Output:</strong></p>\n<pre><code class=\"python\">{'value': 10, 'unit': 'dollar', 'relation': 'gt', 'position': 3}\n</code></pre>\n", "abstract": "I am working on two NLP projects in Python, and both have a similar task to extract numerical values and comparison operators from sentences, like the following: I found two different approaches to solve this problem: How can I parse numerical values out of such sentences? I assume this is a common task in NLP. The desired output would be something like: Input: \"greater than $10\" Output:"}, "answers": [{"id": 45130193, "score": 31, "vote": 0, "content": "<p>I would probably approach this as a chunking task and use <code>nltk</code>'s part of speech tagger combined with its regular expression chunker.  This will allow you to define a regular expression based on the part of speech of the words in your sentences instead of on the words themselves.  For a given sentence, you can do the following:</p>\n<pre><code class=\"python\">import nltk\n\n# example sentence\nsent = 'send me a table with a price greater than $100'\n</code></pre>\n<p>The first thing I would do is to modify your sentences slightly so that you don't confuse the part of speech tagger too much.  Here are some examples of changes that you can make (with very simple regular expressions) but you can experiment and see if there are others:</p>\n<pre><code class=\"python\">$10 -&gt; 10 dollars\n200lbs -&gt; 200 lbs\n5-7 -&gt; 5 - 7 OR 5 to 7\n</code></pre>\n<p>so we get:</p>\n<pre><code class=\"python\">sent = 'send me a table with a price greater than 100 dollars'\n</code></pre>\n<p>now you can get the parts of speech from your sentence:</p>\n<pre><code class=\"python\">sent_pos = nltk.pos_tag(sent.split())\nprint(sent_pos)\n\n[('send', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('table', 'NN'), ('with', 'IN'), ('a', 'DT'), ('price', 'NN'), ('greater', 'JJR'), ('than', 'IN'), ('100', 'CD'), ('dollars', 'NNS')]\n</code></pre>\n<p>We can now create a <a href=\"http://www.nltk.org/book/ch07.html\" rel=\"noreferrer\">chunker</a> which will chunk your POS tagged text according to a (relatively) simple regular expression:</p>\n<pre><code class=\"python\">grammar = 'NumericalPhrase: {&lt;NN|NNS&gt;?&lt;RB&gt;?&lt;JJR&gt;&lt;IN&gt;&lt;CD&gt;&lt;NN|NNS&gt;?}'\nparser = nltk.RegexpParser(grammar)\n</code></pre>\n<p>This defines a parser with a grammar that chunks numerical phrases (what we'll call your phrase type). It defines your numerical phrase as: an optional noun, followed by an optional adverb, followed by a comparative adjective, a preposition, a number, and an optional noun. \nThis is just a suggestion for how you may want to define your phrases, but I think that this will be much simpler than using a regular expression on the words themselves.  </p>\n<p>To get your phrases you can do:</p>\n<pre><code class=\"python\">print(parser.parse(sent_pos))\n(S\n  send/VB\n  me/PRP\n  a/DT\n  table/NN\n  with/IN\n  a/DT\n  (NumericalPhrase price/NN greater/JJR than/IN 100/CD dollars/NNS))  \n</code></pre>\n<p>Or to get only your phrases you can do:</p>\n<pre><code class=\"python\">print([tree.leaves() for tree in parser.parse(sent_pos).subtrees() if tree.label() == 'NumericalPhrase'])\n\n[[('price', 'NN'),\n  ('greater', 'JJR'),\n  ('than', 'IN'),\n  ('100', 'CD'),\n  ('dollars', 'NNS')]]\n</code></pre>\n", "abstract": "I would probably approach this as a chunking task and use nltk's part of speech tagger combined with its regular expression chunker.  This will allow you to define a regular expression based on the part of speech of the words in your sentences instead of on the words themselves.  For a given sentence, you can do the following: The first thing I would do is to modify your sentences slightly so that you don't confuse the part of speech tagger too much.  Here are some examples of changes that you can make (with very simple regular expressions) but you can experiment and see if there are others: so we get: now you can get the parts of speech from your sentence: We can now create a chunker which will chunk your POS tagged text according to a (relatively) simple regular expression: This defines a parser with a grammar that chunks numerical phrases (what we'll call your phrase type). It defines your numerical phrase as: an optional noun, followed by an optional adverb, followed by a comparative adjective, a preposition, a number, and an optional noun. \nThis is just a suggestion for how you may want to define your phrases, but I think that this will be much simpler than using a regular expression on the words themselves.   To get your phrases you can do: Or to get only your phrases you can do:"}]}, {"link": "https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms", "question": {"id": "14489309", "title": "Convert words between verb/noun/adjective forms", "content": "<p>i would like a python library function that translates/converts across different parts of speech. sometimes it should output multiple words (e.g. \"coder\" and \"code\" are both nouns from the verb \"to code\", one's the subject the other's the object)</p>\n<pre><code class=\"python\"># :: String =&gt; List of String\nprint verbify('writer') # =&gt; ['write']\nprint nounize('written') # =&gt; ['writer']\nprint adjectivate('write') # =&gt; ['written']\n</code></pre>\n<p>i mostly care about verbs &lt;=&gt; nouns, for a note taking program i want to write. i.e. i can write \"caffeine antagonizes A1\" or \"caffeine is an A1 antagonist\" and with some NLP it can figure out they mean the same thing. (i know that's not easy, and that it will take NLP that parses and doesn't just tag, but i want to hack up a prototype).</p>\n<p>similar questions ...\n<a href=\"https://stackoverflow.com/questions/7548479/converting-adjectives-and-adverbs-to-their-noun-forms\">Converting adjectives and adverbs to their noun forms</a>\n(this answer only stems down to the root POS. i want to go between POS.)</p>\n<p>ps called Conversion in linguistics <a href=\"http://en.wikipedia.org/wiki/Conversion_%28linguistics%29\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Conversion_%28linguistics%29</a></p>\n", "abstract": "i would like a python library function that translates/converts across different parts of speech. sometimes it should output multiple words (e.g. \"coder\" and \"code\" are both nouns from the verb \"to code\", one's the subject the other's the object) i mostly care about verbs <=> nouns, for a note taking program i want to write. i.e. i can write \"caffeine antagonizes A1\" or \"caffeine is an A1 antagonist\" and with some NLP it can figure out they mean the same thing. (i know that's not easy, and that it will take NLP that parses and doesn't just tag, but i want to hack up a prototype). similar questions ...\nConverting adjectives and adverbs to their noun forms\n(this answer only stems down to the root POS. i want to go between POS.) ps called Conversion in linguistics http://en.wikipedia.org/wiki/Conversion_%28linguistics%29"}, "answers": [{"id": 16752477, "score": 22, "vote": 0, "content": "<p>This is more a heuristic approach. I have just coded it so appologies for the style. It uses the derivationally_related_forms() from wordnet. I have implemented nounify. I guess verbify works analogous. From what I've tested works pretty well:</p>\n<pre><code class=\"python\">from nltk.corpus import wordnet as wn\n\ndef nounify(verb_word):\n    \"\"\" Transform a verb to the closest noun: die -&gt; death \"\"\"\n    verb_synsets = wn.synsets(verb_word, pos=\"v\")\n\n    # Word not found\n    if not verb_synsets:\n        return []\n\n    # Get all verb lemmas of the word\n    verb_lemmas = [l for s in verb_synsets \\\n                   for l in s.lemmas if s.name.split('.')[1] == 'v']\n\n    # Get related forms\n    derivationally_related_forms = [(l, l.derivationally_related_forms()) \\\n                                    for l in    verb_lemmas]\n\n    # filter only the nouns\n    related_noun_lemmas = [l for drf in derivationally_related_forms \\\n                           for l in drf[1] if l.synset.name.split('.')[1] == 'n']\n\n    # Extract the words from the lemmas\n    words = [l.name for l in related_noun_lemmas]\n    len_words = len(words)\n\n    # Build the result in the form of a list containing tuples (word, probability)\n    result = [(w, float(words.count(w))/len_words) for w in set(words)]\n    result.sort(key=lambda w: -w[1])\n\n    # return all the possibilities sorted by probability\n    return result\n</code></pre>\n", "abstract": "This is more a heuristic approach. I have just coded it so appologies for the style. It uses the derivationally_related_forms() from wordnet. I have implemented nounify. I guess verbify works analogous. From what I've tested works pretty well:"}, {"id": 48218093, "score": 13, "vote": 0, "content": "<p>Here is a function that is in theory able to convert words between noun/verb/adjective/adverb form that I updated from <a href=\"http://bogdani.webfactional.com/convert-words-between-verbnounadjectiveadverb-forms-using-wordnet/\" rel=\"noreferrer\">here</a> (originally written by <a href=\"https://stackoverflow.com/users/1607029/bogs\">bogs</a>, I believe) to be compliant with nltk 3.2.5 now that <code>synset.lemmas</code> and <code>sysnset.name</code> are functions.</p>\n<pre><code class=\"python\">from nltk.corpus import wordnet as wn\n\n# Just to make it a bit more readable\nWN_NOUN = 'n'\nWN_VERB = 'v'\nWN_ADJECTIVE = 'a'\nWN_ADJECTIVE_SATELLITE = 's'\nWN_ADVERB = 'r'\n\n\ndef convert(word, from_pos, to_pos):    \n    \"\"\" Transform words given from/to POS tags \"\"\"\n\n    synsets = wn.synsets(word, pos=from_pos)\n\n    # Word not found\n    if not synsets:\n        return []\n\n    # Get all lemmas of the word (consider 'a'and 's' equivalent)\n    lemmas = []\n    for s in synsets:\n        for l in s.lemmas():\n            if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n                lemmas += [l]\n\n    # Get related forms\n    derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n\n    # filter only the desired pos (consider 'a' and 's' equivalent)\n    related_noun_lemmas = []\n\n    for drf in derivationally_related_forms:\n        for l in drf[1]:\n            if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n                related_noun_lemmas += [l]\n\n    # Extract the words from the lemmas\n    words = [l.name() for l in related_noun_lemmas]\n    len_words = len(words)\n\n    # Build the result in the form of a list containing tuples (word, probability)\n    result = [(w, float(words.count(w)) / len_words) for w in set(words)]\n    result.sort(key=lambda w:-w[1])\n\n    # return all the possibilities sorted by probability\n    return result\n\n\nconvert('direct', 'a', 'r')\nconvert('direct', 'a', 'n')\nconvert('quick', 'a', 'r')\nconvert('quickly', 'r', 'a')\nconvert('hunger', 'n', 'v')\nconvert('run', 'v', 'a')\nconvert('tired', 'a', 'r')\nconvert('tired', 'a', 'v')\nconvert('tired', 'a', 'n')\nconvert('tired', 'a', 's')\nconvert('wonder', 'v', 'n')\nconvert('wonder', 'n', 'a')\n</code></pre>\n<p>As you can see below, it doesn't work so great.  It's unable to switch between adjective and adverb form (my specific goal), but it does give some interesting results in other cases.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; convert('direct', 'a', 'r')\n[]\n&gt;&gt;&gt; convert('direct', 'a', 'n')\n[('directness', 0.6666666666666666), ('line', 0.3333333333333333)]\n&gt;&gt;&gt; convert('quick', 'a', 'r')\n[]\n&gt;&gt;&gt; convert('quickly', 'r', 'a')\n[]\n&gt;&gt;&gt; convert('hunger', 'n', 'v')\n[('hunger', 0.75), ('thirst', 0.25)]\n&gt;&gt;&gt; convert('run', 'v', 'a')\n[('persistent', 0.16666666666666666), ('executive', 0.16666666666666666), ('operative', 0.16666666666666666), ('prevalent', 0.16666666666666666), ('meltable', 0.16666666666666666), ('operant', 0.16666666666666666)]\n&gt;&gt;&gt; convert('tired', 'a', 'r')\n[]\n&gt;&gt;&gt; convert('tired', 'a', 'v')\n[]\n&gt;&gt;&gt; convert('tired', 'a', 'n')\n[('triteness', 0.25), ('banality', 0.25), ('tiredness', 0.25), ('commonplace', 0.25)]\n&gt;&gt;&gt; convert('tired', 'a', 's')\n[]\n&gt;&gt;&gt; convert('wonder', 'v', 'n')\n[('wonder', 0.3333333333333333), ('wonderer', 0.2222222222222222), ('marveller', 0.1111111111111111), ('marvel', 0.1111111111111111), ('wonderment', 0.1111111111111111), ('question', 0.1111111111111111)]\n&gt;&gt;&gt; convert('wonder', 'n', 'a')\n[('curious', 0.4), ('wondrous', 0.2), ('marvelous', 0.2), ('marvellous', 0.2)]\n</code></pre>\n<p>hope this is able to save someone a little trouble</p>\n", "abstract": "Here is a function that is in theory able to convert words between noun/verb/adjective/adverb form that I updated from here (originally written by bogs, I believe) to be compliant with nltk 3.2.5 now that synset.lemmas and sysnset.name are functions. As you can see below, it doesn't work so great.  It's unable to switch between adjective and adverb form (my specific goal), but it does give some interesting results in other cases. hope this is able to save someone a little trouble"}, {"id": 14492730, "score": 4, "vote": 0, "content": "<p>I understand that this doesn't answer your whole question, but it does answer a large part of it. I would check out\n<a href=\"http://nodebox.net/code/index.php/Linguistics#verb_conjugation\" rel=\"nofollow\">http://nodebox.net/code/index.php/Linguistics#verb_conjugation</a> \nThis python library is able to conjugate verbs, and recognize whether a word is a verb, noun, or adjective. </p>\n<p>EXAMPLE CODE</p>\n<pre><code class=\"python\">print en.verb.present(\"gave\")\nprint en.verb.present(\"gave\", person=3, negate=False)\n&gt;&gt;&gt; give\n&gt;&gt;&gt; gives\n</code></pre>\n<p>It can also categorize words. </p>\n<pre><code class=\"python\">print en.is_noun(\"banana\")\n&gt;&gt;&gt; True\n</code></pre>\n<p>The download is at the top of the link. </p>\n", "abstract": "I understand that this doesn't answer your whole question, but it does answer a large part of it. I would check out\nhttp://nodebox.net/code/index.php/Linguistics#verb_conjugation \nThis python library is able to conjugate verbs, and recognize whether a word is a verb, noun, or adjective.  EXAMPLE CODE It can also categorize words.  The download is at the top of the link. "}, {"id": 14494220, "score": 3, "vote": 0, "content": "<p>One approach may be to use a dictionary of words with their POS tags and a wordforms mapping. If you get or create such dictionary (which is quite possible if you have access to any conventional dictionary's data, as all the dictionaries list word's POS tags, as well as base forms for all derived forms), you can use something like the following:</p>\n<pre><code class=\"python\">def is_verb(word):\n    if word:\n        tags = pos_tags(word)\n        return 'VB' in tags or 'VBP' in tags or 'VBZ' in tags \\\n               or 'VBD' in tags or 'VBN' in tags:\n\ndef verbify(word):\n    if is_verb(word):\n        return word\n    else:\n       forms = []\n       for tag in pos_tags(word):\n           base = word_form(word, tag[:2])\n           if is_verb(base):\n              forms.append(base)\n       return forms\n</code></pre>\n", "abstract": "One approach may be to use a dictionary of words with their POS tags and a wordforms mapping. If you get or create such dictionary (which is quite possible if you have access to any conventional dictionary's data, as all the dictionaries list word's POS tags, as well as base forms for all derived forms), you can use something like the following:"}]}, {"link": "https://stackoverflow.com/questions/22433884/python-gensim-how-to-calculate-document-similarity-using-the-lda-model", "question": {"id": "22433884", "title": "Python Gensim: how to calculate document similarity using the LDA model?", "content": "<p>I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.\nAfter studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!</p>\n", "abstract": "I've got a trained LDA model and I want to calculate the similarity score between two documents from the corpus I trained my model on.\nAfter studying all the Gensim tutorials and functions, I still can't get my head around it. Can somebody give me a hint? Thanks!"}, "answers": [{"id": 22756647, "score": 36, "vote": 0, "content": "<p>Depends what similarity metric you want to use.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">Cosine similarity</a> is universally useful &amp; <a href=\"http://radimrehurek.com/gensim/matutils.html#gensim.matutils.cossim\" rel=\"noreferrer\">built-in</a>:</p>\n<pre><code class=\"python\">sim = gensim.matutils.cossim(vec_lda1, vec_lda2)\n</code></pre>\n<p><a href=\"http://en.wikipedia.org/wiki/Hellinger_distance\" rel=\"noreferrer\">Hellinger distance</a> is useful for similarity between probability distributions (such as LDA topics):</p>\n<pre><code class=\"python\">import numpy as np\ndense1 = gensim.matutils.sparse2full(lda_vec1, lda.num_topics)\ndense2 = gensim.matutils.sparse2full(lda_vec2, lda.num_topics)\nsim = np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())\n</code></pre>\n", "abstract": "Depends what similarity metric you want to use. Cosine similarity is universally useful & built-in: Hellinger distance is useful for similarity between probability distributions (such as LDA topics):"}, {"id": 22561795, "score": 26, "vote": 0, "content": "<p>Don't know if this'll help but, I managed to attain successful results on document matching and similarities when using the actual document as a query.</p>\n<pre><code class=\"python\">dictionary = corpora.Dictionary.load('dictionary.dict')\ncorpus = corpora.MmCorpus(\"corpus.mm\")\nlda = models.LdaModel.load(\"model.lda\") #result from running online lda (training)\n\nindex = similarities.MatrixSimilarity(lda[corpus])\nindex.save(\"simIndex.index\")\n\ndocname = \"docs/the_doc.txt\"\ndoc = open(docname, 'r').read()\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lda = lda[vec_bow]\n\nsims = index[vec_lda]\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\nprint sims\n</code></pre>\n<p>Your similarity score between all documents residing in the corpus and the document that was used as a query will be the second index of every sim for sims.</p>\n", "abstract": "Don't know if this'll help but, I managed to attain successful results on document matching and similarities when using the actual document as a query. Your similarity score between all documents residing in the corpus and the document that was used as a query will be the second index of every sim for sims."}, {"id": 61560380, "score": 5, "vote": 0, "content": "<p>Provided answers are good, but they aren't very beginner-friendly. I want to start from training the LDA model and calculate cosine similarity.</p>\n<p>Training model part:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">docs = [\"latent Dirichlet allocation (LDA) is a generative statistical model\", \n        \"each document is a mixture of a small number of topics\",\n        \"each document may be viewed as a mixture of various topics\"]\n\n# Convert document to tokens\ndocs = [doc.split() for doc in docs]\n\n# A mapping from token to id in each document\nfrom gensim.corpora import Dictionary\ndictionary = Dictionary(docs)\n\n# Representing the corpus as a bag of words\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n\n# Training the model\nmodel = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)\n</code></pre>\n<p>For extracting the probability assigned to each topic for a document, there are generally two ways. I provide here the both:</p>\n<pre><code class=\"python\"># Some preprocessing for documents like the training the model\ntest_doc = [\"LDA is an example of a topic model\",\n            \"topic modelling refers to the task of identifying topics\"]\ntest_doc = [doc.split() for doc in test_doc]\ntest_corpus = [dictionary.doc2bow(doc) for doc in test_doc]\n\n# Method 1\nfrom gensim.matutils import cossim\ndoc1 = model.get_document_topics(test_corpus[0], minimum_probability=0)\ndoc2 = model.get_document_topics(test_corpus[1], minimum_probability=0)\nprint(cossim(doc1, doc2))\n\n# Method 2\ndoc1 = model[test_corpus[0]]\ndoc2 = model[test_corpus[1]]\nprint(cossim(doc1, doc2))\n</code></pre>\n<p>output:</p>\n<pre><code class=\"python\">#Method 1\n0.8279631530869963\n\n#Method 2\n0.828066885140262\n</code></pre>\n<p>As you can see both of the methods are generally the same, the difference is in the probabilities returned in the 2nd method sometimes doesn't add up to one as discussed <a href=\"https://stackoverflow.com/questions/44571617/probabilities-returned-by-gensims-get-document-topics-method-doesnt-add-up-to/59049417#comment108656303_59049417\">here</a>. \nFor large corpus, the possibility vector could be given by passing the whole corpus:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">#Method 1\npossibility_vector = model.get_document_topics(test_corpus, minimum_probability=0)\n#Method 2\npossiblity_vector = model[test_corpus]\n</code></pre>\n<p><strong>NOTE:</strong> The sum of probability assigned to each topic in a document may become a bit higher than 1 or in some cases a bit less than 1. That is because of the floating-point arithmetic rounding errors.</p>\n", "abstract": "Provided answers are good, but they aren't very beginner-friendly. I want to start from training the LDA model and calculate cosine similarity. Training model part: For extracting the probability assigned to each topic for a document, there are generally two ways. I provide here the both: output: As you can see both of the methods are generally the same, the difference is in the probabilities returned in the 2nd method sometimes doesn't add up to one as discussed here. \nFor large corpus, the possibility vector could be given by passing the whole corpus: NOTE: The sum of probability assigned to each topic in a document may become a bit higher than 1 or in some cases a bit less than 1. That is because of the floating-point arithmetic rounding errors."}]}, {"link": "https://stackoverflow.com/questions/5479333/summarize-text-or-simplify-text", "question": {"id": "5479333", "title": "summarize text or simplify text", "content": "<p>Is there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?</p>\n", "abstract": "Is there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?"}, "answers": [{"id": 19956247, "score": 27, "vote": 0, "content": "<p>Maybe you can try <a href=\"https://github.com/miso-belica/sumy\" rel=\"noreferrer\">sumy</a>. It's a quite small library that I wrote in Python. There are implemented Luhn's and Edmundson's approaches, LSA method, SumBasic, KL-Sum, LexRank and TextRank algorithms. It's Apache2 licensed and supports Czech, Slovak, English, French, Japanese, Chinese, Portuguese, Spanish and German languages.</p>\n<p>Feel free to open an issue or send a pull request if there is something you are missing.</p>\n", "abstract": "Maybe you can try sumy. It's a quite small library that I wrote in Python. There are implemented Luhn's and Edmundson's approaches, LSA method, SumBasic, KL-Sum, LexRank and TextRank algorithms. It's Apache2 licensed and supports Czech, Slovak, English, French, Japanese, Chinese, Portuguese, Spanish and German languages. Feel free to open an issue or send a pull request if there is something you are missing."}, {"id": 5479497, "score": 18, "vote": 0, "content": "<p>I'm not sure if there is currently any libraries that do this, as text summarization, or at least <strong>understandable</strong> text summarization isn't something that will be easily accomplished by a simple plug &amp; play library.</p>\n<p>Here are a few links that I managed to find regarding projects / resources that are related to text summarization to get you started:</p>\n<ul>\n<li><a href=\"http://www.lemurproject.org/\" rel=\"noreferrer\">The Lemur Project</a></li>\n<li><a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Python Natural Language Toolkit</a></li>\n<li><a href=\"http://oreilly.com/catalog/9780596516499\" rel=\"noreferrer\">O'Reilly's Book on Natural Language Processing in Python</a></li>\n<li><a href=\"https://web.archive.org/web/20140821083738/http://nltk.googlecode.com/svn/trunk/doc/book/book.html\" rel=\"noreferrer\">Google Resource on Natural Language Processing</a></li>\n<li><a href=\"https://web.archive.org/web/20140215021123/http://www.mindtwist.de/main/linux/3-linux-tipps/37-how-to-create-a-keyword-summary-of-a-text-with-python.html\" rel=\"noreferrer\">Tutorial : How to create a keyword summary of text in Python</a></li>\n</ul>\n<p>Hope that helps :)</p>\n", "abstract": "I'm not sure if there is currently any libraries that do this, as text summarization, or at least understandable text summarization isn't something that will be easily accomplished by a simple plug & play library. Here are a few links that I managed to find regarding projects / resources that are related to text summarization to get you started: Hope that helps :)"}, {"id": 18476481, "score": 4, "vote": 0, "content": "<p>I needed also the same thing but I couldn't find anything in <strong>Python</strong> that helped me have a <strong>Comprehensive</strong> Result.</p>\n<p>So I found this Web Service really useful, and they have a free <a href=\"http://smmry.com/api\" rel=\"nofollow\">API</a> which gives a JSON result, and I wanted to share it with you.</p>\n<p>Check it out here: <a href=\"http://smmry.com\" rel=\"nofollow\">http://smmry.com</a></p>\n", "abstract": "I needed also the same thing but I couldn't find anything in Python that helped me have a Comprehensive Result. So I found this Web Service really useful, and they have a free API which gives a JSON result, and I wanted to share it with you. Check it out here: http://smmry.com"}, {"id": 6753149, "score": 3, "vote": 0, "content": "<p>Try <a href=\"http://libots.sourceforge.net/\" rel=\"nofollow\">Open Text Summarizer</a> which is released under the GPL open source license.  It works reasonably well but there has been no development work on it since 2007.  </p>\n<p>The original code is written in C (both a library and a command line utility) but there are wrappers to it in a number of languages:</p>\n<ul>\n<li><a href=\"http://www.splitbrain.org/services/ots\" rel=\"nofollow\">Perl</a></li>\n<li><a href=\"http://intridea.com/2010/12/3/summarize-a-ruby-c-binding-for-open-text-summarizer\" rel=\"nofollow\">Ruby</a></li>\n<li><a href=\"http://pypi.python.org/pypi/ots/0.4.2.1\" rel=\"nofollow\">Python</a></li>\n<li><a href=\"http://ots.codeplex.com\" rel=\"nofollow\">C#</a></li>\n</ul>\n", "abstract": "Try Open Text Summarizer which is released under the GPL open source license.  It works reasonably well but there has been no development work on it since 2007.   The original code is written in C (both a library and a command line utility) but there are wrappers to it in a number of languages:"}, {"id": 5502300, "score": 2, "vote": 0, "content": "<p>Not python but <a href=\"http://www.summarization.com/mead/\" rel=\"nofollow\">MEAD</a> will do text summarization (it's in Perl).  Usually what comes out is comprehensible, if not always particularly fluent sounding.  Also check out <a href=\"http://www.summarization.com/\" rel=\"nofollow\">summarization.com</a> for a lot of good information on the text summarization task.</p>\n", "abstract": "Not python but MEAD will do text summarization (it's in Perl).  Usually what comes out is comprehensible, if not always particularly fluent sounding.  Also check out summarization.com for a lot of good information on the text summarization task."}, {"id": 44493106, "score": 2, "vote": 0, "content": "<p>Take a look at this <a href=\"https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/\" rel=\"nofollow noreferrer\">article</a> which does a detailed study of these methods and packages:</p>\n<ol>\n<li>Lex_rank (<a href=\"https://github.com/miso-belica/sumy\" rel=\"nofollow noreferrer\">sumy</a>)</li>\n<li>LSA (sumy)</li>\n<li>Luhn (sumy)</li>\n<li><a href=\"https://github.com/xiaoxu193/PyTeaser\" rel=\"nofollow noreferrer\">PyTeaser</a></li>\n<li><a href=\"https://radimrehurek.com/gensim/models/lsimodel.html\" rel=\"nofollow noreferrer\">Gensim</a> TextRank</li>\n<li><a href=\"https://github.com/ceteri/pytextrank\" rel=\"nofollow noreferrer\">PyTextRank</a></li>\n<li>Google <a href=\"https://github.com/tensorflow/models/tree/master/textsum\" rel=\"nofollow noreferrer\">TextSum</a></li>\n</ol>\n<p>The ending of the article does a '<a href=\"https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#to_summarize\" rel=\"nofollow noreferrer\">summary</a>'. </p>\n<p>The author of sumy @<a href=\"https://stackoverflow.com/users/2988107/miso-belica\">miso.belica</a> has given a description in an answer above.</p>\n<p>Various other ML techniques have risen, such as <a href=\"https://github.com/facebookarchive/NAMAS\" rel=\"nofollow noreferrer\">Facebook/NAMAS</a> and Google/TextSum but still need extensive training in Gigaword Dataset and about 7000 GPU hours. The dataset itself is quite costly.</p>\n<p>In conclusion I would say sumy is the best option in the market right now if you don't have access to high-end machines. Thanks a lot @miso.belica for this wonderful package.</p>\n", "abstract": "Take a look at this article which does a detailed study of these methods and packages: The ending of the article does a 'summary'.  The author of sumy @miso.belica has given a description in an answer above. Various other ML techniques have risen, such as Facebook/NAMAS and Google/TextSum but still need extensive training in Gigaword Dataset and about 7000 GPU hours. The dataset itself is quite costly. In conclusion I would say sumy is the best option in the market right now if you don't have access to high-end machines. Thanks a lot @miso.belica for this wonderful package."}, {"id": 35160942, "score": 1, "vote": 0, "content": "<p>A while back, I wrote a summarization library for python using NLTK, using an algorithm from the Classifier4J library. It's pretty simple but it may suit the needs of anyone that needs summarization: <a href=\"https://github.com/thavelick/summarize\" rel=\"nofollow\">https://github.com/thavelick/summarize</a></p>\n", "abstract": "A while back, I wrote a summarization library for python using NLTK, using an algorithm from the Classifier4J library. It's pretty simple but it may suit the needs of anyone that needs summarization: https://github.com/thavelick/summarize"}]}, {"link": "https://stackoverflow.com/questions/4634787/freqdist-with-nltk", "question": {"id": "4634787", "title": "FreqDist with NLTK", "content": "<p><strong>NLTK</strong> in python has a function <a href=\"http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\" rel=\"noreferrer\">FreqDist</a> which gives you the frequency of words within a text. I am trying to pass my text as an argument but the result is of the form: </p>\n<p><code>[' ', 'e', 'a', 'o', 'n', 'i', 't', 'r', 's', 'l', 'd', 'h', 'c', 'y', 'b', 'u', 'g', '\\n', 'm', 'p', 'w', 'f', ',', 'v', '.', \"'\", 'k', 'B', '\"', 'M', 'H', '9', 'C', '-', 'N', 'S', '1', 'A', 'G', 'P', 'T', 'W', '[', ']', '(', ')', '0', '7', 'E', 'J', 'O', 'R', 'j', 'x']</code> </p>\n<p>whereas in the example in the <strong>NLTK</strong> website the result was whole words not just letters. Im doing it this way:</p>\n<pre><code class=\"python\">file_y = open(fileurl)\np = file_y.read()\nfdist = FreqDist(p)\nvocab = fdist.keys()\nvocab[:100]\n</code></pre>\n<p>DO you know what I have wrong pls? Thanks!</p>\n", "abstract": "NLTK in python has a function FreqDist which gives you the frequency of words within a text. I am trying to pass my text as an argument but the result is of the form:  [' ', 'e', 'a', 'o', 'n', 'i', 't', 'r', 's', 'l', 'd', 'h', 'c', 'y', 'b', 'u', 'g', '\\n', 'm', 'p', 'w', 'f', ',', 'v', '.', \"'\", 'k', 'B', '\"', 'M', 'H', '9', 'C', '-', 'N', 'S', '1', 'A', 'G', 'P', 'T', 'W', '[', ']', '(', ')', '0', '7', 'E', 'J', 'O', 'R', 'j', 'x']  whereas in the example in the NLTK website the result was whole words not just letters. Im doing it this way: DO you know what I have wrong pls? Thanks!"}, "answers": [{"id": 4634924, "score": 48, "vote": 0, "content": "<p><code>FreqDist</code> expects an iterable of tokens. A string is iterable --- the iterator yields every character.</p>\n<p>Pass your text to a tokenizer first, and pass the tokens to <code>FreqDist</code>.</p>\n", "abstract": "FreqDist expects an iterable of tokens. A string is iterable --- the iterator yields every character. Pass your text to a tokenizer first, and pass the tokens to FreqDist."}, {"id": 6500258, "score": 33, "vote": 0, "content": "<p>FreqDist runs on an array of tokens. You're sending it a an array of characters (a string) where you should have tokenized the input first:</p>\n<pre><code class=\"python\">words = nltk.tokenize.word_tokenize(p)\nfdist = FreqDist(words)\n</code></pre>\n", "abstract": "FreqDist runs on an array of tokens. You're sending it a an array of characters (a string) where you should have tokenized the input first:"}, {"id": 4635804, "score": 22, "vote": 0, "content": "<p>NLTK's <code>FreqDist</code> accepts any iterable. As a string is iterated character by character, it is pulling things apart in the way that you're experiencing.</p>\n<p>In order to do count words, you need to feed <code>FreqDist</code> words. How do you do that? Well, you might think (as others have suggested in the answer to your question) to feed the whole file to <code>nltk.tokenize.word_tokenize</code>.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; # first, let's import the dependencies\n&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; from nltk.probability import FreqDist\n\n&gt;&gt;&gt; # wrong :(\n&gt;&gt;&gt; words = nltk.tokenize.word_tokenize(p)\n&gt;&gt;&gt; fdist = FreqDist(words)\n</code></pre>\n<p><code>word_tokenize</code> builds word models from sentences. It needs to be fed each sentence one at a time. It will do a relatively poor job when given whole paragraphs or even documents.</p>\n<p>So, what to do? Easy, add in a sentence tokenizer!</p>\n<pre><code class=\"python\">&gt;&gt;&gt; fdist = FreqDist()\n&gt;&gt;&gt; for sentence in nltk.tokenize.sent_tokenize(p):\n...     for word in nltk.tokenize.word_tokenize(sentence):\n&gt;&gt;&gt;         fdist[word] += 1\n</code></pre>\n<p>One thing to bear in mind is that there are many ways to tokenize text. The modules <code>nltk.tokenize.sent_tokenize</code> and <code>nltk.tokenize.word_tokenize</code> simply pick a reasonable default for relatively clean, English text. There are several other options to chose from, which you can read about in the <a href=\"http://www.nltk.org/api/nltk.tokenize.html\" rel=\"noreferrer\">API documentation</a>.</p>\n", "abstract": "NLTK's FreqDist accepts any iterable. As a string is iterated character by character, it is pulling things apart in the way that you're experiencing. In order to do count words, you need to feed FreqDist words. How do you do that? Well, you might think (as others have suggested in the answer to your question) to feed the whole file to nltk.tokenize.word_tokenize. word_tokenize builds word models from sentences. It needs to be fed each sentence one at a time. It will do a relatively poor job when given whole paragraphs or even documents. So, what to do? Easy, add in a sentence tokenizer! One thing to bear in mind is that there are many ways to tokenize text. The modules nltk.tokenize.sent_tokenize and nltk.tokenize.word_tokenize simply pick a reasonable default for relatively clean, English text. There are several other options to chose from, which you can read about in the API documentation."}, {"id": 18119400, "score": 9, "vote": 0, "content": "<p>You simply have to use it like this:</p>\n<pre><code class=\"python\">import nltk\nfrom nltk.probability import FreqDist\n\nsentence='''This is my sentence'''\ntokens = nltk.tokenize.word_tokenize(sentence)\nfdist=FreqDist(tokens)\n</code></pre>\n<p>The variable fdist is of the type \"class 'nltk.probability.FreqDist\"  and contains the frequency distribution of words.</p>\n", "abstract": "You simply have to use it like this: The variable fdist is of the type \"class 'nltk.probability.FreqDist\"  and contains the frequency distribution of words."}, {"id": 60463723, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">Your_string = \"here is my string\"\ntokens = Your_string.split()\n</code></pre>\n<p>Do this way, and then use the <strong>NLTK</strong> functions</p>\n<p><strong>it will give your tokens in words but not in characters</strong></p>\n", "abstract": "Do this way, and then use the NLTK functions it will give your tokens in words but not in characters"}, {"id": 64186240, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">text_dist = nltk.FreqDist(word for word in list(text) if word.isalpha())\ntop1_text1 = text_dist.max()\nmaxfreq = top1_text1\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/40325980/how-is-the-vader-compound-polarity-score-calculated-in-python-nltk", "question": {"id": "40325980", "title": "How is the Vader &#39;compound&#39; polarity score calculated in Python NLTK?", "content": "<p>I'm using the Vader SentimentAnalyzer to obtain the polarity scores. I used the probability scores for positive/negative/neutral before, but I just realized the \"compound\" score, ranging from -1 (most neg) to 1 (most pos) would provide a single measure of polarity. I wonder how the \"compound\" score computed. Is that calculated from the [pos, neu, neg] vector? </p>\n", "abstract": "I'm using the Vader SentimentAnalyzer to obtain the polarity scores. I used the probability scores for positive/negative/neutral before, but I just realized the \"compound\" score, ranging from -1 (most neg) to 1 (most pos) would provide a single measure of polarity. I wonder how the \"compound\" score computed. Is that calculated from the [pos, neu, neg] vector? "}, "answers": [{"id": 40337646, "score": 90, "vote": 0, "content": "<p>The VADER algorithm outputs sentiment scores to 4 classes of sentiments <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L441\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L441</a>:</p>\n<ul>\n<li><code>neg</code>: Negative</li>\n<li><code>neu</code>: Neutral</li>\n<li><code>pos</code>: Positive </li>\n<li><code>compound</code>: Compound (i.e. aggregated score)</li>\n</ul>\n<p>Let's walk through the code, the first instance of compound is at <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L421\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L421</a>, where it computes:</p>\n<pre><code class=\"python\">compound = normalize(sum_s)\n</code></pre>\n<p>The <code>normalize()</code> function is defined as such at <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L107\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L107</a>:</p>\n<pre><code class=\"python\">def normalize(score, alpha=15):\n    \"\"\"\n    Normalize the score to be between -1 and 1 using an alpha that\n    approximates the max expected value\n    \"\"\"\n    norm_score = score/math.sqrt((score*score) + alpha)\n    return norm_score\n</code></pre>\n<p>So there's a hyper-parameter <code>alpha</code>.</p>\n<p>As for the <code>sum_s</code>, it is a sum of the sentiment arguments passed to the <code>score_valence()</code> function <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L413\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L413</a></p>\n<p>And if we trace back this <code>sentiment</code> argument, we see that it's computed when calling the <code>polarity_scores()</code> function at <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L217\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L217</a>:</p>\n<pre><code class=\"python\">def polarity_scores(self, text):\n    \"\"\"\n    Return a float for sentiment strength based on the input text.\n    Positive values are positive valence, negative value are negative\n    valence.\n    \"\"\"\n    sentitext = SentiText(text)\n    #text, words_and_emoticons, is_cap_diff = self.preprocess(text)\n\n    sentiments = []\n    words_and_emoticons = sentitext.words_and_emoticons\n    for item in words_and_emoticons:\n        valence = 0\n        i = words_and_emoticons.index(item)\n        if (i &lt; len(words_and_emoticons) - 1 and item.lower() == \"kind\" and \\\n            words_and_emoticons[i+1].lower() == \"of\") or \\\n            item.lower() in BOOSTER_DICT:\n            sentiments.append(valence)\n            continue\n\n        sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n\n    sentiments = self._but_check(words_and_emoticons, sentiments)\n</code></pre>\n<p>Looking at the <code>polarity_scores</code> function, what it's doing is to iterate through the whole SentiText lexicon and checks with the rule-based <code>sentiment_valence()</code> function to assign the valence score to the sentiment <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L243\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L243</a>, see Section 2.1.1 of <a href=\"http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\">http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf</a></p>\n<p>So going back to the compound score, we see that:</p>\n<ul>\n<li>the <code>compound</code> score is a normalized score of <code>sum_s</code> and</li>\n<li><code>sum_s</code> is the sum of valence computed based on some heuristics and a sentiment lexicon (aka. Sentiment Intensity) and</li>\n<li>the normalized score is simply the <code>sum_s</code> divided by its square plus an alpha parameter that increases the denominator of the normalization function. </li>\n</ul>\n<hr/>\n<p><strong>Is that calculated from the [pos, neu, neg] vector?</strong></p>\n<p>Not really =)</p>\n<p>If we take a look at the <code>score_valence</code> function <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L411\">https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L411</a>, we see that the compound score is computed with the <code>sum_s</code> before the pos, neg and neu scores are computed using <code>_sift_sentiment_scores()</code> that computes the invidiual pos, neg and neu scores using the raw scores from <code>sentiment_valence()</code> without the sum.</p>\n<hr/>\n<p>If we take a look at this <code>alpha</code> mathemagic, it seems the output of the normalization is rather unstable (if left unconstrained), depending on the value of <code>alpha</code>:</p>\n<p><code>alpha=0</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/F8A7n.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/F8A7n.png\"/></a></p>\n<p><code>alpha=15</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/p8De8.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/p8De8.png\"/></a> </p>\n<p><code>alpha=50000</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/69WGl.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/69WGl.png\"/></a></p>\n<p><code>alpha=0.001</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/v1l31.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/v1l31.png\"/></a></p>\n<p>It gets funky when it's negative:</p>\n<p><code>alpha=-10</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/knkJ1.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/knkJ1.png\"/></a></p>\n<p><code>alpha=-1,000,000</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/lPkAf.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/lPkAf.png\"/></a></p>\n<p><code>alpha=-1,000,000,000</code>:</p>\n<p><a href=\"https://i.stack.imgur.com/6ICZK.png\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/6ICZK.png\"/></a></p>\n", "abstract": "The VADER algorithm outputs sentiment scores to 4 classes of sentiments https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L441: Let's walk through the code, the first instance of compound is at https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L421, where it computes: The normalize() function is defined as such at https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L107: So there's a hyper-parameter alpha. As for the sum_s, it is a sum of the sentiment arguments passed to the score_valence() function https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L413 And if we trace back this sentiment argument, we see that it's computed when calling the polarity_scores() function at https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L217: Looking at the polarity_scores function, what it's doing is to iterate through the whole SentiText lexicon and checks with the rule-based sentiment_valence() function to assign the valence score to the sentiment https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L243, see Section 2.1.1 of http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf So going back to the compound score, we see that: Is that calculated from the [pos, neu, neg] vector? Not really =) If we take a look at the score_valence function https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L411, we see that the compound score is computed with the sum_s before the pos, neg and neu scores are computed using _sift_sentiment_scores() that computes the invidiual pos, neg and neu scores using the raw scores from sentiment_valence() without the sum. If we take a look at this alpha mathemagic, it seems the output of the normalization is rather unstable (if left unconstrained), depending on the value of alpha: alpha=0:  alpha=15:   alpha=50000:  alpha=0.001:  It gets funky when it's negative: alpha=-10:  alpha=-1,000,000:  alpha=-1,000,000,000: "}, {"id": 55973381, "score": 7, "vote": 0, "content": "<p>\"About the Scoring\" section at the <a href=\"https://github.com/cjhutto/vaderSentiment\" rel=\"noreferrer\">github repo</a> has a description. </p>\n", "abstract": "\"About the Scoring\" section at the github repo has a description. "}]}, {"link": "https://stackoverflow.com/questions/3531746/what-s-a-good-python-profanity-filter-library", "question": {"id": "3531746", "title": "What\u2019s a good Python profanity filter library?", "content": "<p>Like <a href=\"https://stackoverflow.com/questions/1521646/best-profanity-filter\">https://stackoverflow.com/questions/1521646/best-profanity-filter</a>, but for Python \u2014 and I\u2019m looking for libraries I can run and control myself locally, as opposed to web services.</p>\n<p>(And whilst it\u2019s always great to hear your fundamental objections of principle to profanity filtering, I\u2019m not specifically looking for them here. I know profanity filtering can\u2019t pick up every hurtful thing being said. I know swearing, in the grand scheme of things, isn\u2019t a particularly big issue. I know you need some human input to deal with issues of content. I\u2019d just like to find a good library, and see what use I can make of it.)</p>\n", "abstract": "Like https://stackoverflow.com/questions/1521646/best-profanity-filter, but for Python \u2014 and I\u2019m looking for libraries I can run and control myself locally, as opposed to web services. (And whilst it\u2019s always great to hear your fundamental objections of principle to profanity filtering, I\u2019m not specifically looking for them here. I know profanity filtering can\u2019t pick up every hurtful thing being said. I know swearing, in the grand scheme of things, isn\u2019t a particularly big issue. I know you need some human input to deal with issues of content. I\u2019d just like to find a good library, and see what use I can make of it.)"}, "answers": [{"id": 3533322, "score": 44, "vote": 0, "content": "<p>I didn't found any Python profanity library, so I made one myself.</p>\n<h2>Parameters</h2>\n<hr/>\n<h3><code>filterlist</code></h3>\n<p>A list of regular expressions that match a forbidden word. Please do not use <code>\\b</code>, it will be inserted depending on <code>inside_words</code>.</p>\n<p>Example:\n <code>['bad', 'un\\w+']</code></p>\n<h3><code>ignore_case</code></h3>\n<p>Default: <code>True</code></p>\n<p>Self-explanatory.</p>\n<h3><code>replacements</code></h3>\n<p>Default: <code>\"$@%-?!\"</code></p>\n<p>A string with characters from which the replacements strings will be randomly generated.</p>\n<p>Examples: <code>\"%&amp;$?!\"</code> or <code>\"-\"</code> etc.</p>\n<h3><code>complete</code></h3>\n<p>Default: <code>True</code></p>\n<p>Controls if the entire string will be replaced or if the first and last chars will be kept.</p>\n<h3><code>inside_words</code></h3>\n<p>Default: <code>False</code></p>\n<p>Controls if words are searched inside other words too. Disabling this </p>\n<h2>Module source</h2>\n<hr/>\n<p>(examples at the end)</p>\n<pre><code class=\"python\">\"\"\"\nModule that provides a class that filters profanities\n\n\"\"\"\n\n__author__ = \"leoluk\"\n__version__ = '0.0.1'\n\nimport random\nimport re\n\nclass ProfanitiesFilter(object):\n    def __init__(self, filterlist, ignore_case=True, replacements=\"$@%-?!\", \n                 complete=True, inside_words=False):\n        \"\"\"\n        Inits the profanity filter.\n\n        filterlist -- a list of regular expressions that\n        matches words that are forbidden\n        ignore_case -- ignore capitalization\n        replacements -- string with characters to replace the forbidden word\n        complete -- completely remove the word or keep the first and last char?\n        inside_words -- search inside other words?\n\n        \"\"\"\n\n        self.badwords = filterlist\n        self.ignore_case = ignore_case\n        self.replacements = replacements\n        self.complete = complete\n        self.inside_words = inside_words\n\n    def _make_clean_word(self, length):\n        \"\"\"\n        Generates a random replacement string of a given length\n        using the chars in self.replacements.\n\n        \"\"\"\n        return ''.join([random.choice(self.replacements) for i in\n                  range(length)])\n\n    def __replacer(self, match):\n        value = match.group()\n        if self.complete:\n            return self._make_clean_word(len(value))\n        else:\n            return value[0]+self._make_clean_word(len(value)-2)+value[-1]\n\n    def clean(self, text):\n        \"\"\"Cleans a string from profanity.\"\"\"\n\n        regexp_insidewords = {\n            True: r'(%s)',\n            False: r'\\b(%s)\\b',\n            }\n\n        regexp = (regexp_insidewords[self.inside_words] % \n                  '|'.join(self.badwords))\n\n        r = re.compile(regexp, re.IGNORECASE if self.ignore_case else 0)\n\n        return r.sub(self.__replacer, text)\n\n\nif __name__ == '__main__':\n\n    f = ProfanitiesFilter(['bad', 'un\\w+'], replacements=\"-\")    \n    example = \"I am doing bad ungood badlike things.\"\n\n    print f.clean(example)\n    # Returns \"I am doing --- ------ badlike things.\"\n\n    f.inside_words = True    \n    print f.clean(example)\n    # Returns \"I am doing --- ------ ---like things.\"\n\n    f.complete = False    \n    print f.clean(example)\n    # Returns \"I am doing b-d u----d b-dlike things.\"\n</code></pre>\n", "abstract": "I didn't found any Python profanity library, so I made one myself. A list of regular expressions that match a forbidden word. Please do not use \\b, it will be inserted depending on inside_words. Example:\n ['bad', 'un\\w+'] Default: True Self-explanatory. Default: \"$@%-?!\" A string with characters from which the replacements strings will be randomly generated. Examples: \"%&$?!\" or \"-\" etc. Default: True Controls if the entire string will be replaced or if the first and last chars will be kept. Default: False Controls if words are searched inside other words too. Disabling this  (examples at the end)"}, {"id": 17706025, "score": 21, "vote": 0, "content": "<pre><code class=\"python\">arrBad = [\n'2g1c',\n'2 girls 1 cup',\n'acrotomophilia',\n'anal',\n'anilingus',\n'anus',\n'arsehole',\n'ass',\n'asshole',\n'assmunch',\n'auto erotic',\n'autoerotic',\n'babeland',\n'baby batter',\n'ball gag',\n'ball gravy',\n'ball kicking',\n'ball licking',\n'ball sack',\n'ball sucking',\n'bangbros',\n'bareback',\n'barely legal',\n'barenaked',\n'bastardo',\n'bastinado',\n'bbw',\n'bdsm',\n'beaver cleaver',\n'beaver lips',\n'bestiality',\n'bi curious',\n'big black',\n'big breasts',\n'big knockers',\n'big tits',\n'bimbos',\n'birdlock',\n'bitch',\n'black cock',\n'blonde action',\n'blonde on blonde action',\n'blow j',\n'blow your l',\n'blue waffle',\n'blumpkin',\n'bollocks',\n'bondage',\n'boner',\n'boob',\n'boobs',\n'booty call',\n'brown showers',\n'brunette action',\n'bukkake',\n'bulldyke',\n'bullet vibe',\n'bung hole',\n'bunghole',\n'busty',\n'butt',\n'buttcheeks',\n'butthole',\n'camel toe',\n'camgirl',\n'camslut',\n'camwhore',\n'carpet muncher',\n'carpetmuncher',\n'chocolate rosebuds',\n'circlejerk',\n'cleveland steamer',\n'clit',\n'clitoris',\n'clover clamps',\n'clusterfuck',\n'cock',\n'cocks',\n'coprolagnia',\n'coprophilia',\n'cornhole',\n'cum',\n'cumming',\n'cunnilingus',\n'cunt',\n'darkie',\n'date rape',\n'daterape',\n'deep throat',\n'deepthroat',\n'dick',\n'dildo',\n'dirty pillows',\n'dirty sanchez',\n'dog style',\n'doggie style',\n'doggiestyle',\n'doggy style',\n'doggystyle',\n'dolcett',\n'domination',\n'dominatrix',\n'dommes',\n'donkey punch',\n'double dong',\n'double penetration',\n'dp action',\n'eat my ass',\n'ecchi',\n'ejaculation',\n'erotic',\n'erotism',\n'escort',\n'ethical slut',\n'eunuch',\n'faggot',\n'fecal',\n'felch',\n'fellatio',\n'feltch',\n'female squirting',\n'femdom',\n'figging',\n'fingering',\n'fisting',\n'foot fetish',\n'footjob',\n'frotting',\n'fuck',\n'fucking',\n'fuck buttons',\n'fudge packer',\n'fudgepacker',\n'futanari',\n'g-spot',\n'gang bang',\n'gay sex',\n'genitals',\n'giant cock',\n'girl on',\n'girl on top',\n'girls gone wild',\n'goatcx',\n'goatse',\n'gokkun',\n'golden shower',\n'goo girl',\n'goodpoop',\n'goregasm',\n'grope',\n'group sex',\n'guro',\n'hand job',\n'handjob',\n'hard core',\n'hardcore',\n'hentai',\n'homoerotic',\n'honkey',\n'hooker',\n'hot chick',\n'how to kill',\n'how to murder',\n'huge fat',\n'humping',\n'incest',\n'intercourse',\n'jack off',\n'jail bait',\n'jailbait',\n'jerk off',\n'jigaboo',\n'jiggaboo',\n'jiggerboo',\n'jizz',\n'juggs',\n'kike',\n'kinbaku',\n'kinkster',\n'kinky',\n'knobbing',\n'leather restraint',\n'leather straight jacket',\n'lemon party',\n'lolita',\n'lovemaking',\n'make me come',\n'male squirting',\n'masturbate',\n'menage a trois',\n'milf',\n'missionary position',\n'motherfucker',\n'mound of venus',\n'mr hands',\n'muff diver',\n'muffdiving',\n'nambla',\n'nawashi',\n'negro',\n'neonazi',\n'nig nog',\n'nigga',\n'nigger',\n'nimphomania',\n'nipple',\n'nipples',\n'nsfw images',\n'nude',\n'nudity',\n'nympho',\n'nymphomania',\n'octopussy',\n'omorashi',\n'one cup two girls',\n'one guy one jar',\n'orgasm',\n'orgy',\n'paedophile',\n'panties',\n'panty',\n'pedobear',\n'pedophile',\n'pegging',\n'penis',\n'phone sex',\n'piece of shit',\n'piss pig',\n'pissing',\n'pisspig',\n'playboy',\n'pleasure chest',\n'pole smoker',\n'ponyplay',\n'poof',\n'poop chute',\n'poopchute',\n'porn',\n'porno',\n'pornography',\n'prince albert piercing',\n'pthc',\n'pubes',\n'pussy',\n'queaf',\n'raghead',\n'raging boner',\n'rape',\n'raping',\n'rapist',\n'rectum',\n'reverse cowgirl',\n'rimjob',\n'rimming',\n'rosy palm',\n'rosy palm and her 5 sisters',\n'rusty trombone',\n's&amp;m',\n'sadism',\n'scat',\n'schlong',\n'scissoring',\n'semen',\n'sex',\n'sexo',\n'sexy',\n'shaved beaver',\n'shaved pussy',\n'shemale',\n'shibari',\n'shit',\n'shota',\n'shrimping',\n'slanteye',\n'slut',\n'smut',\n'snatch',\n'snowballing',\n'sodomize',\n'sodomy',\n'spic',\n'spooge',\n'spread legs',\n'strap on',\n'strapon',\n'strappado',\n'strip club',\n'style doggy',\n'suck',\n'sucks',\n'suicide girls',\n'sultry women',\n'swastika',\n'swinger',\n'tainted love',\n'taste my',\n'tea bagging',\n'threesome',\n'throating',\n'tied up',\n'tight white',\n'tit',\n'tits',\n'titties',\n'titty',\n'tongue in a',\n'topless',\n'tosser',\n'towelhead',\n'tranny',\n'tribadism',\n'tub girl',\n'tubgirl',\n'tushy',\n'twat',\n'twink',\n'twinkie',\n'two girls one cup',\n'undressing',\n'upskirt',\n'urethra play',\n'urophilia',\n'vagina',\n'venus mound',\n'vibrator',\n'violet blue',\n'violet wand',\n'vorarephilia',\n'voyeur',\n'vulva',\n'wank',\n'wet dream',\n'wetback',\n'white power',\n'women rapping',\n'wrapping men',\n'wrinkled starfish',\n'xx',\n'xxx',\n'yaoi',\n'yellow showers',\n'yiffy',\n'zoophilia']\n\ndef profanityFilter(text):\nbrokenStr1 = text.split()\nbadWordMask = '!@#$%!@#$%^~!@%^~@#$%!@#$%^~!'\nnew = ''\nfor word in brokenStr1:\n    if word in arrBad:\n        print word + ' &lt;--Bad word!'\n        text = text.replace(word,badWordMask[:len(word)])\n        #print new\n\nreturn text\n\nprint profanityFilter(\"this thing sucks sucks sucks fucking stuff\")\n</code></pre>\n<p>You can add or remove from the bad words list,arrBad, as you please.</p>\n", "abstract": "You can add or remove from the bad words list,arrBad, as you please."}, {"id": 13125387, "score": 5, "vote": 0, "content": "<p>WebPurify is a Profanity Filter Library for Python</p>\n", "abstract": "WebPurify is a Profanity Filter Library for Python"}, {"id": 3878301, "score": 4, "vote": 0, "content": "<p>You could probably combine <a href=\"http://spambayes.sourceforge.net/\" rel=\"nofollow\">http://spambayes.sourceforge.net/</a> and <a href=\"http://www.cs.cmu.edu/~biglou/resources/bad-words.txt\" rel=\"nofollow\">http://www.cs.cmu.edu/~biglou/resources/bad-words.txt</a>.</p>\n", "abstract": "You could probably combine http://spambayes.sourceforge.net/ and http://www.cs.cmu.edu/~biglou/resources/bad-words.txt."}, {"id": 3531913, "score": 1, "vote": 0, "content": "<p>Profanity? What the f***'s that? ;-)</p>\n<p>It will still take a couple of years before a computer will really be able to recognize swearing and cursing and it is my sincere hope that people will have understood by then that profanity is human and not \"dangerous.\"</p>\n<p>Instead of a dumb filter, have a smart human moderator who can balance the tone of discussion as appropriate. A moderator who can detect abuse like:</p>\n<p>\"If you were my husband, I'd poison your tea.\" - \"If you were my wife, I'd drink it.\"</p>\n<p>(that was from Winston Churchill, btw.)</p>\n", "abstract": "Profanity? What the f***'s that? ;-) It will still take a couple of years before a computer will really be able to recognize swearing and cursing and it is my sincere hope that people will have understood by then that profanity is human and not \"dangerous.\" Instead of a dumb filter, have a smart human moderator who can balance the tone of discussion as appropriate. A moderator who can detect abuse like: \"If you were my husband, I'd poison your tea.\" - \"If you were my wife, I'd drink it.\" (that was from Winston Churchill, btw.)"}, {"id": 3878279, "score": 0, "vote": 0, "content": "<p>It's possible for users to work around this, of course, but it should do a fairly thorough job of removing profanity:</p>\n<pre><code class=\"python\">import re\ndef remove_profanity(s):\n    def repl(word):\n        m = re.match(r\"(\\w+)(.*)\", word)\n        if not m:\n            return word\n        word = \"Bork\" if m.group(1)[0].isupper() else \"bork\"\n        word += m.group(2)\n        return word\n    return \" \".join([repl(w) for w in s.split(\" \")])\n\nprint remove_profanity(\"You just come along with me and have a good time. The Galaxy's a fun place. You'll need to have this fish in your ear.\")\n</code></pre>\n", "abstract": "It's possible for users to work around this, of course, but it should do a fairly thorough job of removing profanity:"}]}, {"link": "https://stackoverflow.com/questions/43510778/python-how-to-intuit-word-from-abbreviated-text-using-nlp", "question": {"id": "43510778", "title": "Python - How to intuit word from abbreviated text using NLP?", "content": "<p>I was recently working on a data set that used abbreviations for various words. For example,</p>\n<pre><code class=\"python\">wtrbtl = water bottle\nbwlingbl = bowling ball\nbsktball = basketball\n</code></pre>\n<p>There did not seem to be any consistency in terms of the convention used, i.e. sometimes they used vowels sometimes not. I am trying to build a mapping object like the one above for abbreviations and their corresponding words without a complete corpus or comprehensive list of terms (i.e. abbreviations could be introduced that are not explicitly known). For simplicity sake say it is restricted to stuff you would find in a gym but it could be anything.</p>\n<p>Basically, if you only look at the left hand side of the examples, what kind of model could do the same processing as our brain in terms of relating each abbreviation to the corresponding full text label. </p>\n<p>My ideas have stopped at taking the first and last letter and finding those in a dictionary. Then assign a priori probabilities based on context. But since there are a large number of morphemes without a marker that indicates end of word I don't see how its possible to split them. </p>\n<p>UPDATED: </p>\n<p>I also had the idea to combine a couple string metric algorithms like a Match Rating Algorithm to determine a set of related terms and then calculate the Levenshtein Distance between each word in the set to the target abbreviation. However, I am still in the dark when it comes to abbreviations for words not in a master dictionary. Basically, inferring word construction - may a Naive Bayes model could help but I am concerned that any error in precision caused by using the algorithms above will invalid any model training process. </p>\n<p>Any help is appreciated, as I am really stuck on this one.</p>\n", "abstract": "I was recently working on a data set that used abbreviations for various words. For example, There did not seem to be any consistency in terms of the convention used, i.e. sometimes they used vowels sometimes not. I am trying to build a mapping object like the one above for abbreviations and their corresponding words without a complete corpus or comprehensive list of terms (i.e. abbreviations could be introduced that are not explicitly known). For simplicity sake say it is restricted to stuff you would find in a gym but it could be anything. Basically, if you only look at the left hand side of the examples, what kind of model could do the same processing as our brain in terms of relating each abbreviation to the corresponding full text label.  My ideas have stopped at taking the first and last letter and finding those in a dictionary. Then assign a priori probabilities based on context. But since there are a large number of morphemes without a marker that indicates end of word I don't see how its possible to split them.  UPDATED:  I also had the idea to combine a couple string metric algorithms like a Match Rating Algorithm to determine a set of related terms and then calculate the Levenshtein Distance between each word in the set to the target abbreviation. However, I am still in the dark when it comes to abbreviations for words not in a master dictionary. Basically, inferring word construction - may a Naive Bayes model could help but I am concerned that any error in precision caused by using the algorithms above will invalid any model training process.  Any help is appreciated, as I am really stuck on this one."}, "answers": [{"id": 48162961, "score": 30, "vote": 0, "content": "<p>If you cannot find an exhaustive dictionary, you could build (or download) a probabilistic language model, to generate and evaluate sentence candidates for you. It could be a character n-gram model or a neural network. </p>\n<p>For your abbreviations, you can build a \"noise model\" which predicts probability of character omissions. It can learn from a corpus (you have to label it manually or half-manually) that consonants are missed less frequently than vowels.</p>\n<p>Having a complex language model and a simple noise model, you can combine them using <strong>noisy channel</strong> approach (see e.g. <a href=\"http://web.stanford.edu/~jurafsky/slp3/5.pdf\" rel=\"noreferrer\">the article by Jurafsky</a> for more details), to suggest candidate sentences.</p>\n<p><strong>Update</strong>. I got enthusiastic about this problem and implemented this algorithm:</p>\n<ul>\n<li>language model (character 5-gram trained on the Lord of the Rings text)</li>\n<li>noise model (probability of each symbol being abbreviated)</li>\n<li>beam search algorithm, for candidate phrase suggestion.</li>\n</ul>\n<p>My solution is implemented <a href=\"https://github.com/avidale/weirdMath/blob/master/nlp/abbreviation_spellchecker_Frodo.ipynb\" rel=\"noreferrer\">in this Python notebook</a>. With trained models, it has interface like <code>noisy_channel('bsktball', language_model, error_model)</code>, which, by the way, returns <code>\n{'basket ball': 33.5, 'basket bally': 36.0}</code>. Dictionary values are scores of the suggestions (the lower, the better).</p>\n<p>With other examples it works worse: for 'wtrbtl' it returns </p>\n<pre><code class=\"python\">{'water but all': 23.7, \n 'water but ill': 24.5,\n 'water but lay': 24.8,\n 'water but let': 26.0,\n 'water but lie': 25.9,\n 'water but look': 26.6}\n</code></pre>\n<p>For 'bwlingbl' it gives </p>\n<pre><code class=\"python\">{'bwling belia': 32.3,\n 'bwling bell': 33.6,\n 'bwling below': 32.1,\n 'bwling belt': 32.5,\n 'bwling black': 31.4,\n 'bwling bling': 32.9,\n 'bwling blow': 32.7,\n 'bwling blue': 30.7}\n</code></pre>\n<p>However, when training on an appropriate corpus (e.g. sports magazines and blogs; maybe with oversampling of nouns), and maybe with more generous width of beam search, this model will provide more relevant suggestions.</p>\n", "abstract": "If you cannot find an exhaustive dictionary, you could build (or download) a probabilistic language model, to generate and evaluate sentence candidates for you. It could be a character n-gram model or a neural network.  For your abbreviations, you can build a \"noise model\" which predicts probability of character omissions. It can learn from a corpus (you have to label it manually or half-manually) that consonants are missed less frequently than vowels. Having a complex language model and a simple noise model, you can combine them using noisy channel approach (see e.g. the article by Jurafsky for more details), to suggest candidate sentences. Update. I got enthusiastic about this problem and implemented this algorithm: My solution is implemented in this Python notebook. With trained models, it has interface like noisy_channel('bsktball', language_model, error_model), which, by the way, returns \n{'basket ball': 33.5, 'basket bally': 36.0}. Dictionary values are scores of the suggestions (the lower, the better). With other examples it works worse: for 'wtrbtl' it returns  For 'bwlingbl' it gives  However, when training on an appropriate corpus (e.g. sports magazines and blogs; maybe with oversampling of nouns), and maybe with more generous width of beam search, this model will provide more relevant suggestions."}, {"id": 43513254, "score": 16, "vote": 0, "content": "<p>So I've looked at a similar problem, and came across a fantastic package called <a href=\"http://pythonhosted.org/pyenchant/\" rel=\"noreferrer\">PyEnchant</a>. If you use the build in spell-checker you can get word suggestions, which would be a nice and simple solution. <strong>However</strong> it will only suggest single words (as far as I can tell), and so the situation you have:</p>\n<pre><code class=\"python\">wtrbtl = water bottle\n</code></pre>\n<p>Will not work.</p>\n<p>Here is some code:</p>\n<pre><code class=\"python\">import enchant\n\nwordDict = enchant.Dict(\"en_US\")\n\ninputWords = ['wtrbtl','bwlingbl','bsktball']\nfor word in inputWords:\n    print wordDict.suggest(word)\n</code></pre>\n<p>The output is:</p>\n<pre><code class=\"python\">['rebuttal', 'tribute']\n['bowling', 'blinding', 'blinking', 'bumbling', 'alienable', 'Nibelung']\n['basketball', 'fastball', 'spitball', 'softball', 'executable', 'basketry']\n</code></pre>\n<p>Perhaps if you know what sort of abbreviations there are you can separate the string into two words, e.g.</p>\n<pre><code class=\"python\">'wtrbtl' -&gt; ['wtr', 'btl']\n</code></pre>\n<p>There's also the Natural Language Processing Kit (<a href=\"http://www.nltk.org/\" rel=\"noreferrer\">NLTK</a>), which is AMAZING, and you could use this in combination with the above code by looking at how common each suggested word is, for example.</p>\n<p>Good luck!</p>\n", "abstract": "So I've looked at a similar problem, and came across a fantastic package called PyEnchant. If you use the build in spell-checker you can get word suggestions, which would be a nice and simple solution. However it will only suggest single words (as far as I can tell), and so the situation you have: Will not work. Here is some code: The output is: Perhaps if you know what sort of abbreviations there are you can separate the string into two words, e.g. There's also the Natural Language Processing Kit (NLTK), which is AMAZING, and you could use this in combination with the above code by looking at how common each suggested word is, for example. Good luck!"}, {"id": 48143039, "score": 10, "vote": 0, "content": "<p>One option is to go back in time and compute the <a href=\"https://en.wikipedia.org/wiki/Soundex\" rel=\"noreferrer\">Soundex Algorithm</a> equivalent.</p>\n<p>Soundex drops all the vowels, handles common mispronunciations and crunched up spellings.  The algorithm is simplistic and used to be done by hand. The downside is that has no special word stemming or stop work support.</p>\n", "abstract": "One option is to go back in time and compute the Soundex Algorithm equivalent. Soundex drops all the vowels, handles common mispronunciations and crunched up spellings.  The algorithm is simplistic and used to be done by hand. The downside is that has no special word stemming or stop work support."}, {"id": 48162471, "score": 4, "vote": 0, "content": "<blockquote>\n<p>... abbreviations for words not in a master dictionary.</p>\n</blockquote>\n<p>So, you're looking for a NLP model that can come up with valid English words, without having seen them before?</p>\n<p>It is probably easier to find a more exhaustive word dictionary, or perhaps to map each word in the existing dictionary to common extensions such as <code>+\"es\"</code> or <code>word[:-1] + \"ies\"</code>.</p>\n", "abstract": "... abbreviations for words not in a master dictionary. So, you're looking for a NLP model that can come up with valid English words, without having seen them before? It is probably easier to find a more exhaustive word dictionary, or perhaps to map each word in the existing dictionary to common extensions such as +\"es\" or word[:-1] + \"ies\"."}]}, {"link": "https://stackoverflow.com/questions/3113428/classifying-documents-into-categories", "question": {"id": "3113428", "title": "Classifying Documents into Categories", "content": "<p>I've got about 300k documents stored in a Postgres database that are tagged with topic categories (there are about 150 categories in total).  I have another 150k documents that don't yet have categories.  I'm trying to find the best way to programmaticly categorize them.</p>\n<p>I've been exploring <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">NLTK</a> and its Naive Bayes Classifier.  Seems like a good starting point (if you can suggest a better classification algorithm for this task, I'm all ears).</p>\n<p>My problem is that I don't have enough RAM to train the NaiveBayesClassifier on all 150 categoies/300k documents at once (training on 5 categories used 8GB).  Furthermore, accuracy of the classifier seems to drop as I train on more categories (90% accuracy with 2 categories, 81% with 5, 61% with 10).</p>\n<p>Should I just train a classifier on 5 categories at a time, and run all 150k documents through the classifier to see if there are matches?  It seems like this would work, except that there would be a lot of false positives where documents that don't really match any of the categories get shoe-horned into on by the classifier just because it's the best match available...  Is there a way to have a \"none of the above\" option for the classifier just in case the document doesn't fit into any of the categories?</p>\n<p>Here is my test class <a href=\"http://gist.github.com/451880\" rel=\"noreferrer\">http://gist.github.com/451880</a></p>\n", "abstract": "I've got about 300k documents stored in a Postgres database that are tagged with topic categories (there are about 150 categories in total).  I have another 150k documents that don't yet have categories.  I'm trying to find the best way to programmaticly categorize them. I've been exploring NLTK and its Naive Bayes Classifier.  Seems like a good starting point (if you can suggest a better classification algorithm for this task, I'm all ears). My problem is that I don't have enough RAM to train the NaiveBayesClassifier on all 150 categoies/300k documents at once (training on 5 categories used 8GB).  Furthermore, accuracy of the classifier seems to drop as I train on more categories (90% accuracy with 2 categories, 81% with 5, 61% with 10). Should I just train a classifier on 5 categories at a time, and run all 150k documents through the classifier to see if there are matches?  It seems like this would work, except that there would be a lot of false positives where documents that don't really match any of the categories get shoe-horned into on by the classifier just because it's the best match available...  Is there a way to have a \"none of the above\" option for the classifier just in case the document doesn't fit into any of the categories? Here is my test class http://gist.github.com/451880"}, "answers": [{"id": 3114191, "score": 33, "vote": 0, "content": "<p>You should start by converting your documents into <a href=\"http://en.wikipedia.org/wiki/Vector_space_model\" rel=\"noreferrer\">TF-log(1 + IDF) vectors</a>: term frequencies are sparse so you should use python dict with term as keys and count as values and then divide by total count to get the global frequencies.</p>\n<p>Another solution is to use the abs(hash(term)) for instance as positive integer keys. Then you an use scipy.sparse vectors which are more handy and more efficient to perform linear algebra operation than python dict.</p>\n<p>Also build the 150 frequencies vectors by averaging the frequencies of all the labeled documents belonging to the same category. Then for new document to label, you can compute the <a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">cosine similarity</a>  between the document vector and each category vector and choose the most similar category as label for your document.</p>\n<p>If this is not good enough, then you should try to train a logistic regression model using a L1 penalty as explained in <a href=\"http://github.com/ogrisel/scikit-learn/blob/master/examples/plot_logistic_l1_l2_coef.py\" rel=\"noreferrer\">this example</a> of <a href=\"http://scikit-learn.org/\" rel=\"noreferrer\">scikit-learn</a> (this is a wrapper for liblinear as explained by @ephes). The vectors used to train your logistic regression model should be the previously introduced TD-log(1+IDF) vectors to get good performance (precision and recall). The scikit learn lib offers a sklearn.metrics module with routines to compute those score for a given model and given dataset.</p>\n<p>For larger datasets: you should try the <a href=\"http://github.com/JohnLangford/vowpal_wabbit\" rel=\"noreferrer\">vowpal wabbit</a> which is probably the fastest rabbit on earth for large scale document classification problems (but not easy to use python wrappers AFAIK).</p>\n", "abstract": "You should start by converting your documents into TF-log(1 + IDF) vectors: term frequencies are sparse so you should use python dict with term as keys and count as values and then divide by total count to get the global frequencies. Another solution is to use the abs(hash(term)) for instance as positive integer keys. Then you an use scipy.sparse vectors which are more handy and more efficient to perform linear algebra operation than python dict. Also build the 150 frequencies vectors by averaging the frequencies of all the labeled documents belonging to the same category. Then for new document to label, you can compute the cosine similarity  between the document vector and each category vector and choose the most similar category as label for your document. If this is not good enough, then you should try to train a logistic regression model using a L1 penalty as explained in this example of scikit-learn (this is a wrapper for liblinear as explained by @ephes). The vectors used to train your logistic regression model should be the previously introduced TD-log(1+IDF) vectors to get good performance (precision and recall). The scikit learn lib offers a sklearn.metrics module with routines to compute those score for a given model and given dataset. For larger datasets: you should try the vowpal wabbit which is probably the fastest rabbit on earth for large scale document classification problems (but not easy to use python wrappers AFAIK)."}, {"id": 3113737, "score": 11, "vote": 0, "content": "<p>How big (number of words) are your documents? Memory consumption at 150K trainingdocs should not be an issue.</p>\n<p>Naive Bayes is a good choice especially when you have many categories with only a few training examples or very noisy trainingdata. But in general, linear Support Vector Machines do perform much better.</p>\n<p>Is your problem multiclass (a document belongs only to one category exclusivly) or multilabel (a document belongs to one or more categories)? </p>\n<p>Accuracy is a poor choice to judge classifier performance. You should rather use precision vs recall, precision recall breakeven point (prbp), f1, auc and have to look at the precision vs recall curve where recall (x) is plotted against precision (y) based on the value of your confidence-threshold (wether a document belongs to a category or not). Usually you would build one binary classifier per category (positive training examples of one category vs all other trainingexamples which don't belong to your current category). You'll have to choose an optimal confidence threshold per category. If you want to combine those single measures per category into a global performance measure, you'll have to micro (sum up all true positives, false positives, false negatives and true negatives and calc combined scores) or macro (calc score per category and then average those scores over all categories) average.</p>\n<p>We have a corpus of tens of million documents, millions of training examples and thousands of categories (multilabel). Since we face serious training time problems (the number of documents are new, updated or deleted per day is quite high), we use a modified version of <a href=\"http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" rel=\"noreferrer\">liblinear</a>. But for smaller problems using one of the python wrappers around liblinear (<a href=\"http://www.procoders.net/?p=287\" rel=\"noreferrer\">liblinear2scipy</a> or <a href=\"http://scikit-learn.sourceforge.net/\" rel=\"noreferrer\">scikit-learn</a>) should work fine.</p>\n", "abstract": "How big (number of words) are your documents? Memory consumption at 150K trainingdocs should not be an issue. Naive Bayes is a good choice especially when you have many categories with only a few training examples or very noisy trainingdata. But in general, linear Support Vector Machines do perform much better. Is your problem multiclass (a document belongs only to one category exclusivly) or multilabel (a document belongs to one or more categories)?  Accuracy is a poor choice to judge classifier performance. You should rather use precision vs recall, precision recall breakeven point (prbp), f1, auc and have to look at the precision vs recall curve where recall (x) is plotted against precision (y) based on the value of your confidence-threshold (wether a document belongs to a category or not). Usually you would build one binary classifier per category (positive training examples of one category vs all other trainingexamples which don't belong to your current category). You'll have to choose an optimal confidence threshold per category. If you want to combine those single measures per category into a global performance measure, you'll have to micro (sum up all true positives, false positives, false negatives and true negatives and calc combined scores) or macro (calc score per category and then average those scores over all categories) average. We have a corpus of tens of million documents, millions of training examples and thousands of categories (multilabel). Since we face serious training time problems (the number of documents are new, updated or deleted per day is quite high), we use a modified version of liblinear. But for smaller problems using one of the python wrappers around liblinear (liblinear2scipy or scikit-learn) should work fine."}, {"id": 3113725, "score": 2, "vote": 0, "content": "<blockquote>\n<p>Is there a way to have a \"none of the\n  above\" option for the classifier just\n  in case the document doesn't fit into\n  any of the categories?</p>\n</blockquote>\n<p>You might get this effect simply by having a \"none of the above\" pseudo-category trained each time.  If the max you can train is 5 categories (though I'm not sure why it's eating up quite so much RAM), train 4 actual categories from their actual 2K docs each, and a \"none of the above\" one with its 2K documents taken randomly from all the other 146 categories (about 13-14 from each if you want the \"stratified sampling\" approach, which may be sounder).</p>\n<p>Still feels like a bit of a kludge and you might be better off with a completely different approach -- find a multi-dimensional doc measure that defines your 300K pre-tagged docs into 150 reasonably separable clusters, then just assign each of the other yet-untagged docs to the appropriate cluster as thus determined.  I don't think NLTK has anything directly available to support this kind of thing, but, hey, NLTK's been growing so fast that I may well have missed something...;-)</p>\n", "abstract": "Is there a way to have a \"none of the\n  above\" option for the classifier just\n  in case the document doesn't fit into\n  any of the categories? You might get this effect simply by having a \"none of the above\" pseudo-category trained each time.  If the max you can train is 5 categories (though I'm not sure why it's eating up quite so much RAM), train 4 actual categories from their actual 2K docs each, and a \"none of the above\" one with its 2K documents taken randomly from all the other 146 categories (about 13-14 from each if you want the \"stratified sampling\" approach, which may be sounder). Still feels like a bit of a kludge and you might be better off with a completely different approach -- find a multi-dimensional doc measure that defines your 300K pre-tagged docs into 150 reasonably separable clusters, then just assign each of the other yet-untagged docs to the appropriate cluster as thus determined.  I don't think NLTK has anything directly available to support this kind of thing, but, hey, NLTK's been growing so fast that I may well have missed something...;-)"}]}, {"link": "https://stackoverflow.com/questions/13423919/computing-n-grams-using-python", "question": {"id": "13423919", "title": "Computing N Grams using Python", "content": "<p>I needed to compute the Unigrams,  BiGrams and Trigrams for a text file containing text like: </p>\n<p>\"Cystic fibrosis affects 30,000 children and young adults in the US alone\nInhaling the mists of salt water can reduce the pus and infection that fills the airways of cystic fibrosis sufferers, although side effects include a nasty coughing fit and a harsh taste. \nThat's the conclusion of two studies published in this week's issue of The New England Journal of Medicine.\"</p>\n<p>I started in Python and used the following code:</p>\n<pre><code class=\"python\">#!/usr/bin/env python\n# File: n-gram.py\ndef N_Gram(N,text):\nNList = []                      # start with an empty list\nif N&gt; 1:\n    space = \" \" * (N-1)         # add N - 1 spaces\n    text = space + text + space # add both in front and back\n# append the slices [i:i+N] to NList\nfor i in range( len(text) - (N - 1) ):\n    NList.append(text[i:i+N])\nreturn NList                    # return the list\n# test code\nfor i in range(5):\nprint N_Gram(i+1,\"text\")\n# more test code\nnList = N_Gram(7,\"Here is a lot of text to print\")\nfor ngram in iter(nList):\nprint '\"' + ngram + '\"'\n</code></pre>\n<p><a href=\"http://www.daniweb.com/software-development/python/threads/39109/generating-n-grams-from-a-word\" rel=\"noreferrer\">http://www.daniweb.com/software-development/python/threads/39109/generating-n-grams-from-a-word</a></p>\n<p>But it works for all the n-grams within a word, when I want it from between words as in CYSTIC and FIBROSIS or CYSTIC FIBROSIS. Can someone help me out as to how I can get this done? </p>\n", "abstract": "I needed to compute the Unigrams,  BiGrams and Trigrams for a text file containing text like:  \"Cystic fibrosis affects 30,000 children and young adults in the US alone\nInhaling the mists of salt water can reduce the pus and infection that fills the airways of cystic fibrosis sufferers, although side effects include a nasty coughing fit and a harsh taste. \nThat's the conclusion of two studies published in this week's issue of The New England Journal of Medicine.\" I started in Python and used the following code: http://www.daniweb.com/software-development/python/threads/39109/generating-n-grams-from-a-word But it works for all the n-grams within a word, when I want it from between words as in CYSTIC and FIBROSIS or CYSTIC FIBROSIS. Can someone help me out as to how I can get this done? "}, "answers": [{"id": 30609050, "score": 49, "vote": 0, "content": "<p>A short Pythonesque solution from this <a href=\"http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\">blog</a>:</p>\n<pre><code class=\"python\">def find_ngrams(input_list, n):\n  return zip(*[input_list[i:] for i in range(n)])\n</code></pre>\n<p>Usage:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n&gt;&gt;&gt; find_ngrams(input_list, 1)\n[('all',), ('this',), ('happened',), ('more',), ('or',), ('less',)]\n&gt;&gt;&gt; find_ngrams(input_list, 2)\n[('all', 'this'), ('this', 'happened'), ('happened', 'more'), ('more', 'or'), ('or', 'less')]\n&gt;&gt;&gt; find_ngrams(input_list, 3))\n[('all', 'this', 'happened'), ('this', 'happened', 'more'), ('happened', 'more', 'or'), ('more', 'or', 'less')]\n</code></pre>\n", "abstract": "A short Pythonesque solution from this blog: Usage:"}, {"id": 13424002, "score": 41, "vote": 0, "content": "<p>Assuming input is a string contains space separated words, like <code>x = \"a b c d\"</code> you can use the following function (edit: see the last function for a possibly more complete solution):</p>\n<pre><code class=\"python\">def ngrams(input, n):\n    input = input.split(' ')\n    output = []\n    for i in range(len(input)-n+1):\n        output.append(input[i:i+n])\n    return output\n\nngrams('a b c d', 2) # [['a', 'b'], ['b', 'c'], ['c', 'd']]\n</code></pre>\n<p>If you want those joined back into strings, you might call something like:</p>\n<pre><code class=\"python\">[' '.join(x) for x in ngrams('a b c d', 2)] # ['a b', 'b c', 'c d']\n</code></pre>\n<p>Lastly, that doesn't summarize things into totals, so if your input was <code>'a a a a'</code>, you need to count them up into a dict:</p>\n<pre><code class=\"python\">for g in (' '.join(x) for x in ngrams(input, 2)):\n    grams.setdefault(g, 0)\n    grams[g] += 1\n</code></pre>\n<p>Putting that all together into one final function gives:</p>\n<pre><code class=\"python\">def ngrams(input, n):\n   input = input.split(' ')\n   output = {}\n   for i in range(len(input)-n+1):\n       g = ' '.join(input[i:i+n])\n       output.setdefault(g, 0)\n       output[g] += 1\n   return output\n\nngrams('a a a a', 2) # {'a a': 3}\n</code></pre>\n", "abstract": "Assuming input is a string contains space separated words, like x = \"a b c d\" you can use the following function (edit: see the last function for a possibly more complete solution): If you want those joined back into strings, you might call something like: Lastly, that doesn't summarize things into totals, so if your input was 'a a a a', you need to count them up into a dict: Putting that all together into one final function gives:"}, {"id": 13431956, "score": 27, "vote": 0, "content": "<p>Use NLTK (the Natural Language Toolkit) and use the functions to tokenize (split) your text into a list and then find bigrams and trigrams.</p>\n<pre><code class=\"python\">import nltk\nwords = nltk.word_tokenize(my_text)\nmy_bigrams = nltk.bigrams(words)\nmy_trigrams = nltk.trigrams(words)\n</code></pre>\n", "abstract": "Use NLTK (the Natural Language Toolkit) and use the functions to tokenize (split) your text into a list and then find bigrams and trigrams."}, {"id": 26655378, "score": 11, "vote": 0, "content": "<p>There is one more interesting module into python called Scikit. Here is the code. This will help u to get all the grams given in a particular range. Here is the code</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import CountVectorizer \ntext = \"this is a foo bar sentences and i want to ngramize it\"\nvectorizer = CountVectorizer(ngram_range=(1,6))\nanalyzer = vectorizer.build_analyzer()\nprint analyzer(text)\n</code></pre>\n<p>Output is</p>\n<pre><code class=\"python\">[u'this', u'is', u'foo', u'bar', u'sentences', u'and', u'want', u'to', u'ngramize', u'it', u'this is', u'is foo', u'foo bar', u'bar sentences', u'sentences and', u'and want', u'want to', u'to ngramize', u'ngramize it', u'this is foo', u'is foo bar', u'foo bar sentences', u'bar sentences and', u'sentences and want', u'and want to', u'want to ngramize', u'to ngramize it', u'this is foo bar', u'is foo bar sentences', u'foo bar sentences and', u'bar sentences and want', u'sentences and want to', u'and want to ngramize', u'want to ngramize it', u'this is foo bar sentences', u'is foo bar sentences and', u'foo bar sentences and want', u'bar sentences and want to', u'sentences and want to ngramize', u'and want to ngramize it', u'this is foo bar sentences and', u'is foo bar sentences and want', u'foo bar sentences and want to', u'bar sentences and want to ngramize', u'sentences and want to ngramize it']\n</code></pre>\n<p>Here it gives all the grams given in a range 1 to 6. Its using the method called countVectorizer. Here is the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" rel=\"noreferrer\">link</a> for that.</p>\n", "abstract": "There is one more interesting module into python called Scikit. Here is the code. This will help u to get all the grams given in a particular range. Here is the code Output is Here it gives all the grams given in a range 1 to 6. Its using the method called countVectorizer. Here is the link for that."}, {"id": 13424088, "score": 3, "vote": 0, "content": "<p>Using <code>collections.deque</code>:</p>\n<pre><code class=\"python\">from collections import deque\nfrom itertools import islice\n\ndef ngrams(message, n=1):\n    it = iter(message.split())\n    window = deque(islice(it, n), maxlen=n)\n    yield tuple(window)\n    for item in it:\n        window.append(item)\n        yield tuple(window)\n</code></pre>\n<p>...or maybe you could do it in one line as a list comprehension:</p>\n<pre><code class=\"python\">n = 2\nmessage = \"Hello, how are you?\".split()\nmyNgrams = [message[i:i+n] for i in range(len(message) - n + 1)]\n</code></pre>\n", "abstract": "Using collections.deque: ...or maybe you could do it in one line as a list comprehension:"}, {"id": 39758695, "score": 2, "vote": 0, "content": "<p>nltk has native support for ngrams</p>\n<p>'n' is the ngram size ex: n=3 is for a trigram</p>\n<pre><code class=\"python\">from nltk import ngrams\n\ndef ngramize(texts, n):\n    output=[]\n    for text in texts:\n        output += ngrams(text,n)\n    return output\n</code></pre>\n", "abstract": "nltk has native support for ngrams 'n' is the ngram size ex: n=3 is for a trigram"}, {"id": 48315959, "score": 2, "vote": 0, "content": "<p>If efficiency is an issue and you have to build multiple different n-grams I would consider using the following code (building up on Franck's excellent answer):</p>\n<pre><code class=\"python\">from itertools import chain\n\ndef n_grams(seq, n=1):\n    \"\"\"Returns an iterator over the n-grams given a list_tokens\"\"\"\n    shift_token = lambda i: (el for j,el in enumerate(seq) if j&gt;=i)\n    shifted_tokens = (shift_token(i) for i in range(n))\n    tuple_ngrams = zip(*shifted_tokens)\n    return tuple_ngrams # if join in generator : (\" \".join(i) for i in tuple_ngrams)\n\ndef range_ngrams(list_tokens, ngram_range=(1,2)):\n    \"\"\"Returns an itirator over all n-grams for n in range(ngram_range) given a list_tokens.\"\"\"\n    return chain(*(n_grams(list_tokens, i) for i in range(*ngram_range)))\n</code></pre>\n<p>Usage : </p>\n<pre><code class=\"python\">&gt;&gt;&gt; input_list = input_list = 'test the ngrams generator'.split()\n&gt;&gt;&gt; list(range_ngrams(input_list, ngram_range=(1,3)))\n[('test',), ('the',), ('ngrams',), ('generator',), ('test', 'the'), ('the', 'ngrams'), ('ngrams', 'generator'), ('test', 'the', 'ngrams'), ('the', 'ngrams', 'generator')]\n</code></pre>\n<p>~Same speed as NLTK:</p>\n<pre><code class=\"python\">import nltk\n%%timeit\ninput_list = 'test the ngrams interator vs nltk '*10**6\nnltk.ngrams(input_list,n=5)\n# 7.02 ms \u00b1 79 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%%timeit\ninput_list = 'test the ngrams interator vs nltk '*10**6\nn_grams(input_list,n=5)\n# 7.01 ms \u00b1 103 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%%timeit\ninput_list = 'test the ngrams interator vs nltk '*10**6\nnltk.ngrams(input_list,n=1)\nnltk.ngrams(input_list,n=2)\nnltk.ngrams(input_list,n=3)\nnltk.ngrams(input_list,n=4)\nnltk.ngrams(input_list,n=5)\n# 7.32 ms \u00b1 241 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%%timeit\ninput_list = 'test the ngrams interator vs nltk '*10**6\nrange_ngrams(input_list, ngram_range=(1,6))\n# 7.13 ms \u00b1 165 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n", "abstract": "If efficiency is an issue and you have to build multiple different n-grams I would consider using the following code (building up on Franck's excellent answer): Usage :  ~Same speed as NLTK:"}, {"id": 46323015, "score": 1, "vote": 0, "content": "<p>Though the post is old, I thought to mention my answer here so that most of the ngrams creation logic can be in one post.</p>\n<p>There is something by name TextBlob in Python. It creates ngrams very easily similar to NLTK.</p>\n<p>Below is the code snippet with its output for easy understanding.</p>\n<pre><code class=\"python\">sent = \"\"\"This is to show the usage of Text Blob in Python\"\"\"\nblob = TextBlob(sent)\nunigrams = blob.ngrams(n=1)\nbigrams = blob.ngrams(n=2)\ntrigrams = blob.ngrams(n=3)\n</code></pre>\n<p>And the output is :</p>\n<pre><code class=\"python\">unigrams\n[WordList(['This']),\n WordList(['is']),\n WordList(['to']),\n WordList(['show']),\n WordList(['the']),\n WordList(['usage']),\n WordList(['of']),\n WordList(['Text']),\n WordList(['Blob']),\n WordList(['in']),\n WordList(['Python'])]\n\nbigrams\n[WordList(['This', 'is']),\n WordList(['is', 'to']),\n WordList(['to', 'show']),\n WordList(['show', 'the']),\n WordList(['the', 'usage']),\n WordList(['usage', 'of']),\n WordList(['of', 'Text']),\n WordList(['Text', 'Blob']),\n WordList(['Blob', 'in']),\n WordList(['in', 'Python'])]\n\ntrigrams\n[WordList(['This', 'is', 'to']),\n WordList(['is', 'to', 'show']),\n WordList(['to', 'show', 'the']),\n WordList(['show', 'the', 'usage']),\n WordList(['the', 'usage', 'of']),\n WordList(['usage', 'of', 'Text']),\n WordList(['of', 'Text', 'Blob']),\n WordList(['Text', 'Blob', 'in']),\n WordList(['Blob', 'in', 'Python'])]\n</code></pre>\n<p>As simple as that. </p>\n<p>There is more to this that are being done by TextBlob. Please have a look at this doc for more details - <a href=\"https://textblob.readthedocs.io/en/dev/\" rel=\"nofollow noreferrer\">https://textblob.readthedocs.io/en/dev/</a></p>\n", "abstract": "Though the post is old, I thought to mention my answer here so that most of the ngrams creation logic can be in one post. There is something by name TextBlob in Python. It creates ngrams very easily similar to NLTK. Below is the code snippet with its output for easy understanding. And the output is : As simple as that.  There is more to this that are being done by TextBlob. Please have a look at this doc for more details - https://textblob.readthedocs.io/en/dev/"}]}, {"link": "https://stackoverflow.com/questions/40011896/nltk-vs-stanford-nlp", "question": {"id": "40011896", "title": "NLTK vs Stanford NLP", "content": "<p>I have recently started to use NLTK toolkit for creating few solutions using Python.</p>\n<p>I hear a lot of community activity regarding using Stanford NLP.\nCan anyone tell me the difference between NLTK and Stanford NLP? Are they two different libraries? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail.</p>\n<p>Can Stanford NLP be used using Python?</p>\n", "abstract": "I have recently started to use NLTK toolkit for creating few solutions using Python. I hear a lot of community activity regarding using Stanford NLP.\nCan anyone tell me the difference between NLTK and Stanford NLP? Are they two different libraries? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail. Can Stanford NLP be used using Python?"}, "answers": [{"id": 40028128, "score": 38, "vote": 0, "content": "<blockquote>\n<p>Can anyone tell me what is the difference between NLTK and Stanford NLP? Are they 2 different libraries ? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail.</p>\n</blockquote>\n<p>(I'm assuming you mean \"<a href=\"http://stanfordnlp.github.io/CoreNLP/\" rel=\"noreferrer\">Stanford CoreNLP</a>\".)</p>\n<p>They are two different libraries.</p>\n<ul>\n<li><strong>Stanford CoreNLP</strong> is written in Java</li>\n<li><strong>NLTK</strong> is a Python library</li>\n</ul>\n<p>The main functional difference is that NLTK has multiple versions or interfaces to other versions of NLP tools, while Stanford CoreNLP only has their version. NLTK also supports installing third-party Java projects, and even includes <a href=\"https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#stanford-tagger-ner-tokenizer-and-parser\" rel=\"noreferrer\">instructions for installing some Stanford NLP packages on the wiki</a>.</p>\n<p>Both have good support for English, but if you are dealing with other languages:</p>\n<ul>\n<li><strong>Stanford CoreNLP</strong> comes with <a href=\"http://stanfordnlp.github.io/CoreNLP/human-languages.html\" rel=\"noreferrer\">models for English, Chinese, French, German, Spanish, and Arabic</a>.</li>\n<li><strong>NLTK</strong> comes with <a href=\"http://www.nltk.org/nltk_data/\" rel=\"noreferrer\">corpora in additional languages like Portugese, Russian, and Polish</a>. Individual tools may support even more languages (e.g. no Danish corpora, but has a <a href=\"http://www.nltk.org/api/nltk.stem.html#nltk.stem.snowball.DanishStemmer\" rel=\"noreferrer\">DanishStemmer</a>).</li>\n</ul>\n<p>That said, which one is \"best\" will depend on your specific application and required performance (what features you are using, language, vocabulary, desired speed, etc.).</p>\n<blockquote>\n<p>Can Stanford NLP be used using Python?</p>\n</blockquote>\n<p><a href=\"http://stanfordnlp.github.io/CoreNLP/other-languages.html#python\" rel=\"noreferrer\">Yes, there are a number of interfaces and packages for using Stanford CoreNLP in Python</a>  (independent of NLTK).</p>\n", "abstract": "Can anyone tell me what is the difference between NLTK and Stanford NLP? Are they 2 different libraries ? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail. (I'm assuming you mean \"Stanford CoreNLP\".) They are two different libraries. The main functional difference is that NLTK has multiple versions or interfaces to other versions of NLP tools, while Stanford CoreNLP only has their version. NLTK also supports installing third-party Java projects, and even includes instructions for installing some Stanford NLP packages on the wiki. Both have good support for English, but if you are dealing with other languages: That said, which one is \"best\" will depend on your specific application and required performance (what features you are using, language, vocabulary, desired speed, etc.). Can Stanford NLP be used using Python? Yes, there are a number of interfaces and packages for using Stanford CoreNLP in Python  (independent of NLTK)."}, {"id": 51023058, "score": 11, "vote": 0, "content": "<p>The choice will depend upon your use case. <strong>NLTK</strong> is great for <strong>pre-processing</strong> and <strong>tokenizing</strong> text. It also includes a good <strong>POS</strong> tagger. <strong>Standford Core NLP</strong> for only tokenizing/POS tagging is a bit of overkill, because Standford NLP requires more resources.<br/>\n But one fundamental difference is, you can't parse <strong>syntactic dependencies</strong> out of the box with NLTK. You need to specify a Grammar for that which can be very tedious if the text domain is not restricted. Whereas Standford NLP provides a probabilistic parser for general text as a down-loadable model, which is quite accurate. It also has built in NER (Named Entity Recognition) and more. Also I will recomend to take a look at <a href=\"https://spacy.io/\" rel=\"noreferrer\">Spacy</a>, which is written in python, easy to use and much <a href=\"https://spacy.io/usage/facts-figures#benchmarks\" rel=\"noreferrer\">faster</a> than CoreNLP.  </p>\n", "abstract": "The choice will depend upon your use case. NLTK is great for pre-processing and tokenizing text. It also includes a good POS tagger. Standford Core NLP for only tokenizing/POS tagging is a bit of overkill, because Standford NLP requires more resources.\n But one fundamental difference is, you can't parse syntactic dependencies out of the box with NLTK. You need to specify a Grammar for that which can be very tedious if the text domain is not restricted. Whereas Standford NLP provides a probabilistic parser for general text as a down-loadable model, which is quite accurate. It also has built in NER (Named Entity Recognition) and more. Also I will recomend to take a look at Spacy, which is written in python, easy to use and much faster than CoreNLP.  "}, {"id": 57649698, "score": 5, "vote": 0, "content": "<p>It appears that you are new to NLP. </p>\n<blockquote>\n<p><em>I have recently started to use NLTK toolkit</em> </p>\n</blockquote>\n<p>If indeed you are new to NLP, then the best thing would be to start simple. So ideally you would start off with nltk. I am relatively new to natural language processing (a few months old). I can confirm that for beginners, nltk is better, since it has a great and free <a href=\"https://www.nltk.org/book\" rel=\"noreferrer\">online book</a> which helps the beginner learn quickly. </p>\n<p>Once you are comfortable and actually have a problem to solve, look at Stanford Core NLP to see if it will be better at solving your problem. </p>\n<p>If you want to stick to NLTK, you can also access the Stanford CoreNLP API in <a href=\"https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\" rel=\"noreferrer\">NLTK</a>. </p>\n<p>Now for the similarities and differences:</p>\n<blockquote>\n<p>Can anyone tell me what is the difference between NLTK and Stanford\n  NLP ? \n  Are they 2 different libraries?</p>\n</blockquote>\n<p>Both offer natural language processing. Some of the most useful parts of Stanford Core NLP include the part-of-speech tagger, the named entity recognizer, sentiment analysis, and pattern learning. </p>\n<p>The named entity recognizer is better in the Stanford Core NLP. Stanford Core NLP is better at grammatical functions for instance picking up subject, object, predictae (that is partially why I switched from nltk to Stanford Core NLP). As @user812786 said, NLTK has multiple interfaces to other versions of NLP tools. NLTK is also better for learning NLP. If you need to use multiple corpora, use NLTK, as you can easily access a wide multitude of <a href=\"http://www.nltk.org/howto/corpus.html\" rel=\"noreferrer\">text corpora and lexical resources</a>. Both have POS tagging and sentiment analysis. </p>\n<blockquote>\n<p>Can stanford NLP be used using Python ?</p>\n</blockquote>\n<p>Yes absolutely. You can use StanfordNLP which is a Python natural language analysis package that is able to call the CoreNLP Java package. There are also multiple Python packages using the <a href=\"https://stanfordnlp.github.io/CoreNLP/other-languages.html\" rel=\"noreferrer\">Stanford CoreNLP server</a></p>\n", "abstract": "It appears that you are new to NLP.  I have recently started to use NLTK toolkit  If indeed you are new to NLP, then the best thing would be to start simple. So ideally you would start off with nltk. I am relatively new to natural language processing (a few months old). I can confirm that for beginners, nltk is better, since it has a great and free online book which helps the beginner learn quickly.  Once you are comfortable and actually have a problem to solve, look at Stanford Core NLP to see if it will be better at solving your problem.  If you want to stick to NLTK, you can also access the Stanford CoreNLP API in NLTK.  Now for the similarities and differences: Can anyone tell me what is the difference between NLTK and Stanford\n  NLP ? \n  Are they 2 different libraries? Both offer natural language processing. Some of the most useful parts of Stanford Core NLP include the part-of-speech tagger, the named entity recognizer, sentiment analysis, and pattern learning.  The named entity recognizer is better in the Stanford Core NLP. Stanford Core NLP is better at grammatical functions for instance picking up subject, object, predictae (that is partially why I switched from nltk to Stanford Core NLP). As @user812786 said, NLTK has multiple interfaces to other versions of NLP tools. NLTK is also better for learning NLP. If you need to use multiple corpora, use NLTK, as you can easily access a wide multitude of text corpora and lexical resources. Both have POS tagging and sentiment analysis.  Can stanford NLP be used using Python ? Yes absolutely. You can use StanfordNLP which is a Python natural language analysis package that is able to call the CoreNLP Java package. There are also multiple Python packages using the Stanford CoreNLP server"}, {"id": 50968392, "score": 1, "vote": 0, "content": "<p>I would add to this answer that if you are looking to parse date/time events StanfordCoreNLP contains SuTime which is the best datetime parser available. The support for arbitrary texts like 'Next Monday afternoon' is not present in any other package. </p>\n", "abstract": "I would add to this answer that if you are looking to parse date/time events StanfordCoreNLP contains SuTime which is the best datetime parser available. The support for arbitrary texts like 'Next Monday afternoon' is not present in any other package. "}, {"id": 57003384, "score": 1, "vote": 0, "content": "<p>NLTK can be used for the learning phase to and perform natural language process from scratch and basic level.\nStandford NLP gives you high-level flexibility to done task very fast and easiest way.</p>\n<p>If you want fast and production use, can go for Standford NLP.</p>\n", "abstract": "NLTK can be used for the learning phase to and perform natural language process from scratch and basic level.\nStandford NLP gives you high-level flexibility to done task very fast and easiest way. If you want fast and production use, can go for Standford NLP."}, {"id": 61256153, "score": 1, "vote": 0, "content": "<p>In 2020, Stanford released STANZA, Python library based on Stanford NLP.\nYou  can find it here <a href=\"https://stanfordnlp.github.io/stanza/\" rel=\"nofollow noreferrer\">https://stanfordnlp.github.io/stanza/</a></p>\n<p>If you familiar with Spacy NLP, it quite similar :</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import stanza\n&gt;&gt;&gt; stanza.download('en') # download English model\n&gt;&gt;&gt; nlp = stanza.Pipeline('en') # initialize English neural pipeline\n&gt;&gt;&gt; doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence\n</code></pre>\n", "abstract": "In 2020, Stanford released STANZA, Python library based on Stanford NLP.\nYou  can find it here https://stanfordnlp.github.io/stanza/ If you familiar with Spacy NLP, it quite similar :"}, {"id": 60407493, "score": 0, "vote": 0, "content": "<p>Those 2 are different libraries. </p>\n<p>They are written in different languages like <a href=\"https://stanfordnlp.github.io/CoreNLP/\" rel=\"nofollow noreferrer\">Standford CoreNLP</a> is written in Java and <a href=\"https://www.nltk.org/\" rel=\"nofollow noreferrer\">NLTK</a> is written in python, you can check the documentation in the main website, in my point of view <strong>NLTK</strong> is much more useful to be used for <strong>tokenizing</strong> and Data <strong>PRE-PROCESSING</strong>.</p>\n", "abstract": "Those 2 are different libraries.  They are written in different languages like Standford CoreNLP is written in Java and NLTK is written in python, you can check the documentation in the main website, in my point of view NLTK is much more useful to be used for tokenizing and Data PRE-PROCESSING."}]}, {"link": "https://stackoverflow.com/questions/44395656/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing", "question": {"id": "44395656", "title": "Applying Spacy Parser to Pandas DataFrame w/ Multiprocessing", "content": "<p>Say I have a dataset, like</p>\n<pre><code class=\"python\">iris = pd.DataFrame(sns.load_dataset('iris'))\n</code></pre>\n<p>I can use <code>Spacy</code> and <code>.apply</code> to parse a string column into tokens (my real dataset has &gt;1 word/token per entry of course)</p>\n<pre><code class=\"python\">import spacy # (I have version 1.8.2)\nnlp = spacy.load('en')\niris['species_parsed'] = iris['species'].apply(nlp)\n</code></pre>\n<p>result:</p>\n<pre><code class=\"python\">   sepal_length   ... species    species_parsed\n0           1.4   ... setosa          (setosa)\n1           1.4   ... setosa          (setosa)\n2           1.3   ... setosa          (setosa)\n</code></pre>\n<p>I can also use this convenient multiprocessing function (<a href=\"http://www.racketracer.com/2016/07/06/pandas-in-parallel/\" rel=\"noreferrer\">thanks to this blogpost</a>) to do most arbitrary apply functions on a dataframe in parallel:</p>\n<pre><code class=\"python\">from multiprocessing import Pool, cpu_count\ndef parallelize_dataframe(df, func, num_partitions):\n\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_partitions)\n    df = pd.concat(pool.map(func, df_split))\n\n    pool.close()\n    pool.join()\n    return df\n</code></pre>\n<p>for example:</p>\n<pre><code class=\"python\">def my_func(df):\n    df['length_of_word'] = df['species'].apply(lambda x: len(x))\n    return df\n\nnum_cores = cpu_count()\niris = parallelize_dataframe(iris, my_func, num_cores)\n</code></pre>\n<p>result:</p>\n<pre><code class=\"python\">   sepal_length species  length_of_word\n0           5.1  setosa               6\n1           4.9  setosa               6\n2           4.7  setosa               6\n</code></pre>\n<p>...But for some reason, I can't apply the Spacy parser to a dataframe using multiprocessing this way. </p>\n<pre><code class=\"python\">def add_parsed(df):\n    df['species_parsed'] = df['species'].apply(nlp)\n    return df\n\niris = parallelize_dataframe(iris, add_parsed, num_cores)\n</code></pre>\n<p>result:</p>\n<pre><code class=\"python\">   sepal_length species  length_of_word species_parsed\n0           5.1  setosa               6             ()\n1           4.9  setosa               6             ()\n2           4.7  setosa               6             ()\n</code></pre>\n<p>Is there some other way to do this? I'm loving Spacy for NLP but I have a lot of text data and so I'd like to parallelize some processing functions, but ran into this issue.</p>\n", "abstract": "Say I have a dataset, like I can use Spacy and .apply to parse a string column into tokens (my real dataset has >1 word/token per entry of course) result: I can also use this convenient multiprocessing function (thanks to this blogpost) to do most arbitrary apply functions on a dataframe in parallel: for example: result: ...But for some reason, I can't apply the Spacy parser to a dataframe using multiprocessing this way.  result: Is there some other way to do this? I'm loving Spacy for NLP but I have a lot of text data and so I'd like to parallelize some processing functions, but ran into this issue."}, "answers": [{"id": 44764557, "score": 40, "vote": 0, "content": "<p>Spacy is highly optimised and does the multiprocessing for you. As a result, I think your best bet is to take the data out of the Dataframe and pass it to the Spacy pipeline as a list rather than trying to use <code>.apply</code> directly.</p>\n<p>You then need to the collate the results of the parse, and put this back into the Dataframe. </p>\n<p>So, in your example, you could use something like:</p>\n<pre><code class=\"python\">tokens = []\nlemma = []\npos = []\n\nfor doc in nlp.pipe(df['species'].astype('unicode').values, batch_size=50,\n                        n_threads=3):\n    if doc.is_parsed:\n        tokens.append([n.text for n in doc])\n        lemma.append([n.lemma_ for n in doc])\n        pos.append([n.pos_ for n in doc])\n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n        tokens.append(None)\n        lemma.append(None)\n        pos.append(None)\n\ndf['species_tokens'] = tokens\ndf['species_lemma'] = lemma\ndf['species_pos'] = pos\n</code></pre>\n<p>This approach will work fine on small datasets, but it eats up your memory, so not great if you want to process huge amounts of text.</p>\n", "abstract": "Spacy is highly optimised and does the multiprocessing for you. As a result, I think your best bet is to take the data out of the Dataframe and pass it to the Spacy pipeline as a list rather than trying to use .apply directly. You then need to the collate the results of the parse, and put this back into the Dataframe.  So, in your example, you could use something like: This approach will work fine on small datasets, but it eats up your memory, so not great if you want to process huge amounts of text."}]}, {"link": "https://stackoverflow.com/questions/8590370/what-is-nltk-pos-tagger-asking-me-to-download", "question": {"id": "8590370", "title": "What is NLTK POS tagger asking me to download?", "content": "<p>I just started using a part-of-speech tagger, and I am facing many problems. </p>\n<p>I started POS tagging with the following:</p>\n<pre><code class=\"python\">import nltk\ntext=nltk.word_tokenize(\"We are going out.Just you and me.\")\n</code></pre>\n<p>When I want to print <code>'text'</code>, the following happens:</p>\n<pre><code class=\"python\">print nltk.pos_tag(text)\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 63, in pos_tag\ntagger = nltk.data.load(_POS_TAGGER)\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 594, in load\nresource_val = pickle.load(_open(resource_url))\nFile \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 673, in _open\n return find(path).open()\n File \"F:\\Python26\\lib\\site-packages\\nltk\\data.py\", line 455, in find\n   raise LookupError(resource_not_found)`  \nLookupError:\n Resource 'taggers/maxent_treebank_pos_tagger/english.pickle' not\n found.  Please use the NLTK Downloader to obtain the resource:\n\n&gt;&gt;&gt; nltk.download().\n\n Searched in:\n    - 'C:\\\\Documents and Settings\\\\Administrator/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:\\\\Python26\\\\nltk_data'\n    - 'F:\\\\Python26\\\\lib\\\\nltk_data'\n    - 'C:\\\\Documents and Settings\\\\Administrator\\\\Application Data\\\\nltk_data'\n</code></pre>\n<p>I used <code>nltk.download()</code> but it did not work.</p>\n", "abstract": "I just started using a part-of-speech tagger, and I am facing many problems.  I started POS tagging with the following: When I want to print 'text', the following happens: I used nltk.download() but it did not work."}, "answers": [{"id": 37651321, "score": 36, "vote": 0, "content": "<p>From <code>NLTK</code> versions higher than v3.2, please use:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.__version__\n'3.2.1'\n&gt;&gt;&gt; nltk.download('averaged_perceptron_tagger')\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/alvas/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-date!\nTrue\n</code></pre>\n<p>For <code>NLTK</code> versions using the old MaxEnt model, i.e. v3.1 and below, please use:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.download('maxent_treebank_pos_tagger')\n[nltk_data] Downloading package maxent_treebank_pos_tagger to\n[nltk_data]     /home/alvas/nltk_data...\n[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-date!\nTrue\n</code></pre>\n<p>For more details on the change in the default <code>pos_tag</code>, please see <a href=\"https://github.com/nltk/nltk/pull/1143\" rel=\"noreferrer\">https://github.com/nltk/nltk/pull/1143</a> </p>\n", "abstract": "From NLTK versions higher than v3.2, please use: For NLTK versions using the old MaxEnt model, i.e. v3.1 and below, please use: For more details on the change in the default pos_tag, please see https://github.com/nltk/nltk/pull/1143 "}, {"id": 8599513, "score": 31, "vote": 0, "content": "<p>When you type <code>nltk.download()</code> in Python, an NLTK Downloader interface gets displayed automatically.<br/>\nClick on Models and choose maxent_treebank_pos_. It gets installed automatically.  </p>\n<pre><code class=\"python\">import nltk \ntext=nltk.word_tokenize(\"We are going out.Just you and me.\")\nprint nltk.pos_tag(text)\n[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'JJ'),\n ('you', 'PRP'), ('and', 'CC'), ('me', 'PRP'), ('.', '.')]\n</code></pre>\n", "abstract": "When you type nltk.download() in Python, an NLTK Downloader interface gets displayed automatically.\nClick on Models and choose maxent_treebank_pos_. It gets installed automatically.  "}, {"id": 32600664, "score": 5, "vote": 0, "content": "<p>From the shell/terminal, you can use:</p>\n<pre><code class=\"python\">python -m nltk.downloader maxent_treebank_pos_tagger\n</code></pre>\n<p>(might need to be sudo on Linux)</p>\n<p>It will install <code>maxent_treebank_pos_tagger</code> (i.e. the standard treebank POS tagger in NLTK) and fix your issue.</p>\n", "abstract": "From the shell/terminal, you can use: (might need to be sudo on Linux) It will install maxent_treebank_pos_tagger (i.e. the standard treebank POS tagger in NLTK) and fix your issue."}, {"id": 15819864, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">import nltk\ntext = \"Obama delivers his first speech.\"\n\nsent  =  nltk.sent_tokenize(text)\n\n\nloftags = []\nfor s in sent:\n    d = nltk.word_tokenize(s)   \n\n    print nltk.pos_tag(d)\n</code></pre>\n<p>Result :</p>\n<blockquote>\n<p>akshayy@ubuntu:~/summ$ python nn1.py [('Obama', 'NNP'), ('delivers',\n  'NNS'), ('his', 'PRP$'), ('first', 'JJ'), ('speech', 'NN'), ('.',\n  '.')]</p>\n</blockquote>\n<p>( I just asked another question where used this code )</p>\n", "abstract": "Result : akshayy@ubuntu:~/summ$ python nn1.py [('Obama', 'NNP'), ('delivers',\n  'NNS'), ('his', 'PRP$'), ('first', 'JJ'), ('speech', 'NN'), ('.',\n  '.')] ( I just asked another question where used this code )"}, {"id": 18210732, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">nltk.download()\n</code></pre>\n<p>Click on Models and choose maxent_treebank_pos_. It gets installed automatically.</p>\n<pre><code class=\"python\">import nltk \ntext=nltk.word_tokenize(\"We are going out.Just you and me.\")\nprint nltk.pos_tag(text)\n[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'JJ'),\n ('you', 'PRP'), ('and', 'CC'), ('me', 'PRP'), ('.', '.')]\n</code></pre>\n", "abstract": "Click on Models and choose maxent_treebank_pos_. It gets installed automatically."}, {"id": 58607053, "score": 0, "vote": 0, "content": "<p>If nltk version is 3.4.5, do the below:</p>\n<pre><code class=\"python\">import nltk\nnltk.download('averaged_perceptron_tagger')\n</code></pre>\n<p>To check you nltk version, do the below:</p>\n<pre><code class=\"python\">print (nltk.__version__)\n</code></pre>\n", "abstract": "If nltk version is 3.4.5, do the below: To check you nltk version, do the below:"}, {"id": 69394203, "score": 0, "vote": 0, "content": "<p>I am using Google Colab and the current version of NLTK is <code>3.2.5</code> as of today.</p>\n<p>This is what worked for me.</p>\n<pre><code class=\"python\">import nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.tokenize import word_tokenize\n\ntext = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\nnltk.pos_tag(text)\n</code></pre>\n", "abstract": "I am using Google Colab and the current version of NLTK is 3.2.5 as of today. This is what worked for me."}]}, {"link": "https://stackoverflow.com/questions/29332851/what-does-nn-vbd-in-dt-nns-rb-means-in-nltk", "question": {"id": "29332851", "title": "What does NN VBD IN DT NNS RB means in NLTK?", "content": "<p>when I chunk text, I get lots of codes in the output like\n<code>NN, VBD, IN, DT, NNS, RB</code>.\nIs there a list documented somewhere which tells me the meaning of these? \nI have tried googling <code>nltk chunk code</code> <code>nltk chunk grammar</code> <code>nltk chunk tokens</code>.</p>\n<p>But I am not able to find any documentation which explains what these codes mean.</p>\n", "abstract": "when I chunk text, I get lots of codes in the output like\nNN, VBD, IN, DT, NNS, RB.\nIs there a list documented somewhere which tells me the meaning of these? \nI have tried googling nltk chunk code nltk chunk grammar nltk chunk tokens. But I am not able to find any documentation which explains what these codes mean."}, "answers": [{"id": 29333217, "score": 25, "vote": 0, "content": "<p>The tags that you see are not a result of the chunks but the POS tagging that happens before chunking. It's the Penn Treebank tagset, see <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" rel=\"noreferrer\">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk\n&gt;&gt;&gt; sent = \"This is a Foo Bar sentence.\"\n# POS tag.\n&gt;&gt;&gt; nltk.pos_tag(word_tokenize(sent))\n[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('Foo', 'NNP'), ('Bar', 'NNP'), ('sentence', 'NN'), ('.', '.')]\n&gt;&gt;&gt; tagged_sent = nltk.pos_tag(word_tokenize(sent))\n# Chunk.\n&gt;&gt;&gt; ne_chunk(tagged_sent)\nTree('S', [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), Tree('ORGANIZATION', [('Foo', 'NNP'), ('Bar', 'NNP')]), ('sentence', 'NN'), ('.', '.')])\n</code></pre>\n<p>To get the chunks look for subtrees within the chunked outputs. From the above output, the <code>Tree('ORGANIZATION', [('Foo', 'NNP'), ('Bar', 'NNP')])</code> indicates the chunk.</p>\n<p>This tutorial site is pretty helpful to explain the chunking process in NLTK: <a href=\"https://web.archive.org/web/20150412115803/http://www.eecis.udel.edu:80/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf\" rel=\"noreferrer\">http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf</a>. </p>\n<p>For official documentation, see <a href=\"http://www.nltk.org/howto/chunk.html\" rel=\"noreferrer\">http://www.nltk.org/howto/chunk.html</a></p>\n", "abstract": "The tags that you see are not a result of the chunks but the POS tagging that happens before chunking. It's the Penn Treebank tagset, see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html To get the chunks look for subtrees within the chunked outputs. From the above output, the Tree('ORGANIZATION', [('Foo', 'NNP'), ('Bar', 'NNP')]) indicates the chunk. This tutorial site is pretty helpful to explain the chunking process in NLTK: http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf.  For official documentation, see http://www.nltk.org/howto/chunk.html"}, {"id": 54869627, "score": 22, "vote": 0, "content": "<p>Even though the above links have all kinds. But hope this is still helpful for someone, added a few that are missed on other links.</p>\n<p><strong>CC</strong>: Coordinating conjunction</p>\n<p><strong>CD</strong>: Cardinal number</p>\n<p><strong>DT</strong>: Determiner</p>\n<p><strong>EX</strong>: Existential there</p>\n<p><strong>FW</strong>: Foreign word</p>\n<p><strong>IN</strong>: Preposition or subordinating conjunction</p>\n<p><strong>JJ</strong>: Adjective</p>\n<p><strong>VP</strong>: Verb Phrase</p>\n<p><strong>JJR</strong>: Adjective, comparative</p>\n<p><strong>JJS</strong>: Adjective, superlative</p>\n<p><strong>LS</strong>: List item marker</p>\n<p><strong>MD</strong>: Modal</p>\n<p><strong>NN</strong>: Noun, singular or mass</p>\n<p><strong>NNS</strong>: Noun, plural</p>\n<p><strong>PP</strong>: Preposition Phrase</p>\n<p><strong>NNP</strong>: Proper noun, singular Phrase</p>\n<p><strong>NNPS</strong>: Proper noun, plural</p>\n<p><strong>PDT</strong>: Pre determiner</p>\n<p><strong>POS</strong>: Possessive ending</p>\n<p><strong>PRP</strong>: Personal pronoun Phrase</p>\n<p><strong>PRP</strong>: Possessive pronoun Phrase</p>\n<p><strong>RB</strong>: Adverb</p>\n<p><strong>RBR</strong>: Adverb, comparative</p>\n<p><strong>RBS</strong>: Adverb, superlative</p>\n<p><strong>RP</strong>: Particle</p>\n<p><strong>S</strong>: Simple declarative clause</p>\n<p><strong>SBAR</strong>: Clause introduced by a (possibly empty) subordinating conjunction</p>\n<p><strong>SBARQ</strong>: Direct question introduced by a wh-word or a wh-phrase. </p>\n<p><strong>SINV</strong>: Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal.</p>\n<p><strong>SQ</strong>: Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ.</p>\n<p><strong>SYM</strong>: Symbol</p>\n<p><strong>VBD</strong>: Verb, past tense</p>\n<p><strong>VBG</strong>: Verb, gerund or present participle</p>\n<p><strong>VBN</strong>: Verb, past participle</p>\n<p><strong>VBP</strong>: Verb, non-3rd person singular present</p>\n<p><strong>VBZ</strong>: Verb, 3rd person singular present</p>\n<p><strong>WDT</strong>: Wh-determiner</p>\n<p><strong>WP</strong>: Wh-pronoun</p>\n<p><strong>WP</strong>: Possessive wh-pronoun</p>\n<p><strong>WRB</strong>: Wh-adverb</p>\n", "abstract": "Even though the above links have all kinds. But hope this is still helpful for someone, added a few that are missed on other links. CC: Coordinating conjunction CD: Cardinal number DT: Determiner EX: Existential there FW: Foreign word IN: Preposition or subordinating conjunction JJ: Adjective VP: Verb Phrase JJR: Adjective, comparative JJS: Adjective, superlative LS: List item marker MD: Modal NN: Noun, singular or mass NNS: Noun, plural PP: Preposition Phrase NNP: Proper noun, singular Phrase NNPS: Proper noun, plural PDT: Pre determiner POS: Possessive ending PRP: Personal pronoun Phrase PRP: Possessive pronoun Phrase RB: Adverb RBR: Adverb, comparative RBS: Adverb, superlative RP: Particle S: Simple declarative clause SBAR: Clause introduced by a (possibly empty) subordinating conjunction SBARQ: Direct question introduced by a wh-word or a wh-phrase.  SINV: Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal. SQ: Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ. SYM: Symbol VBD: Verb, past tense VBG: Verb, gerund or present participle VBN: Verb, past participle VBP: Verb, non-3rd person singular present VBZ: Verb, 3rd person singular present WDT: Wh-determiner WP: Wh-pronoun WP: Possessive wh-pronoun WRB: Wh-adverb"}, {"id": 29339135, "score": 2, "vote": 0, "content": "<p>As told by Alvas above, these tags are part-of-speech which tells whether a word/phrase is Noun phrase,Adverb,determiner,verb etc... </p>\n<p>Here are the <a href=\"http://nishutayaltech.blogspot.in/2015/02/penn-treebank-pos-tags-in-natural.html\" rel=\"nofollow\">POS Tag</a> details you can refer.</p>\n<pre><code class=\"python\">Chunking recovers the phrased from the Part of speech tags\n</code></pre>\n<p>You can refer this <a href=\"http://www.nltk.org/book/ch07.html\" rel=\"nofollow\">link</a> for reading for about chunking.</p>\n", "abstract": "As told by Alvas above, these tags are part-of-speech which tells whether a word/phrase is Noun phrase,Adverb,determiner,verb etc...  Here are the POS Tag details you can refer. You can refer this link for reading for about chunking."}]}, {"link": "https://stackoverflow.com/questions/36800654/how-is-the-tfidfvectorizer-in-scikit-learn-supposed-to-work", "question": {"id": "36800654", "title": "How is the TFIDFVectorizer in scikit-learn supposed to work?", "content": "<p>I'm trying to get words that are distinctive of certain documents using the TfIDFVectorizer class in scikit-learn. It creates a tfidf matrix with all the words and their scores in all the documents, but then it seems to count common words, as well. This is some of the code I'm running: </p>\n<pre><code class=\"python\">vectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(contents)\nfeature_names = vectorizer.get_feature_names()\ndense = tfidf_matrix.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names, index=characters)\ns = pd.Series(df.loc['Adam'])\ns[s &gt; 0].sort_values(ascending=False)[:10]\n</code></pre>\n<p>I expected this to return a list of distinctive words for the document 'Adam', but what it does it return a list of common words: </p>\n<pre><code class=\"python\">and     0.497077\nto      0.387147\nthe     0.316648\nof      0.298724\nin      0.186404\nwith    0.144583\nhis     0.140998\n</code></pre>\n<p>I might not understand it perfectly, but as I understand it, tf-idf is supposed to find words that are distinctive of one document in a corpus, finding words that appear frequently in one document, but not in other documents. Here, <code>and</code> appears frequently in other documents, so I don't know why it's returning a high value here. </p>\n<p>The complete code I'm using to generate this is <a href=\"https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-scikit.ipynb\" rel=\"noreferrer\">in this Jupyter notebook</a>. </p>\n<p>When I compute tf/idfs semi-manually, using the NLTK and computing scores for each word, I get the appropriate results. For the 'Adam' document: </p>\n<pre><code class=\"python\">fresh        0.000813\nprime        0.000813\nbone         0.000677\nrelate       0.000677\nblame        0.000677\nenough       0.000677\n</code></pre>\n<p>That looks about right, since these are words that appear in the 'Adam' document, but not as much in other documents in the corpus. The complete code used to generate this is in <a href=\"https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-nltk.ipynb\" rel=\"noreferrer\">this Jupyter notebook</a>. </p>\n<p>Am I doing something wrong with the scikit code? Is there another way to initialize this class where it returns the right results? Of course, I can ignore stopwords by passing <code>stop_words = 'english'</code>, but that doesn't really solve the problem, since common words of any sort shouldn't have high scores here. </p>\n", "abstract": "I'm trying to get words that are distinctive of certain documents using the TfIDFVectorizer class in scikit-learn. It creates a tfidf matrix with all the words and their scores in all the documents, but then it seems to count common words, as well. This is some of the code I'm running:  I expected this to return a list of distinctive words for the document 'Adam', but what it does it return a list of common words:  I might not understand it perfectly, but as I understand it, tf-idf is supposed to find words that are distinctive of one document in a corpus, finding words that appear frequently in one document, but not in other documents. Here, and appears frequently in other documents, so I don't know why it's returning a high value here.  The complete code I'm using to generate this is in this Jupyter notebook.  When I compute tf/idfs semi-manually, using the NLTK and computing scores for each word, I get the appropriate results. For the 'Adam' document:  That looks about right, since these are words that appear in the 'Adam' document, but not as much in other documents in the corpus. The complete code used to generate this is in this Jupyter notebook.  Am I doing something wrong with the scikit code? Is there another way to initialize this class where it returns the right results? Of course, I can ignore stopwords by passing stop_words = 'english', but that doesn't really solve the problem, since common words of any sort shouldn't have high scores here. "}, "answers": [{"id": 36801712, "score": 8, "vote": 0, "content": "<p>From scikit-learn documentation:</p>\n<p>As tf\u2013idf is very often used for text features, there is also another class called TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model.</p>\n<p>As you can see, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\" rel=\"noreferrer\">TfidfVectorizer</a> is a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\" rel=\"noreferrer\">CountVectorizer</a> followed by <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\" rel=\"noreferrer\">TfidfTransformer</a>.</p>\n<p>What you are probably looking for is <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\" rel=\"noreferrer\">TfidfTransformer</a> and not <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\" rel=\"noreferrer\">TfidfVectorizer</a></p>\n", "abstract": "From scikit-learn documentation: As tf\u2013idf is very often used for text features, there is also another class called TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model. As you can see, TfidfVectorizer is a CountVectorizer followed by TfidfTransformer. What you are probably looking for is TfidfTransformer and not TfidfVectorizer"}, {"id": 36814522, "score": 7, "vote": 0, "content": "<p>I believe your issue lies in using different stopword lists. Scikit-learn and NLTK use different stopword lists by default. For scikit-learn it is usually a good idea to have a custom stop_words list passed to TfidfVectorizer, e.g.:</p>\n<pre><code class=\"python\">my_stopword_list = ['and','to','the','of']\nmy_vectorizer = TfidfVectorizer(stop_words=my_stopword_list)\n</code></pre>\n<p>Doc page for TfidfVectorizer class: [<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html][1]\" rel=\"noreferrer\">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html][1]</a></p>\n", "abstract": "I believe your issue lies in using different stopword lists. Scikit-learn and NLTK use different stopword lists by default. For scikit-learn it is usually a good idea to have a custom stop_words list passed to TfidfVectorizer, e.g.: Doc page for TfidfVectorizer class: [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html][1]"}, {"id": 51489113, "score": 6, "vote": 0, "content": "<p>using below code I get much better results</p>\n<pre><code class=\"python\">vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n</code></pre>\n<p>Output</p>\n<pre><code class=\"python\">sustain    0.045090\nbone       0.045090\nthou       0.044417\nthee       0.043673\ntimely     0.043269\nthy        0.042731\nprime      0.041628\nabsence    0.041234\nrib        0.041234\nfeel       0.040259\nName: Adam, dtype: float64\n</code></pre>\n<p>and</p>\n<pre><code class=\"python\">thee          0.071188\nthy           0.070549\nforbids       0.069358\nthou          0.068068\nearly         0.064642\nearliest      0.062229\ndreamed       0.062229\nfirmness      0.062229\nglistering    0.062229\nsweet         0.060770\nName: Eve, dtype: float64\n</code></pre>\n", "abstract": "using below code I get much better results Output and"}, {"id": 36815074, "score": 3, "vote": 0, "content": "<p>I'm not sure why it's not the default, but you probably want <code>sublinear_tf=True</code> in the initialization for TfidfVectorizer. I forked your repo and sent you a PR with an example that probably looks more like what you want.</p>\n", "abstract": "I'm not sure why it's not the default, but you probably want sublinear_tf=True in the initialization for TfidfVectorizer. I forked your repo and sent you a PR with an example that probably looks more like what you want."}, {"id": 58977344, "score": 0, "vote": 0, "content": "<p>The answer to your question may lie in the size of your corpus and source codes for different implementations. I haven't looked into the nltk code in detail, but 3-8 documents (in scikit code) are probably not big enough to construct a corpus. When constructing corpuses; news archives with with hundreds of thousands of articles or thousands of books are used. Maybe frequency of words like 'the' in 8 documents were not large overall to account for commonness of these words among those documents. </p>\n<p>If you look at source codes, you might be able to find differences in implementation, whether they follow different normalization steps or frequency distributions (<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html</a> has common tfidf variants)</p>\n<p>Another thing that may help could be looking at the term frequencies (CountVectorizer in scikit) and making sure that words like 'the' are over represented in all documents.</p>\n", "abstract": "The answer to your question may lie in the size of your corpus and source codes for different implementations. I haven't looked into the nltk code in detail, but 3-8 documents (in scikit code) are probably not big enough to construct a corpus. When constructing corpuses; news archives with with hundreds of thousands of articles or thousands of books are used. Maybe frequency of words like 'the' in 8 documents were not large overall to account for commonness of these words among those documents.  If you look at source codes, you might be able to find differences in implementation, whether they follow different normalization steps or frequency distributions (https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html has common tfidf variants) Another thing that may help could be looking at the term frequencies (CountVectorizer in scikit) and making sure that words like 'the' are over represented in all documents."}]}, {"link": "https://stackoverflow.com/questions/8842817/selecting-the-most-fluent-text-from-a-set-of-possibilities-via-grammar-checking", "question": {"id": "8842817", "title": "Selecting the most fluent text from a set of possibilities via grammar checking (Python)", "content": "<h1>Some background</h1>\n<p>I am a literature student at New College of Florida, currently working on an overly ambitious creative project. <strong>The project is geared towards the algorithmic generation of poetry</strong>. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.</p>\n<p><strong>I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak.</strong> About a month ago some friendly folks on SO <a href=\"https://stackoverflow.com/questions/8541447/some-nlp-stuff-to-do-with-grammar-tagging-stemming-and-word-sense-disambiguat\">gave me some advice on how I might solve this issue</a> by using an <strong>ngram language modeller</strong>, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)</p>\n<h1>Perhaps I need something like AtD, but hopefully less complex</h1>\n<p><strong>I think need something that works like <a href=\"http://afterthedeadline.com/\" rel=\"nofollow noreferrer\">After the Deadline</a></strong> or <a href=\"http://queequeg.sourceforge.net/index-e.html\" rel=\"nofollow noreferrer\">Queequeg</a>, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.</p>\n<p>On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the <a href=\"http://blog.afterthedeadline.com/2009/09/15/python-bindings-for-atd/\" rel=\"nofollow noreferrer\">python bindings</a> for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of \"services\" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)</p>\n<h2>Things I'd like to avoid</h2>\n<p><strong>Building Ngram language models myself doesn't seem right for the task.</strong> my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)</p>\n<p><strong>Strictly checking grammar is neither right for the task.</strong> the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.</p>\n<p>In fact, I don't even need any kind of <em>suggestions</em> for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.</p>\n<h1>A simple solution? Scoring fluency by detecting obvious errors</h1>\n<p>If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.</p>\n<p>Let's say we have a tiny bit of text already laid out:</p>\n<p><code>existing_text = \"The old river\"</code></p>\n<p>Now let's say my script needs to figure out which inflection of the verb \"to bear\" could come next. I'm open to suggestions about this routine. <strong>But I need help mostly with step #2</strong>, rating fluency by tallying grammatical errors:</p>\n<ol>\n<li>Use the Verb Conjugation methods in <a href=\"http://nodebox.net/code/index.php/Linguistics\" rel=\"nofollow noreferrer\">NodeBox Linguistics</a> to come up with all conjugations of this verb; <code>['bear', 'bears', 'bearing', 'bore', 'borne']</code>.</li>\n<li>Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from <code>existing_text + \" \" + possibility</code> (\"The old river bear\", \"The old river bears\", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be \"The old river bear\".</li>\n<li>Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.</li>\n</ol>\n", "abstract": "I am a literature student at New College of Florida, currently working on an overly ambitious creative project. The project is geared towards the algorithmic generation of poetry. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock. I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak. About a month ago some friendly folks on SO gave me some advice on how I might solve this issue by using an ngram language modeller, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.) I think need something that works like After the Deadline or Queequeg, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement. On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the python bindings for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of \"services\" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.) Building Ngram language models myself doesn't seem right for the task. my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.) Strictly checking grammar is neither right for the task. the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles. In fact, I don't even need any kind of suggestions for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues. If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach. Let's say we have a tiny bit of text already laid out: existing_text = \"The old river\" Now let's say my script needs to figure out which inflection of the verb \"to bear\" could come next. I'm open to suggestions about this routine. But I need help mostly with step #2, rating fluency by tallying grammatical errors:"}, "answers": [{"id": 8875033, "score": 2, "vote": 0, "content": "<p>Very cool project, first of all. </p>\n<p>I found a <a href=\"http://www.languagetool.org/usage/\" rel=\"nofollow\">java grammar checker</a>. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere. </p>\n<p>I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail.</p>\n", "abstract": "Very cool project, first of all.  I found a java grammar checker. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere.  I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail."}, {"id": 9091708, "score": 1, "vote": 0, "content": "<p>Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above.</p>\n<p>Michael Heilman uses this approach for question generation.  For more details, read these papers:\n<a href=\"http://aclweb.org/anthology/N/N10/N10-1086.pdf\" rel=\"nofollow\">Good Question! Statistical Ranking for Question Generation</a> and\n<a href=\"http://aclweb.org/anthology/W/W10/W10-0705.pdf\" rel=\"nofollow\">Rating Computer-Generated Questions with Mechanical Turk</a>.</p>\n", "abstract": "Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above. Michael Heilman uses this approach for question generation.  For more details, read these papers:\nGood Question! Statistical Ranking for Question Generation and\nRating Computer-Generated Questions with Mechanical Turk."}, {"id": 19502557, "score": 1, "vote": 0, "content": "<p>The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at:</p>\n<p><a href=\"https://pypi.python.org/pypi/pylinkgrammar\" rel=\"nofollow\">https://pypi.python.org/pypi/pylinkgrammar</a></p>\n", "abstract": "The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at: https://pypi.python.org/pypi/pylinkgrammar"}]}, {"link": "https://stackoverflow.com/questions/23704510/how-do-i-test-whether-an-nltk-resource-is-already-installed-on-the-machine-runni", "question": {"id": "23704510", "title": "How do I test whether an nltk resource is already installed on the machine running my code?", "content": "<p>I just started my first NLTK project and am confused about the proper setup. I need several resources like the Punkt Tokenizer and the maxent pos tagger. I myself downloaded them using the GUI <code>nltk.download()</code>. For my collaborators I of course want that this things get downloaded automatically. I haven't found any idiomatic code for that in the docu. </p>\n<p>Am I supposed to just put <code>nltk.data.load('tokenizers/punkt/english.pickle')</code> and their like into the code? Is this going to download the resources every time the script is run? Am I to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? There MUST be gear out there that does the job, right? :)</p>\n<p>//Edit To explify my question: <br/>\n<strong>How do I test whether an nltk resource (like the Punkt Tokenizer) is already installed on the machine running my code, and install it if it is not?</strong> </p>\n", "abstract": "I just started my first NLTK project and am confused about the proper setup. I need several resources like the Punkt Tokenizer and the maxent pos tagger. I myself downloaded them using the GUI nltk.download(). For my collaborators I of course want that this things get downloaded automatically. I haven't found any idiomatic code for that in the docu.  Am I supposed to just put nltk.data.load('tokenizers/punkt/english.pickle') and their like into the code? Is this going to download the resources every time the script is run? Am I to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? There MUST be gear out there that does the job, right? :) //Edit To explify my question: \nHow do I test whether an nltk resource (like the Punkt Tokenizer) is already installed on the machine running my code, and install it if it is not? "}, "answers": [{"id": 23715469, "score": 42, "vote": 0, "content": "<p>You can use the <code>nltk.data.find()</code> function, see <a href=\"https://github.com/nltk/nltk/blob/develop/nltk/data.py\" rel=\"noreferrer\">https://github.com/nltk/nltk/blob/develop/nltk/data.py</a>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; nltk.data.find('tokenizers/punkt.zip')\nZipFilePathPointer(u'/home/alvas/nltk_data/tokenizers/punkt.zip', u'')\n</code></pre>\n<p>When the resource is not available you'll find the error:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python2.7/dist-packages/nltk-3.0a3-py2.7.egg/nltk/data.py\", line 615, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource u'punkt.zip' not found.  Please use the NLTK Downloader\n  to obtain the resource:  &gt;&gt;&gt; nltk.download()\n  Searched in:\n    - '/home/alvas/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n</code></pre>\n<p>Most probably, you would like to do something like this to ensure that your collaborators have the package:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; try:\n...     nltk.data.find('tokenizers/punkt')\n... except LookupError:\n...     nltk.download('punkt')\n... \n[nltk_data] Downloading package punkt to /home/alvas/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTrue\n</code></pre>\n", "abstract": "You can use the nltk.data.find() function, see https://github.com/nltk/nltk/blob/develop/nltk/data.py: When the resource is not available you'll find the error: Most probably, you would like to do something like this to ensure that your collaborators have the package:"}, {"id": 73166593, "score": 1, "vote": 0, "content": "<p>After Somnath comment, I am posting an example of the try-except workaround. Here we search for the comtrans module that is not in the nltk data by default.</p>\n<pre><code class=\"python\">from nltk.corpus import comtrans\nfrom nltk import download\n\ntry:\n    words = comtrans.words('alignment-en-fr.txt')\nexcept LookupError:\n    print('resource not found. Downloading now...')\n    download('comtrans')\n    words = comtrans.words('alignment-en-fr.txt')\n</code></pre>\n", "abstract": "After Somnath comment, I am posting an example of the try-except workaround. Here we search for the comtrans module that is not in the nltk data by default."}]}, {"link": "https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing", "question": {"id": "25735644", "title": "Python - RegEx for splitting text into sentences (sentence-tokenizing)", "content": "<p>I want to make a list of sentences from a string and then print them out. I don't want to use NLTK to do this.  So it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   This is attempt at regex that doesn't work.</p>\n<pre><code class=\"python\">import re\n\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nsentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n\nfor stuff in sentences:\n        print(stuff)    \n</code></pre>\n<p>Example output of what it should look like</p>\n<pre><code class=\"python\">Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. \nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.\n</code></pre>\n", "abstract": "I want to make a list of sentences from a string and then print them out. I don't want to use NLTK to do this.  So it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   This is attempt at regex that doesn't work. Example output of what it should look like"}, "answers": [{"id": 25736082, "score": 47, "vote": 0, "content": "<pre><code class=\"python\">(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s\n</code></pre>\n<p>Try this. split your string this.You can also check demo.</p>\n<p><a href=\"http://regex101.com/r/nG1gU7/27\" rel=\"noreferrer\">http://regex101.com/r/nG1gU7/27</a></p>\n", "abstract": "Try this. split your string this.You can also check demo. http://regex101.com/r/nG1gU7/27"}, {"id": 25735848, "score": 35, "vote": 0, "content": "<p>Ok so sentence-tokenizers are something I looked at in a little detail, using regexes, nltk, CoreNLP, spaCy. You end up writing your own and it depends on the application. This stuff is tricky and valuable and people don't just give their tokenizer code away. (Ultimately, tokenization is not a deterministic procedure, it's probabilistic, and also depends very heavily on your corpus or domain, e.g. legal/financial documents vs social-media posts vs Yelp reviews vs biomedical papers...)</p>\n<p><strong>In general you can't rely on one single Great White infallible regex</strong>, you have to write a function which uses several regexes (both positive and negative); also a dictionary of abbreviations, and some basic language parsing which knows that e.g. 'I', 'USA', 'FCC', 'TARP' are capitalized in English.</p>\n<p><strong>To illustrate how easily this can get seriously complicated, let's try to write you that functional spec for a deterministic tokenizer <em>just</em> to decide whether single or multiple period ('.'/'...') indicates end-of-sentence</strong>, or something else:</p>\n<p><code>function isEndOfSentence(leftContext, rightContext)</code></p>\n<ol>\n<li>Return False for decimals inside numbers or currency e.g. <em>1.23 , $1.23, \"That's just my $.02\"</em> Consider also section references like 1.2.A.3.a, European date formats like 09.07.2014, IP addresses like 192.168.1.1, MAC addresses...</li>\n<li>Return False (and don't tokenize into individual letters) for known abbreviations e.g. \"U.S. stocks are falling\" ; this requires a dictionary of known abbreviations. Anything outside that dictionary you will get wrong, unless you add code to detect unknown abbreviations like A.B.C. and add them to a list.</li>\n<li>Ellipses '...' at ends of sentences are terminal, but in the middle of sentences are not. This is not as easy as you might think: you need to look at the left context and the right context, specifically is the RHS capitalized and again consider capitalized words like 'I' and   abbreviations. Here's an example proving ambiguity which : <em>She asked me to stay... I left an hour later.</em> (Was that one sentence or two? Impossible to determine)</li>\n<li>You may also want to write a few patterns to detect and reject miscellaneous non-sentence-ending uses of punctuation: emoticons :-), ASCII art, spaced ellipses . . . and other stuff esp. Twitter. (Making that adaptive is even harder). How do we tell if @midnight is a Twitter user, the <a href=\"http://www.cc.com/shows/-midnight\" rel=\"nofollow noreferrer\">show on Comedy Central</a>, text shorthand, or simply unwanted/junk/typo punctuation? Seriously non-trivial.</li>\n<li>After you handle all those negative cases, you could arbitrarily say that any isolated period followed by whitespace is likely to be an end of sentence. (Ultimately, if you really want to buy extra accuracy, you end up writing your own probabilistic sentence-tokenizer which uses weights, and training it on a specific corpus(e.g. legal texts, broadcast media, StackOverflow, Twitter, forums comments etc.)) Then you have to manually review exemplars and training errors. See Manning and Jurafsky book or Coursera course [a].\nUltimately you get as much correctness as you are prepared to pay for.</li>\n<li>All of the above is clearly specific to the English-language/ abbreviations, US number/time/date formats. If you want to make it country- and language-independent, that's a bigger proposition, you'll need corpora, native-speaking people to label and QA it all, etc.</li>\n<li>All of the above is still only ASCII, which is practically speaking only 96 characters. Allow the input to be Unicode, and things get harder still (and the training-set necessarily must be either much bigger or much sparser)</li>\n</ol>\n<p>In the simple (deterministic) case, <code>function isEndOfSentence(leftContext, rightContext)</code> would return boolean, but in the more general sense, it's probabilistic: it returns a float 0.0-1.0 (confidence level that that particular '.' is a sentence end).</p>\n<p>References: [a] Coursera video: \"Basic Text Processing 2-5 - Sentence Segmentation - Stanford NLP - Professor Dan Jurafsky &amp; Chris Manning\" <a href=\"https://www.youtube.com/watch?v=di0N3kXfGYg\" rel=\"nofollow noreferrer\">[UPDATE: an unofficial version used to be on YouTube, was taken down]</a></p>\n", "abstract": "Ok so sentence-tokenizers are something I looked at in a little detail, using regexes, nltk, CoreNLP, spaCy. You end up writing your own and it depends on the application. This stuff is tricky and valuable and people don't just give their tokenizer code away. (Ultimately, tokenization is not a deterministic procedure, it's probabilistic, and also depends very heavily on your corpus or domain, e.g. legal/financial documents vs social-media posts vs Yelp reviews vs biomedical papers...) In general you can't rely on one single Great White infallible regex, you have to write a function which uses several regexes (both positive and negative); also a dictionary of abbreviations, and some basic language parsing which knows that e.g. 'I', 'USA', 'FCC', 'TARP' are capitalized in English. To illustrate how easily this can get seriously complicated, let's try to write you that functional spec for a deterministic tokenizer just to decide whether single or multiple period ('.'/'...') indicates end-of-sentence, or something else: function isEndOfSentence(leftContext, rightContext) In the simple (deterministic) case, function isEndOfSentence(leftContext, rightContext) would return boolean, but in the more general sense, it's probabilistic: it returns a float 0.0-1.0 (confidence level that that particular '.' is a sentence end). References: [a] Coursera video: \"Basic Text Processing 2-5 - Sentence Segmentation - Stanford NLP - Professor Dan Jurafsky & Chris Manning\" [UPDATE: an unofficial version used to be on YouTube, was taken down]"}, {"id": 25736515, "score": 5, "vote": 0, "content": "<p>Try to split the input according to the spaces rather than a dot or <code>?</code>, if you do like this then the dot or <code>?</code> won't be printed in the final result.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import re\n&gt;&gt;&gt; s = \"\"\"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\"\"\n&gt;&gt;&gt; m = re.split(r'(?&lt;=[^A-Z].[.?]) +(?=[A-Z])', s)\n&gt;&gt;&gt; for i in m:\n...     print i\n... \nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.\n</code></pre>\n", "abstract": "Try to split the input according to the spaces rather than a dot or ?, if you do like this then the dot or ? won't be printed in the final result."}, {"id": 35676984, "score": 2, "vote": 0, "content": "<pre><code class=\"python\">sent = re.split('(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)(\\s|[A-Z].*)',text)\nfor s in sent:\n    print s\n</code></pre>\n<p>Here the regex used is : <code>(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)(\\s|[A-Z].*)</code></p>\n<p>First block: <code>(?&lt;!\\w\\.\\w.)</code> : this pattern searches in a negative feedback loop <code>(?&lt;!)</code> for all words <code>(\\w)</code> followed by fullstop <code>(\\.)</code> , followed by other words <code>(\\.)</code></p>\n<p>Second block: <code>(?&lt;![A-Z][a-z]\\.)</code>: this pattern searches in a negative feedback loop for anything starting with uppercase alphabets <code>([A-Z])</code>, followed by lower case alphabets <code>([a-z])</code> till a dot <code>(\\.)</code> is found.</p>\n<p>Third block: <code>(?&lt;=\\.|\\?)</code>: this pattern searches in a feedback loop of dot <code>(\\.)</code> OR question mark <code>(\\?)</code></p>\n<p>Fourth block: <code>(\\s|[A-Z].*)</code>: this pattern searches after the dot OR question mark from the third block. It searches for blank space <code>(\\s)</code> OR any sequence of characters starting with a upper case alphabet <code>([A-Z].*)</code>.\nThis block is important to split if the input is as </p>\n<blockquote>\n<p>Hello world.Hi I am here today.</p>\n</blockquote>\n<p>i.e. if there is space or no space after the dot.</p>\n", "abstract": "Here the regex used is : (?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)(\\s|[A-Z].*) First block: (?<!\\w\\.\\w.) : this pattern searches in a negative feedback loop (?<!) for all words (\\w) followed by fullstop (\\.) , followed by other words (\\.) Second block: (?<![A-Z][a-z]\\.): this pattern searches in a negative feedback loop for anything starting with uppercase alphabets ([A-Z]), followed by lower case alphabets ([a-z]) till a dot (\\.) is found. Third block: (?<=\\.|\\?): this pattern searches in a feedback loop of dot (\\.) OR question mark (\\?) Fourth block: (\\s|[A-Z].*): this pattern searches after the dot OR question mark from the third block. It searches for blank space (\\s) OR any sequence of characters starting with a upper case alphabet ([A-Z].*).\nThis block is important to split if the input is as  Hello world.Hi I am here today. i.e. if there is space or no space after the dot."}, {"id": 25736333, "score": 1, "vote": 0, "content": "<p>Naive approach for proper english sentences not starting with non-alphas and not containing quoted parts of speech:</p>\n<pre><code class=\"python\">import re\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nEndPunctuation = re.compile(r'([\\.\\?\\!]\\s+)')\nNonEndings = re.compile(r'(?:Mrs?|Jr|i\\.e)\\.\\s*$')\nparts = EndPunctuation.split(text)\nsentence = []\nfor part in parts:\n  if len(part) and len(sentence) and EndPunctuation.match(sentence[-1]) and not NonEndings.search(''.join(sentence)):\n    print(''.join(sentence))\n    sentence = []\n  if len(part):\n    sentence.append(part)\nif len(sentence):\n  print(''.join(sentence))\n</code></pre>\n<p>False positive splitting may be reduced by extending NonEndings a bit. Other cases will require additional code. Handling typos in a sensible way will prove difficult with this approach.</p>\n<p>You will never reach perfection with this approach. But depending on the task it might just work \"enough\"...</p>\n", "abstract": "Naive approach for proper english sentences not starting with non-alphas and not containing quoted parts of speech: False positive splitting may be reduced by extending NonEndings a bit. Other cases will require additional code. Handling typos in a sensible way will prove difficult with this approach. You will never reach perfection with this approach. But depending on the task it might just work \"enough\"..."}, {"id": 48480953, "score": 1, "vote": 0, "content": "<p>I'm not great at regular expressions, but a simpler version, \"brute force\" actually, of above is </p>\n<pre><code class=\"python\">sentence = re.compile(\"([\\'\\\"][A-Z]|([A-Z][a-z]*\\. )|[A-Z])(([a-z]*\\.[a-z]*\\.)|([A-Za-z0-9]*\\.[A-Za-z0-9])|([A-Z][a-z]*\\. [A-Za-z]*)|[^\\.?]|[A-Za-z])*[\\.?]\")\n</code></pre>\n<p>which means \nstart acceptable units are '[A-Z] or \"[A-Z]<br/>\nplease note, most regular expressions are greedy so the order is very important when we do <strong>|</strong>(or). That's, why I have written <strong>i.e.</strong> regular expression first, then is come forms like <strong>Inc.</strong> </p>\n", "abstract": "I'm not great at regular expressions, but a simpler version, \"brute force\" actually, of above is  which means \nstart acceptable units are '[A-Z] or \"[A-Z]\nplease note, most regular expressions are greedy so the order is very important when we do |(or). That's, why I have written i.e. regular expression first, then is come forms like Inc. "}, {"id": 25735939, "score": 0, "vote": 0, "content": "<p>Try this:</p>\n<pre><code class=\"python\">(?&lt;!\\b(?:[A-Z][a-z]|\\d|[i.e]))\\.(?!\\b(?:com|\\d+)\\b)\n</code></pre>\n", "abstract": "Try this:"}, {"id": 28094620, "score": 0, "vote": 0, "content": "<p>I wrote this taking into consideration smci's comments above.  It is a middle-of-the-road approach that doesn't require external libraries and doesn't use regex.  It allows you to provide a list of abbreviations and accounts for sentences ended by terminators in wrappers, such as a period and quote: [.\", ?', .)].</p>\n<pre><code class=\"python\">abbreviations = {'dr.': 'doctor', 'mr.': 'mister', 'bro.': 'brother', 'bro': 'brother', 'mrs.': 'mistress', 'ms.': 'miss', 'jr.': 'junior', 'sr.': 'senior', 'i.e.': 'for example', 'e.g.': 'for example', 'vs.': 'versus'}\nterminators = ['.', '!', '?']\nwrappers = ['\"', \"'\", ')', ']', '}']\n\n\ndef find_sentences(paragraph):\n   end = True\n   sentences = []\n   while end &gt; -1:\n       end = find_sentence_end(paragraph)\n       if end &gt; -1:\n           sentences.append(paragraph[end:].strip())\n           paragraph = paragraph[:end]\n   sentences.append(paragraph)\n   sentences.reverse()\n   return sentences\n\n\ndef find_sentence_end(paragraph):\n    [possible_endings, contraction_locations] = [[], []]\n    contractions = abbreviations.keys()\n    sentence_terminators = terminators + [terminator + wrapper for wrapper in wrappers for terminator in terminators]\n    for sentence_terminator in sentence_terminators:\n        t_indices = list(find_all(paragraph, sentence_terminator))\n        possible_endings.extend(([] if not len(t_indices) else [[i, len(sentence_terminator)] for i in t_indices]))\n    for contraction in contractions:\n        c_indices = list(find_all(paragraph, contraction))\n        contraction_locations.extend(([] if not len(c_indices) else [i + len(contraction) for i in c_indices]))\n    possible_endings = [pe for pe in possible_endings if pe[0] + pe[1] not in contraction_locations]\n    if len(paragraph) in [pe[0] + pe[1] for pe in possible_endings]:\n        max_end_start = max([pe[0] for pe in possible_endings])\n        possible_endings = [pe for pe in possible_endings if pe[0] != max_end_start]\n    possible_endings = [pe[0] + pe[1] for pe in possible_endings if sum(pe) &gt; len(paragraph) or (sum(pe) &lt; len(paragraph) and paragraph[sum(pe)] == ' ')]\n    end = (-1 if not len(possible_endings) else max(possible_endings))\n    return end\n\n\ndef find_all(a_str, sub):\n    start = 0\n    while True:\n        start = a_str.find(sub, start)\n        if start == -1:\n            return\n        yield start\n        start += len(sub)\n</code></pre>\n<p>I used Karl's find_all function from this entry: <a href=\"https://stackoverflow.com/questions/4664850/find-all-occurrences-of-a-substring-in-python\">Find all occurrences of a substring in Python</a></p>\n", "abstract": "I wrote this taking into consideration smci's comments above.  It is a middle-of-the-road approach that doesn't require external libraries and doesn't use regex.  It allows you to provide a list of abbreviations and accounts for sentences ended by terminators in wrappers, such as a period and quote: [.\", ?', .)]. I used Karl's find_all function from this entry: Find all occurrences of a substring in Python"}, {"id": 52194399, "score": 0, "vote": 0, "content": "<p>My example is based on the example of Ali, adapted to Brazilian Portuguese. Thanks Ali.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">ABREVIACOES = ['sra?s?', 'exm[ao]s?', 'ns?', 'nos?', 'doc', 'ac', 'publ', 'ex', 'lv', 'vlr?', 'vls?',\n               'exmo(a)', 'ilmo(a)', 'av', 'of', 'min', 'livr?', 'co?ls?', 'univ', 'resp', 'cli', 'lb',\n               'dra?s?', '[a-z]+r\\(as?\\)', 'ed', 'pa?g', 'cod', 'prof', 'op', 'plan', 'edf?', 'func', 'ch',\n               'arts?', 'artigs?', 'artg', 'pars?', 'rel', 'tel', 'res', '[a-z]', 'vls?', 'gab', 'bel',\n               'ilm[oa]', 'parc', 'proc', 'adv', 'vols?', 'cels?', 'pp', 'ex[ao]', 'eg', 'pl', 'ref',\n               '[0-9]+', 'reg', 'f[il\u00ed]s?', 'inc', 'par', 'alin', 'fts', 'publ?', 'ex', 'v. em', 'v.rev']\n\nABREVIACOES_RGX = re.compile(r'(?:{})\\.\\s*$'.format('|\\s'.join(ABREVIACOES)), re.IGNORECASE)\n\n        def sentencas(texto, min_len=5):\n            # baseado em https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing\n            texto = re.sub(r'\\s\\s+', ' ', texto)\n            EndPunctuation = re.compile(r'([\\.\\?\\!]\\s+)')\n            # print(NonEndings)\n            parts = EndPunctuation.split(texto)\n            sentencas = []\n            sentence = []\n            for part in parts:\n                txt_sent = ''.join(sentence)\n                q_len = len(txt_sent)\n                if len(part) and len(sentence) and q_len &gt;= min_len and \\\n                        EndPunctuation.match(sentence[-1]) and \\\n                        not ABREVIACOES_RGX.search(txt_sent):\n                    sentencas.append(txt_sent)\n                    sentence = []\n\n                if len(part):\n                    sentence.append(part)\n            if sentence:\n                sentencas.append(''.join(sentence))\n            return sentencas\n</code></pre>\n<p>Full code in: <a href=\"https://github.com/luizanisio/comparador_elastic\" rel=\"nofollow noreferrer\">https://github.com/luizanisio/comparador_elastic</a></p>\n", "abstract": "My example is based on the example of Ali, adapted to Brazilian Portuguese. Thanks Ali. Full code in: https://github.com/luizanisio/comparador_elastic"}, {"id": 25735738, "score": -1, "vote": 0, "content": "<p>If you want to break up sentences at 3 periods (not sure if this is what you want) you can use this regular expresion:</p>\n<blockquote>\n<pre><code class=\"python\">import re\n\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nsentences = re.split(r'\\.{3}', text)\n\nfor stuff in sentences:\n     print(stuff)    \n</code></pre>\n</blockquote>\n", "abstract": "If you want to break up sentences at 3 periods (not sure if this is what you want) you can use this regular expresion:"}]}, {"link": "https://stackoverflow.com/questions/47350942/how-to-verify-installed-spacy-version", "question": {"id": "47350942", "title": "How to verify installed spaCy version?", "content": "<p>I have installed <strong>spaCy</strong> with python for my NLP project.</p>\n<p>I have installed that using <code>pip</code>.  How can I verify installed spaCy version?</p>\n<p>using </p>\n<pre><code class=\"python\">pip install -U spacy\n</code></pre>\n<p>What is command to verify installed spaCy version?</p>\n", "abstract": "I have installed spaCy with python for my NLP project. I have installed that using pip.  How can I verify installed spaCy version? using  What is command to verify installed spaCy version?"}, "answers": [{"id": 47351587, "score": 45, "vote": 0, "content": "<p>You can also do <code>python -m spacy info</code>. If you're updating an existing installation, you might want to run <code>python -m spacy validate</code>, to check that the models you already have are compatible with the version you just installed.</p>\n", "abstract": "You can also do python -m spacy info. If you're updating an existing installation, you might want to run python -m spacy validate, to check that the models you already have are compatible with the version you just installed."}, {"id": 49530619, "score": 9, "vote": 0, "content": "<p>Use command - <strong><code>python -m spacy info</code></strong> to check spacy version </p>\n", "abstract": "Use command - python -m spacy info to check spacy version "}, {"id": 52035352, "score": 8, "vote": 0, "content": "<p>If you ask yourself: How to find any Python pkg version?\nThis one should be used/ as well, not only for Spacy ofc:</p>\n<p>The easiest (if you installed it using pip):</p>\n<pre><code class=\"python\">pip show spacy #pip3 if you installed it using pip3\n</code></pre>\n<p>Or:</p>\n<pre><code class=\"python\">python -m spacy --version\n</code></pre>\n<p>Or... just run python (with the version that you installed Spacy on) and use the <strong>version</strong> method</p>\n<p>If you want to know the version of any Python pkg (package) you are working with this would work for you every time!</p>\n<p>run:</p>\n<pre><code class=\"python\">python\n&gt;&gt; import spacy\n&gt;&gt; print(spacy.__version__)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/D9m1y.png\" rel=\"nofollow noreferrer\"><img alt=\"Easy to do it with Pycharm built-in IDE console (Jupyter notebook)\" src=\"https://i.stack.imgur.com/D9m1y.png\"/></a></p>\n<p>Or, Either:</p>\n<pre><code class=\"python\">python -m spacy --version\n</code></pre>\n<p>or</p>\n<pre><code class=\"python\">python3 -m spacy --version #depends where it is install (python or python3)\n</code></pre>\n", "abstract": "If you ask yourself: How to find any Python pkg version?\nThis one should be used/ as well, not only for Spacy ofc: The easiest (if you installed it using pip): Or: Or... just run python (with the version that you installed Spacy on) and use the version method If you want to know the version of any Python pkg (package) you are working with this would work for you every time! run:  Or, Either: or"}, {"id": 47351160, "score": 3, "vote": 0, "content": "<p>If you installed with <code>pip</code> you can try to find it with <code>pip list</code> and get version info with <code>pip show &lt;name&gt;</code></p>\n", "abstract": "If you installed with pip you can try to find it with pip list and get version info with pip show <name>"}, {"id": 48338198, "score": 2, "vote": 0, "content": "<p>If you are using python3, you can use your package manager (pip) <code>pip3 list</code> and find spacy's version. <br/>\nFor Python 2.7+ <code>pip list</code> does the job</p>\n", "abstract": "If you are using python3, you can use your package manager (pip) pip3 list and find spacy's version. \nFor Python 2.7+ pip list does the job"}, {"id": 71661958, "score": 1, "vote": 0, "content": "<p><a href=\"https://i.stack.imgur.com/6mc5g.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/6mc5g.png\"/></a></p>\n<p>Simply use <code>!python -m spacy info</code> to get details on Jupyter notebook, remove <code>!</code> for normal python command check.</p>\n<p>Check the above screenshot to see the result details.</p>\n<p>Thanks</p>\n", "abstract": " Simply use !python -m spacy info to get details on Jupyter notebook, remove ! for normal python command check. Check the above screenshot to see the result details. Thanks"}, {"id": 68654169, "score": 0, "vote": 0, "content": "<p>Another way to get versions of Spacy and the dependencies is to use: <code>pip freeze requirements.txt</code>. See <a href=\"https://pip.pypa.io/en/stable/cli/pip_freeze/\" rel=\"nofollow noreferrer\">this</a> link for the official documentation for both Mac and Windows OSs.</p>\n<p>The main benefit I find with this approach is that you get a list of all dependencies plus the versions. Libraries are often times very picky about the versions. Using this method you can just share the requirements.txt with your collaborators and then they are good to go too :)</p>\n<p>Edit:\nThanks to hc_dev for the valuable comment.</p>\n", "abstract": "Another way to get versions of Spacy and the dependencies is to use: pip freeze requirements.txt. See this link for the official documentation for both Mac and Windows OSs. The main benefit I find with this approach is that you get a list of all dependencies plus the versions. Libraries are often times very picky about the versions. Using this method you can just share the requirements.txt with your collaborators and then they are good to go too :) Edit:\nThanks to hc_dev for the valuable comment."}]}, {"link": "https://stackoverflow.com/questions/1639855/pos-tagging-in-german", "question": {"id": "1639855", "title": "POS tagging in German", "content": "<p>I am using NLTK to extract nouns from a text-string starting with the following command:</p>\n<pre><code class=\"python\">tagged_text = nltk.pos_tag(nltk.Text(nltk.word_tokenize(some_string)))\n</code></pre>\n<p>It works fine in English. <strong>Is there an easy way to make it work for German as well?</strong> </p>\n<p>(I have no experience with natural language programming, but I managed to use the python nltk library which is great so far.)</p>\n", "abstract": "I am using NLTK to extract nouns from a text-string starting with the following command: It works fine in English. Is there an easy way to make it work for German as well?  (I have no experience with natural language programming, but I managed to use the python nltk library which is great so far.)"}, "answers": [{"id": 1640001, "score": 23, "vote": 0, "content": "<p>Natural language software does its magic by leveraging corpora and the statistics they provide. You'll need to tell nltk about some German corpus to help it tokenize German correctly. I believe the <a href=\"http://www.statmt.org/europarl/\" rel=\"nofollow noreferrer\">EUROPARL</a> corpus might help get you going. </p>\n<p>See <a href=\"https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus.europarl_raw\" rel=\"nofollow noreferrer\">nltk.corpus.europarl_raw</a> and <a href=\"https://stackoverflow.com/a/34191333/101827\">this answer</a> for example configuration.</p>\n<p>Also, consider tagging this question with \"nlp\".</p>\n", "abstract": "Natural language software does its magic by leveraging corpora and the statistics they provide. You'll need to tell nltk about some German corpus to help it tokenize German correctly. I believe the EUROPARL corpus might help get you going.  See nltk.corpus.europarl_raw and this answer for example configuration. Also, consider tagging this question with \"nlp\"."}, {"id": 15278298, "score": 18, "vote": 0, "content": "<p>The <a href=\"https://www.clips.uantwerpen.be/clips.bak/pages/pattern-de\" rel=\"nofollow noreferrer\">Pattern library</a> includes a function for parsing German sentences and the result includes the part-of-speech tags. The following is copied from their documentation:</p>\n<pre><code class=\"python\">from pattern.de import parse, split\ns = parse('Die Katze liegt auf der Matte.')\ns = split(s)\nprint s.sentences[0]\n\n&gt;&gt;&gt;   Sentence('Die/DT/B-NP/O Katze/NN/I-NP/O liegt/VB/B-VP/O'\n     'auf/IN/B-PP/B-PNP der/DT/B-NP/I-PNP Matte/NN/I-NP/I-PNP ././O/O')\n</code></pre>\n<p><strong>Update</strong>: Another option is <a href=\"https://spacy.io/\" rel=\"nofollow noreferrer\">spacy</a>, there is a quick example in this <a href=\"https://explosion.ai/blog/german-model\" rel=\"nofollow noreferrer\">blog article:</a></p>\n<pre><code class=\"python\">import spacy\n\nnlp = spacy.load('de')\ndoc = nlp(u'Ich bin ein Berliner.')\n\n# show universal pos tags\nprint(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in doc))\n# output: Ich/PRON bin/AUX ein/DET Berliner/NOUN ./PUNCT\n</code></pre>\n", "abstract": "The Pattern library includes a function for parsing German sentences and the result includes the part-of-speech tags. The following is copied from their documentation: Update: Another option is spacy, there is a quick example in this blog article:"}, {"id": 1640109, "score": 4, "vote": 0, "content": "<p>Part-of-Speech (POS) tagging is very specific to a particular [natural] language.  NLTK includes many different taggers, which use distinct techniques to infer the tag of a given token in a given token.  Most (but not all) of these taggers use a statistical model of sorts as the main or sole device to \"do the trick\".  Such taggers require some \"training data\" upon which to build this statistical representation of the language, and the training data comes in the form of corpora.</p>\n<p>The NTLK \"distribution\" itself includes many of these corpora, as well a set of \"corpora readers\" which provide an API to read different types of corpora. I don't know the state of affairs in NTLK proper, and if this includes any german corpus.  You can however locate free some free corpora which you'll then need to convert to a format that satisfies the proper NTLK corpora reader, and then you can use this to train a POS tagger for the German language.</p>\n<p>You can even create your own corpus, but that is a hell of a painstaking job; if you work in a univeristy, you gotta find ways of bribing and otherwise coercing students to do that for you ;-) </p>\n", "abstract": "Part-of-Speech (POS) tagging is very specific to a particular [natural] language.  NLTK includes many different taggers, which use distinct techniques to infer the tag of a given token in a given token.  Most (but not all) of these taggers use a statistical model of sorts as the main or sole device to \"do the trick\".  Such taggers require some \"training data\" upon which to build this statistical representation of the language, and the training data comes in the form of corpora. The NTLK \"distribution\" itself includes many of these corpora, as well a set of \"corpora readers\" which provide an API to read different types of corpora. I don't know the state of affairs in NTLK proper, and if this includes any german corpus.  You can however locate free some free corpora which you'll then need to convert to a format that satisfies the proper NTLK corpora reader, and then you can use this to train a POS tagger for the German language. You can even create your own corpus, but that is a hell of a painstaking job; if you work in a univeristy, you gotta find ways of bribing and otherwise coercing students to do that for you ;-) "}, {"id": 20816272, "score": 4, "vote": 0, "content": "<p>Possibly you can use the Stanford POS tagger. Below is a recipe I wrote. There are python recipes for German NLP that I've compiled and you can access them on <a href=\"http://htmlpreview.github.io/?https://github.com/alvations/DLTK/blob/master/docs/index.html\" rel=\"nofollow\">http://htmlpreview.github.io/?https://github.com/alvations/DLTK/blob/master/docs/index.html</a></p>\n<pre><code class=\"python\">#-*- coding: utf8 -*-\n\nimport os, glob, codecs\n\ndef installStanfordTag():\n    if not os.path.exists('stanford-postagger-full-2013-06-20'):\n        os.system('wget http://nlp.stanford.edu/software/stanford-postagger-full-2013-06-20.zip')\n        os.system('unzip stanford-postagger-full-2013-06-20.zip')\n    return\n\ndef tag(infile):\n    cmd = \"./stanford-postagger.sh \"+models[m]+\" \"+infile\n    tagout = os.popen(cmd).readlines()\n    return [i.strip() for i in tagout]\n\ndef taglinebyline(sents):\n    tagged = []\n    for ss in sents:\n        os.popen(\"echo '''\"+ss+\"''' &gt; stanfordtemp.txt\")\n        tagged.append(tag('stanfordtemp.txt')[0])\n    return tagged\n\ninstallStanfordTag()\nstagdir = './stanford-postagger-full-2013-06-20/'\nmodels = {'fast':'models/german-fast.tagger',\n          'dewac':'models/german-dewac.tagger',\n          'hgc':'models/german-hgc.tagger'}\nos.chdir(stagdir)\nprint os.getcwd()\n\n\nm = 'fast' # It's best to use the fast german tagger if your data is small.\n\nsentences = ['Ich bin schwanger .','Ich bin wieder schwanger .','Ich verstehe nur Bahnhof .']\n\ntagged_sents = taglinebyline(sentences) # Call the stanford tagger\n\nfor sent in tagged_sents:\n    print sent\n</code></pre>\n", "abstract": "Possibly you can use the Stanford POS tagger. Below is a recipe I wrote. There are python recipes for German NLP that I've compiled and you can access them on http://htmlpreview.github.io/?https://github.com/alvations/DLTK/blob/master/docs/index.html"}, {"id": 4758556, "score": 2, "vote": 0, "content": "<p>I have written a blog-post about how to convert the German annotated TIGER Corpus in order to use it with the NLTK. <a href=\"http://web.archive.org/web/20130402043354/http://experimentallabor.de/setting-up-a-pos-tagged-german-corpus-to-use-with-nltk/\" rel=\"nofollow\">Have a look at it here.</a></p>\n", "abstract": "I have written a blog-post about how to convert the German annotated TIGER Corpus in order to use it with the NLTK. Have a look at it here."}, {"id": 70591007, "score": 0, "vote": 0, "content": "<p>It seems to be a little late to answer the question, but it might be helpful for anyone who finds this question by googling like i did. So i'd like to share the things I found out.</p>\n<p>The <a href=\"https://github.com/wartaal/HanTa\" rel=\"nofollow noreferrer\">HannoverTagger</a> might be a useful tool for this Task.\nYou can find tutorials <a href=\"https://github.com/wartaal/HanTa/blob/master/Demo.ipynb\" rel=\"nofollow noreferrer\">here</a> and <a href=\"https://textmining.wp.hs-hannover.de/Preprocessing.html#Lemmatisierung-und-Wortarterkennung\" rel=\"nofollow noreferrer\">here(german)</a>, but the second one is in german.</p>\n<p>The Tagger seems to use the <a href=\"https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html\" rel=\"nofollow noreferrer\">STTS Tagset</a>, if you need a complete list of all Tags.</p>\n", "abstract": "It seems to be a little late to answer the question, but it might be helpful for anyone who finds this question by googling like i did. So i'd like to share the things I found out. The HannoverTagger might be a useful tool for this Task.\nYou can find tutorials here and here(german), but the second one is in german. The Tagger seems to use the STTS Tagset, if you need a complete list of all Tags."}]}, {"link": "https://stackoverflow.com/questions/36034454/what-meaning-does-the-length-of-a-word2vec-vector-have", "question": {"id": "36034454", "title": "What meaning does the length of a Word2vec vector have?", "content": "<p>I am using Word2vec through <a href=\"https://radimrehurek.com/gensim/\" rel=\"noreferrer\"><em>gensim</em></a> with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the <code>Word2Vec</code> object are not unit vectors:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; from gensim.models import Word2Vec\n&gt;&gt;&gt; w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n&gt;&gt;&gt; king_vector = w2v['king']\n&gt;&gt;&gt; numpy.linalg.norm(king_vector)\n2.9022589\n</code></pre>\n<p>However, in the <a href=\"https://github.com/piskvorky/gensim/blob/0.12.4/gensim/models/word2vec.py#L1153-L1213\" rel=\"noreferrer\"><code>most_similar</code></a> method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented <code>.syn0norm</code> property, which contains only unit vectors:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; w2v.init_sims()\n&gt;&gt;&gt; unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]\n&gt;&gt;&gt; numpy.linalg.norm(unit_king_vector)\n0.99999994\n</code></pre>\n<p>The larger vector is just a scaled up version of the unit vector:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; king_vector - numpy.linalg.norm(king_vector) * unit_king_vector\narray([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n        ... (some lines omitted) ...\n        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)\n</code></pre>\n<p>Given that word similarity comparisons in Word2Vec are done by <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noreferrer\">cosine similarity</a>, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean <em>something</em>, since gensim exposes them to me rather than only exposing the unit vectors in <code>.syn0norm</code>.</p>\n<p>How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?</p>\n", "abstract": "I am using Word2vec through gensim with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the Word2Vec object are not unit vectors: However, in the most_similar method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented .syn0norm property, which contains only unit vectors: The larger vector is just a scaled up version of the unit vector: Given that word similarity comparisons in Word2Vec are done by cosine similarity, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean something, since gensim exposes them to me rather than only exposing the unit vectors in .syn0norm. How are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?"}, "answers": [{"id": 50550236, "score": 30, "vote": 0, "content": "<p>I think the answer you are looking for is described in the 2015 paper <a href=\"https://arxiv.org/pdf/1508.02297.pdf\" rel=\"noreferrer\">Measuring Word Significance\nusing\nDistributed Representations of Words</a> by Adriaan Schakel and Benjamin Wilson. The key points:</p>\n<blockquote>\n<p>When a word appears\n  in different contexts, its vector gets moved in\n  different directions during updates. The final vector\n  then represents some sort of weighted average\n  over the various contexts. Averaging over vectors\n  that point in different directions typically results in\n  a vector that gets shorter with increasing number\n  of different contexts in which the word appears.\n  For words to be used in many different contexts,\n  they must carry little meaning. Prime examples of\n  such insignificant words are high-frequency stop\n  words, which are indeed represented by short vectors\n  despite their high term frequencies ...</p>\n</blockquote>\n<hr/>\n<blockquote>\n<p>For given term frequency,\n  the vector length is seen to take values only in a\n  narrow interval. That interval initially shifts upwards\n  with increasing frequency. Around a frequency\n  of about 30, that trend reverses and the interval\n  shifts downwards.</p>\n<p>...</p>\n<p>Both forces determining the length of a word\n  vector are seen at work here. Small-frequency\n  words tend to be used consistently, so that the\n  more frequently such words appear, the longer\n  their vectors. This tendency is reflected by the upwards\n  trend in Fig. 3 at low frequencies. High-frequency\n  words, on the other hand, tend to be\n  used in many different contexts, the more so, the\n  more frequently they occur. The averaging over\n  an increasing number of different contexts shortens\n  the vectors representing such words. This tendency\n  is clearly reflected by the downwards trend\n  in Fig. 3 at high frequencies, culminating in punctuation\n  marks and stop words with short vectors at\n  the very end.</p>\n<p>...</p>\n<p><a href=\"https://i.stack.imgur.com/NI9je.png\" rel=\"noreferrer\"><img alt=\"Graph showing the trend described in the previous excerpt\" src=\"https://i.stack.imgur.com/NI9je.png\"/></a></p>\n<p>Figure 3: Word vector length <em>v</em> versus term frequency\n  <em>tf</em> of all words in the hep-th vocabulary.\n  Note the logarithmic scale used on the frequency\n  axis. The dark symbols denote bin means with the\n  <i>k</i>th bin containing the frequencies in the interval\n  [2<sup><i>k\u22121</i></sup>, 2<sup><i>k</i></sup> \u2212 1] with <em>k</em> = 1, 2, 3, . . .. These means\n  are included as a guide to the eye. The horizontal\n  line indicates the length <em>v</em> = 1.37 of the mean\n  vector</p>\n</blockquote>\n<hr/>\n<blockquote>\n<h3>4 Discussion</h3>\n<p>Most applications of distributed representations of\n  words obtained through word2vec so far centered\n  around semantics. A host of experiments have\n  demonstrated the extent to which the direction of\n  word vectors captures semantics. In this brief report,\n  it was pointed out that not only the direction,\n  but also the length of word vectors carries important\n  information. Specifically, it was shown that\n  word vector length furnishes, in combination with\n  term frequency, a useful measure of word significance. </p>\n</blockquote>\n", "abstract": "I think the answer you are looking for is described in the 2015 paper Measuring Word Significance\nusing\nDistributed Representations of Words by Adriaan Schakel and Benjamin Wilson. The key points: When a word appears\n  in different contexts, its vector gets moved in\n  different directions during updates. The final vector\n  then represents some sort of weighted average\n  over the various contexts. Averaging over vectors\n  that point in different directions typically results in\n  a vector that gets shorter with increasing number\n  of different contexts in which the word appears.\n  For words to be used in many different contexts,\n  they must carry little meaning. Prime examples of\n  such insignificant words are high-frequency stop\n  words, which are indeed represented by short vectors\n  despite their high term frequencies ... For given term frequency,\n  the vector length is seen to take values only in a\n  narrow interval. That interval initially shifts upwards\n  with increasing frequency. Around a frequency\n  of about 30, that trend reverses and the interval\n  shifts downwards. ... Both forces determining the length of a word\n  vector are seen at work here. Small-frequency\n  words tend to be used consistently, so that the\n  more frequently such words appear, the longer\n  their vectors. This tendency is reflected by the upwards\n  trend in Fig. 3 at low frequencies. High-frequency\n  words, on the other hand, tend to be\n  used in many different contexts, the more so, the\n  more frequently they occur. The averaging over\n  an increasing number of different contexts shortens\n  the vectors representing such words. This tendency\n  is clearly reflected by the downwards trend\n  in Fig. 3 at high frequencies, culminating in punctuation\n  marks and stop words with short vectors at\n  the very end. ...  Figure 3: Word vector length v versus term frequency\n  tf of all words in the hep-th vocabulary.\n  Note the logarithmic scale used on the frequency\n  axis. The dark symbols denote bin means with the\n  kth bin containing the frequencies in the interval\n  [2k\u22121, 2k \u2212 1] with k = 1, 2, 3, . . .. These means\n  are included as a guide to the eye. The horizontal\n  line indicates the length v = 1.37 of the mean\n  vector Most applications of distributed representations of\n  words obtained through word2vec so far centered\n  around semantics. A host of experiments have\n  demonstrated the extent to which the direction of\n  word vectors captures semantics. In this brief report,\n  it was pointed out that not only the direction,\n  but also the length of word vectors carries important\n  information. Specifically, it was shown that\n  word vector length furnishes, in combination with\n  term frequency, a useful measure of word significance. "}]}, {"link": "https://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python", "question": {"id": "15016025", "title": "How to print the LDA topics models from gensim? Python", "content": "<p>Using <code>gensim</code> I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models?</p>\n<p>When printing the <code>lda.print_topics(10)</code> the code gave the following error because <code>print_topics()</code> return a <code>NoneType</code>:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"/home/alvas/workspace/XLINGTOP/xlingtop.py\", line 93, in &lt;module&gt;\n    for top in lda.print_topics(2):\nTypeError: 'NoneType' object is not iterable\n</code></pre>\n<p>The code:</p>\n<pre><code class=\"python\">from gensim import corpora, models, similarities\nfrom gensim.models import hdpmodel, ldamodel\nfrom itertools import izip\n\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\n\n# remove common words and tokenize\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in documents]\n\n# remove words that appear only once\nall_tokens = sum(texts, [])\ntokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\ntexts = [[word for word in text if word not in tokens_once]\n         for text in texts]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# I can print out the topics for LSA\nlsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\ncorpus_lsi = lsi[corpus]\n\nfor l,t in izip(corpus_lsi,corpus):\n  print l,\"#\",t\nprint\nfor top in lsi.print_topics(2):\n  print top\n\n# I can print out the documents and which is the most probable topics for each doc.\nlda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=50)\ncorpus_lda = lda[corpus]\n\nfor l,t in izip(corpus_lda,corpus):\n  print l,\"#\",t\nprint\n\n# But I am unable to print out the topics, how should i do it?\nfor top in lda.print_topics(10):\n  print top\n</code></pre>\n", "abstract": "Using gensim I was able to extract topics from a set of documents in LSA but how do I access the topics generated from the LDA models? When printing the lda.print_topics(10) the code gave the following error because print_topics() return a NoneType: The code:"}, "answers": [{"id": 15016117, "score": 21, "vote": 0, "content": "<p>After some messing around, it seems like <code>print_topics(numoftopics)</code> for the <code>ldamodel</code> has some bug. So my workaround is to use <code>print_topic(topicid)</code>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; print lda.print_topics()\nNone\n&gt;&gt;&gt; for i in range(0, lda.num_topics-1):\n&gt;&gt;&gt;  print lda.print_topic(i)\n0.083*response + 0.083*interface + 0.083*time + 0.083*human + 0.083*user + 0.083*survey + 0.083*computer + 0.083*eps + 0.083*trees + 0.083*system\n...\n</code></pre>\n", "abstract": "After some messing around, it seems like print_topics(numoftopics) for the ldamodel has some bug. So my workaround is to use print_topic(topicid):"}, {"id": 29939046, "score": 12, "vote": 0, "content": "<p>I think syntax of show_topics has changed over time:</p>\n<pre><code class=\"python\">show_topics(num_topics=10, num_words=10, log=False, formatted=True)\n</code></pre>\n<p>For num_topics number of topics, return num_words most significant words (10 words per topic, by default).</p>\n<p>The topics are returned as a list \u2013 a list of strings if formatted is True, or a list of (probability, word) 2-tuples if False.</p>\n<p>If log is True, also output this result to log.</p>\n<p>Unlike LSA, there is no natural ordering between the topics in LDA. The returned num_topics &lt;= self.num_topics subset of all topics is therefore arbitrary and may change between two LDA training runs.</p>\n", "abstract": "I think syntax of show_topics has changed over time: For num_topics number of topics, return num_words most significant words (10 words per topic, by default). The topics are returned as a list \u2013 a list of strings if formatted is True, or a list of (probability, word) 2-tuples if False. If log is True, also output this result to log. Unlike LSA, there is no natural ordering between the topics in LDA. The returned num_topics <= self.num_topics subset of all topics is therefore arbitrary and may change between two LDA training runs."}, {"id": 52938577, "score": 9, "vote": 0, "content": "<p>I think it is alway more helpful to see the topics as a list of words. The following code snippet helps acchieve that goal. I assume you already have an lda model called <code>lda_model</code>.</p>\n<pre><code class=\"python\">for index, topic in lda_model.show_topics(formatted=False, num_words= 30):\n    print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n</code></pre>\n<p>In the above code, I have decided to show the first 30 words belonging to each topic. For simplicity, I have shown the first topic I get.</p>\n<pre><code class=\"python\">Topic: 0 \nWords: ['associate', 'incident', 'time', 'task', 'pain', 'amcare', 'work', 'ppe', 'train', 'proper', 'report', 'standard', 'pmv', 'level', 'perform', 'wear', 'date', 'factor', 'overtime', 'location', 'area', 'yes', 'new', 'treatment', 'start', 'stretch', 'assign', 'condition', 'participate', 'environmental']\nTopic: 1 \nWords: ['work', 'associate', 'cage', 'aid', 'shift', 'leave', 'area', 'eye', 'incident', 'aider', 'hit', 'pit', 'manager', 'return', 'start', 'continue', 'pick', 'call', 'come', 'right', 'take', 'report', 'lead', 'break', 'paramedic', 'receive', 'get', 'inform', 'room', 'head']\n</code></pre>\n<p>I don't really like the way the above topics look so I usually modify my code to as shown:</p>\n<pre><code class=\"python\">for idx, topic in lda_model.show_topics(formatted=False, num_words= 30):\n    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))\n</code></pre>\n<p>... and the output (first 2 topics shown) will look like.</p>\n<pre><code class=\"python\">Topic: 0 \nWords: associate|incident|time|task|pain|amcare|work|ppe|train|proper|report|standard|pmv|level|perform|wear|date|factor|overtime|location|area|yes|new|treatment|start|stretch|assign|condition|participate|environmental\nTopic: 1 \nWords: work|associate|cage|aid|shift|leave|area|eye|incident|aider|hit|pit|manager|return|start|continue|pick|call|come|right|take|report|lead|break|paramedic|receive|get|inform|room|head\n</code></pre>\n", "abstract": "I think it is alway more helpful to see the topics as a list of words. The following code snippet helps acchieve that goal. I assume you already have an lda model called lda_model. In the above code, I have decided to show the first 30 words belonging to each topic. For simplicity, I have shown the first topic I get. I don't really like the way the above topics look so I usually modify my code to as shown: ... and the output (first 2 topics shown) will look like."}, {"id": 15197772, "score": 6, "vote": 0, "content": "<p>Are you using any logging? <code>print_topics</code> prints to the logfile as stated in the <a href=\"http://radimrehurek.com/gensim/tut2.html#transforming-vectors\" rel=\"noreferrer\">docs</a>.</p>\n<p>As @mac389 says, <code>lda.show_topics()</code> is the way to go to print to screen.</p>\n", "abstract": "Are you using any logging? print_topics prints to the logfile as stated in the docs. As @mac389 says, lda.show_topics() is the way to go to print to screen."}, {"id": 57684925, "score": 5, "vote": 0, "content": "<p>Using Gensim for cleaning it's own topic format.</p>\n<pre><code class=\"python\">from gensim.parsing.preprocessing import preprocess_string, strip_punctuation,\nstrip_numeric\n\nlda_topics = lda.show_topics(num_words=5)\n\ntopics = []\nfilters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n\nfor topic in lda_topics:\n    print(topic)\n    topics.append(preprocess_string(topic[1], filters))\n\nprint(topics)\n</code></pre>\n<p>Output :</p>\n<pre><code class=\"python\">(0, '0.020*\"business\" + 0.018*\"data\" + 0.012*\"experience\" + 0.010*\"learning\" + 0.008*\"analytics\"')\n(1, '0.027*\"data\" + 0.020*\"experience\" + 0.013*\"business\" + 0.010*\"role\" + 0.009*\"science\"')\n(2, '0.026*\"data\" + 0.016*\"experience\" + 0.012*\"learning\" + 0.011*\"machine\" + 0.009*\"business\"')\n(3, '0.028*\"data\" + 0.015*\"analytics\" + 0.015*\"experience\" + 0.008*\"business\" + 0.008*\"skills\"')\n(4, '0.014*\"data\" + 0.009*\"learning\" + 0.009*\"machine\" + 0.009*\"business\" + 0.008*\"experience\"')\n\n\n[\n  ['business', 'data', 'experience', 'learning', 'analytics'], \n  ['data', 'experience', 'business', 'role', 'science'], \n  ['data', 'experience', 'learning', 'machine', 'business'], \n  ['data', 'analytics', 'experience', 'business', 'skills'], \n  ['data', 'learning', 'machine', 'business', 'experience']\n]\n</code></pre>\n", "abstract": "Using Gensim for cleaning it's own topic format. Output :"}, {"id": 23529054, "score": 3, "vote": 0, "content": "<p>Here is sample code to print topics:</p>\n<pre><code class=\"python\">def ExtractTopics(filename, numTopics=5):\n    # filename is a pickle file where I have lists of lists containing bag of words\n    texts = pickle.load(open(filename, \"rb\"))\n\n    # generate dictionary\n    dict = corpora.Dictionary(texts)\n\n    # remove words with low freq.  3 is an arbitrary number I have picked here\n    low_occerance_ids = [tokenid for tokenid, docfreq in dict.dfs.iteritems() if docfreq == 3]\n    dict.filter_tokens(low_occerance_ids)\n    dict.compactify()\n    corpus = [dict.doc2bow(t) for t in texts]\n    # Generate LDA Model\n    lda = models.ldamodel.LdaModel(corpus, num_topics=numTopics)\n    i = 0\n    # We print the topics\n    for topic in lda.show_topics(num_topics=numTopics, formatted=False, topn=20):\n        i = i + 1\n        print \"Topic #\" + str(i) + \":\",\n        for p, id in topic:\n            print dict[int(id)],\n\n        print \"\"\n</code></pre>\n", "abstract": "Here is sample code to print topics:"}, {"id": 38695625, "score": 3, "vote": 0, "content": "<p>you can use:</p>\n<pre><code class=\"python\">for i in  lda_model.show_topics():\n    print i[0], i[1]\n</code></pre>\n", "abstract": "you can use:"}, {"id": 47005462, "score": 1, "vote": 0, "content": "<p>Recently, came across a similar issue while working with Python 3 and Gensim 2.3.0. <code>print_topics()</code> and <code>show_topics()</code> weren't giving any error but also not printing anything. Turns out that <code>show_topics()</code> returns a list. So one can simply do:</p>\n<pre><code class=\"python\">topic_list = show_topics()\nprint(topic_list)\n</code></pre>\n", "abstract": "Recently, came across a similar issue while working with Python 3 and Gensim 2.3.0. print_topics() and show_topics() weren't giving any error but also not printing anything. Turns out that show_topics() returns a list. So one can simply do:"}, {"id": 50866713, "score": 1, "vote": 0, "content": "<p>You can also export the top words from each topic to a csv file. <code>topn</code> controls how many words under each topic to export. </p>\n<pre><code class=\"python\">import pandas as pd\n\ntop_words_per_topic = []\nfor t in range(lda_model.num_topics):\n    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 5)])\n\npd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")\n</code></pre>\n<p>The CSV file has the following format</p>\n<pre><code class=\"python\">Topic Word  P  \n0     w1    0.004437  \n0     w2    0.003553  \n0     w3    0.002953  \n0     w4    0.002866  \n0     w5    0.008813  \n1     w6    0.003393  \n1     w7    0.003289  \n1     w8    0.003197 \n... \n</code></pre>\n", "abstract": "You can also export the top words from each topic to a csv file. topn controls how many words under each topic to export.  The CSV file has the following format"}, {"id": 54684278, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">****This code works fine but I want to know the topic name instead of Topic: 0 and Topic:1, How do i know which topic this word comes in**?** \n\n\n\nfor index, topic in lda_model.show_topics(formatted=False, num_words= 30):\n        print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n\nTopic: 0 \nWords: ['associate', 'incident', 'time', 'task', 'pain', 'amcare', 'work', 'ppe', 'train', 'proper', 'report', 'standard', 'pmv', 'level', 'perform', 'wear', 'date', 'factor', 'overtime', 'location', 'area', 'yes', 'new', 'treatment', 'start', 'stretch', 'assign', 'condition', 'participate', 'environmental']\nTopic: 1 \nWords: ['work', 'associate', 'cage', 'aid', 'shift', 'leave', 'area', 'eye', 'incident', 'aider', 'hit', 'pit', 'manager', 'return', 'start', 'continue', 'pick', 'call', 'come', 'right', 'take', 'report', 'lead', 'break', 'paramedic', 'receive', 'get', 'inform', 'room', 'head']\n</code></pre>\n", "abstract": ""}]}, {"link": "https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert", "question": {"id": "55619176", "title": "How to cluster similar sentences using BERT", "content": "<p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>\n<p>A good example of the implementation can be seen in this short article: <a href=\"http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/\" rel=\"noreferrer\">http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/</a></p>\n<p>I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence?</p>\n<p>Any information would be helpful.</p>\n", "abstract": "For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a sentence and using HDBSCAN/KMeans clustering to group similar sentences. A good example of the implementation can be seen in this short article: http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/ I would like to do the same thing using BERT (using the BERT python package from hugging face), however I am rather unfamiliar with how to extract the raw word/sentence vectors in order to input them into a clustering algorithm. I know that BERT can output sentence representations - so how would I actually extract the raw vectors from a sentence? Any information would be helpful."}, "answers": [{"id": 62859000, "score": 20, "vote": 0, "content": "<p>You can use <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">Sentence Transformers</a> to generate the sentence embeddings. These embeddings are much more meaningful as compared to the one obtained from bert-as-service, as they have been fine-tuned such that semantically similar sentences have higher similarity score. You can use FAISS based clustering algorithm if number of sentences to be clustered are in millions or more as vanilla K-means like clustering algorithm takes quadratic time.</p>\n", "abstract": "You can use Sentence Transformers to generate the sentence embeddings. These embeddings are much more meaningful as compared to the one obtained from bert-as-service, as they have been fine-tuned such that semantically similar sentences have higher similarity score. You can use FAISS based clustering algorithm if number of sentences to be clustered are in millions or more as vanilla K-means like clustering algorithm takes quadratic time."}, {"id": 56779044, "score": 12, "vote": 0, "content": "<p>You will need to generate bert embeddidngs for the sentences first. \nbert-as-service provides a very easy way to generate embeddings for sentences.</p>\n<p>This is how you can geberate bert vectors for a list of sentences you need to cluster. It is explained very well in the bert-as-service repository:\n<a href=\"https://github.com/hanxiao/bert-as-service\" rel=\"noreferrer\">https://github.com/hanxiao/bert-as-service</a></p>\n<p>Installations:</p>\n<pre><code class=\"python\">pip install bert-serving-server  # server\npip install bert-serving-client  # client, independent of `bert-serving-server`\n</code></pre>\n<p>Download one of the pre-trained models available at <a href=\"https://github.com/google-research/bert\" rel=\"noreferrer\">https://github.com/google-research/bert</a> </p>\n<p>Start the service:</p>\n<pre><code class=\"python\">bert-serving-start -model_dir /your_model_directory/ -num_worker=4 \n</code></pre>\n<p>Generate the vectors for the list of sentences:</p>\n<pre><code class=\"python\">from bert_serving.client import BertClient\nbc = BertClient()\nvectors=bc.encode(your_list_of_sentences)\n</code></pre>\n<p>This would give you a list of vectors, you could write them into a csv and use any clustering algorithm as the sentences are reduced to numbers. </p>\n", "abstract": "You will need to generate bert embeddidngs for the sentences first. \nbert-as-service provides a very easy way to generate embeddings for sentences. This is how you can geberate bert vectors for a list of sentences you need to cluster. It is explained very well in the bert-as-service repository:\nhttps://github.com/hanxiao/bert-as-service Installations: Download one of the pre-trained models available at https://github.com/google-research/bert  Start the service: Generate the vectors for the list of sentences: This would give you a list of vectors, you could write them into a csv and use any clustering algorithm as the sentences are reduced to numbers. "}, {"id": 68728666, "score": 4, "vote": 0, "content": "<p>As <a href=\"https://stackoverflow.com/users/4935974/subham-kumar\">Subham Kumar</a> <a href=\"https://stackoverflow.com/a/62859000/395857\">mentioned</a>, one can use this Python 3 library to compute sentence similarity: <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers</a></p>\n<p>The library has a few <a href=\"https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/clustering\" rel=\"nofollow noreferrer\">code examples</a> to perform clustering:</p>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/fast_clustering.py\" rel=\"nofollow noreferrer\"><code>fast_clustering.py</code></a>:</p>\n<pre><code class=\"python\">\"\"\"\nThis is a more complex example on performing clustering on large scale dataset.\n\nThis examples find in a large set of sentences local communities, i.e., groups of sentences that are highly\nsimilar. You can freely configure the threshold what is considered as similar. A high threshold will\nonly find extremely similar sentences, a lower threshold will find more sentence that are less similar.\n\nA second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.\n\nThe method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).\n\nIn this example, we download a large set of questions from Quora and then find similar questions in this set.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer, util\nimport os\nimport csv\nimport time\n\n\n# Model for computing sentence embeddings. We use one trained for similar questions detection\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n# and find similar question in it\nurl = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\ndataset_path = \"quora_duplicate_questions.tsv\"\nmax_corpus_size = 50000 # We limit our corpus to only the first 50k questions\n\n\n# Check if the dataset exists. If not, download and extract\n# Download dataset if needed\nif not os.path.exists(dataset_path):\n    print(\"Download dataset\")\n    util.http_get(url, dataset_path)\n\n# Get all unique sentences from the file\ncorpus_sentences = set()\nwith open(dataset_path, encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n    for row in reader:\n        corpus_sentences.add(row['question1'])\n        corpus_sentences.add(row['question2'])\n        if len(corpus_sentences) &gt;= max_corpus_size:\n            break\n\ncorpus_sentences = list(corpus_sentences)\nprint(\"Encode the corpus. This might take a while\")\ncorpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n\n\nprint(\"Start clustering\")\nstart_time = time.time()\n\n#Two parameters to tune:\n#min_cluster_size: Only consider cluster that have at least 25 elements\n#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\nclusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)\n\nprint(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n\n#Print for all clusters the top 3 and bottom 3 elements\nfor i, cluster in enumerate(clusters):\n    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n    for sentence_id in cluster[0:3]:\n        print(\"\\t\", corpus_sentences[sentence_id])\n    print(\"\\t\", \"...\")\n    for sentence_id in cluster[-3:]:\n        print(\"\\t\", corpus_sentences[sentence_id])\n\n</code></pre>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py\" rel=\"nofollow noreferrer\"><code>kmeans.py</code></a>:</p>\n<pre><code class=\"python\">\"\"\"\nThis is a simple application for sentence embeddings: clustering\n\nSentences are mapped to sentence embeddings and then k-mean clustering is applied.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\n\nembedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.'\n          ]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Perform kmean clustering\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = [[] for i in range(num_clusters)]\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    clustered_sentences[cluster_id].append(corpus[sentence_id])\n\nfor i, cluster in enumerate(clustered_sentences):\n    print(\"Cluster \", i+1)\n    print(cluster)\n    print(\"\")\n</code></pre>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/agglomerative.py\" rel=\"nofollow noreferrer\"><code>agglomerative.py</code></a>:</p>\n<pre><code class=\"python\">\"\"\"\nThis is a simple application for sentence embeddings: clustering\n\nSentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import AgglomerativeClustering\nimport numpy as np\n\nembedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.'\n          ]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Normalize the embeddings to unit length\ncorpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n\n# Perform kmean clustering\nclustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n\n    clustered_sentences[cluster_id].append(corpus[sentence_id])\n\nfor i, cluster in clustered_sentences.items():\n    print(\"Cluster \", i+1)\n    print(cluster)\n    print(\"\")\n</code></pre>\n", "abstract": "As Subham Kumar mentioned, one can use this Python 3 library to compute sentence similarity: https://github.com/UKPLab/sentence-transformers The library has a few code examples to perform clustering: fast_clustering.py: kmeans.py: agglomerative.py:"}, {"id": 56303299, "score": 3, "vote": 0, "content": "<p>Bert adds a special [CLS] token at the beginning of each sample/sentence. After fine-tuning on a downstream task, the embedding of this [CLS] token or pooled_output as they call it in the hugging face implementation represents the sentence embedding.</p>\n<p>But I think that you don't have labels so you won't be able to fine-tune, therefore you cannot use the pooled_output as a sentence embedding. Instead you should use the word embeddings in encoded_layers which is a tensor with dimensions (12,seq_len, 768). In this tensor you have the embeddings(dimension 768) from each of the 12 layers in Bert. To get the word embeddings you can use the output of the last layer, you can concatenate or sum the output of the last 4 layers and so on.</p>\n<p>Here is the script for extracting the features: <a href=\"https://github.com/ethanjperez/pytorch-pretrained-BERT/blob/master/examples/extract_features.py\" rel=\"nofollow noreferrer\">https://github.com/ethanjperez/pytorch-pretrained-BERT/blob/master/examples/extract_features.py</a></p>\n", "abstract": "Bert adds a special [CLS] token at the beginning of each sample/sentence. After fine-tuning on a downstream task, the embedding of this [CLS] token or pooled_output as they call it in the hugging face implementation represents the sentence embedding. But I think that you don't have labels so you won't be able to fine-tune, therefore you cannot use the pooled_output as a sentence embedding. Instead you should use the word embeddings in encoded_layers which is a tensor with dimensions (12,seq_len, 768). In this tensor you have the embeddings(dimension 768) from each of the 12 layers in Bert. To get the word embeddings you can use the output of the last layer, you can concatenate or sum the output of the last 4 layers and so on. Here is the script for extracting the features: https://github.com/ethanjperez/pytorch-pretrained-BERT/blob/master/examples/extract_features.py"}, {"id": 65180425, "score": 0, "vote": 0, "content": "<p>Not sure if you still need it but recently a paper mentioned how to use document embeddings to cluster documents and extract words from each cluster to represent a topic. Here's the link:\n<a href=\"https://arxiv.org/pdf/2008.09470.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/2008.09470.pdf</a>, <a href=\"https://github.com/ddangelov/Top2Vec\" rel=\"nofollow noreferrer\">https://github.com/ddangelov/Top2Vec</a></p>\n<p>Inspired by the above paper, another algorithm for topic modelling using BERT to generate sentence embeddings is mentioned here:\n<a href=\"https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6</a>, <a href=\"https://github.com/MaartenGr/BERTopic\" rel=\"nofollow noreferrer\">https://github.com/MaartenGr/BERTopic</a></p>\n<p>The above two libraries provide an end-to-end solution to extract topics from a corpus. But if you're interested only in generating sentence embeddings, look at Gensim's doc2vec (<a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\" rel=\"nofollow noreferrer\">https://radimrehurek.com/gensim/models/doc2vec.html</a>) or at sentence-transformers (<a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers</a>) as mentioned in the other answers. If you go with sentence-transformers, it is suggested that you train a model on you're domain specific corpus to get good results.</p>\n", "abstract": "Not sure if you still need it but recently a paper mentioned how to use document embeddings to cluster documents and extract words from each cluster to represent a topic. Here's the link:\nhttps://arxiv.org/pdf/2008.09470.pdf, https://github.com/ddangelov/Top2Vec Inspired by the above paper, another algorithm for topic modelling using BERT to generate sentence embeddings is mentioned here:\nhttps://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6, https://github.com/MaartenGr/BERTopic The above two libraries provide an end-to-end solution to extract topics from a corpus. But if you're interested only in generating sentence embeddings, look at Gensim's doc2vec (https://radimrehurek.com/gensim/models/doc2vec.html) or at sentence-transformers (https://github.com/UKPLab/sentence-transformers) as mentioned in the other answers. If you go with sentence-transformers, it is suggested that you train a model on you're domain specific corpus to get good results."}]}, {"link": "https://stackoverflow.com/questions/4858467/combining-a-tokenizer-into-a-grammar-and-parser-with-nltk", "question": {"id": "4858467", "title": "Combining a Tokenizer into a Grammar and Parser with NLTK", "content": "<p>I am making my way through the NLTK book and I can't seem to do something that would appear to be a natural first step for building a decent grammar.</p>\n<p>My goal is to build a grammar for a particular text corpus. </p>\n<p><em>(Initial question: Should I even try to start a grammar from scratch or should I start with a predefined grammar? If I should start with another grammar, which is a good one to start with for English?)</em></p>\n<p>Suppose I have the following simple grammar:</p>\n<pre><code class=\"python\">simple_grammar = nltk.parse_cfg(\"\"\"\nS -&gt; NP VP\nPP -&gt; P NP\nNP -&gt; Det N | Det N PP\nVP -&gt; V NP | VP PP\nDet -&gt; 'a' | 'A'\nN -&gt; 'car' | 'door'\nV -&gt; 'has'\nP -&gt; 'in' | 'for'\n \"\"\");\n</code></pre>\n<p>This grammar can parse a very simple sentence, such as:</p>\n<pre><code class=\"python\">parser = nltk.ChartParser(simple_grammar)\ntrees = parser.nbest_parse(\"A car has a door\")\n</code></pre>\n<p>Now I want to extend this grammar to handle sentences with other nouns and verbs. How do I add those nouns and verbs to my grammar without manually defining them in the grammar? </p>\n<p>For example, suppose I want to be able to parse the sentence \"A car has wheels\". I know that the supplied tokenizers can <em>magically</em> figure out which words are verbs/nouns, etc. How can I use the output of the tokenizer to tell the grammar that \"wheels\" is a noun?</p>\n", "abstract": "I am making my way through the NLTK book and I can't seem to do something that would appear to be a natural first step for building a decent grammar. My goal is to build a grammar for a particular text corpus.  (Initial question: Should I even try to start a grammar from scratch or should I start with a predefined grammar? If I should start with another grammar, which is a good one to start with for English?) Suppose I have the following simple grammar: This grammar can parse a very simple sentence, such as: Now I want to extend this grammar to handle sentences with other nouns and verbs. How do I add those nouns and verbs to my grammar without manually defining them in the grammar?  For example, suppose I want to be able to parse the sentence \"A car has wheels\". I know that the supplied tokenizers can magically figure out which words are verbs/nouns, etc. How can I use the output of the tokenizer to tell the grammar that \"wheels\" is a noun?"}, "answers": [{"id": 4860826, "score": 16, "vote": 0, "content": "<p>You could run a POS tagger over your text and then adapt your grammar to work on POS tags instead of words.</p>\n<pre><code class=\"python\">&gt; text = nltk.word_tokenize(\"A car has a door\")\n['A', 'car', 'has', 'a', 'door']\n\n&gt; tagged_text = nltk.pos_tag(text)\n[('A', 'DT'), ('car', 'NN'), ('has', 'VBZ'), ('a', 'DT'), ('door', 'NN')]\n\n&gt; pos_tags = [pos for (token,pos) in nltk.pos_tag(text)]\n['DT', 'NN', 'VBZ', 'DT', 'NN']\n\n&gt; simple_grammar = nltk.parse_cfg(\"\"\"\n  S -&gt; NP VP\n  PP -&gt; P NP\n  NP -&gt; Det N | Det N PP\n  VP -&gt; V NP | VP PP\n  Det -&gt; 'DT'\n  N -&gt; 'NN'\n  V -&gt; 'VBZ'\n  P -&gt; 'PP'\n  \"\"\")\n\n&gt; parser = nltk.ChartParser(simple_grammar)\n&gt; tree = parser.parse(pos_tags)\n</code></pre>\n", "abstract": "You could run a POS tagger over your text and then adapt your grammar to work on POS tags instead of words."}, {"id": 11780678, "score": 12, "vote": 0, "content": "<p>I know this is a year later but I wanted to add some thoughts.</p>\n<p>I take a lot of different sentences and tag them with parts of speech for a project I'm working on. From there I was doing as StompChicken suggested, pulling the tags from the tuples (word, tag) and using those tags as the \"terminals\" (the bottom nodes of tree as we create a completely tagged sentence).</p>\n<p>Ultimately this doesn't suite my desire to mark head nouns in noun phrases, since I can't pull the head noun \"word\" into the grammar, since the grammar only has the tags.</p>\n<p>So what I did was instead use the set of (word, tag) tuples to create a dictionary of tags, with all the words with that tag as values for that tag. Then I print this dictionary to the screen/grammar.cfg (context free grammar) file.</p>\n<p>The form I use to print it works perfectly with setting up a parser through loading a grammar file (<code>parser = nltk.load_parser('grammar.cfg')</code>). One of the lines it generates looks like this:</p>\n<p><code>VBG -&gt; \"fencing\" | \"bonging\" | \"amounting\" | \"living\" ... over 30 more words...</code></p>\n<p>So now my grammar has the actual words as terminals and assigns the same tags that <code>nltk.tag_pos</code> does.</p>\n<p>Hope this helps anyone else wanting to automate tagging a large corpus and still have the actual words as terminals in their grammar.</p>\n<pre><code class=\"python\">import nltk\nfrom collections import defaultdict\n\ntag_dict = defaultdict(list)\n\n...\n    \"\"\" (Looping through sentences) \"\"\"\n\n        # Tag\n        tagged_sent = nltk.pos_tag(tokens)\n\n        # Put tags and words into the dictionary\n        for word, tag in tagged_sent:\n            if tag not in tag_dict:\n                tag_dict[tag].append(word)\n            elif word not in tag_dict.get(tag):\n                tag_dict[tag].append(word)\n\n# Printing to screen\nfor tag, words in tag_dict.items():\n    print tag, \"-&gt;\",\n    first_word = True\n    for word in words:\n        if first_word:\n            print \"\\\"\" + word + \"\\\"\",\n            first_word = False\n        else:\n            print \"| \\\"\" + word + \"\\\"\",\n    print ''\n</code></pre>\n", "abstract": "I know this is a year later but I wanted to add some thoughts. I take a lot of different sentences and tag them with parts of speech for a project I'm working on. From there I was doing as StompChicken suggested, pulling the tags from the tuples (word, tag) and using those tags as the \"terminals\" (the bottom nodes of tree as we create a completely tagged sentence). Ultimately this doesn't suite my desire to mark head nouns in noun phrases, since I can't pull the head noun \"word\" into the grammar, since the grammar only has the tags. So what I did was instead use the set of (word, tag) tuples to create a dictionary of tags, with all the words with that tag as values for that tag. Then I print this dictionary to the screen/grammar.cfg (context free grammar) file. The form I use to print it works perfectly with setting up a parser through loading a grammar file (parser = nltk.load_parser('grammar.cfg')). One of the lines it generates looks like this: VBG -> \"fencing\" | \"bonging\" | \"amounting\" | \"living\" ... over 30 more words... So now my grammar has the actual words as terminals and assigns the same tags that nltk.tag_pos does. Hope this helps anyone else wanting to automate tagging a large corpus and still have the actual words as terminals in their grammar."}, {"id": 4876291, "score": 11, "vote": 0, "content": "<p>Parsing is a tricky problem, alot of things can go wrong!</p>\n<p>You want (at least) three components here, a tokenizer, a tagger and finally the parser.</p>\n<p>First you need to tokenize the running text into a list of tokens. This can be as easy as splitting the input string around whitespace, but if you are parsing more general text you will also need to handle numbers and punctuation, which is non trivial. For instance sentence ending periods are often not regarded as part of the word it is attached to, but periods marking an abbreviation often are.</p>\n<p>When you have a list of input tokens you can use a tagger to try to determine the POS of each word, and use it to disambiguate input tag sequences. This has two main advantages: First it speeds up parsing as we no longer have to consider alternate hypothesis licensed by ambiguous words, as the POS-tagger has already done this. Second it improves unknown word handling, ie. words not in your grammar, by also assigning those words a tag (hopefully the right one). Combining a parser and a tagger in this way is commonplace.</p>\n<p>The POS-tags will then make up the pre-terminals in your grammar, The pre-terminals are the left-hand sides of productions with only terminals as their right-hand side. Ie in N -&gt; \"house\", V -&gt; \"jump\" etc. N and V are preterminals. It is fairly common to have the grammar with syntactic, only non-terminals on both-sides, productions and lexical productions, one non-terminal going to one terminal. This makes linguistic sense most of the time, and most CFG-parsers require the grammar to be in this form. However one could represent any CFG in this way by creating \"dummy productions\" from any terminals in RHSes with non-terminals in them.</p>\n<p>It could be neccesary to have some sort of mapping between POS-tags and pre-terminals if you want to make more (or less) fine grained tag distinctions in your grammar than what your tagger outputs. You can then initialize the chart with the results from the tagger, ie. passive items of the appropriate category spanning each input token. Sadly I do not know NTLK, but I'm sure there is a simple way to do this. When the chart is seeded, parsing can contiune as normal, and any parse-trees can be extracted (also including the words) in the regular fashion.</p>\n<p>However, in most practical applications you will find that the parser can return several different analyses as natural language is highly ambiguous. I don't know what kind of text corpus you are trying to parse, but if it's anything like natural language you probably will have to construct some sort of parse-selection model, this will require a treebank,a collection of parse-trees of some size ranging from a couple of hundred to several thousand parses, all depending on your grammar and how accurate results you need. Given this treebank one can automagically infer a PCFG corresponding to it. The PCFG can then be used as a simple model for ranking the parse trees.</p>\n<p>All of this is a lot of work to do yourself. What are you using the  parse results for? Have you looked at other resources in the NTLK or other packages such as the StanfordParser or the BerkeleyParser?</p>\n", "abstract": "Parsing is a tricky problem, alot of things can go wrong! You want (at least) three components here, a tokenizer, a tagger and finally the parser. First you need to tokenize the running text into a list of tokens. This can be as easy as splitting the input string around whitespace, but if you are parsing more general text you will also need to handle numbers and punctuation, which is non trivial. For instance sentence ending periods are often not regarded as part of the word it is attached to, but periods marking an abbreviation often are. When you have a list of input tokens you can use a tagger to try to determine the POS of each word, and use it to disambiguate input tag sequences. This has two main advantages: First it speeds up parsing as we no longer have to consider alternate hypothesis licensed by ambiguous words, as the POS-tagger has already done this. Second it improves unknown word handling, ie. words not in your grammar, by also assigning those words a tag (hopefully the right one). Combining a parser and a tagger in this way is commonplace. The POS-tags will then make up the pre-terminals in your grammar, The pre-terminals are the left-hand sides of productions with only terminals as their right-hand side. Ie in N -> \"house\", V -> \"jump\" etc. N and V are preterminals. It is fairly common to have the grammar with syntactic, only non-terminals on both-sides, productions and lexical productions, one non-terminal going to one terminal. This makes linguistic sense most of the time, and most CFG-parsers require the grammar to be in this form. However one could represent any CFG in this way by creating \"dummy productions\" from any terminals in RHSes with non-terminals in them. It could be neccesary to have some sort of mapping between POS-tags and pre-terminals if you want to make more (or less) fine grained tag distinctions in your grammar than what your tagger outputs. You can then initialize the chart with the results from the tagger, ie. passive items of the appropriate category spanning each input token. Sadly I do not know NTLK, but I'm sure there is a simple way to do this. When the chart is seeded, parsing can contiune as normal, and any parse-trees can be extracted (also including the words) in the regular fashion. However, in most practical applications you will find that the parser can return several different analyses as natural language is highly ambiguous. I don't know what kind of text corpus you are trying to parse, but if it's anything like natural language you probably will have to construct some sort of parse-selection model, this will require a treebank,a collection of parse-trees of some size ranging from a couple of hundred to several thousand parses, all depending on your grammar and how accurate results you need. Given this treebank one can automagically infer a PCFG corresponding to it. The PCFG can then be used as a simple model for ranking the parse trees. All of this is a lot of work to do yourself. What are you using the  parse results for? Have you looked at other resources in the NTLK or other packages such as the StanfordParser or the BerkeleyParser?"}]}, {"link": "https://stackoverflow.com/questions/8683588/understanding-nltk-collocation-scoring-for-bigrams-and-trigrams", "question": {"id": "8683588", "title": "Understanding NLTK collocation scoring for bigrams and trigrams", "content": "<p><strong>Background:</strong></p>\n<p>I am trying to compare pairs of words to see which pair is \"more likely to occur\" in US English than another pair.  My plan is/was to use the collocation facilities in NLTK to score word pairs, with the higher scoring pair being the most likely.</p>\n<p><strong>Approach:</strong></p>\n<p>I coded the following in Python using NLTK (several steps and imports removed for brevity):</p>\n<pre><code class=\"python\">bgm    = nltk.collocations.BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(tokens)\nscored = finder.score_ngrams( bgm.likelihood_ratio  )\nprint scored\n</code></pre>\n<p><strong>Results:</strong></p>\n<p>I then examined the results using 2 word pairs, one of which should be highly likely to co-occur, and one pair which should not (\"roasted cashews\" and \"gasoline cashews\"). I was surprised to see these word pairing score identically:</p>\n<pre><code class=\"python\">[(('roasted', 'cashews'), 5.545177444479562)]\n[(('gasoline', 'cashews'), 5.545177444479562)]\n</code></pre>\n<p>I would have expected 'roasted cashews' to score higher than 'gasoline cashews' in my test.</p>\n<p><strong>Questions:</strong></p>\n<ol>\n<li>Am I misunderstanding the use of collocations?</li>\n<li>Is my code incorrect?</li>\n<li>Is my assumption that the scores should be different wrong, and if so why?</li>\n</ol>\n<p>Thank you very much for any information or help!</p>\n", "abstract": "Background: I am trying to compare pairs of words to see which pair is \"more likely to occur\" in US English than another pair.  My plan is/was to use the collocation facilities in NLTK to score word pairs, with the higher scoring pair being the most likely. Approach: I coded the following in Python using NLTK (several steps and imports removed for brevity): Results: I then examined the results using 2 word pairs, one of which should be highly likely to co-occur, and one pair which should not (\"roasted cashews\" and \"gasoline cashews\"). I was surprised to see these word pairing score identically: I would have expected 'roasted cashews' to score higher than 'gasoline cashews' in my test. Questions: Thank you very much for any information or help!"}, "answers": [{"id": 8684029, "score": 34, "vote": 0, "content": "<p>The NLTK collocations document seems pretty good to me.  <a href=\"http://www.nltk.org/howto/collocations.html\" rel=\"noreferrer\">http://www.nltk.org/howto/collocations.html</a></p>\n<p>You need to give the scorer some actual sizable corpus to work with.  Here is a working example using the Brown corpus built into NLTK.  It takes about 30 seconds to run.</p>\n<pre><code class=\"python\">import nltk.collocations\nimport nltk.corpus\nimport collections\n\nbgm    = nltk.collocations.BigramAssocMeasures()\nfinder = nltk.collocations.BigramCollocationFinder.from_words(\n    nltk.corpus.brown.words())\nscored = finder.score_ngrams( bgm.likelihood_ratio  )\n\n# Group bigrams by first word in bigram.                                        \nprefix_keys = collections.defaultdict(list)\nfor key, scores in scored:\n   prefix_keys[key[0]].append((key[1], scores))\n\n# Sort keyed bigrams by strongest association.                                  \nfor key in prefix_keys:\n   prefix_keys[key].sort(key = lambda x: -x[1])\n\nprint 'doctor', prefix_keys['doctor'][:5]\nprint 'baseball', prefix_keys['baseball'][:5]\nprint 'happy', prefix_keys['happy'][:5]\n</code></pre>\n<p>The output seems reasonable, works well for baseball, less so for doctor and happy.</p>\n<pre><code class=\"python\">doctor [('bills', 35.061321987405748), (',', 22.963930079491501), \n  ('annoys', 19.009636692022365), \n  ('had', 16.730384189212423), ('retorted', 15.190847940499127)]\n\nbaseball [('game', 32.110754519752291), ('cap', 27.81891372457088), \n  ('park', 23.509042621473505), ('games', 23.105033513054011), \n  (\"player's\",    16.227872863424668)]\n\nhappy [(\"''\", 20.296341424483998), ('Spahn', 13.915820697905589), \n ('family', 13.734352182441569), \n (',', 13.55077617193821), ('bodybuilder', 13.513265447290536)\n</code></pre>\n", "abstract": "The NLTK collocations document seems pretty good to me.  http://www.nltk.org/howto/collocations.html You need to give the scorer some actual sizable corpus to work with.  Here is a working example using the Brown corpus built into NLTK.  It takes about 30 seconds to run. The output seems reasonable, works well for baseball, less so for doctor and happy."}]}, {"link": "https://stackoverflow.com/questions/59956670/parsing-city-of-origin-destination-city-from-a-string", "question": {"id": "59956670", "title": "Parsing city of origin / destination city from a string", "content": "<p>I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').</p>\n<p>The data:</p>\n<pre><code class=\"python\">df_col = [\n    'new york to venice, italy for usd271',\n    'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n    'from los angeles to guadalajara, mexico for usd191',\n    'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags'\n]\n</code></pre>\n<p>This should result in:</p>\n<pre><code class=\"python\">Origin: New York, USA; Destination: Venice, Italy\nOrigin: Brussels, BEL; Destination: Bangkok, Thailand\nOrigin: Los Angeles, USA; Destination: Guadalajara, Mexico\nOrigin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)\n</code></pre>\n<p>Thus far I have tried:\nA variety of NLTK methods, but what has gotten me closest is using the <code>nltk.pos_tag</code> method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...</p>\n<pre><code class=\"python\">[('Fly', 'NNP'), ('to', 'TO'), ('Australia', 'NNP'), ('&amp;', 'CC'), ('New', 'NNP'), ('Zealand', 'NNP'), ('from', 'IN'), ('Paris', 'NNP'), ('from', 'IN'), ('\u00e2\u201a\u00ac422', 'NNP'), ('return', 'NN'), ('including', 'VBG'), ('2', 'CD'), ('checked', 'VBD'), ('bags', 'NNS'), ('!', '.')]\n</code></pre>\n<p>I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks.</p>\n", "abstract": "I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination'). The data: This should result in: Thus far I have tried:\nA variety of NLTK methods, but what has gotten me closest is using the nltk.pos_tag method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example... I am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks."}, "answers": [{"id": 59959188, "score": 146, "vote": 0, "content": "<h1>TL;DR</h1>\n<p>Pretty much impossible at first glance, unless you have access to some API that contains pretty sophisticated components. </p>\n<h1>In Long</h1>\n<p>From first look, it seems like you're asking to solve a natural language problem magically. But lets break it down and scope it to a point where something is buildable. </p>\n<p>First, to identify countries and cities, you need data that enumerates them, so lets try: <a href=\"https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json\" rel=\"noreferrer\">https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json</a> </p>\n<p>And top of the search results, we find <a href=\"https://datahub.io/core/world-cities\" rel=\"noreferrer\">https://datahub.io/core/world-cities</a> that leads to the world-cities.json file. Now we load them into sets of countries and cities. </p>\n<pre><code class=\"python\">import requests\nimport json\n\ncities_url = \"https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json\"\ncities_json = json.loads(requests.get(cities_url).content.decode('utf8'))\n\ncountries = set([city['country'] for city in cities_json])\ncities = set([city['name'] for city in cities_json])\n</code></pre>\n<h2>Now given data, lets try to build <strong>component ONE</strong>:</h2>\n<ul>\n<li><strong>Task:</strong> Detect if any substring in the texts matches a city/country.</li>\n<li><strong>Tool:</strong> <a href=\"https://github.com/vi3k6i5/flashtext\" rel=\"noreferrer\">https://github.com/vi3k6i5/flashtext</a> (a fast string search/match)</li>\n<li><strong>Metric:</strong> No. of correctly identified cities/countries in string</li>\n</ul>\n<p>Lets put them together.</p>\n<pre><code class=\"python\">import requests\nimport json\nfrom flashtext import KeywordProcessor\n\ncities_url = \"https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json\"\ncities_json = json.loads(requests.get(cities_url).content.decode('utf8'))\n\ncountries = set([city['country'] for city in cities_json])\ncities = set([city['name'] for city in cities_json])\n\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\n\ntexts = ['new york to venice, italy for usd271',\n'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n'from los angeles to guadalajara, mexico for usd191',\n'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags']\nkeyword_processor.extract_keywords(texts[0])\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">['York', 'Venice', 'Italy']\n</code></pre>\n<h2>Hey, what went wrong?!</h2>\n<p>Doing due diligence, first hunch is that \"new york\" is not in the data, </p>\n<pre><code class=\"python\">&gt;&gt;&gt; \"New York\" in cities\nFalse\n</code></pre>\n<p>What the?! #$%^&amp;* For sanity sake, we check these:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; len(countries)\n244\n&gt;&gt;&gt; len(cities)\n21940\n</code></pre>\n<p>Yes, you cannot just trust a single data source, so lets try to fetch all data sources.</p>\n<p>From <a href=\"https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json\" rel=\"noreferrer\">https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json</a>, you find another link <a href=\"https://github.com/dr5hn/countries-states-cities-database\" rel=\"noreferrer\">https://github.com/dr5hn/countries-states-cities-database</a> Lets munge this...</p>\n<pre><code class=\"python\">import requests\nimport json\n\ncities_url = \"https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json\"\ncities1_json = json.loads(requests.get(cities_url).content.decode('utf8'))\n\ncountries1 = set([city['country'] for city in cities1_json])\ncities1 = set([city['name'] for city in cities1_json])\n\ndr5hn_cities_url = \"https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/cities.json\"\ndr5hn_countries_url = \"https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/countries.json\"\n\ncities2_json = json.loads(requests.get(dr5hn_cities_url).content.decode('utf8'))\ncountries2_json = json.loads(requests.get(dr5hn_countries_url).content.decode('utf8'))\n\ncountries2 = set([c['name'] for c in countries2_json])\ncities2 = set([c['name'] for c in cities2_json])\n\ncountries = countries2.union(countries1)\ncities = cities2.union(cities1)\n</code></pre>\n<h2>And now that we are neurotic, we do sanity checks.</h2>\n<pre><code class=\"python\">&gt;&gt;&gt; len(countries)\n282\n&gt;&gt;&gt; len(cities)\n127793\n</code></pre>\n<p>Wow, that's a lot more cities than previously. </p>\n<p>Lets try the <code>flashtext</code> code again.</p>\n<pre><code class=\"python\">from flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\ntexts = ['new york to venice, italy for usd271',\n'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n'from los angeles to guadalajara, mexico for usd191',\n'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags']\n\nkeyword_processor.extract_keywords(texts[0])\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">['York', 'Venice', 'Italy']\n</code></pre>\n<h2>Seriously?! There is no New York?! $%^&amp;*</h2>\n<p>Okay, for more sanity checks, lets just look for \"york\" in the list of cities.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; [c for c in cities if 'york' in c.lower()]\n['Yorklyn',\n 'West York',\n 'West New York',\n 'Yorktown Heights',\n 'East Riding of Yorkshire',\n 'Yorke Peninsula',\n 'Yorke Hill',\n 'Yorktown',\n 'Jefferson Valley-Yorktown',\n 'New York Mills',\n 'City of York',\n 'Yorkville',\n 'Yorkton',\n 'New York County',\n 'East York',\n 'East New York',\n 'York Castle',\n 'York County',\n 'Yorketown',\n 'New York City',\n 'York Beach',\n 'Yorkshire',\n 'North Yorkshire',\n 'Yorkeys Knob',\n 'York',\n 'York Town',\n 'York Harbor',\n 'North York']\n</code></pre>\n<h2>Eureka! It's because it's call \"New York City\" and not \"New York\"!</h2>\n<p><strong>You:</strong> What kind of prank is this?! </p>\n<p><strong>Linguist:</strong> Welcome to the world of <strong>natural language</strong> processing, where natural language is a social construct subjective to communal and idiolectal variant. </p>\n<p><strong>You</strong>: Cut the crap, tell me how to solve this. </p>\n<p><strong>NLP Practitioner</strong> (A real one that works on noisy user-generate texts): You just have to add to the list. But before that, check your <em>metric</em> given the list you already have.</p>\n<h3>For every texts in your sample \"test set\", you should provide some truth labels to make sure you can \"measure your metric\".</h3>\n<pre><code class=\"python\">from itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\ntexts_labels = [('new york to venice, italy for usd271', ('New York', 'Venice', 'Italy')),\n('return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407', ('Brussels', 'Bangkok')),\n('from los angeles to guadalajara, mexico for usd191', ('Los Angeles', 'Guadalajara')),\n('fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags', ('Australia', 'New Zealand', 'Paris'))]\n\n# No. of correctly extracted terms.\ntrue_positives = 0\nfalse_positives = 0\ntotal_truth = 0\n\nfor text, label in texts_labels:\n    extracted = keyword_processor.extract_keywords(text)\n\n    # We're making some assumptions here that the order of \n    # extracted and the truth must be the same.\n    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)\n    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)\n    total_truth += len(label)\n\n    # Just visualization candies.\n    print(text)\n    print(extracted)\n    print(label)\n    print()\n</code></pre>\n<p>Actually, it doesn't look that bad. We get an accuracy of 90%:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; true_positives / total_truth\n0.9\n</code></pre>\n<h3>But I %^&amp;*(-ing want 100% extraction!!</h3>\n<p>Alright, alright, so look at the \"only\" error that the above approach is making, it's simply that \"New York\" isn't in the list of cities. </p>\n<p><strong>You</strong>: Why don't we just add \"New York\" to the list of cities, i.e. </p>\n<pre><code class=\"python\">keyword_processor.add_keyword('New York')\n\nprint(texts[0])\nprint(keyword_processor.extract_keywords(texts[0]))\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">['New York', 'Venice', 'Italy']\n</code></pre>\n<p><strong>You</strong>: See, I did it!!! Now I deserve a beer.\n<strong>Linguist</strong>: How about <code>'I live in Marawi'</code>?</p>\n<pre><code class=\"python\">&gt;&gt;&gt; keyword_processor.extract_keywords('I live in Marawi')\n[]\n</code></pre>\n<p><strong>NLP Practitioner</strong> (chiming in): How about <code>'I live in Jeju'</code>? </p>\n<pre><code class=\"python\">&gt;&gt;&gt; keyword_processor.extract_keywords('I live in Jeju')\n[]\n</code></pre>\n<p><strong>A Raymond Hettinger fan</strong> (from farway): \"There must be a better way!\"</p>\n<p>Yes, there is what if we just try something silly like adding keywords of cities that ends with \"City\" into our <code>keyword_processor</code>?</p>\n<pre><code class=\"python\">for c in cities:\n    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n            print(c[:-5])\n</code></pre>\n<h1>It works!</h1>\n<p>Now lets retry our regression test examples:</p>\n<pre><code class=\"python\">from itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\ntexts_labels = [('new york to venice, italy for usd271', ('New York', 'Venice', 'Italy')),\n('return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407', ('Brussels', 'Bangkok')),\n('from los angeles to guadalajara, mexico for usd191', ('Los Angeles', 'Guadalajara')),\n('fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags', ('Australia', 'New Zealand', 'Paris')),\n('I live in Florida', ('Florida')), \n('I live in Marawi', ('Marawi')), \n('I live in jeju', ('Jeju'))]\n\n# No. of correctly extracted terms.\ntrue_positives = 0\nfalse_positives = 0\ntotal_truth = 0\n\nfor text, label in texts_labels:\n    extracted = keyword_processor.extract_keywords(text)\n\n    # We're making some assumptions here that the order of \n    # extracted and the truth must be the same.\n    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)\n    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)\n    total_truth += len(label)\n\n    # Just visualization candies.\n    print(text)\n    print(extracted)\n    print(label)\n    print()\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">new york to venice, italy for usd271\n['New York', 'Venice', 'Italy']\n('New York', 'Venice', 'Italy')\n\nreturn flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407\n['Brussels', 'Bangkok']\n('Brussels', 'Bangkok')\n\nfrom los angeles to guadalajara, mexico for usd191\n['Los Angeles', 'Guadalajara', 'Mexico']\n('Los Angeles', 'Guadalajara')\n\nfly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags\n['Australia', 'New Zealand', 'Paris']\n('Australia', 'New Zealand', 'Paris')\n\nI live in Florida\n['Florida']\nFlorida\n\nI live in Marawi\n['Marawi']\nMarawi\n\nI live in jeju\n['Jeju']\nJeju\n</code></pre>\n<h3>100% Yeah, NLP-bunga !!!</h3>\n<p>But seriously, this is only the tip of the problem. What happens if you have a sentence like this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; keyword_processor.extract_keywords('Adam flew to Bangkok from Singapore and then to China')\n['Adam', 'Bangkok', 'Singapore', 'China']\n</code></pre>\n<p><strong>WHY is <code>Adam</code> extracted as a city?!</strong></p>\n<p>Then you do some more neurotic checks:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; 'Adam' in cities\nAdam\n</code></pre>\n<p>Congratulations, you've jumped into another NLP rabbit hole of polysemy where the same word has different meaning, in this case, <code>Adam</code> most probably refer to a person in the sentence but it is also coincidentally the name of a city (according to the data you've pulled from).</p>\n<h3>I see what you did there... Even if we ignore this polysemy nonsense, you are still not giving me the desired output:</h3>\n<p>[in]:</p>\n<pre><code class=\"python\">['new york to venice, italy for usd271',\n'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n'from los angeles to guadalajara, mexico for usd191',\n'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags'\n]\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">Origin: New York, USA; Destination: Venice, Italy\nOrigin: Brussels, BEL; Destination: Bangkok, Thailand\nOrigin: Los Angeles, USA; Destination: Guadalajara, Mexico\nOrigin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)\n</code></pre>\n<p><strong>Linguist</strong>: Even with the assumption that the preposition (e.g. <code>from</code>, <code>to</code>) preceding the city gives you the \"origin\" / \"destination\" tag, how are you going to handle the case of \"multi-leg\" flights, e.g. </p>\n<pre><code class=\"python\">&gt;&gt;&gt; keyword_processor.extract_keywords('Adam flew to Bangkok from Singapore and then to China')\n</code></pre>\n<p>What's the desired output of this sentence:</p>\n<pre><code class=\"python\">&gt; Adam flew to Bangkok from Singapore and then to China\n</code></pre>\n<p>Perhaps like this? What is the specification? How (un-)structured is your input text?</p>\n<pre><code class=\"python\">&gt; Origin: Singapore\n&gt; Departure: Bangkok\n&gt; Departure: China\n</code></pre>\n<h2>Lets try to build component TWO to detect prepositions.</h2>\n<p>Lets take that assumption you have and try some hacks to the same <code>flashtext</code> methods. </p>\n<p><strong>What if we add <code>to</code> and <code>from</code> to the list?</strong></p>\n<pre><code class=\"python\">from itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\nkeyword_processor.add_keyword('to')\nkeyword_processor.add_keyword('from')\n\ntexts = ['new york to venice, italy for usd271',\n'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n'from los angeles to guadalajara, mexico for usd191',\n'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags']\n\n\nfor text in texts:\n    extracted = keyword_processor.extract_keywords(text)\n    print(text)\n    print(extracted)\n    print()\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">new york to venice, italy for usd271\n['New York', 'to', 'Venice', 'Italy']\n\nreturn flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407\n['from', 'Brussels', 'to', 'Bangkok', 'from']\n\nfrom los angeles to guadalajara, mexico for usd191\n['from', 'Los Angeles', 'to', 'Guadalajara', 'Mexico']\n\nfly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags\n['to', 'Australia', 'New Zealand', 'from', 'Paris', 'from']\n</code></pre>\n<h3>Heh, that's pretty crappy rule to use to/from,</h3>\n<ol>\n<li>What if the \"from\" is referring the price of the ticket?</li>\n<li>What if there's no \"to/from\" preceding the country/city? </li>\n</ol>\n<p>Okay, lets work with the above output and see what we do about the problem 1. <strong>Maybe check if the term after the from is city, if not, remove the to/from?</strong></p>\n<pre><code class=\"python\">from itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if 'city' in c.lower() and c.endswith('City') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\nkeyword_processor.add_keyword('to')\nkeyword_processor.add_keyword('from')\n\ntexts = ['new york to venice, italy for usd271',\n'return flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407',\n'from los angeles to guadalajara, mexico for usd191',\n'fly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags']\n\n\nfor text in texts:\n    extracted = keyword_processor.extract_keywords(text)\n    print(text)\n\n    new_extracted = []\n    extracted_next = extracted[1:]\n    for e_i, e_iplus1 in zip_longest(extracted, extracted_next):\n        if e_i == 'from' and e_iplus1 not in cities and e_iplus1 not in countries:\n            print(e_i, e_iplus1)\n            continue\n        elif e_i == 'from' and e_iplus1 == None: # last word in the list.\n            continue\n        else:\n            new_extracted.append(e_i)\n\n    print(new_extracted)\n    print()\n</code></pre>\n<p>That seems to do the trick and remove the <code>from</code> that doesn't precede a city/country. </p>\n<p>[out]:</p>\n<pre><code class=\"python\">new york to venice, italy for usd271\n['New York', 'to', 'Venice', 'Italy']\n\nreturn flights from brussels to bangkok with etihad from \u00e2\u201a\u00ac407\nfrom None\n['from', 'Brussels', 'to', 'Bangkok']\n\nfrom los angeles to guadalajara, mexico for usd191\n['from', 'Los Angeles', 'to', 'Guadalajara', 'Mexico']\n\nfly to australia new zealand from paris from \u00e2\u201a\u00ac422 return including 2 checked bags\nfrom None\n['to', 'Australia', 'New Zealand', 'from', 'Paris']\n</code></pre>\n<h3>But the \"from New York\" still isn't solve!!</h3>\n<p><strong>Linguist</strong>: Think carefully, should ambiguity be resolved by making an informed decision to make ambiguous phrase obvious? If so, what is the \"information\" in the informed decision? Should it follow a certain template first to detect the information before filling in the ambiguity?</p>\n<p><strong>You</strong>: I'm losing my patience with you... You're bringing me in circles and circles, where's that AI that can understand human language that I keep hearing from the news and Google and Facebook and all?!</p>\n<p><strong>You</strong>: What you gave me are rule based and where's the AI in all these?</p>\n<p><strong>NLP Practitioner</strong>: Didn't you wanted 100%? Writing \"business logics\" or rule-based systems would be the only way to really achieve that \"100%\" given a specific data set without any preset data set that one can use for \"training an AI\".</p>\n<p><strong>You</strong>: What do you mean by training an AI? Why can't I just use Google or Facebook or Amazon or Microsoft or even IBM's AI? </p>\n<p><strong>NLP Practitioner</strong>: Let me introduce you to </p>\n<ul>\n<li><a href=\"https://learning.oreilly.com/library/view/data-science-from/9781492041122/\" rel=\"noreferrer\">https://learning.oreilly.com/library/view/data-science-from/9781492041122/</a></li>\n<li><a href=\"https://allennlp.org/tutorials\" rel=\"noreferrer\">https://allennlp.org/tutorials</a></li>\n<li><a href=\"https://www.aclweb.org/anthology/\" rel=\"noreferrer\">https://www.aclweb.org/anthology/</a></li>\n</ul>\n<p>Welcome to the world of Computational Linguistics and NLP!</p>\n<h1>In Short</h1>\n<p>Yes, there's no real ready-made magical solution and if you want to use an \"AI\" or machine learning algorithm, most probably you would need a lot more training data like the <code>texts_labels</code> pairs shown in the above example.</p>\n", "abstract": "Pretty much impossible at first glance, unless you have access to some API that contains pretty sophisticated components.  From first look, it seems like you're asking to solve a natural language problem magically. But lets break it down and scope it to a point where something is buildable.  First, to identify countries and cities, you need data that enumerates them, so lets try: https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json  And top of the search results, we find https://datahub.io/core/world-cities that leads to the world-cities.json file. Now we load them into sets of countries and cities.  Lets put them together. [out]: Doing due diligence, first hunch is that \"new york\" is not in the data,  What the?! #$%^&* For sanity sake, we check these: Yes, you cannot just trust a single data source, so lets try to fetch all data sources. From https://www.google.com/search?q=list+of+countries+and+cities+in+the+world+json, you find another link https://github.com/dr5hn/countries-states-cities-database Lets munge this... Wow, that's a lot more cities than previously.  Lets try the flashtext code again. [out]: Okay, for more sanity checks, lets just look for \"york\" in the list of cities. You: What kind of prank is this?!  Linguist: Welcome to the world of natural language processing, where natural language is a social construct subjective to communal and idiolectal variant.  You: Cut the crap, tell me how to solve this.  NLP Practitioner (A real one that works on noisy user-generate texts): You just have to add to the list. But before that, check your metric given the list you already have. Actually, it doesn't look that bad. We get an accuracy of 90%: Alright, alright, so look at the \"only\" error that the above approach is making, it's simply that \"New York\" isn't in the list of cities.  You: Why don't we just add \"New York\" to the list of cities, i.e.  [out]: You: See, I did it!!! Now I deserve a beer.\nLinguist: How about 'I live in Marawi'? NLP Practitioner (chiming in): How about 'I live in Jeju'?  A Raymond Hettinger fan (from farway): \"There must be a better way!\" Yes, there is what if we just try something silly like adding keywords of cities that ends with \"City\" into our keyword_processor? Now lets retry our regression test examples: [out]: But seriously, this is only the tip of the problem. What happens if you have a sentence like this: WHY is Adam extracted as a city?! Then you do some more neurotic checks: Congratulations, you've jumped into another NLP rabbit hole of polysemy where the same word has different meaning, in this case, Adam most probably refer to a person in the sentence but it is also coincidentally the name of a city (according to the data you've pulled from). [in]: [out]: Linguist: Even with the assumption that the preposition (e.g. from, to) preceding the city gives you the \"origin\" / \"destination\" tag, how are you going to handle the case of \"multi-leg\" flights, e.g.  What's the desired output of this sentence: Perhaps like this? What is the specification? How (un-)structured is your input text? Lets take that assumption you have and try some hacks to the same flashtext methods.  What if we add to and from to the list? [out]: Okay, lets work with the above output and see what we do about the problem 1. Maybe check if the term after the from is city, if not, remove the to/from? That seems to do the trick and remove the from that doesn't precede a city/country.  [out]: Linguist: Think carefully, should ambiguity be resolved by making an informed decision to make ambiguous phrase obvious? If so, what is the \"information\" in the informed decision? Should it follow a certain template first to detect the information before filling in the ambiguity? You: I'm losing my patience with you... You're bringing me in circles and circles, where's that AI that can understand human language that I keep hearing from the news and Google and Facebook and all?! You: What you gave me are rule based and where's the AI in all these? NLP Practitioner: Didn't you wanted 100%? Writing \"business logics\" or rule-based systems would be the only way to really achieve that \"100%\" given a specific data set without any preset data set that one can use for \"training an AI\". You: What do you mean by training an AI? Why can't I just use Google or Facebook or Amazon or Microsoft or even IBM's AI?  NLP Practitioner: Let me introduce you to  Welcome to the world of Computational Linguistics and NLP! Yes, there's no real ready-made magical solution and if you want to use an \"AI\" or machine learning algorithm, most probably you would need a lot more training data like the texts_labels pairs shown in the above example."}]}, {"link": "https://stackoverflow.com/questions/56927602/unable-to-load-the-spacy-model-en-core-web-lg-on-google-colab", "question": {"id": "56927602", "title": "Unable to load the spacy model &#39;en_core_web_lg&#39; on Google colab", "content": "<p>I am using spacy in google colab to build an NER model for which I have downloaded the spaCy 'en_core_web_lg' model using</p>\n<pre><code class=\"python\">import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\n</code></pre>\n<p>and I get a message saying</p>\n<pre><code class=\"python\">\u2714 Download and installation successful\nYou can now load the model via spacy.load('en_core_web_lg')\n</code></pre>\n<p>However then when i try to load the model</p>\n<pre><code class=\"python\">nlp = spacy.load('en_core_web_lg')\n</code></pre>\n<p>the following error is printed:</p>\n<pre><code class=\"python\">OSError: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n</code></pre>\n<p>Could anyone help me with this problem?</p>\n", "abstract": "I am using spacy in google colab to build an NER model for which I have downloaded the spaCy 'en_core_web_lg' model using and I get a message saying However then when i try to load the model the following error is printed: Could anyone help me with this problem?"}, "answers": [{"id": 56949134, "score": 78, "vote": 0, "content": "<p>Running</p>\n<pre><code class=\"python\">import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\nnlp = spacy.load(\"en_core_web_lg\")\n</code></pre>\n<p>shouldn't yield any errors anymore with recent spaCy versions.</p>\n<p>If running the code still gives errors, you should be all set with running in one cell (takes a while, but gives you visual feedback about progress, differently from <code>spacy.cli</code>)</p>\n<pre><code class=\"python\">!python -m spacy download en_core_web_lg\n</code></pre>\n<p>Then, *** <strong>restart the colab runtime</strong> *** via</p>\n<ul>\n<li>the colab menu <code>Runtime &gt; Restart runtime</code>, or</li>\n<li>use the keyboard shortcut <code>Ctrl+M .</code></li>\n</ul>\n<p>After that, executing</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load('en_core_web_lg')\n</code></pre>\n<p>should work flawlessly.</p>\n", "abstract": "Running shouldn't yield any errors anymore with recent spaCy versions. If running the code still gives errors, you should be all set with running in one cell (takes a while, but gives you visual feedback about progress, differently from spacy.cli) Then, *** restart the colab runtime *** via After that, executing should work flawlessly."}, {"id": 58773322, "score": 21, "vote": 0, "content": "<p>In Google Colab Notebooks, you should <a href=\"https://spacy.io/usage/models#usage-import\" rel=\"noreferrer\">import the model as a package</a>.</p>\n<p>However you download and install the model:</p>\n<pre><code class=\"python\">!pip install &lt;model_s3_url&gt; # tar.gz file e.g. from release notes like https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-2.3.1\n!pip install en_core_web_lg\nimport spacy\n</code></pre>\n<p>you don't have permission in Colab to load the model with normal spacy usage:</p>\n<pre><code class=\"python\">nlp = spacy.load(\"en_core_web_lg\") # not via packages\nnlp = spacy.load(\"/path/to/en_core_web_lg\") #not via paths\nnlp = spacy.load(\"en\") # nor via shortcut links\nspacy.load()\n</code></pre>\n<p>Instead, import the <em><strong>model</strong></em> and load it directly:</p>\n<pre><code class=\"python\">import en_core_web_lg\nnlp = en_core_web_lg.load()\n</code></pre>\n<p>Then use as directed:</p>\n<pre><code class=\"python\">doc = nlp(\"This is a sentence. Soon, it will be knowledge.\")\n</code></pre>\n", "abstract": "In Google Colab Notebooks, you should import the model as a package. However you download and install the model: you don't have permission in Colab to load the model with normal spacy usage: Instead, import the model and load it directly: Then use as directed:"}, {"id": 59197634, "score": 5, "vote": 0, "content": "<p>It seems the best answer is on this thread: <a href=\"https://stackoverflow.com/questions/49259404/how-to-install-models-download-packages-on-google-colab?rq=1\">How to install models/download packages on Google Colab?</a> </p>\n<pre><code class=\"python\">import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\n</code></pre>\n", "abstract": "It seems the best answer is on this thread: How to install models/download packages on Google Colab? "}, {"id": 56960353, "score": 0, "vote": 0, "content": "<p>I ran into a similar issue on google colab with: </p>\n<pre><code class=\"python\">nlp = spacy.load('en_core_web_md') \n</code></pre>\n<p>I suspect it may have something to do with the size of the model.  It worked for me using the small spacy model.</p>\n<pre><code class=\"python\">spacy download en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\n</code></pre>\n", "abstract": "I ran into a similar issue on google colab with:  I suspect it may have something to do with the size of the model.  It worked for me using the small spacy model."}]}, {"link": "https://stackoverflow.com/questions/60186935/how-to-build-semantic-search-for-a-given-domain", "question": {"id": "60186935", "title": "How to build semantic search for a given domain", "content": "<p>There is a problem we are trying to solve where we want to do a semantic search on our set of data,\ni.e we have a domain-specific data (example: sentences talking about automobiles)</p>\n<p>Our data is just a bunch of sentences and what we want is to give a phrase and get back the sentences which are:</p>\n<ol>\n<li>Similar to that phrase</li>\n<li>Has a part of a sentence that is similar to the phrase</li>\n<li>A sentence which is having contextually similar meanings </li>\n</ol>\n<p><br/></p>\n<p>Let me try giving you an example suppose I search for the phrase \"Buying Experience\", I should get the sentences like:</p>\n<ul>\n<li>I never thought car buying could take less than 30 minutes to sign\nand buy.</li>\n<li><p>I found a car that i liked and the purchase process was<br/>\nstraightforward and easy</p></li>\n<li><p>I absolutely hated going car shopping, but today i\u2019m glad i did</p></li>\n</ul>\n<p><br>\nI want to lay emphasis on the fact that we are looking for <strong>contextual similarity</strong> and not just a brute force word search.</br></p>\n<p>If the sentence uses different words then also it should be able to find it.</p>\n<p>Things that we have already tried:</p>\n<ol>\n<li><p><a href=\"https://www.opensemanticsearch.org/\" rel=\"noreferrer\">Open Semantic Search</a> the problem we faced here is generating ontology from the data we have, or\nfor that sake searching for available ontology from different domains of our interest.</p></li>\n<li><p>Elastic Search(BM25 + Vectors(tf-idf)), we tried this where it gave a few sentences but precision was not that great. The accuracy was bad\nas well. We tried against a human-curated dataset, it was able to get around 10% of the sentences only.</p></li>\n<li><p>We tried different embeddings like the once mentioned in <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">sentence-transformers</a> and also went through the <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.p\" rel=\"noreferrer\">example</a> and tried evaluating against our human-curated set\nand that also had very low accuracy.</p></li>\n<li><p>We tried <a href=\"https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604\" rel=\"noreferrer\">ELMO</a>. This was better but still lower accuracy than we expected and there is a\ncognitive load to decide the cosine value below which we shouldn't consider the sentences. This even applies to point 3.</p></li>\n</ol>\n<p>Any help will be appreciated. Thanks a lot for the help in advance</p>\n", "abstract": "There is a problem we are trying to solve where we want to do a semantic search on our set of data,\ni.e we have a domain-specific data (example: sentences talking about automobiles) Our data is just a bunch of sentences and what we want is to give a phrase and get back the sentences which are:  Let me try giving you an example suppose I search for the phrase \"Buying Experience\", I should get the sentences like: I found a car that i liked and the purchase process was\nstraightforward and easy I absolutely hated going car shopping, but today i\u2019m glad i did \nI want to lay emphasis on the fact that we are looking for contextual similarity and not just a brute force word search. If the sentence uses different words then also it should be able to find it. Things that we have already tried: Open Semantic Search the problem we faced here is generating ontology from the data we have, or\nfor that sake searching for available ontology from different domains of our interest. Elastic Search(BM25 + Vectors(tf-idf)), we tried this where it gave a few sentences but precision was not that great. The accuracy was bad\nas well. We tried against a human-curated dataset, it was able to get around 10% of the sentences only. We tried different embeddings like the once mentioned in sentence-transformers and also went through the example and tried evaluating against our human-curated set\nand that also had very low accuracy. We tried ELMO. This was better but still lower accuracy than we expected and there is a\ncognitive load to decide the cosine value below which we shouldn't consider the sentences. This even applies to point 3. Any help will be appreciated. Thanks a lot for the help in advance"}, "answers": [{"id": 60312352, "score": 10, "vote": 0, "content": "<p>I would highly suggest that you watch Trey Grainger's lecture on how to build a semantic search system =&gt; <a href=\"https://www.youtube.com/watch?v=4fMZnunTRF8\" rel=\"noreferrer\">https://www.youtube.com/watch?v=4fMZnunTRF8</a>. He talks about the anatomy of a semantic search system and each of the pieces used to fit together to deliver a final solution.</p>\n<p>A great example of the contextual similarity is Bing's search engine: <a href=\"https://i.stack.imgur.com/AL5X2.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/AL5X2.png\"/></a></p>\n<p>The original query had the terms {canned soda} and bing's search results can refer to {canned diet soda} , {soft drinks}, {unopened room temperature pop} or {carbonated drinks}. How did bing do this?: </p>\n<p>Well, words that have similar meanings get similar vectors and then these vectors can be projected onto a 2-dimensional graph to be easily visualized. These vectors are trained by ensuring words with similar meanings are physically near each other. You can train your own vector based model by training the <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">GloVe model</a><a href=\"https://i.stack.imgur.com/QRGwS.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/QRGwS.png\"/></a> </p>\n<p>The closer the distances of the vectors are to each other the better. Now you can search for nearest neighbour queries based of the distance of their vectors. For example, for the query {how to stop animals from destroying my garden}, the nearest neighbour gives these results: </p>\n<p><a href=\"https://i.stack.imgur.com/6Ek4A.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/6Ek4A.png\"/></a></p>\n<p><a href=\"https://blogs.bing.com/search-quality-insights/May-2018/Towards-More-Intelligent-Search-Deep-Learning-for-Query-Semantics\" rel=\"noreferrer\">You can learn more about it here.</a> For your case you can find a threshold for the maximum distance a vector of a sentence can be from the original search query for it to be consider a contextually similar sentence. </p>\n<p>The contextual similarity can also possibly be done by reducing the vocabulary dimension using something like LSI(Latent Semantic Indexing). To do this in Python I would highly suggest you check out the genism library for python: <a href=\"https://radimrehurek.com/gensim/about.html\" rel=\"noreferrer\">https://radimrehurek.com/gensim/about.html</a>. </p>\n", "abstract": "I would highly suggest that you watch Trey Grainger's lecture on how to build a semantic search system => https://www.youtube.com/watch?v=4fMZnunTRF8. He talks about the anatomy of a semantic search system and each of the pieces used to fit together to deliver a final solution. A great example of the contextual similarity is Bing's search engine:  The original query had the terms {canned soda} and bing's search results can refer to {canned diet soda} , {soft drinks}, {unopened room temperature pop} or {carbonated drinks}. How did bing do this?:  Well, words that have similar meanings get similar vectors and then these vectors can be projected onto a 2-dimensional graph to be easily visualized. These vectors are trained by ensuring words with similar meanings are physically near each other. You can train your own vector based model by training the GloVe model  The closer the distances of the vectors are to each other the better. Now you can search for nearest neighbour queries based of the distance of their vectors. For example, for the query {how to stop animals from destroying my garden}, the nearest neighbour gives these results:   You can learn more about it here. For your case you can find a threshold for the maximum distance a vector of a sentence can be from the original search query for it to be consider a contextually similar sentence.  The contextual similarity can also possibly be done by reducing the vocabulary dimension using something like LSI(Latent Semantic Indexing). To do this in Python I would highly suggest you check out the genism library for python: https://radimrehurek.com/gensim/about.html. "}, {"id": 60361827, "score": 1, "vote": 0, "content": "<p>You might be interested in looking into <a href=\"https://github.com/semi-technologies/weaviate\" rel=\"nofollow noreferrer\">Weaviate</a> to help you solve this problem. It is a smart graph based on the <a href=\"https://www.semi.technology/documentation/weaviate/current/about/philosophy.html\" rel=\"nofollow noreferrer\">vectorization of data objects</a>.</p>\n<p>If you have domain-specific language (e.g., abbreviations) you can extend Weaviate with <a href=\"https://www.semi.technology/documentation/weaviate/current/features/adding-synonyms.html\" rel=\"nofollow noreferrer\">custom concepts</a>.</p>\n<p>You might be able to solve your problem with the semantic search features (i.e., <code>Explore{}</code>) or the automatic classification features.</p>\n<h2>Explore function</h2>\n<p>Because all data objects get vectorized, you can do a semantic search like the following (this example comes <a href=\"https://www.semi.technology/documentation/weaviate/current/query-data/filters.html#explore-filter\" rel=\"nofollow noreferrer\">from the docs</a>, you can try it out <a href=\"http://playground.semi.technology/?weaviate_uri=https%3A%2F%2Fdemo.dataset.playground.semi.technology%2Fv1%2Fgraphql&amp;graphiql&amp;gqlquery=%7B%0D%0A++Get%7B%0D%0A++++Things%7B%0D%0A++++++Publication%28%0D%0A++++++++explore%3A+%7B%0D%0A++++++++++concepts%3A+%5B%22fashion%22%5D%2C%0D%0A++++++++++certainty%3A+0.7%2C%0D%0A++++++++++moveAwayFrom%3A+%7B%0D%0A++++++++++++concepts%3A+%5B%22finance%22%5D%2C%0D%0A++++++++++++force%3A+0.45%0D%0A++++++++++%7D%2C%0D%0A++++++++++moveTo%3A+%7B%0D%0A++++++++++++concepts%3A+%5B%22haute+couture%22%5D%2C%0D%0A++++++++++++force%3A+0.85%0D%0A++++++++++%7D%0D%0A++++++++%7D%0D%0A++++++%29%7B%0D%0A++++++++name%0D%0A++++++%7D%0D%0A++++%7D%0D%0A++%7D%0D%0A%7D\" rel=\"nofollow noreferrer\">here</a> using GraphQL):</p>\n<pre><code class=\"python\">{\n  Get{\n    Things{\n      Publication(\n        explore: {\n          concepts: [\"fashion\"],\n          certainty: 0.7,\n          moveAwayFrom: {\n            concepts: [\"finance\"],\n            force: 0.45\n          },\n          moveTo: {\n            concepts: [\"haute couture\"],\n            force: 0.85\n          }\n        }\n      ){\n        name\n      }\n    }\n  }\n}\n</code></pre>\n<p>If you structure <a href=\"https://www.semi.technology/documentation/weaviate/current/add-data/define_schema.html\" rel=\"nofollow noreferrer\">your graph schema</a> based on -for example- the class name \"Sentence\", a similar query might look something like this:</p>\n<pre><code class=\"python\">{\n  Get{\n    Things{\n      Sentence(\n        # Explore (i.e., semantically) for \"Buying Experience\"\n        explore: {\n          concepts: [\"Buying Experience\"]\n        }\n        # Result must include the word \"car\" \n        where: {\n          operator: Like\n          path: [\"content\"]\n          valueString: \"*car*\"\n        }\n      ){\n        content\n      }\n    }\n  }\n}\n</code></pre>\n<p>Note:<br/>\nYou can also <a href=\"https://www.semi.technology/documentation/weaviate/current/query-data/explore.html#explore-function\" rel=\"nofollow noreferrer\">explore the graph</a> semantically as a whole.</p>\n<h2>Automatic classification</h2>\n<p>An alternative might be working with the <a href=\"https://www.semi.technology/documentation/weaviate/current/features/contextual-classification.html\" rel=\"nofollow noreferrer\">contextual</a> or <a href=\"https://www.semi.technology/documentation/weaviate/current/features/knn-classification.html\" rel=\"nofollow noreferrer\">KNN</a> classification features.</p>\n<p>In your case, you might use the class Sentence and relate them to a class called Experience, which would have the property: <code>buying</code> (there are of course many other configurations and strategies you can choose from).</p>\n<p>PS:<br/>\n<a href=\"https://www.youtube.com/watch?v=3NfcAF4qm2k\" rel=\"nofollow noreferrer\">This video</a> gives a bit more context if you like.</p>\n", "abstract": "You might be interested in looking into Weaviate to help you solve this problem. It is a smart graph based on the vectorization of data objects. If you have domain-specific language (e.g., abbreviations) you can extend Weaviate with custom concepts. You might be able to solve your problem with the semantic search features (i.e., Explore{}) or the automatic classification features. Because all data objects get vectorized, you can do a semantic search like the following (this example comes from the docs, you can try it out here using GraphQL): If you structure your graph schema based on -for example- the class name \"Sentence\", a similar query might look something like this: Note:\nYou can also explore the graph semantically as a whole. An alternative might be working with the contextual or KNN classification features. In your case, you might use the class Sentence and relate them to a class called Experience, which would have the property: buying (there are of course many other configurations and strategies you can choose from). PS:\nThis video gives a bit more context if you like."}, {"id": 60336376, "score": 0, "vote": 0, "content": "<p>As far as I know, I don\u2019t think any theoretical model exists for building a semantic search engine. However, I believe a semantic search engine should be designed to cater to the specific requirements at hand. Having said that, any semantic search engine that is able to successfully understand the intent of the user as well as the context of the search term, needs to work with natural language processing (NLP) and machine learning as the building blocks.</p>\n<p>Even though search engines work differently from search tools, you can refer to enterprise search tools to get an idea about a semantic search model that works. The new age platforms like 3RDi Search work on the principles of semantic search and have proved to be the ideal solution for the unstructured data that enterprises have to deal with. Google is very likely working on a model to introduce advanced semantics in search engine.</p>\n", "abstract": "As far as I know, I don\u2019t think any theoretical model exists for building a semantic search engine. However, I believe a semantic search engine should be designed to cater to the specific requirements at hand. Having said that, any semantic search engine that is able to successfully understand the intent of the user as well as the context of the search term, needs to work with natural language processing (NLP) and machine learning as the building blocks. Even though search engines work differently from search tools, you can refer to enterprise search tools to get an idea about a semantic search model that works. The new age platforms like 3RDi Search work on the principles of semantic search and have proved to be the ideal solution for the unstructured data that enterprises have to deal with. Google is very likely working on a model to introduce advanced semantics in search engine."}]}, {"link": "https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list", "question": {"id": "31836058", "title": "NLTK Named Entity recognition to a Python list", "content": "<p>I used NLTK's <code>ne_chunk</code> to extract named entities from a text:</p>\n<pre><code class=\"python\">my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n\n\nnltk.ne_chunk(my_sent, binary=True)\n</code></pre>\n<p>But I can't figure out how to save these entities to a list? E.g. \u2013</p>\n<pre><code class=\"python\">print Entity_list\n('WASHINGTON', 'New York', 'Loretta', 'Brooklyn', 'African')\n</code></pre>\n<p>Thanks.</p>\n", "abstract": "I used NLTK's ne_chunk to extract named entities from a text: But I can't figure out how to save these entities to a list? E.g. \u2013 Thanks."}, "answers": [{"id": 31838373, "score": 36, "vote": 0, "content": "<p><code>nltk.ne_chunk</code> returns a nested <code>nltk.tree.Tree</code> object so you would have to traverse the <code>Tree</code> object to get to the NEs.</p>\n<p>Take a look at <a href=\"https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk\">Named Entity Recognition with Regular Expression: NLTK</a></p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize\n&gt;&gt;&gt; from nltk.tree import Tree\n&gt;&gt;&gt; \n&gt;&gt;&gt; def get_continuous_chunks(text):\n...     chunked = ne_chunk(pos_tag(word_tokenize(text)))\n...     continuous_chunk = []\n...     current_chunk = []\n...     for i in chunked:\n...             if type(i) == Tree:\n...                     current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n...             if current_chunk:\n...                     named_entity = \" \".join(current_chunk)\n...                     if named_entity not in continuous_chunk:\n...                             continuous_chunk.append(named_entity)\n...                             current_chunk = []\n...             else:\n...                     continue\n...     return continuous_chunk\n... \n&gt;&gt;&gt; my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n&gt;&gt;&gt; get_continuous_chunks(my_sent)\n['WASHINGTON', 'New York', 'Loretta E. Lynch', 'Brooklyn']\n\n\n&gt;&gt;&gt; my_sent = \"How's the weather in New York and Brooklyn\"\n&gt;&gt;&gt; get_continuous_chunks(my_sent)\n['New York', 'Brooklyn']\n</code></pre>\n", "abstract": "nltk.ne_chunk returns a nested nltk.tree.Tree object so you would have to traverse the Tree object to get to the NEs. Take a look at Named Entity Recognition with Regular Expression: NLTK"}, {"id": 45704548, "score": 22, "vote": 0, "content": "<p>You can also extract the <code>label</code> of each Name Entity in the text using this code:</p>\n<pre><code class=\"python\">import nltk\nfor sent in nltk.sent_tokenize(sentence):\n   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n      if hasattr(chunk, 'label'):\n         print(chunk.label(), ' '.join(c[0] for c in chunk))\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code class=\"python\">GPE WASHINGTON\nGPE New York\nPERSON Loretta E. Lynch\nGPE Brooklyn\n</code></pre>\n<p>You can see <code>Washington</code>, <code>New York</code> and <code>Brooklyn</code> are <code>GPE</code> means <strong>geo-political entities</strong></p>\n<p>and <code>Loretta E. Lynch</code> is a <code>PERSON</code></p>\n", "abstract": "You can also extract the label of each Name Entity in the text using this code: Output: You can see Washington, New York and Brooklyn are GPE means geo-political entities and Loretta E. Lynch is a PERSON"}, {"id": 31837224, "score": 8, "vote": 0, "content": "<p>As you get a <a href=\"http://www.nltk.org/_modules/nltk/tree.html\" rel=\"noreferrer\"><code>tree</code></a> as a return value, I guess you want to pick those subtrees that are labeled with <code>NE</code></p>\n<p>Here is a simple example to gather all those in a list:</p>\n<pre><code class=\"python\">import nltk\n\nmy_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n\nparse_tree = nltk.ne_chunk(nltk.tag.pos_tag(my_sent.split()), binary=True)  # POS tagging before chunking!\n\nnamed_entities = []\n\nfor t in parse_tree.subtrees():\n    if t.label() == 'NE':\n        named_entities.append(t)\n        # named_entities.append(list(t))  # if you want to save a list of tagged words instead of a tree\n\nprint named_entities\n</code></pre>\n<p>This gives:</p>\n<pre><code class=\"python\">[Tree('NE', [('WASHINGTON', 'NNP')]), Tree('NE', [('New', 'NNP'), ('York', 'NNP')])]\n</code></pre>\n<p>or as a list of lists:</p>\n<pre><code class=\"python\">[[('WASHINGTON', 'NNP')], [('New', 'NNP'), ('York', 'NNP')]]\n</code></pre>\n<p>Also see: <a href=\"https://stackoverflow.com/questions/14841997/how-to-navigate-a-nltk-tree-tree\">How to navigate a nltk.tree.Tree?</a></p>\n", "abstract": "As you get a tree as a return value, I guess you want to pick those subtrees that are labeled with NE Here is a simple example to gather all those in a list: This gives: or as a list of lists: Also see: How to navigate a nltk.tree.Tree?"}, {"id": 48738383, "score": 6, "vote": 0, "content": "<p>use tree2conlltags from nltk.chunk. Also ne_chunk needs pos tagging which tags word tokens (thus needs word_tokenize).</p>\n<pre><code class=\"python\">from nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk.chunk import tree2conlltags\n\nsentence = \"Mark and John are working at Google.\"\nprint(tree2conlltags(ne_chunk(pos_tag(word_tokenize(sentence))\n\"\"\"[('Mark', 'NNP', 'B-PERSON'), \n    ('and', 'CC', 'O'), ('John', 'NNP', 'B-PERSON'), \n    ('are', 'VBP', 'O'), ('working', 'VBG', 'O'), \n    ('at', 'IN', 'O'), ('Google', 'NNP', 'B-ORGANIZATION'), \n    ('.', '.', 'O')] \"\"\"\n</code></pre>\n<p>This will give you a list of tuples: [(token, pos_tag, name_entity_tag)]\nIf this list is not exactly what you want, it is certainly easier to parse the list you want out of this list then an nltk tree.</p>\n<p>Code and details from <a href=\"http://nlpforhackers.io/named-entity-extraction/\" rel=\"nofollow noreferrer\">this link</a>; check it out for more information</p>\n<p>You can also continue by only extracting the words, with the following function:</p>\n<pre><code class=\"python\">def wordextractor(tuple1):\n\n    #bring the tuple back to lists to work with it\n    words, tags, pos = zip(*tuple1)\n    words = list(words)\n    pos = list(pos)\n    c = list()\n    i=0\n    while i&lt;= len(tuple1)-1:\n        #get words with have pos B-PERSON or I-PERSON\n        if pos[i] == 'B-PERSON':\n            c = c+[words[i]]\n        elif pos[i] == 'I-PERSON':\n            c = c+[words[i]]\n        i=i+1\n\n    return c\n\nprint(wordextractor(tree2conlltags(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))))\n</code></pre>\n<p><strong>Edit</strong> Added output docstring\n**Edit* Added Output only for B-Person</p>\n", "abstract": "use tree2conlltags from nltk.chunk. Also ne_chunk needs pos tagging which tags word tokens (thus needs word_tokenize). This will give you a list of tuples: [(token, pos_tag, name_entity_tag)]\nIf this list is not exactly what you want, it is certainly easier to parse the list you want out of this list then an nltk tree. Code and details from this link; check it out for more information You can also continue by only extracting the words, with the following function: Edit Added output docstring\n**Edit* Added Output only for B-Person"}, {"id": 49327568, "score": 4, "vote": 0, "content": "<p>You may also consider using Spacy:</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load('en')\n\ndoc = nlp('WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.')\n\nprint([ent for ent in doc.ents])\n\n&gt;&gt;&gt; [WASHINGTON, New York, the 1990s, Loretta E. Lynch, Brooklyn, African-Americans]\n</code></pre>\n", "abstract": "You may also consider using Spacy:"}, {"id": 44294377, "score": 3, "vote": 0, "content": "<p>A <code>Tree</code> is a list. Chunks are subtrees, non-chunked words are regular strings. So let's go down the list, extract the words from each chunk, and join them.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; chunked = nltk.ne_chunk(my_sent)\n&gt;&gt;&gt;\n&gt;&gt;&gt;  [ \" \".join(w for w, t in elt) for elt in chunked if isinstance(elt, nltk.Tree) ]\n['WASHINGTON', 'New York', 'Loretta E. Lynch', 'Brooklyn']\n</code></pre>\n", "abstract": "A Tree is a list. Chunks are subtrees, non-chunked words are regular strings. So let's go down the list, extract the words from each chunk, and join them."}, {"id": 60829206, "score": 1, "vote": 0, "content": "<p>nltk.ne_chunk returns a nested nltk.tree.Tree object so you would have to traverse the Tree object to get to the NEs. You can use list comprehension to do the same.</p>\n<pre><code class=\"python\">import nltk   \nmy_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n\nword = nltk.word_tokenize(my_sent)   \npos_tag = nltk.pos_tag(word)   \nchunk = nltk.ne_chunk(pos_tag)   \nNE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]   \nprint (NE)\n</code></pre>\n", "abstract": "nltk.ne_chunk returns a nested nltk.tree.Tree object so you would have to traverse the Tree object to get to the NEs. You can use list comprehension to do the same."}]}, {"link": "https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim", "question": {"id": "27139908", "title": "Load PreComputed Vectors Gensim", "content": "<p>I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. <a href=\"http://www-nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">http://www-nlp.stanford.edu/projects/glove/</a>). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch?</p>\n<p>Thanks! </p>\n", "abstract": "I am using the Gensim Python package to learn a neural language model, and I know that you can provide a training corpus to learn the model. However, there already exist many precomputed word vectors available in text format (e.g. http://www-nlp.stanford.edu/projects/glove/). Is there some way to initialize a Gensim Word2Vec model that just makes use of some precomputed vectors, rather than having to learn the vectors from scratch? Thanks! "}, "answers": [{"id": 41990999, "score": 45, "vote": 0, "content": "<p>The GloVe dump from the Stanford site is in a format that is little different from the word2vec format. You can convert the GloVe file into word2vec format using:</p>\n<pre><code class=\"python\">python -m gensim.scripts.glove2word2vec --input  glove.840B.300d.txt --output glove.840B.300d.w2vformat.txt\n</code></pre>\n", "abstract": "The GloVe dump from the Stanford site is in a format that is little different from the word2vec format. You can convert the GloVe file into word2vec format using:"}, {"id": 27462798, "score": 24, "vote": 0, "content": "<p>You can download pre-trained word vectors from here (get the file 'GoogleNews-vectors-negative300.bin'):\n<a href=\"https://code.google.com/p/word2vec/\" rel=\"noreferrer\">word2vec</a></p>\n<p>Extract the file and then you can load it in python like:</p>\n<pre><code class=\"python\">model = gensim.models.word2vec.Word2Vec.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)\n\nmodel.most_similar('dog')\n</code></pre>\n<p>EDIT (May 2017):\nAs the above code is now deprecated, this is how you'd load the vectors now:</p>\n<pre><code class=\"python\">model = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(os.path.dirname(__file__), 'GoogleNews-vectors-negative300.bin'), binary=True)\n</code></pre>\n", "abstract": "You can download pre-trained word vectors from here (get the file 'GoogleNews-vectors-negative300.bin'):\nword2vec Extract the file and then you can load it in python like: EDIT (May 2017):\nAs the above code is now deprecated, this is how you'd load the vectors now:"}, {"id": 53788439, "score": 0, "vote": 0, "content": "<p>As far as I know, Gensim can load two binary formats, word2vec and fastText, and a generic plain text format which can be created by most word embedding tools. The generic plain text format looks like this (in this example 20000 is the size of the vocabulary and 100 is the length of vector)</p>\n<pre><code class=\"python\">20000 100\nthe 0.476841 -0.620207 -0.002157 0.359706 -0.591816 [98 more numbers...]\nand 0.223408  0.231993 -0.231131 -0.900311 -0.225111 [98 more numbers..]\n[19998 more lines...]\n</code></pre>\n<p>Chaitanya Shivade has explained in his answer here, how to use a script provided by Gensim to convert the Glove format (each line: word + vector) into the generic format. </p>\n<p>Loading the different formats is easy, but it is also easy to get them mixed up:</p>\n<pre><code class=\"python\">import gensim\nmodel_file = path/to/model/file\n</code></pre>\n<p>1) Loading binary word2vec</p>\n<pre><code class=\"python\">model = gensim.models.word2vec.Word2Vec.load_word2vec_format(model_file)\n</code></pre>\n<p>2) Loading binary fastText</p>\n<pre><code class=\"python\">model = gensim.models.fasttext.FastText.load_fasttext_format(model_file)\n</code></pre>\n<p>3) Loading the generic plain text format (which has been introduced by word2vec)</p>\n<pre><code class=\"python\">model = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format(model_file)\n</code></pre>\n<p>If you only plan to use the word embeddings and not to continue to train them in Gensim, you may want to use the KeyedVector class. This will reduce the amount of memory you need to load the vectors considerably (<a href=\"https://radimrehurek.com/gensim/models/keyedvectors.html#why-use-keyedvectors-instead-of-a-full-model\" rel=\"nofollow noreferrer\">detailed explanation</a>). </p>\n<p>The following will load the binary word2vec format as keyedvectors:</p>\n<pre><code class=\"python\">model = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format(model_file, binary=True)\n</code></pre>\n", "abstract": "As far as I know, Gensim can load two binary formats, word2vec and fastText, and a generic plain text format which can be created by most word embedding tools. The generic plain text format looks like this (in this example 20000 is the size of the vocabulary and 100 is the length of vector) Chaitanya Shivade has explained in his answer here, how to use a script provided by Gensim to convert the Glove format (each line: word + vector) into the generic format.  Loading the different formats is easy, but it is also easy to get them mixed up: 1) Loading binary word2vec 2) Loading binary fastText 3) Loading the generic plain text format (which has been introduced by word2vec) If you only plan to use the word embeddings and not to continue to train them in Gensim, you may want to use the KeyedVector class. This will reduce the amount of memory you need to load the vectors considerably (detailed explanation).  The following will load the binary word2vec format as keyedvectors:"}]}, {"link": "https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn", "question": {"id": "36182502", "title": "add stemming support to CountVectorizer (sklearn)", "content": "<p>I'm trying to add stemming to my pipeline in NLP with sklearn.</p>\n<pre><code class=\"python\">from nltk.stem.snowball import FrenchStemmer\n\nstop = stopwords.words('french')\nstemmer = FrenchStemmer()\n\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def __init__(self, stemmer):\n        super(StemmedCountVectorizer, self).__init__()\n        self.stemmer = stemmer\n\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc:(self.stemmer.stem(w) for w in analyzer(doc))\n\nstem_vectorizer = StemmedCountVectorizer(stemmer)\ntext_clf = Pipeline([('vect', stem_vectorizer), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='linear', C=1)) ])\n</code></pre>\n<p>When using this pipeline with the CountVectorizer of sklearn it works. And if I create manually the features like this it works also.<br/></p>\n<pre><code class=\"python\">vectorizer = StemmedCountVectorizer(stemmer)\nvectorizer.fit_transform(X)\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)\n</code></pre>\n<p><strong>EDIT</strong>:</p>\n<p>If I try this pipeline on my IPython Notebook it displays the [*] and nothing happens. When I look at my terminal, it gives this error :<br/></p>\n<pre><code class=\"python\">Process PoolWorker-12:\nTraceback (most recent call last):\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\n    self.run()\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Anaconda2\\lib\\multiprocessing\\pool.py\", line 102, in worker\n    task = get()\n  File \"C:\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\pool.py\", line 360, in get\n    return recv()\nAttributeError: 'module' object has no attribute 'StemmedCountVectorizer'\n</code></pre>\n<p><em>Example</em></p>\n<p>Here is the complete example</p>\n<pre><code class=\"python\">from sklearn.pipeline import Pipeline\nfrom sklearn import grid_search\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom nltk.stem.snowball import FrenchStemmer\n\nstemmer = FrenchStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemming(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\nX = ['le chat est beau', 'le ciel est nuageux', 'les gens sont gentils', 'Paris est magique', 'Marseille est tragique', 'JCVD est fou']\nY = [1,0,1,1,0,0]\n\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC())])\nparameters = { 'vect__analyzer': ['word', stemming]}\n\ngs_clf = grid_search.GridSearchCV(text_clf, parameters, n_jobs=-1)\ngs_clf.fit(X, Y)\n</code></pre>\n<p>If you remove stemming from the parameters it works otherwise it doesn't work.</p>\n<p><strong>UPDATE</strong>:</p>\n<p>The problem seems to be in the parallelization process because when removing <strong>n_jobs=-1</strong> the problem disappear.</p>\n", "abstract": "I'm trying to add stemming to my pipeline in NLP with sklearn. When using this pipeline with the CountVectorizer of sklearn it works. And if I create manually the features like this it works also. EDIT: If I try this pipeline on my IPython Notebook it displays the [*] and nothing happens. When I look at my terminal, it gives this error : Example Here is the complete example If you remove stemming from the parameters it works otherwise it doesn't work. UPDATE: The problem seems to be in the parallelization process because when removing n_jobs=-1 the problem disappear."}, "answers": [{"id": 36191362, "score": 34, "vote": 0, "content": "<p>You can pass a callable as <code>analyzer</code> to the <code>CountVectorizer</code> constructor to provide a custom analyzer. This appears to work for me.</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem.snowball import FrenchStemmer\n\nstemmer = FrenchStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\nstem_vectorizer = CountVectorizer(analyzer=stemmed_words)\nprint(stem_vectorizer.fit_transform(['Tu marches dans la rue']))\nprint(stem_vectorizer.get_feature_names())\n</code></pre>\n<p>Prints out:</p>\n<pre><code class=\"python\">  (0, 4)    1\n  (0, 2)    1\n  (0, 0)    1\n  (0, 1)    1\n  (0, 3)    1\n[u'dan', u'la', u'march', u'ru', u'tu']\n</code></pre>\n", "abstract": "You can pass a callable as analyzer to the CountVectorizer constructor to provide a custom analyzer. This appears to work for me. Prints out:"}, {"id": 41377484, "score": 20, "vote": 0, "content": "<p>I know I am little late in posting my answer.\nBut here it is, in case someone still needs help.</p>\n<p>Following is the cleanest approach to add language stemmer to count vectorizer by overriding <code>build_analyser()</code></p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import CountVectorizer\nimport nltk.stem\n\nfrench_stemmer = nltk.stem.SnowballStemmer('french')\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([french_stemmer.stem(w) for w in analyzer(doc)])\n\nvectorizer_s = StemmedCountVectorizer(min_df=3, analyzer=\"word\", stop_words='french')\n</code></pre>\n<p>You can freely call <code>fit</code> and <code>transform</code> functions of CountVectorizer class over your <code>vectorizer_s</code> object</p>\n", "abstract": "I know I am little late in posting my answer.\nBut here it is, in case someone still needs help. Following is the cleanest approach to add language stemmer to count vectorizer by overriding build_analyser() You can freely call fit and transform functions of CountVectorizer class over your vectorizer_s object"}, {"id": 36183388, "score": 1, "vote": 0, "content": "<p>You can try:</p>\n<pre><code class=\"python\">def build_analyzer(self):\n    analyzer = super(CountVectorizer, self).build_analyzer()\n    return lambda doc:(stemmer.stem(w) for w in analyzer(doc))\n</code></pre>\n<p>and remove the <code>__init__</code> method.</p>\n", "abstract": "You can try: and remove the __init__ method."}]}, {"link": "https://stackoverflow.com/questions/37253326/how-to-find-the-most-common-words-using-spacy", "question": {"id": "37253326", "title": "How to find the most common words using spacy?", "content": "<p>I'm using spacy with python and its working fine for tagging each word but I was wondering if it was possible to find the most common words in a string. Also is it possible to get the most common nouns, verbs, adverbs and so on?</p>\n<p>There's a count_by function included but I cant seem to get it to run in any meaningful way.</p>\n", "abstract": "I'm using spacy with python and its working fine for tagging each word but I was wondering if it was possible to find the most common words in a string. Also is it possible to get the most common nouns, verbs, adverbs and so on? There's a count_by function included but I cant seem to get it to run in any meaningful way."}, "answers": [{"id": 41425016, "score": 37, "vote": 0, "content": "<p>I recently had to count frequency of all the tokens in a text file. You can filter out words to get POS tokens you like using the pos_ attribute. Here is a simple example:</p>\n<pre><code class=\"python\">import spacy\nfrom collections import Counter\nnlp = spacy.load('en')\ndoc = nlp(u'Your text here')\n# all tokens that arent stop words or punctuations\nwords = [token.text\n         for token in doc\n         if not token.is_stop and not token.is_punct]\n\n# noun tokens that arent stop words or punctuations\nnouns = [token.text\n         for token in doc\n         if (not token.is_stop and\n             not token.is_punct and\n             token.pos_ == \"NOUN\")]\n\n# five most common tokens\nword_freq = Counter(words)\ncommon_words = word_freq.most_common(5)\n\n# five most common noun tokens\nnoun_freq = Counter(nouns)\ncommon_nouns = noun_freq.most_common(5)\n</code></pre>\n", "abstract": "I recently had to count frequency of all the tokens in a text file. You can filter out words to get POS tokens you like using the pos_ attribute. Here is a simple example:"}, {"id": 37253693, "score": 12, "vote": 0, "content": "<p>This should look basically the same as counting anything else in Python. spaCy lets you just iterate over the document, and you get back a sequence of Token objects. These can be used to access the annotations.</p>\n<pre><code class=\"python\">from __future__ import print_function, unicode_literals\nimport spacy\nfrom collections import defaultdict, Counter\n\nnlp = spacy.load('en')\n\npos_counts = defaultdict(Counter)\ndoc = nlp(u'My text here.')\n\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n\nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id])\n</code></pre>\n<p>Note that the .orth and .pos attributes are integers. You can get the strings that they map to via the .orth_ and .pos_ attributes. The .orth attribute is the unnormalised view of the token, there's also the .lower, .lemma etc string-view. You might want to bind a .norm function, to do your own string normalisation. See the docs for details.</p>\n<p>The integers are useful for your counts because you can make your counting program much more memory efficient, if you're counting over a large corpus. You could also store the frequent counts in a numpy array, for additional speed and efficiency. If you don't want to bother with this, feel free to count with the .orth_ attribute directly, or use its alias .text.</p>\n<p>Note that the .pos attribute in the snippet above gives a coarse-grained set of part-of-speech tags. The richer treebank tags are available on the .tag attribute.</p>\n", "abstract": "This should look basically the same as counting anything else in Python. spaCy lets you just iterate over the document, and you get back a sequence of Token objects. These can be used to access the annotations. Note that the .orth and .pos attributes are integers. You can get the strings that they map to via the .orth_ and .pos_ attributes. The .orth attribute is the unnormalised view of the token, there's also the .lower, .lemma etc string-view. You might want to bind a .norm function, to do your own string normalisation. See the docs for details. The integers are useful for your counts because you can make your counting program much more memory efficient, if you're counting over a large corpus. You could also store the frequent counts in a numpy array, for additional speed and efficiency. If you don't want to bother with this, feel free to count with the .orth_ attribute directly, or use its alias .text. Note that the .pos attribute in the snippet above gives a coarse-grained set of part-of-speech tags. The richer treebank tags are available on the .tag attribute."}, {"id": 58030377, "score": 5, "vote": 0, "content": "<p>I'm adding to this thread quite late. HOWEVER, there is, in fact, a built-in way to do this using the doc.count_by() function in spacy.</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import spacy\nimport spacy.attrs\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"It all happened between November 2007 and November 2008\")\n\n# Returns integers that map to parts of speech\ncounts_dict = doc.count_by(spacy.attrs.IDS['POS'])\n\n# Print the human readable part of speech tags\nfor pos, count in counts_dict.items():\n    human_readable_tag = doc.vocab[pos].text\n    print(human_readable_tag, count)\n\n</code></pre>\n<p>The output is :</p>\n<p>VERB 1</p>\n<p>ADP 1</p>\n<p>CCONJ 1</p>\n<p>DET 1</p>\n<p>NUM 2</p>\n<p>PRON 1</p>\n<p>PROPN 2</p>\n", "abstract": "I'm adding to this thread quite late. HOWEVER, there is, in fact, a built-in way to do this using the doc.count_by() function in spacy. The output is : VERB 1 ADP 1 CCONJ 1 DET 1 NUM 2 PRON 1 PROPN 2"}]}, {"link": "https://stackoverflow.com/questions/45420466/gensim-keyerror-word-not-in-vocabulary", "question": {"id": "45420466", "title": "Gensim: KeyError: &quot;word not in vocabulary&quot;", "content": "<p>I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34:</p>\n<pre><code class=\"python\">b = ['let',\n 'know',\n 'buy',\n 'someth',\n 'featur',\n 'mashabl',\n 'might',\n 'earn',\n 'affili',\n 'commiss',\n 'fifti',\n 'year',\n 'ago',\n 'graduat',\n '21yearold',\n 'dustin',\n 'hoffman',\n 'pull',\n 'asid',\n 'given',\n 'one',\n 'piec',\n 'unsolicit',\n 'advic',\n 'percent',\n 'buy']\n</code></pre>\n<p><strong>Model</strong></p>\n<pre><code class=\"python\">model = gensim.models.Word2Vec(b,min_count=1,size=32)\nprint(model) \n### prints: Word2Vec(vocab=34, size=32, alpha=0.025) ####\n</code></pre>\n<p>if I try to get the similarity score by doing <code>model['buy']</code> of one the words in the list, I get the </p>\n<blockquote>\n<p>KeyError: \"word 'buy' not in vocabulary\"</p>\n</blockquote>\n<p>Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. </p>\n", "abstract": "I have a trained Word2vec model using Python's Gensim Library. I have a tokenized list as below. The vocab size is 34 but I am just giving few out of 34: Model if I try to get the similarity score by doing model['buy'] of one the words in the list, I get the  KeyError: \"word 'buy' not in vocabulary\" Can you guys suggest me what I am doing wrong and what are the ways to check the model which can be further used to train PCA or t-sne in order to visualize similar words forming a topic? Thank you. "}, "answers": [{"id": 45420886, "score": 39, "vote": 0, "content": "<p>The first parameter passed to <code>gensim.models.Word2Vec</code> is an iterable of sentences. Sentences themselves are a list of words. From the docs:</p>\n<blockquote>\n<p>Initialize the model from an iterable of <code>sentences</code>. Each sentence is a\n  list of words (unicode strings) that will be used for training.</p>\n</blockquote>\n<p>Right now, it thinks that each word in your list <code>b</code> is a sentence and so it is doing <code>Word2Vec</code> for each <strong>character</strong> in each word, as opposed to each word in your <code>b</code>. Right now you can do:</p>\n<pre><code class=\"python\">model = gensim.models.Word2Vec(b,min_count=1,size=32)\n\nprint(model['a'])\narray([  7.42487283e-03,  -5.65282721e-03,   1.28707094e-02, ... ]\n</code></pre>\n<p>To get it to work for words, simply wrap <code>b</code> in another list so that it is interpreted correctly:</p>\n<pre><code class=\"python\">model = gensim.models.Word2Vec([b],min_count=1,size=32)\n\nprint(model['buy'])\narray([-0.01331611,  0.00496594, -0.00165093, -0.01444992,  0.01393849, ... ]\n</code></pre>\n", "abstract": "The first parameter passed to gensim.models.Word2Vec is an iterable of sentences. Sentences themselves are a list of words. From the docs: Initialize the model from an iterable of sentences. Each sentence is a\n  list of words (unicode strings) that will be used for training. Right now, it thinks that each word in your list b is a sentence and so it is doing Word2Vec for each character in each word, as opposed to each word in your b. Right now you can do: To get it to work for words, simply wrap b in another list so that it is interpreted correctly:"}, {"id": 51355252, "score": 5, "vote": 0, "content": "<p>From the docs you need to pass iterable sentences so whatever you pass to the function it treats input as a iterable so here you are passing only words so it counts word2vec vector for each in charecter in the whole corpus.</p>\n<p>So In order to avoid that problem, pass the list of words inside a list.</p>\n<pre><code class=\"python\">word2vec_model = gensim.models.Word2Vec([b],min_count=1,size=32)\n</code></pre>\n", "abstract": "From the docs you need to pass iterable sentences so whatever you pass to the function it treats input as a iterable so here you are passing only words so it counts word2vec vector for each in charecter in the whole corpus. So In order to avoid that problem, pass the list of words inside a list."}]}, {"link": "https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file", "question": {"id": "47118678", "title": "Difference between Fasttext .vec and .bin file", "content": "<p>I recently downloaded fasttext pretrained model for english. I got two files:</p>\n<ol>\n<li>wiki.en.vec</li>\n<li>wiki.en.bin</li>\n</ol>\n<p>I am not sure what is the difference between the two files?</p>\n", "abstract": "I recently downloaded fasttext pretrained model for english. I got two files: I am not sure what is the difference between the two files?"}, "answers": [{"id": 49439794, "score": 26, "vote": 0, "content": "<p>The <code>.vec</code> files contain only the aggregated word vectors, in plain-text. The <code>.bin</code> files <em>in addition</em> contain the model parameters, and crucially, the vectors for all the n-grams. </p>\n<p>So if you want to encode words you did not train with using those n-grams (FastText's famous \"subword information\"), you <em>need</em> to find an API that can handle FastText <code>.bin</code> files (most only support the <code>.vec</code> files, however...).</p>\n", "abstract": "The .vec files contain only the aggregated word vectors, in plain-text. The .bin files in addition contain the model parameters, and crucially, the vectors for all the n-grams.  So if you want to encode words you did not train with using those n-grams (FastText's famous \"subword information\"), you need to find an API that can handle FastText .bin files (most only support the .vec files, however...)."}, {"id": 47119362, "score": 16, "vote": 0, "content": "<p>As the <a href=\"https://github.com/facebookresearch/fastText/blob/master/README.md\" rel=\"noreferrer\">documentation</a> says, </p>\n<blockquote>\n<p><code>model.vec</code> is a text file containing the word vectors, one per line.\n  <code>model.bin</code> is a binary file containing the parameters of the model\n  along with the dictionary and all hyper parameters.</p>\n</blockquote>\n<p>In other words, <code>.vec</code> file format is the same as <code>.txt</code> file format, and you could use it in other applications (for example, to exchange data between your FastText model and your Word2Vec model since <code>.vec</code> file is similar to <code>.txt</code> file generated by Word2Vec). And the <code>.bin</code> file could be used if you want to continue training the vectors or to restart the optimization.</p>\n", "abstract": "As the documentation says,  model.vec is a text file containing the word vectors, one per line.\n  model.bin is a binary file containing the parameters of the model\n  along with the dictionary and all hyper parameters. In other words, .vec file format is the same as .txt file format, and you could use it in other applications (for example, to exchange data between your FastText model and your Word2Vec model since .vec file is similar to .txt file generated by Word2Vec). And the .bin file could be used if you want to continue training the vectors or to restart the optimization."}]}, {"link": "https://stackoverflow.com/questions/48019843/pca-on-word2vec-embeddings", "question": {"id": "48019843", "title": "PCA on word2vec embeddings", "content": "<p>I am trying to reproduce the results of this paper: <a href=\"https://arxiv.org/pdf/1607.06520.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1607.06520.pdf</a></p>\n<p>Specifically this part:</p>\n<blockquote>\n<p>To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.</p>\n</blockquote>\n<p><a href=\"https://i.stack.imgur.com/EOJJK.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/EOJJK.png\"/></a></p>\n<p>I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec. </p>\n<p>The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:</p>\n<p><a href=\"https://i.stack.imgur.com/7b6Dj.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/7b6Dj.png\"/></a></p>\n<p>I've computed the differences between each normalized vector in the following way:</p>\n<pre><code class=\"python\">model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-\nnegative300.bin', binary = True)\nmodel.init_sims()\n\npairs = [('she', 'he'),\n('her', 'his'),\n('woman', 'man'),\n('Mary', 'John'),\n('herself', 'himself'),\n('daughter', 'son'),\n('mother', 'father'),\n('gal', 'guy'),\n('girl', 'boy'),\n('female', 'male')]\n\ndifference_matrix = np.array([model.word_vec(a[0], use_norm=True) - model.word_vec(a[1], use_norm=True) for a in pairs])\n</code></pre>\n<p>I then perform PCA on the resulting matrix, with 10 components, as per the paper:</p>\n<pre><code class=\"python\">from sklearn.decomposition import PCA\npca = PCA(n_components=10)\npca.fit(difference_matrix)\n</code></pre>\n<p>However I get very different results when I look at <code>pca.explained_variance_ratio_</code> :</p>\n<pre><code class=\"python\">array([  2.83391436e-01,   2.48616155e-01,   1.90642492e-01,\n         9.98411858e-02,   5.61260498e-02,   5.29706681e-02,\n         2.75670634e-02,   2.21957722e-02,   1.86491774e-02,\n         1.99108478e-32])\n</code></pre>\n<p>or with a chart:</p>\n<p><a href=\"https://i.stack.imgur.com/RuNEi.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/RuNEi.png\"/></a></p>\n<p>The first component accounts for less than 30% of the variance when it should be above 60%! </p>\n<p>The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what.</p>\n<p>Note: I've tried without normalizing the vectors, but I get the same results.</p>\n", "abstract": "I am trying to reproduce the results of this paper: https://arxiv.org/pdf/1607.06520.pdf Specifically this part: To identify the gender subspace, we took the ten gender pair difference vectors and computed its principal components (PCs). As Figure 6 shows, there is a single direction that explains the majority of variance in these vectors. The first eigenvalue is significantly larger than the rest.  I am using the same set of word vectors as the authors (Google News Corpus, 300 dimensions), which I load into word2vec.  The 'ten gender pair difference vectors' the authors refer to are computed from the following word pairs:  I've computed the differences between each normalized vector in the following way: I then perform PCA on the resulting matrix, with 10 components, as per the paper: However I get very different results when I look at pca.explained_variance_ratio_ : or with a chart:  The first component accounts for less than 30% of the variance when it should be above 60%!  The results I get are similar to what I get when I try to do the PCA on randomly selected vectors, so I must be doing something wrong, but I can't figure out what. Note: I've tried without normalizing the vectors, but I get the same results."}, "answers": [{"id": 53110437, "score": 13, "vote": 0, "content": "<p>They released the code for the paper on github: <a href=\"https://github.com/tolga-b/debiaswe\" rel=\"noreferrer\">https://github.com/tolga-b/debiaswe</a></p>\n<p>Specifically, you can see their code for creating the PCA plot in <a href=\"https://github.com/tolga-b/debiaswe/blob/master/debiaswe/we.py\" rel=\"noreferrer\">this</a> file. </p>\n<p>Here is the relevant snippet of code from that file:</p>\n<pre><code class=\"python\">def doPCA(pairs, embedding, num_components = 10):\n    matrix = []\n    for a, b in pairs:\n        center = (embedding.v(a) + embedding.v(b))/2\n        matrix.append(embedding.v(a) - center)\n        matrix.append(embedding.v(b) - center)\n    matrix = np.array(matrix)\n    pca = PCA(n_components = num_components)\n    pca.fit(matrix)\n    # bar(range(num_components), pca.explained_variance_ratio_)\n    return pca\n</code></pre>\n<p>Based on the code, looks like they are taking the difference between each word in a pair and the average vector of the pair. To me, it's not clear this is what they meant in the paper. However, I ran this code with their pairs and was able to recreate the graph from the paper:</p>\n<p><a href=\"https://i.stack.imgur.com/Cy2wi.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/Cy2wi.png\"/></a></p>\n", "abstract": "They released the code for the paper on github: https://github.com/tolga-b/debiaswe Specifically, you can see their code for creating the PCA plot in this file.  Here is the relevant snippet of code from that file: Based on the code, looks like they are taking the difference between each word in a pair and the average vector of the pair. To me, it's not clear this is what they meant in the paper. However, I ran this code with their pairs and was able to recreate the graph from the paper: "}, {"id": 53233561, "score": 4, "vote": 0, "content": "<p>To expand on oregano's answer: </p>\n<p>For each pair, a and b, they calculate the center, c = (a + b) / 2 and then include vectors pointing in both directions, a - c and b - c.  </p>\n<p>The reason this is critical is that PCA gives you the vector along which the most variance occurs.  All of your vectors point in the same direction, so there is very little variance in precisely the direction you are trying to reveal.</p>\n<p>Their set includes vectors pointing in both directions in the gender subspace, so PCA clearly reveals gender variation.</p>\n", "abstract": "To expand on oregano's answer:  For each pair, a and b, they calculate the center, c = (a + b) / 2 and then include vectors pointing in both directions, a - c and b - c.   The reason this is critical is that PCA gives you the vector along which the most variance occurs.  All of your vectors point in the same direction, so there is very little variance in precisely the direction you are trying to reveal. Their set includes vectors pointing in both directions in the gender subspace, so PCA clearly reveals gender variation."}]}, {"link": "https://stackoverflow.com/questions/30843011/save-and-reuse-tfidfvectorizer-in-scikit-learn", "question": {"id": "30843011", "title": "Save and reuse TfidfVectorizer in scikit learn", "content": "<p>I am using TfidfVectorizer in scikit learn to create a matrix from text data. Now I need to save this object for reusing it later. I tried to use pickle, but it gave the following error.</p>\n<pre><code class=\"python\">loc=open('vectorizer.obj','w')\npickle.dump(self.vectorizer,loc)\n*** TypeError: can't pickle instancemethod objects\n</code></pre>\n<p>I tried using joblib in sklearn.externals, which again gave similar error. Is there any way to save this object so that I can reuse it later?</p>\n<p>Here is my full object:</p>\n<pre><code class=\"python\">class changeToMatrix(object):\ndef __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    self.vectorizer = TfidfVectorizer(ngram_range=ngram_range,analyzer='word',lowercase=True,\\\n                                          token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=tokenizer)\n\ndef load_ref_text(self,text_file):\n    textfile = open(text_file,'r')\n    lines=textfile.readlines()\n    textfile.close()\n    lines = ' '.join(lines)\n    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    sentences = [ sent_tokenizer.tokenize(lines.strip()) ]\n    sentences1 = [item.strip().strip('.') for sublist in sentences for item in sublist]      \n    chk2=pd.DataFrame(self.vectorizer.fit_transform(sentences1).toarray()) #vectorizer is transformed in this step \n    return sentences1,[chk2]\n\ndef get_processed_data(self,data_loc):\n    ref_sentences,ref_dataframes=self.load_ref_text(data_loc)\n    loc=open(\"indexedData/vectorizer.obj\",\"w\")\n    pickle.dump(self.vectorizer,loc) #getting error here\n    loc.close()\n    return ref_sentences,ref_dataframes\n</code></pre>\n", "abstract": "I am using TfidfVectorizer in scikit learn to create a matrix from text data. Now I need to save this object for reusing it later. I tried to use pickle, but it gave the following error. I tried using joblib in sklearn.externals, which again gave similar error. Is there any way to save this object so that I can reuse it later? Here is my full object:"}, "answers": [{"id": 30845049, "score": 17, "vote": 0, "content": "<p>Firstly, it's better to leave the import at the top of your code instead of within your class:</p>\n<pre><code class=\"python\">from sklearn.feature_extraction.text import TfidfVectorizer\nclass changeToMatrix(object):\n  def __init__(self,ngram_range=(1,1),tokenizer=StemTokenizer()):\n    ...\n</code></pre>\n<p>Next <code>StemTokenizer</code> don't seem to be a canonical class. Possibly you've got it from <a href=\"http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html\" rel=\"noreferrer\">http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html</a> or maybe somewhere else so <strong><em>we'll assume it returns a list of strings</em></strong>.</p>\n<pre><code class=\"python\">class StemTokenizer(object):\n    def __init__(self):\n        self.ignore_set = {'footnote', 'nietzsche', 'plato', 'mr.'}\n\n    def __call__(self, doc):\n        words = []\n        for word in word_tokenize(doc):\n            word = word.lower()\n            w = wn.morphy(word)\n            if w and len(w) &gt; 1 and w not in self.ignore_set:\n                words.append(w)\n        return words\n</code></pre>\n<p>Now to answer your actual question, it's possible that you need to open a file in byte mode before dumping a pickle, i.e.:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; from nltk import word_tokenize\n&gt;&gt;&gt; import cPickle as pickle\n&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=word_tokenize)\n&gt;&gt;&gt; vectorizer\nTfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n        dtype=&lt;type 'numpy.int64'&gt;, encoding=u'utf-8', input=u'content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(0, 2), norm=u'l2', preprocessor=None, smooth_idf=True,\n        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n        token_pattern='[a-zA-Z0-9]+',\n        tokenizer=&lt;function word_tokenize at 0x7f5ea68e88c0&gt;, use_idf=True,\n        vocabulary=None)\n&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:\n...     pickle.dump(vectorizer, fin)\n... \n&gt;&gt;&gt; exit()\nalvas@ubi:~$ ls -lah vectorizer.pk \n-rw-rw-r-- 1 alvas alvas 763 Jun 15 14:18 vectorizer.pk\n</code></pre>\n<p><strong>Note</strong>: Using the <code>with</code> idiom for i/o file access automatically closes the file once you get out of the <code>with</code> scope.</p>\n<p>Regarding the issue with <code>SnowballStemmer()</code>, note that <code>SnowballStemmer('english')</code> is an object while the stemming function is <code>SnowballStemmer('english').stem</code>. </p>\n<p><strong>IMPORTANT</strong>:</p>\n<ul>\n<li><code>TfidfVectorizer</code>'s tokenizer parameter expects to take a string and return a list of string</li>\n<li>But Snowball stemmer does not take a string as input and return a list of string.</li>\n</ul>\n<p>So you will need to do this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.stem import SnowballStemmer\n&gt;&gt;&gt; from nltk import word_tokenize\n&gt;&gt;&gt; stemmer = SnowballStemmer('english').stem\n&gt;&gt;&gt; def stem_tokenize(text):\n...     return [stemmer(i) for i in word_tokenize(text)]\n... \n&gt;&gt;&gt; vectorizer = TfidfVectorizer(ngram_range=(0,2),analyzer='word',lowercase=True, token_pattern='[a-zA-Z0-9]+',strip_accents='unicode',tokenizer=stem_tokenize)\n&gt;&gt;&gt; with open('vectorizer.pk', 'wb') as fin:\n...     pickle.dump(vectorizer, fin)\n...\n&gt;&gt;&gt; exit()\nalvas@ubi:~$ ls -lah vectorizer.pk \n-rw-rw-r-- 1 alvas alvas 758 Jun 15 15:55 vectorizer.pk\n</code></pre>\n", "abstract": "Firstly, it's better to leave the import at the top of your code instead of within your class: Next StemTokenizer don't seem to be a canonical class. Possibly you've got it from http://sahandsaba.com/visualizing-philosophers-and-scientists-by-the-words-they-used-with-d3js-and-python.html or maybe somewhere else so we'll assume it returns a list of strings. Now to answer your actual question, it's possible that you need to open a file in byte mode before dumping a pickle, i.e.: Note: Using the with idiom for i/o file access automatically closes the file once you get out of the with scope. Regarding the issue with SnowballStemmer(), note that SnowballStemmer('english') is an object while the stemming function is SnowballStemmer('english').stem.  IMPORTANT: So you will need to do this:"}]}, {"link": "https://stackoverflow.com/questions/1789254/clustering-text-in-python", "question": {"id": "1789254", "title": "Clustering text in Python", "content": "<p>I need to cluster some text documents and have been researching various options.  It looks like LingPipe can cluster plain text without prior conversion (to vector space etc), but it's the only tool I've seen that explicitly claims to work on strings.</p>\n<p>Are there any Python tools that can cluster text directly?  If not, what's the best way to handle this?</p>\n", "abstract": "I need to cluster some text documents and have been researching various options.  It looks like LingPipe can cluster plain text without prior conversion (to vector space etc), but it's the only tool I've seen that explicitly claims to work on strings. Are there any Python tools that can cluster text directly?  If not, what's the best way to handle this?"}, "answers": [{"id": 1791009, "score": 45, "vote": 0, "content": "<p>The quality of text-clustering depends mainly on two factors:</p>\n<ol>\n<li><p>Some notion of similarity between the documents you want to cluster. For example, it's easy to distinguish between newsarticles about sports and politics in vector space via tfidf-cosine-distance. It's a lot harder to cluster product-reviews in \"good\" or \"bad\" based on this measure.</p></li>\n<li><p>The clustering method itself. You know how many cluster there'll be? Ok, use kmeans. You don't care about accuracy but want to show a nice tree-structure for navigation of search-results? Use some kind of hierarchical clustering.</p></li>\n</ol>\n<p>There is no text-clustering solution, that would work well under any circumstances. And therefore it's probably not enough to take some clustering software out of the box and throw your data at it.</p>\n<p>Having said that, here's some experimental code i used some time ago to play around with text-clustering. The documents are represented as normalized tfidf-vectors and the similarity is measured as cosine distance. The clustering method itself is <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.3073&amp;rep=rep1&amp;type=pdf\" rel=\"noreferrer\">majorclust</a>.</p>\n<pre><code class=\"python\">import sys\nfrom math import log, sqrt\nfrom itertools import combinations\n\ndef cosine_distance(a, b):\n    cos = 0.0\n    a_tfidf = a[\"tfidf\"]\n    for token, tfidf in b[\"tfidf\"].iteritems():\n        if token in a_tfidf:\n            cos += tfidf * a_tfidf[token]\n    return cos\n\ndef normalize(features):\n    norm = 1.0 / sqrt(sum(i**2 for i in features.itervalues()))\n    for k, v in features.iteritems():\n        features[k] = v * norm\n    return features\n\ndef add_tfidf_to(documents):\n    tokens = {}\n    for id, doc in enumerate(documents):\n        tf = {}\n        doc[\"tfidf\"] = {}\n        doc_tokens = doc.get(\"tokens\", [])\n        for token in doc_tokens:\n            tf[token] = tf.get(token, 0) + 1\n        num_tokens = len(doc_tokens)\n        if num_tokens &gt; 0:\n            for token, freq in tf.iteritems():\n                tokens.setdefault(token, []).append((id, float(freq) / num_tokens))\n\n    doc_count = float(len(documents))\n    for token, docs in tokens.iteritems():\n        idf = log(doc_count / len(docs))\n        for id, tf in docs:\n            tfidf = tf * idf\n            if tfidf &gt; 0:\n                documents[id][\"tfidf\"][token] = tfidf\n\n    for doc in documents:\n        doc[\"tfidf\"] = normalize(doc[\"tfidf\"])\n\ndef choose_cluster(node, cluster_lookup, edges):\n    new = cluster_lookup[node]\n    if node in edges:\n        seen, num_seen = {}, {}\n        for target, weight in edges.get(node, []):\n            seen[cluster_lookup[target]] = seen.get(\n                cluster_lookup[target], 0.0) + weight\n        for k, v in seen.iteritems():\n            num_seen.setdefault(v, []).append(k)\n        new = num_seen[max(num_seen)][0]\n    return new\n\ndef majorclust(graph):\n    cluster_lookup = dict((node, i) for i, node in enumerate(graph.nodes))\n\n    count = 0\n    movements = set()\n    finished = False\n    while not finished:\n        finished = True\n        for node in graph.nodes:\n            new = choose_cluster(node, cluster_lookup, graph.edges)\n            move = (node, cluster_lookup[node], new)\n            if new != cluster_lookup[node] and move not in movements:\n                movements.add(move)\n                cluster_lookup[node] = new\n                finished = False\n\n    clusters = {}\n    for k, v in cluster_lookup.iteritems():\n        clusters.setdefault(v, []).append(k)\n\n    return clusters.values()\n\ndef get_distance_graph(documents):\n    class Graph(object):\n        def __init__(self):\n            self.edges = {}\n\n        def add_edge(self, n1, n2, w):\n            self.edges.setdefault(n1, []).append((n2, w))\n            self.edges.setdefault(n2, []).append((n1, w))\n\n    graph = Graph()\n    doc_ids = range(len(documents))\n    graph.nodes = set(doc_ids)\n    for a, b in combinations(doc_ids, 2):\n        graph.add_edge(a, b, cosine_distance(documents[a], documents[b]))\n    return graph\n\ndef get_documents():\n    texts = [\n        \"foo blub baz\",\n        \"foo bar baz\",\n        \"asdf bsdf csdf\",\n        \"foo bab blub\",\n        \"csdf hddf kjtz\",\n        \"123 456 890\",\n        \"321 890 456 foo\",\n        \"123 890 uiop\",\n    ]\n    return [{\"text\": text, \"tokens\": text.split()}\n             for i, text in enumerate(texts)]\n\ndef main(args):\n    documents = get_documents()\n    add_tfidf_to(documents)\n    dist_graph = get_distance_graph(documents)\n\n    for cluster in majorclust(dist_graph):\n        print \"=========\"\n        for doc_id in cluster:\n            print documents[doc_id][\"text\"]\n\nif __name__ == '__main__':\n    main(sys.argv)\n</code></pre>\n<p>For real applications, you would use a decent tokenizer, use integers instead of token-strings and don't calc a O(n^2) distance-matrix... </p>\n", "abstract": "The quality of text-clustering depends mainly on two factors: Some notion of similarity between the documents you want to cluster. For example, it's easy to distinguish between newsarticles about sports and politics in vector space via tfidf-cosine-distance. It's a lot harder to cluster product-reviews in \"good\" or \"bad\" based on this measure. The clustering method itself. You know how many cluster there'll be? Ok, use kmeans. You don't care about accuracy but want to show a nice tree-structure for navigation of search-results? Use some kind of hierarchical clustering. There is no text-clustering solution, that would work well under any circumstances. And therefore it's probably not enough to take some clustering software out of the box and throw your data at it. Having said that, here's some experimental code i used some time ago to play around with text-clustering. The documents are represented as normalized tfidf-vectors and the similarity is measured as cosine distance. The clustering method itself is majorclust. For real applications, you would use a decent tokenizer, use integers instead of token-strings and don't calc a O(n^2) distance-matrix... "}, {"id": 1789423, "score": 2, "vote": 0, "content": "<p>It seems to be possible by using simple UNIX command line tools to extract the text contents of those documents into text files, then using a pure Python solution for the actual clustering.</p>\n<p>I found a code snippet for clustering data in general:</p>\n<p><a href=\"http://www.daniweb.com/code/snippet216641.html\" rel=\"nofollow noreferrer\">http://www.daniweb.com/code/snippet216641.html</a></p>\n<p>A Python package for this:</p>\n<p><a href=\"http://python-cluster.sourceforge.net/\" rel=\"nofollow noreferrer\">http://python-cluster.sourceforge.net/</a></p>\n<p>Another python package (used mainly for bioinformatics):</p>\n<p><a href=\"http://bonsai.ims.u-tokyo.ac.jp/~mdehoon/software/cluster/software.htm#pycluster\" rel=\"nofollow noreferrer\">http://bonsai.ims.u-tokyo.ac.jp/~mdehoon/software/cluster/software.htm#pycluster</a></p>\n", "abstract": "It seems to be possible by using simple UNIX command line tools to extract the text contents of those documents into text files, then using a pure Python solution for the actual clustering. I found a code snippet for clustering data in general: http://www.daniweb.com/code/snippet216641.html A Python package for this: http://python-cluster.sourceforge.net/ Another python package (used mainly for bioinformatics): http://bonsai.ims.u-tokyo.ac.jp/~mdehoon/software/cluster/software.htm#pycluster"}, {"id": 10309649, "score": 0, "vote": 0, "content": "<p>There is Python library <a href=\"http://www.nltk.org/\" rel=\"nofollow\">NLTK</a> that supports linguistic analysis including clustering text</p>\n", "abstract": "There is Python library NLTK that supports linguistic analysis including clustering text"}]}, {"link": "https://stackoverflow.com/questions/26890605/filter-twitter-feeds-only-by-language", "question": {"id": "26890605", "title": "Filter Twitter feeds only by language", "content": "<p>I am using Tweepy API for extracting Twitter feeds. I want to extract all Twitter feeds of a specific language only. The language filter works only if <code>track</code> filter is provided. The following code returns 406 error:</p>\n<pre><code class=\"python\">l = StdOutListener()\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\nstream = Stream(auth, l)\nstream.filter(languages=[\"en\"])\n</code></pre>\n<p>How can I extract <strong>all</strong> the tweets from certain language using Tweepy? </p>\n", "abstract": "I am using Tweepy API for extracting Twitter feeds. I want to extract all Twitter feeds of a specific language only. The language filter works only if track filter is provided. The following code returns 406 error: How can I extract all the tweets from certain language using Tweepy? "}, "answers": [{"id": 26921161, "score": 28, "vote": 0, "content": "<p>You can't (without special access). Streaming <strong>all</strong> the tweets (unfiltered) requires a connection to <a href=\"https://dev.twitter.com/streaming/reference/get/statuses/firehose\" rel=\"noreferrer\">the firehose</a>, which is granted only in specific use cases by Twitter. Honestly, the firehose isn't really necessary--proper use of <a href=\"https://dev.twitter.com/streaming/overview/request-parameters#track\" rel=\"noreferrer\"><code>track</code></a> can get you more tweets than you know what to do with.</p>\n<p>Try using something like this:</p>\n<pre><code class=\"python\">stream.filter(languages=[\"en\"], track=[\"a\", \"the\", \"i\", \"you\", \"u\"]) # etc\n</code></pre>\n<p>Filtering by words like that will get you many, many tweets. If you want real data for the most-used words, check out this article from Time: <a href=\"http://techland.time.com/2009/06/08/the-500-most-frequently-used-words-on-twitter/\" rel=\"noreferrer\">The 500 Most Frequently Used Words on Twitter</a>. You can use <a href=\"https://twittercommunity.com/t/streaming-api-limit/8707\" rel=\"noreferrer\">up to 400</a> keywords, but that will likely approach the 1% limit of tweets at a given time interval. If your <code>track</code> parameter matches 60% of all tweets at a given time, you will still only get 1% (which is a LOT of tweets).</p>\n", "abstract": "You can't (without special access). Streaming all the tweets (unfiltered) requires a connection to the firehose, which is granted only in specific use cases by Twitter. Honestly, the firehose isn't really necessary--proper use of track can get you more tweets than you know what to do with. Try using something like this: Filtering by words like that will get you many, many tweets. If you want real data for the most-used words, check out this article from Time: The 500 Most Frequently Used Words on Twitter. You can use up to 400 keywords, but that will likely approach the 1% limit of tweets at a given time interval. If your track parameter matches 60% of all tweets at a given time, you will still only get 1% (which is a LOT of tweets)."}, {"id": 49614257, "score": 8, "vote": 0, "content": "<p>Try <code>lang='en'</code> param in <code>Cursor()</code>  e.g.</p>\n<p><code>tweepy.Cursor(.. lang='en')</code></p>\n", "abstract": "Try lang='en' param in Cursor()  e.g. tweepy.Cursor(.. lang='en')"}, {"id": 40693339, "score": 4, "vote": 0, "content": "<p>Other than getting filtered tweets directly, you can filter it after getting all tweets of different languages by:</p>\n<pre><code class=\"python\">tweets = api.search(\"python\")\nfor tweet in tweets:\n   if tweet.lang == \"en\":\n      print(tweet.text)\n      #Do the stuff here\n</code></pre>\n<p>Hope it helps.</p>\n", "abstract": "Other than getting filtered tweets directly, you can filter it after getting all tweets of different languages by: Hope it helps."}, {"id": 54445150, "score": 2, "vote": 0, "content": "<p>You can see the arguments for the track method in the github code <a href=\"https://github.com/tweepy/tweepy/blob/master/tweepy/streaming.py\" rel=\"nofollow noreferrer\">https://github.com/tweepy/tweepy/blob/master/tweepy/streaming.py</a></p>\n<p>Put languages in a array of ISO_639-1_codes.</p>\n<p>They are:</p>\n<pre><code class=\"python\">filter(self, follow=None, track=None, is_async=False, locations=None,\n               stall_warnings=False, languages=None, encoding='utf8', filter_level=None):\n</code></pre>\n<p>So to track by languages just put:</p>\n<pre><code class=\"python\">class Listener(StreamListener):\n\n    def on_data(self, data):\n        j = json.loads(data)\n        t = {\n          'screenName' : j['user']['screen_name'],\n          'text:': j['text']\n          }\n        print(t)\n        return(True)\n\n    def on_status(self, status):\n        print(status.text)\n\n\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\nstream = Stream(auth=auth, listener=Listener(),wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n\nstream.filter(track=['Trump'],languages=[\"en\",\"fr\",\"es\"])\n</code></pre>\n", "abstract": "You can see the arguments for the track method in the github code https://github.com/tweepy/tweepy/blob/master/tweepy/streaming.py Put languages in a array of ISO_639-1_codes. They are: So to track by languages just put:"}, {"id": 58293135, "score": 1, "vote": 0, "content": "<p><a href=\"http://docs.tweepy.org/en/latest/api.html\" rel=\"nofollow noreferrer\">Tweepy search</a> allows to fetch tweets for specific language. You can use ISO 639-1 code to specify the value for language parameter.\nFollowing code will fetch tweets with full text in specified language (English for below example)</p>\n<pre><code class=\"python\">    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n    api = tweepy.API(auth)\n    tweets = api.search(q = keywordtosearch, lang = 'en', count = 100, truncated = False, tweet_mode = 'extended')\n    for tweet in tweets:\n        print(tweet.full_text)\n        #add your code\n</code></pre>\n", "abstract": "Tweepy search allows to fetch tweets for specific language. You can use ISO 639-1 code to specify the value for language parameter.\nFollowing code will fetch tweets with full text in specified language (English for below example)"}, {"id": 54501945, "score": 0, "vote": 0, "content": "<p>This worked for me.</p>\n<pre><code class=\"python\">auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)\na=input(\"Enter Tag: \")\ntweets = api.search(a, count=200)\na=[]\nfor tweet in tweets:\n    if tweet.lang == \"en\":\n        a.append(tweet.text)\n</code></pre>\n", "abstract": "This worked for me."}, {"id": 62000133, "score": 0, "vote": 0, "content": "<p>With the help of GetOldTweets3 (<a href=\"https://pypi.org/project/GetOldTweets3/\" rel=\"nofollow noreferrer\">https://pypi.org/project/GetOldTweets3/</a>), you can download tweets (even old ones) by filtering over few criteria, as shown below:</p>\n<pre><code class=\"python\">tweetCriteria = got.manager.TweetCriteria().setQuerySearch('Coronavirus')\\\n                                       .setSince(\"2020-02-15\")\\\n                                       .setUntil(\"2020-03-29\")\\\n                                       .setMaxTweets(5)\\\n                                       .setNear('India')\\\n                                       .setLang('en')\ntweets = got.manager.TweetManager.getTweets(tweetCriteria)\nfor tweet in tweets:\n    print(tweet.text)\n    print(tweet.date)\n    print(tweet.geo)\n    print(tweet.id)\n    print(tweet.permalink)\n    print(tweet.username)\n    print(tweet.retweets)\n    print(tweet.favorites)\n    print(tweet.mentions)\n    print(tweet.hashtags)\n    print('*'*50)\n</code></pre>\n", "abstract": "With the help of GetOldTweets3 (https://pypi.org/project/GetOldTweets3/), you can download tweets (even old ones) by filtering over few criteria, as shown below:"}]}, {"link": "https://stackoverflow.com/questions/10098533/implementing-bag-of-words-naive-bayes-classifier-in-nltk", "question": {"id": "10098533", "title": "Implementing Bag-of-Words Naive-Bayes classifier in NLTK", "content": "<p>I basically have the <a href=\"https://stackoverflow.com/questions/2162718/python-nltk-code-snippet-to-train-a-classifier-naive-bayes-using-feature-frequ\">same question as this guy</a>.. The <a href=\"http://nltk.googlecode.com/svn/trunk/doc/book/ch06.html#document-classify-all-words\" rel=\"nofollow noreferrer\">example in the NLTK book</a> for the Naive Bayes classifier considers only whether a word occurs in a document as a feature.. it doesn't consider the frequency of the words as the feature to look at (\"bag-of-words\").</p>\n<p><a href=\"https://stackoverflow.com/a/2226115/378622\">One of the answers</a> seems to suggest this can't be done with the built in NLTK classifiers.  Is that the case?  How can I do frequency/bag-of-words NB classification with NLTK?</p>\n", "abstract": "I basically have the same question as this guy.. The example in the NLTK book for the Naive Bayes classifier considers only whether a word occurs in a document as a feature.. it doesn't consider the frequency of the words as the feature to look at (\"bag-of-words\"). One of the answers seems to suggest this can't be done with the built in NLTK classifiers.  Is that the case?  How can I do frequency/bag-of-words NB classification with NLTK?"}, "answers": [{"id": 10098909, "score": 32, "vote": 0, "content": "<p><a href=\"http://scikit-learn.org\" rel=\"noreferrer\">scikit-learn</a> has <a href=\"http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes\" rel=\"noreferrer\">an implementation of multinomial naive Bayes</a>, which is the right variant of naive Bayes in this situation. A support vector machine (SVM) would probably work better, though.</p>\n<p>As Ken pointed out in the comments, NLTK has <a href=\"https://github.com/nltk/nltk/blob/master/nltk/classify/scikitlearn.py\" rel=\"noreferrer\">a nice wrapper for scikit-learn classifiers</a>. Modified from the docs, here's a somewhat complicated one that does TF-IDF weighting, chooses the 1000 best features based on a chi2 statistic, and then passes that into a multinomial naive Bayes classifier. (I bet this is somewhat clumsy, as I'm not super familiar with either NLTK or scikit-learn.)</p>\n<pre><code class=\"python\">import numpy as np\nfrom nltk.probability import FreqDist\nfrom nltk.classify import SklearnClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('tfidf', TfidfTransformer()),\n                     ('chi2', SelectKBest(chi2, k=1000)),\n                     ('nb', MultinomialNB())])\nclassif = SklearnClassifier(pipeline)\n\nfrom nltk.corpus import movie_reviews\npos = [FreqDist(movie_reviews.words(i)) for i in movie_reviews.fileids('pos')]\nneg = [FreqDist(movie_reviews.words(i)) for i in movie_reviews.fileids('neg')]\nadd_label = lambda lst, lab: [(x, lab) for x in lst]\nclassif.train(add_label(pos[:100], 'pos') + add_label(neg[:100], 'neg'))\n\nl_pos = np.array(classif.classify_many(pos[100:]))\nl_neg = np.array(classif.classify_many(neg[100:]))\nprint \"Confusion matrix:\\n%d\\t%d\\n%d\\t%d\" % (\n          (l_pos == 'pos').sum(), (l_pos == 'neg').sum(),\n          (l_neg == 'pos').sum(), (l_neg == 'neg').sum())\n</code></pre>\n<p>This printed for me:</p>\n<pre><code class=\"python\">Confusion matrix:\n524     376\n202     698\n</code></pre>\n<p>Not perfect, but decent, considering it's not a super easy problem and it's only trained on 100/100.</p>\n", "abstract": "scikit-learn has an implementation of multinomial naive Bayes, which is the right variant of naive Bayes in this situation. A support vector machine (SVM) would probably work better, though. As Ken pointed out in the comments, NLTK has a nice wrapper for scikit-learn classifiers. Modified from the docs, here's a somewhat complicated one that does TF-IDF weighting, chooses the 1000 best features based on a chi2 statistic, and then passes that into a multinomial naive Bayes classifier. (I bet this is somewhat clumsy, as I'm not super familiar with either NLTK or scikit-learn.) This printed for me: Not perfect, but decent, considering it's not a super easy problem and it's only trained on 100/100."}, {"id": 10098707, "score": 7, "vote": 0, "content": "<p>The features in the NLTK bayes classifier are \"nominal\", not numeric. This means they can take a finite number of discrete values (labels), but they can't be treated as frequencies.</p>\n<p>So with the Bayes classifier, you cannot <em>directly</em> use word frequency as a feature-- you could do something like use the 50 more frequent words from each text as your feature set, but that's quite a different thing</p>\n<p>But maybe there are other classifiers in the NLTK that depend on frequency. I wouldn't know, but have you looked? I'd say it's worth checking out.</p>\n", "abstract": "The features in the NLTK bayes classifier are \"nominal\", not numeric. This means they can take a finite number of discrete values (labels), but they can't be treated as frequencies. So with the Bayes classifier, you cannot directly use word frequency as a feature-- you could do something like use the 50 more frequent words from each text as your feature set, but that's quite a different thing But maybe there are other classifiers in the NLTK that depend on frequency. I wouldn't know, but have you looked? I'd say it's worth checking out."}, {"id": 22303324, "score": 3, "vote": 0, "content": "<ul>\n<li>put the string you are looking at into a list, broken into words </li>\n<li>for each item in the list, ask: is this item a feature I have in my feature list. </li>\n<li>If it is, add the log prob as normal, if not, ignore it.</li>\n</ul>\n<p>If your sentence has the same word multiple times, it will just add the probs multiple times.  If the word appears multiple times in the same class, your training data should reflect that in the word count.</p>\n<p>For added accuracy, count all bi-grams, tri-grams, etc as separate features.</p>\n<p>It helps to manually write your own classifiers so that you understand exactly what is happening and what you need to do to imporve accuracy.  If you use a pre-packaged solution and it doesn't work well enough, there is not much you can do about it.</p>\n", "abstract": "If your sentence has the same word multiple times, it will just add the probs multiple times.  If the word appears multiple times in the same class, your training data should reflect that in the word count. For added accuracy, count all bi-grams, tri-grams, etc as separate features. It helps to manually write your own classifiers so that you understand exactly what is happening and what you need to do to imporve accuracy.  If you use a pre-packaged solution and it doesn't work well enough, there is not much you can do about it."}]}, {"link": "https://stackoverflow.com/questions/60492839/how-to-compare-sentence-similarities-using-embeddings-from-bert", "question": {"id": "60492839", "title": "How to compare sentence similarities using embeddings from BERT", "content": "<p>I am using the HuggingFace Transformers package to access pretrained models. As my use case needs functionality for both English and Arabic, I am using the <a href=\"https://github.com/google-research/bert/blob/master/multilingual.md\" rel=\"noreferrer\">bert-base-multilingual-cased</a> pretrained model. I need to be able to compare the similarity of sentences using something such as cosine similarity. To use  this, I first need to get an embedding vector for each sentence, and can then compute the cosine similarity.</p>\n<p>Firstly, what is the best way to extratc the semantic embedding from the BERT model? Would taking the last hidden state of the model after being fed the sentence suffice?</p>\n<pre><code class=\"python\">import torch\nfrom transformers import BertModel, BertTokenizer\n\nmodel_class = BertModel\ntokenizer_class = BertTokenizer\npretrained_weights = 'bert-base-multilingual-cased'\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\nsentence = 'this is a test sentence'\n\ninput_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\nwith torch.no_grad():\n    output_tuple = model(input_ids)\n    last_hidden_states = output_tuple[0]\n\nprint(last_hidden_states.size(), last_hidden_states)\n</code></pre>\n<p>Secondly, if this is a sufficient way to get embeddings from my sentence, I now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. The shapes output are <code>[1, n, vocab_size]</code>, where <code>n</code> can have any value. </p>\n<p>In order to compute two vectors' cosine similarity, they need to be the same  length. How can I do this here? Could something as naive as first summing across <code>axis=1</code> still work? What other options do I have? </p>\n", "abstract": "I am using the HuggingFace Transformers package to access pretrained models. As my use case needs functionality for both English and Arabic, I am using the bert-base-multilingual-cased pretrained model. I need to be able to compare the similarity of sentences using something such as cosine similarity. To use  this, I first need to get an embedding vector for each sentence, and can then compute the cosine similarity. Firstly, what is the best way to extratc the semantic embedding from the BERT model? Would taking the last hidden state of the model after being fed the sentence suffice? Secondly, if this is a sufficient way to get embeddings from my sentence, I now have another problem where the embedding vectors have different lengths depending on the length of the original sentence. The shapes output are [1, n, vocab_size], where n can have any value.  In order to compute two vectors' cosine similarity, they need to be the same  length. How can I do this here? Could something as naive as first summing across axis=1 still work? What other options do I have? "}, "answers": [{"id": 60504075, "score": 21, "vote": 0, "content": "<p>In addition to an already great accepted answer, I want to point you to <a href=\"https://arxiv.org/abs/1908.10084\" rel=\"noreferrer\"><code>sentence-BERT</code></a>, which discusses the similarity aspect and implications of specific metrics (like cosine similarity) in greater detail.\nThey also have a <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"noreferrer\">very convenient implementation</a> online. The main advantage here is that they seemingly gain a lot of processing speed compared to a \"naive\" sentence embedding comparison, but I am not familiar enough with the implementation itself.</p>\n<p>Importantly, there is also generally a more fine-grained distinction in <em>what kind of similarity</em> you want to look at. Specifically for that, there is also a great discussion in one of the <a href=\"https://www.aclweb.org/anthology/S14-2001.pdf\" rel=\"noreferrer\">task papers</a> from SemEval 2014 (SICK dataset), which goes into more detail about this. From your task description, I am assuming that you are already using data from one of the later SemEval tasks, which also extended this to multilingual similarity.</p>\n", "abstract": "In addition to an already great accepted answer, I want to point you to sentence-BERT, which discusses the similarity aspect and implications of specific metrics (like cosine similarity) in greater detail.\nThey also have a very convenient implementation online. The main advantage here is that they seemingly gain a lot of processing speed compared to a \"naive\" sentence embedding comparison, but I am not familiar enough with the implementation itself. Importantly, there is also generally a more fine-grained distinction in what kind of similarity you want to look at. Specifically for that, there is also a great discussion in one of the task papers from SemEval 2014 (SICK dataset), which goes into more detail about this. From your task description, I am assuming that you are already using data from one of the later SemEval tasks, which also extended this to multilingual similarity."}, {"id": 60493083, "score": 12, "vote": 0, "content": "<p>You can use the <code>[CLS]</code> token as a representation for the entire sequence. This token is typically prepended to your sentence during the preprocessing step. This token that is typically used for classification tasks (see figure 2 and paragraph 3.2 in the <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">BERT paper</a>).</p>\n<p>It is the very first token of the embedding.</p>\n<p>Alternatively you can take the average vector of the sequence (like you say over the first(?) axis), which can yield better results according to the <a href=\"https://huggingface.co/transformers/model_doc/bert.html\" rel=\"noreferrer\">huggingface documentation</a> (3rd tip).</p>\n<p>Note that BERT was not designed for sentence similarity using the cosine distance, though in my experience it does yield decent results. </p>\n", "abstract": "You can use the [CLS] token as a representation for the entire sequence. This token is typically prepended to your sentence during the preprocessing step. This token that is typically used for classification tasks (see figure 2 and paragraph 3.2 in the BERT paper). It is the very first token of the embedding. Alternatively you can take the average vector of the sequence (like you say over the first(?) axis), which can yield better results according to the huggingface documentation (3rd tip). Note that BERT was not designed for sentence similarity using the cosine distance, though in my experience it does yield decent results. "}, {"id": 71300267, "score": 1, "vote": 0, "content": "<p><strong>You should NOT use BERT's output as sentence embeddings for semantic similarity.</strong> BERT is not pretrained for semantic similarity, which will result in poor results, even worse than simple Glove Embeddings. See below a comment from Jacob Devlin (first author in BERT's paper) and a piece from the Sentence-BERT paper, which discusses in detail sentence embeddings.</p>\n<blockquote>\n<p><strong>Jacob Devlin's comment:</strong> I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally). (<a href=\"https://github.com/google-research/bert/issues/164#issuecomment-441324222\" rel=\"nofollow noreferrer\">https://github.com/google-research/bert/issues/164#issuecomment-441324222</a>)</p>\n</blockquote>\n<blockquote>\n<p><strong>From Sentence-BERT paper:</strong> The results show that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLS token output only achieves an average correlation of 29.19. Both are worse than computing average GloVe embeddings. (<a href=\"https://arxiv.org/pdf/1908.10084.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/1908.10084.pdf</a>)</p>\n</blockquote>\n<p><strong>You should use instead a model pre-trained specifically for sentence similarity</strong>, such as Sentence-BERT. Sentence-BERT and several other pretrained models for sentence similarity are available in the sentence-transformers library (<a href=\"https://www.sbert.net/docs/pretrained_models.html\" rel=\"nofollow noreferrer\">https://www.sbert.net/docs/pretrained_models.html</a>), which is fully compatible with the amazing HuggingFace transformers library. With these libraries, you can obtain sentence embeddings in just a line of code.</p>\n", "abstract": "You should NOT use BERT's output as sentence embeddings for semantic similarity. BERT is not pretrained for semantic similarity, which will result in poor results, even worse than simple Glove Embeddings. See below a comment from Jacob Devlin (first author in BERT's paper) and a piece from the Sentence-BERT paper, which discusses in detail sentence embeddings. Jacob Devlin's comment: I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally). (https://github.com/google-research/bert/issues/164#issuecomment-441324222) From Sentence-BERT paper: The results show that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLS token output only achieves an average correlation of 29.19. Both are worse than computing average GloVe embeddings. (https://arxiv.org/pdf/1908.10084.pdf) You should use instead a model pre-trained specifically for sentence similarity, such as Sentence-BERT. Sentence-BERT and several other pretrained models for sentence similarity are available in the sentence-transformers library (https://www.sbert.net/docs/pretrained_models.html), which is fully compatible with the amazing HuggingFace transformers library. With these libraries, you can obtain sentence embeddings in just a line of code."}, {"id": 72803274, "score": 1, "vote": 0, "content": "<p>As a complement to <a href=\"https://stackoverflow.com/users/3607203/dennlinger\">dennlinger</a>'s <a href=\"https://stackoverflow.com/a/60504075/395857\">answer</a>, I'll add a code example from <a href=\"https://www.sbert.net/docs/usage/semantic_textual_similarity.html\" rel=\"nofollow noreferrer\">https://www.sbert.net/docs/usage/semantic_textual_similarity.html</a> to compare sentence similarities using embeddings from BERT:</p>\n<pre><code class=\"python\">from sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n\n# Two lists of sentences\nsentences1 = ['The cat sits outside',\n             'A man is playing guitar',\n             'The new movie is awesome']\n\nsentences2 = ['The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n\n#Compute cosine-similarits\ncosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n</code></pre>\n<p>The library contains the state-of-the-art sentence embedding models.</p>\n<p>See <a href=\"https://stackoverflow.com/a/68728666/395857\">https://stackoverflow.com/a/68728666/395857</a> to perform sentence clustering.</p>\n", "abstract": "As a complement to dennlinger's answer, I'll add a code example from https://www.sbert.net/docs/usage/semantic_textual_similarity.html to compare sentence similarities using embeddings from BERT: The library contains the state-of-the-art sentence embedding models. See https://stackoverflow.com/a/68728666/395857 to perform sentence clustering."}]}, {"link": "https://stackoverflow.com/questions/30653642/combining-bag-of-words-and-other-features-in-one-model-using-sklearn-and-pandas", "question": {"id": "30653642", "title": "Combining bag of words and other features in one model using sklearn and pandas", "content": "<p>I am trying to model the score that a post receives, based on both the text of the post, and other features (time of day, length of post, etc.)</p>\n<p>I am wondering how to best combine these different types of features into one model. Right now, I have something like the following (stolen from <a href=\"https://stackoverflow.com/questions/22687365/concatenate-custom-features-with-countvectorizer\">here</a> and <a href=\"https://stackoverflow.com/questions/27993058/pandas-apply-to-dateframe-produces-built-in-method-values-of\">here</a>). </p>\n<pre><code class=\"python\">import pandas as pd\n...\n\ndef features(p):\n    terms = vectorizer(p[0])\n    d = {'feature_1': p[1], 'feature_2': p[2]}\n    for t in terms:\n        d[t] = d.get(t, 0) + 1\n    return d\n\nposts = pd.read_csv('path/to/csv')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2)).build_tokenizer()\ny = posts[\"score\"].values.astype(np.float32) \nvect = DictVectorizer()\n\n# This is the part I want to fix\ntemp = zip(list(posts.message), list(posts.feature_1), list(posts.feature_2))\ntokenized = map(lambda x: features(x), temp)\nX = vect.fit_transform(tokenized)\n</code></pre>\n<p>It seems very silly to extract all of the features I want out of the pandas dataframe, just to zip them all back together. Is there a better way of doing this step?</p>\n<p>The CSV looks something like the following:</p>\n<pre><code class=\"python\">ID,message,feature_1,feature_2\n1,'This is the text',4,7\n2,'This is more text',3,2\n...\n</code></pre>\n", "abstract": "I am trying to model the score that a post receives, based on both the text of the post, and other features (time of day, length of post, etc.) I am wondering how to best combine these different types of features into one model. Right now, I have something like the following (stolen from here and here).  It seems very silly to extract all of the features I want out of the pandas dataframe, just to zip them all back together. Is there a better way of doing this step? The CSV looks something like the following:"}, "answers": [{"id": 31351465, "score": 29, "vote": 0, "content": "<p>You could do everything with your map and lambda:</p>\n<pre><code class=\"python\">tokenized=map(lambda msg, ft1, ft2: features([msg,ft1,ft2]), posts.message,posts.feature_1, posts.feature_2)\n</code></pre>\n<p>This saves doing your interim temp step and iterates through the 3 columns.</p>\n<p>Another solution would be convert the messages into their CountVectorizer sparse matrix and join this matrix with the feature values from the posts dataframe (this skips having to construct a dict and produces a sparse matrix similar to what you would get with DictVectorizer):</p>\n<pre><code class=\"python\">import scipy as sp\nposts = pd.read_csv('post.csv')\n\n# Create vectorizer for function to use\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\ny = posts[\"score\"].values.astype(np.float32) \n\nX = sp.sparse.hstack((vectorizer.fit_transform(posts.message),posts[['feature_1','feature_2']].values),format='csr')\nX_columns=vectorizer.get_feature_names()+posts[['feature_1','feature_2']].columns.tolist()\n\n\nposts\nOut[38]: \n   ID              message  feature_1  feature_2  score\n0   1   'This is the text'          4          7     10\n1   2  'This is more text'          3          2      9\n2   3   'More random text'          3          2      9\n\nX_columns\nOut[39]: \n[u'is',\n u'is more',\n u'is the',\n u'more',\n u'more random',\n u'more text',\n u'random',\n u'random text',\n u'text',\n u'the',\n u'the text',\n u'this',\n u'this is',\n 'feature_1',\n 'feature_2']\n\nX.toarray()\nOut[40]: \narray([[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 4, 7],\n       [1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 3, 2],\n       [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 3, 2]])\n</code></pre>\n<p>Additionally sklearn-pandas has DataFrameMapper which does what you're looking for too:</p>\n<pre><code class=\"python\">from sklearn_pandas import DataFrameMapper\nmapper = DataFrameMapper([\n    (['feature_1', 'feature_2'], None),\n    ('message',CountVectorizer(binary=True, ngram_range=(1, 2)))\n])\nX=mapper.fit_transform(posts)\n\nX\nOut[71]: \narray([[4, 7, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [3, 2, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1],\n       [3, 2, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0]])\n</code></pre>\n<p>Note:X is not sparse when using this last method.</p>\n<pre><code class=\"python\">X_columns=mapper.features[0][0]+mapper.features[1][1].get_feature_names()\n\nX_columns\nOut[76]: \n['feature_1',\n 'feature_2',\n u'is',\n u'is more',\n u'is the',\n u'more',\n u'more random',\n u'more text',\n u'random',\n u'random text',\n u'text',\n u'the',\n u'the text',\n u'this',\n u'this is']\n</code></pre>\n", "abstract": "You could do everything with your map and lambda: This saves doing your interim temp step and iterates through the 3 columns. Another solution would be convert the messages into their CountVectorizer sparse matrix and join this matrix with the feature values from the posts dataframe (this skips having to construct a dict and produces a sparse matrix similar to what you would get with DictVectorizer): Additionally sklearn-pandas has DataFrameMapper which does what you're looking for too: Note:X is not sparse when using this last method."}]}, {"link": "https://stackoverflow.com/questions/34628224/pronoun-resolution-backwards", "question": {"id": "34628224", "title": "pronoun resolution backwards", "content": "<p>The usual coreference resolution works in the following way:</p>\n<p>Provided</p>\n<pre><code class=\"python\">The man likes math. He really does.\n</code></pre>\n<p>it figures out that </p>\n<pre><code class=\"python\">he \n</code></pre>\n<p>refers to </p>\n<pre><code class=\"python\">the man.\n</code></pre>\n<p>There are plenty of tools to do this.</p>\n<p>However, is there a way to do it backwards? </p>\n<p>For example,</p>\n<p>given</p>\n<pre><code class=\"python\">The man likes math. The man really does.\n</code></pre>\n<p>I want to do the pronoun resolution \"backwards,\"</p>\n<p>so that I get an output like</p>\n<pre><code class=\"python\">The man likes math. He really does.\n</code></pre>\n<p>My input text will mostly be 3~10 sentences, and I'm working with python.</p>\n", "abstract": "The usual coreference resolution works in the following way: Provided it figures out that  refers to  There are plenty of tools to do this. However, is there a way to do it backwards?  For example, given I want to do the pronoun resolution \"backwards,\" so that I get an output like My input text will mostly be 3~10 sentences, and I'm working with python."}, "answers": [{"id": 34708531, "score": 9, "vote": 0, "content": "<p>This is perhaps not really an answer to be happy with, but I think the answer is that there's no such functionality built in anywhere, though you can code it yourself without too much difficulty. Giving an outline of how I'd do it with CoreNLP:</p>\n<ol>\n<li><p>Still run coref. This'll tell you that \"the man\" and \"the man\" are coreferent, and so you can replace the second one with a pronoun.</p></li>\n<li><p>Run the <code>gender</code> annotator from CoreNLP. This is a poorly-documented and even more poorly advertised annotator that tries to attach gender to tokens in a sentence.</p></li>\n<li><p>Somehow figure out plurals. Most of the time you could use the part-of-speech tag: plural nouns get the tags NNS or NNPS, but there are some complications so you might also want to consider (1) the existence of conjunctions in the antecedent; (2) the lemma of a word being different from its text; (3) especially in conjunction with 2, the word ending in 's' or 'es' -- this can distinguish between lemmatizations which strip out plurals versus lemmatizations which strip out tenses, etc.</p></li>\n<li><p>This is enough to figure out the right pronoun. Now it's just a matter of chopping up the sentence and putting it back together. This is a bit of a pain if you do it in CoreNLP -- the code is just not set up to change the text of a sentence -- but in the worst case you can always just re-annotate a new surface form.</p></li>\n</ol>\n<p>Hope this helps somewhat!</p>\n", "abstract": "This is perhaps not really an answer to be happy with, but I think the answer is that there's no such functionality built in anywhere, though you can code it yourself without too much difficulty. Giving an outline of how I'd do it with CoreNLP: Still run coref. This'll tell you that \"the man\" and \"the man\" are coreferent, and so you can replace the second one with a pronoun. Run the gender annotator from CoreNLP. This is a poorly-documented and even more poorly advertised annotator that tries to attach gender to tokens in a sentence. Somehow figure out plurals. Most of the time you could use the part-of-speech tag: plural nouns get the tags NNS or NNPS, but there are some complications so you might also want to consider (1) the existence of conjunctions in the antecedent; (2) the lemma of a word being different from its text; (3) especially in conjunction with 2, the word ending in 's' or 'es' -- this can distinguish between lemmatizations which strip out plurals versus lemmatizations which strip out tenses, etc. This is enough to figure out the right pronoun. Now it's just a matter of chopping up the sentence and putting it back together. This is a bit of a pain if you do it in CoreNLP -- the code is just not set up to change the text of a sentence -- but in the worst case you can always just re-annotate a new surface form. Hope this helps somewhat!"}]}, {"link": "https://stackoverflow.com/questions/20827741/nltk-naivebayesclassifier-training-for-sentiment-analysis", "question": {"id": "20827741", "title": "nltk NaiveBayesClassifier training for sentiment analysis", "content": "<p>I am training the <code>NaiveBayesClassifier</code> in Python using sentences, and it gives me the error below. I do not understand what the error might be, and any help would be good. </p>\n<p>I have tried many other input formats, but the error remains. The code given below:</p>\n<pre><code class=\"python\">from text.classifiers import NaiveBayesClassifier\nfrom text.blob import TextBlob\ntrain = [('I love this sandwich.', 'pos'),\n         ('This is an amazing place!', 'pos'),\n         ('I feel very good about these beers.', 'pos'),\n         ('This is my best work.', 'pos'),\n         (\"What an awesome view\", 'pos'),\n         ('I do not like this restaurant', 'neg'),\n         ('I am tired of this stuff.', 'neg'),\n         (\"I can't deal with this\", 'neg'),\n         ('He is my sworn enemy!', 'neg'),\n         ('My boss is horrible.', 'neg') ]\n\ntest = [('The beer was good.', 'pos'),\n        ('I do not enjoy my job', 'neg'),\n        (\"I ain't feeling dandy today.\", 'neg'),\n        (\"I feel amazing!\", 'pos'),\n        ('Gary is a friend of mine.', 'pos'),\n        (\"I can't believe I'm doing this.\", 'neg') ]\nclassifier = nltk.NaiveBayesClassifier.train(train)\n</code></pre>\n<p>I am including the traceback below.</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"C:\\Users\\5460\\Desktop\\train01.py\", line 15, in &lt;module&gt;\n    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n  File \"C:\\Users\\5460\\Desktop\\train01.py\", line 15, in &lt;genexpr&gt;\n    all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n  File \"C:\\Python27\\lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 87, in word_tokenize\n    return _word_tokenize(text)\n  File \"C:\\Python27\\lib\\site-packages\\nltk\\tokenize\\treebank.py\", line 67, in tokenize\n    text = re.sub(r'^\\\"', r'``', text)\n  File \"C:\\Python27\\lib\\re.py\", line 151, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or buffer\n</code></pre>\n", "abstract": "I am training the NaiveBayesClassifier in Python using sentences, and it gives me the error below. I do not understand what the error might be, and any help would be good.  I have tried many other input formats, but the error remains. The code given below: I am including the traceback below."}, "answers": [{"id": 20827919, "score": 46, "vote": 0, "content": "<p>You need to change your data structure. Here is your <code>train</code> list as it currently stands:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; train = [('I love this sandwich.', 'pos'),\n('This is an amazing place!', 'pos'),\n('I feel very good about these beers.', 'pos'),\n('This is my best work.', 'pos'),\n(\"What an awesome view\", 'pos'),\n('I do not like this restaurant', 'neg'),\n('I am tired of this stuff.', 'neg'),\n(\"I can't deal with this\", 'neg'),\n('He is my sworn enemy!', 'neg'),\n('My boss is horrible.', 'neg')]\n</code></pre>\n<p>The problem is, though, that the first element of each tuple should be a dictionary of features. So I will change your list into a data structure that the classifier can work with:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from nltk.tokenize import word_tokenize # or use some other tokenizer\n&gt;&gt;&gt; all_words = set(word.lower() for passage in train for word in word_tokenize(passage[0]))\n&gt;&gt;&gt; t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n</code></pre>\n<p>Your data should now be structured like this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; t\n[({'this': True, 'love': True, 'deal': False, 'tired': False, 'feel': False, 'is': False, 'am': False, 'an': False, 'sandwich': True, 'ca': False, 'best': False, '!': False, 'what': False, '.': True, 'amazing': False, 'horrible': False, 'sworn': False, 'awesome': False, 'do': False, 'good': False, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'these': False, 'of': False, 'work': False, \"n't\": False, 'i': False, 'stuff': False, 'place': False, 'my': False, 'view': False}, 'pos'), . . .]\n</code></pre>\n<p>Note that the first element of each tuple is now a dictionary. Now that your data is in place and the first element of each tuple is a dictionary, you can train the classifier like so:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(t)\n&gt;&gt;&gt; classifier.show_most_informative_features()\nMost Informative Features\n                    this = True              neg : pos    =      2.3 : 1.0\n                    this = False             pos : neg    =      1.8 : 1.0\n                      an = False             neg : pos    =      1.6 : 1.0\n                       . = True              pos : neg    =      1.4 : 1.0\n                       . = False             neg : pos    =      1.4 : 1.0\n                 awesome = False             neg : pos    =      1.2 : 1.0\n                      of = False             pos : neg    =      1.2 : 1.0\n                    feel = False             neg : pos    =      1.2 : 1.0\n                   place = False             neg : pos    =      1.2 : 1.0\n                horrible = False             pos : neg    =      1.2 : 1.0\n</code></pre>\n<p>If you want to use the classifier, you can do it like this. First, you begin with a test sentence:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; test_sentence = \"This is the best band I've ever heard!\"\n</code></pre>\n<p>Then, you tokenize the sentence and figure out which words the sentence shares with all_words. These constitute the sentence's features.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; test_sent_features = {word: (word in word_tokenize(test_sentence.lower())) for word in all_words}\n</code></pre>\n<p>Your features will now look like this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; test_sent_features\n{'love': False, 'deal': False, 'tired': False, 'feel': False, 'is': True, 'am': False, 'an': False, 'sandwich': False, 'ca': False, 'best': True, '!': True, 'what': False, 'i': True, '.': False, 'amazing': False, 'horrible': False, 'sworn': False, 'awesome': False, 'do': False, 'good': False, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'this': True, 'of': False, 'work': False, \"n't\": False, 'these': False, 'stuff': False, 'place': False, 'my': False, 'view': False}\n</code></pre>\n<p>Then you simply classify those features:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; classifier.classify(test_sent_features)\n'pos' # note 'best' == True in the sentence features above\n</code></pre>\n<p>This test sentence appears to be positive.</p>\n", "abstract": "You need to change your data structure. Here is your train list as it currently stands: The problem is, though, that the first element of each tuple should be a dictionary of features. So I will change your list into a data structure that the classifier can work with: Your data should now be structured like this: Note that the first element of each tuple is now a dictionary. Now that your data is in place and the first element of each tuple is a dictionary, you can train the classifier like so: If you want to use the classifier, you can do it like this. First, you begin with a test sentence: Then, you tokenize the sentence and figure out which words the sentence shares with all_words. These constitute the sentence's features. Your features will now look like this: Then you simply classify those features: This test sentence appears to be positive."}, {"id": 20833372, "score": 20, "vote": 0, "content": "<p>@275365's tutorial on the data structure for NLTK's bayesian classifier is great. From a more high level, we can look at it as,</p>\n<p>We have inputs sentences with sentiment tags:</p>\n<pre><code class=\"python\">training_data = [('I love this sandwich.', 'pos'),\n('This is an amazing place!', 'pos'),\n('I feel very good about these beers.', 'pos'),\n('This is my best work.', 'pos'),\n(\"What an awesome view\", 'pos'),\n('I do not like this restaurant', 'neg'),\n('I am tired of this stuff.', 'neg'),\n(\"I can't deal with this\", 'neg'),\n('He is my sworn enemy!', 'neg'),\n('My boss is horrible.', 'neg')]\n</code></pre>\n<p>Let's consider our feature sets to be individual words, so we extract a list of all possible words from the training data (let's call it vocabulary) as such:</p>\n<pre><code class=\"python\">from nltk.tokenize import word_tokenize\nfrom itertools import chain\nvocabulary = set(chain(*[word_tokenize(i[0].lower()) for i in training_data]))\n</code></pre>\n<p>Essentially, <code>vocabulary</code> here is the same @275365's <code>all_word</code></p>\n<pre><code class=\"python\">&gt;&gt;&gt; all_words = set(word.lower() for passage in training_data for word in word_tokenize(passage[0]))\n&gt;&gt;&gt; vocabulary = set(chain(*[word_tokenize(i[0].lower()) for i in training_data]))\n&gt;&gt;&gt; print vocabulary == all_words\nTrue\n</code></pre>\n<p>From each data point, (i.e. each sentence and the pos/neg tag), we want to say whether a feature (i.e. a word from the vocabulary) exist or not.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; sentence = word_tokenize('I love this sandwich.'.lower())\n&gt;&gt;&gt; print {i:True for i in vocabulary if i in sentence}\n{'this': True, 'i': True, 'sandwich': True, 'love': True, '.': True}\n</code></pre>\n<p>But we also want to tell the classifier which word don't exist in the sentence but in the vocabulary, so for each data point, we list out all possible words in the vocabulary and say whether a word exist or not:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; sentence = word_tokenize('I love this sandwich.'.lower())\n&gt;&gt;&gt; x =  {i:True for i in vocabulary if i in sentence}\n&gt;&gt;&gt; y =  {i:False for i in vocabulary if i not in sentence}\n&gt;&gt;&gt; x.update(y)\n&gt;&gt;&gt; print x\n{'love': True, 'deal': False, 'tired': False, 'feel': False, 'is': False, 'am': False, 'an': False, 'good': False, 'best': False, '!': False, 'these': False, 'what': False, '.': True, 'amazing': False, 'horrible': False, 'sworn': False, 'ca': False, 'do': False, 'sandwich': True, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'this': True, 'of': False, 'work': False, \"n't\": False, 'i': True, 'stuff': False, 'place': False, 'my': False, 'awesome': False, 'view': False}\n</code></pre>\n<p>But since this loops through the vocabulary twice, it's more efficient to do this:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; sentence = word_tokenize('I love this sandwich.'.lower())\n&gt;&gt;&gt; x = {i:(i in sentence) for i in vocabulary}\n{'love': True, 'deal': False, 'tired': False, 'feel': False, 'is': False, 'am': False, 'an': False, 'good': False, 'best': False, '!': False, 'these': False, 'what': False, '.': True, 'amazing': False, 'horrible': False, 'sworn': False, 'ca': False, 'do': False, 'sandwich': True, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'this': True, 'of': False, 'work': False, \"n't\": False, 'i': True, 'stuff': False, 'place': False, 'my': False, 'awesome': False, 'view': False}\n</code></pre>\n<p>So for each sentence, we want to tell the classifier for each sentence which word exist and which word doesn't and also give it the pos/neg tag. We can call that a <code>feature_set</code>, it's a tuple made up of a <code>x</code> (as shown above) and its tag. </p>\n<pre><code class=\"python\">&gt;&gt;&gt; feature_set = [({i:(i in word_tokenize(sentence.lower())) for i in vocabulary},tag) for sentence, tag in training_data]\n[({'this': True, 'love': True, 'deal': False, 'tired': False, 'feel': False, 'is': False, 'am': False, 'an': False, 'sandwich': True, 'ca': False, 'best': False, '!': False, 'what': False, '.': True, 'amazing': False, 'horrible': False, 'sworn': False, 'awesome': False, 'do': False, 'good': False, 'very': False, 'boss': False, 'beers': False, 'not': False, 'with': False, 'he': False, 'enemy': False, 'about': False, 'like': False, 'restaurant': False, 'these': False, 'of': False, 'work': False, \"n't\": False, 'i': False, 'stuff': False, 'place': False, 'my': False, 'view': False}, 'pos'), ...]\n</code></pre>\n<p>Then we feed these features and tags in the feature_set into the classifier to train it:</p>\n<pre><code class=\"python\">from nltk import NaiveBayesClassifier as nbc\nclassifier = nbc.train(feature_set)\n</code></pre>\n<p>Now you have a trained classifier and when you want to tag a new sentence, you have to \"featurize\" the new sentence to see which of the word in the new sentence are in the vocabulary that the classifier was trained on:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; test_sentence = \"This is the best band I've ever heard! foobar\"\n&gt;&gt;&gt; featurized_test_sentence = {i:(i in word_tokenize(test_sentence.lower())) for i in vocabulary}\n</code></pre>\n<p><strong>NOTE:</strong> As you can see from the step above, the naive bayes classifier cannot handle out of vocabulary words since the <code>foobar</code> token disappears after you featurize it.</p>\n<p>Then you feed the featurized test sentence into the classifier and ask it to classify:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; classifier.classify(featurized_test_sentence)\n'pos'\n</code></pre>\n<p>Hopefully this gives a clearer picture of how to feed data in to NLTK's naive bayes classifier for sentimental analysis. Here's the full code without the comments and the walkthrough:</p>\n<pre><code class=\"python\">from nltk import NaiveBayesClassifier as nbc\nfrom nltk.tokenize import word_tokenize\nfrom itertools import chain\n\ntraining_data = [('I love this sandwich.', 'pos'),\n('This is an amazing place!', 'pos'),\n('I feel very good about these beers.', 'pos'),\n('This is my best work.', 'pos'),\n(\"What an awesome view\", 'pos'),\n('I do not like this restaurant', 'neg'),\n('I am tired of this stuff.', 'neg'),\n(\"I can't deal with this\", 'neg'),\n('He is my sworn enemy!', 'neg'),\n('My boss is horrible.', 'neg')]\n\nvocabulary = set(chain(*[word_tokenize(i[0].lower()) for i in training_data]))\n\nfeature_set = [({i:(i in word_tokenize(sentence.lower())) for i in vocabulary},tag) for sentence, tag in training_data]\n\nclassifier = nbc.train(feature_set)\n\ntest_sentence = \"This is the best band I've ever heard!\"\nfeaturized_test_sentence =  {i:(i in word_tokenize(test_sentence.lower())) for i in vocabulary}\n\nprint \"test_sent:\",test_sentence\nprint \"tag:\",classifier.classify(featurized_test_sentence)\n</code></pre>\n", "abstract": "@275365's tutorial on the data structure for NLTK's bayesian classifier is great. From a more high level, we can look at it as, We have inputs sentences with sentiment tags: Let's consider our feature sets to be individual words, so we extract a list of all possible words from the training data (let's call it vocabulary) as such: Essentially, vocabulary here is the same @275365's all_word From each data point, (i.e. each sentence and the pos/neg tag), we want to say whether a feature (i.e. a word from the vocabulary) exist or not. But we also want to tell the classifier which word don't exist in the sentence but in the vocabulary, so for each data point, we list out all possible words in the vocabulary and say whether a word exist or not: But since this loops through the vocabulary twice, it's more efficient to do this: So for each sentence, we want to tell the classifier for each sentence which word exist and which word doesn't and also give it the pos/neg tag. We can call that a feature_set, it's a tuple made up of a x (as shown above) and its tag.  Then we feed these features and tags in the feature_set into the classifier to train it: Now you have a trained classifier and when you want to tag a new sentence, you have to \"featurize\" the new sentence to see which of the word in the new sentence are in the vocabulary that the classifier was trained on: NOTE: As you can see from the step above, the naive bayes classifier cannot handle out of vocabulary words since the foobar token disappears after you featurize it. Then you feed the featurized test sentence into the classifier and ask it to classify: Hopefully this gives a clearer picture of how to feed data in to NLTK's naive bayes classifier for sentimental analysis. Here's the full code without the comments and the walkthrough:"}, {"id": 21025079, "score": 5, "vote": 0, "content": "<p>It appears that you are trying to use TextBlob but are training the NLTK NaiveBayesClassifier, which, as pointed out in other answers, must be passed a dictionary of features.</p>\n<p>TextBlob has a default feature extractor that indicates which words in the training set are included in the document (as demonstrated in the other answers). Therefore, TextBlob allows you to pass in your data as is.</p>\n<pre><code class=\"python\">from textblob.classifiers import NaiveBayesClassifier\n\ntrain = [('This is an amazing place!', 'pos'),\n        ('I feel very good about these beers.', 'pos'),\n        ('This is my best work.', 'pos'),\n        (\"What an awesome view\", 'pos'),\n        ('I do not like this restaurant', 'neg'),\n        ('I am tired of this stuff.', 'neg'),\n        (\"I can't deal with this\", 'neg'),\n        ('He is my sworn enemy!', 'neg'),\n        ('My boss is horrible.', 'neg') ] \ntest = [\n        ('The beer was good.', 'pos'),\n        ('I do not enjoy my job', 'neg'),\n        (\"I ain't feeling dandy today.\", 'neg'),\n        (\"I feel amazing!\", 'pos'),\n        ('Gary is a friend of mine.', 'pos'),\n        (\"I can't believe I'm doing this.\", 'neg') ] \n\n\nclassifier = NaiveBayesClassifier(train)  # Pass in data as is\n# When classifying text, features are extracted automatically\nclassifier.classify(\"This is an amazing library!\")  # =&gt; 'pos'\n</code></pre>\n<p>Of course, the simple default extractor is not appropriate for all problems. If you would like to how features are extracted, you just write a function that takes a string of text as input and outputs the dictionary of features and pass that to the classifier.</p>\n<pre><code class=\"python\">classifier = NaiveBayesClassifier(train, feature_extractor=my_extractor_func)\n</code></pre>\n<p>I encourage you to check out the short TextBlob classifier tutorial here: <a href=\"http://textblob.readthedocs.org/en/latest/classifiers.html\" rel=\"nofollow noreferrer\">http://textblob.readthedocs.org/en/latest/classifiers.html</a></p>\n", "abstract": "It appears that you are trying to use TextBlob but are training the NLTK NaiveBayesClassifier, which, as pointed out in other answers, must be passed a dictionary of features. TextBlob has a default feature extractor that indicates which words in the training set are included in the document (as demonstrated in the other answers). Therefore, TextBlob allows you to pass in your data as is. Of course, the simple default extractor is not appropriate for all problems. If you would like to how features are extracted, you just write a function that takes a string of text as input and outputs the dictionary of features and pass that to the classifier. I encourage you to check out the short TextBlob classifier tutorial here: http://textblob.readthedocs.org/en/latest/classifiers.html"}]}, {"link": "https://stackoverflow.com/questions/42094180/spacy-how-to-load-google-news-word2vec-vectors", "question": {"id": "42094180", "title": "SpaCy: how to load Google news word2vec vectors?", "content": "<p>I've tried several methods of loading the google news word2vec vectors (<a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"noreferrer\">https://code.google.com/archive/p/word2vec/</a>):</p>\n<pre><code class=\"python\">en_nlp = spacy.load('en',vector=False)\nen_nlp.vocab.load_vectors_from_bin_loc('GoogleNews-vectors-negative300.bin')\n</code></pre>\n<p>The above gives:</p>\n<pre><code class=\"python\">MemoryError: Error assigning 18446744072820359357 bytes\n</code></pre>\n<p>I've also tried with the .gz packed vectors; or by loading and saving them with gensim to a new format:</p>\n<pre><code class=\"python\">from gensim.models.word2vec import Word2Vec\nmodel = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\nmodel.save_word2vec_format('googlenews2.txt')\n</code></pre>\n<p>This file then contains the words and their word vectors on each line.\nI tried to load them with:</p>\n<pre><code class=\"python\">en_nlp.vocab.load_vectors('googlenews2.txt')\n</code></pre>\n<p>but it returns \"0\".</p>\n<p>What is the correct way to do this?</p>\n<p><strong>Update:</strong></p>\n<p>I can load my own created file into spacy.\nI use a test.txt file with \"string 0.0 0.0 ....\" on each line. Then zip this txt with .bzip2 to test.txt.bz2.\nThen I create a spacy compatible binary file:</p>\n<pre><code class=\"python\">spacy.vocab.write_binary_vectors('test.txt.bz2', 'test.bin')\n</code></pre>\n<p>That I can load into spacy:</p>\n<pre><code class=\"python\">nlp.vocab.load_vectors_from_bin_loc('test.bin')\n</code></pre>\n<p>This works!\nHowever, when I do the same process for the googlenews2.txt, I get the following error:</p>\n<pre><code class=\"python\">lib/python3.6/site-packages/spacy/cfile.pyx in spacy.cfile.CFile.read_into (spacy/cfile.cpp:1279)()\n\nOSError: \n</code></pre>\n", "abstract": "I've tried several methods of loading the google news word2vec vectors (https://code.google.com/archive/p/word2vec/): The above gives: I've also tried with the .gz packed vectors; or by loading and saving them with gensim to a new format: This file then contains the words and their word vectors on each line.\nI tried to load them with: but it returns \"0\". What is the correct way to do this? Update: I can load my own created file into spacy.\nI use a test.txt file with \"string 0.0 0.0 ....\" on each line. Then zip this txt with .bzip2 to test.txt.bz2.\nThen I create a spacy compatible binary file: That I can load into spacy: This works!\nHowever, when I do the same process for the googlenews2.txt, I get the following error:"}, "answers": [{"id": 42115356, "score": 25, "vote": 0, "content": "<p>For spacy 1.x, load Google news vectors into gensim and convert to a new format (each line in .txt contains a single vector: string, vec):</p>\n<pre><code class=\"python\">from gensim.models.word2vec import Word2Vec\nfrom gensim.models import KeyedVectors\nmodel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\nmodel.wv.save_word2vec_format('googlenews.txt')\n</code></pre>\n<p>Remove the first line of the .txt:</p>\n<pre><code class=\"python\">tail -n +2 googlenews.txt &gt; googlenews.new &amp;&amp; mv -f googlenews.new googlenews.txt\n</code></pre>\n<p>Compress the txt as .bz2:</p>\n<pre><code class=\"python\">bzip2 googlenews.txt\n</code></pre>\n<p>Create a SpaCy compatible binary file:</p>\n<pre><code class=\"python\">spacy.vocab.write_binary_vectors('googlenews.txt.bz2','googlenews.bin')\n</code></pre>\n<p>Move the googlenews.bin to /lib/python/site-packages/spacy/data/en_google-1.0.0/vocab/googlenews.bin of your python environment.</p>\n<p>Then load the wordvectors:</p>\n<pre><code class=\"python\">import spacy\nnlp = spacy.load('en',vectors='en_google')\n</code></pre>\n<p>or load them after later:</p>\n<pre><code class=\"python\">nlp.vocab.load_vectors_from_bin_loc('googlenews.bin')\n</code></pre>\n", "abstract": "For spacy 1.x, load Google news vectors into gensim and convert to a new format (each line in .txt contains a single vector: string, vec): Remove the first line of the .txt: Compress the txt as .bz2: Create a SpaCy compatible binary file: Move the googlenews.bin to /lib/python/site-packages/spacy/data/en_google-1.0.0/vocab/googlenews.bin of your python environment. Then load the wordvectors: or load them after later:"}, {"id": 50091148, "score": 12, "vote": 0, "content": "<p>I know that this question has already been answered, but I am going to offer a simpler solution. This solution will load google news vectors into a blank spacy nlp object.</p>\n<pre><code class=\"python\">import gensim\nimport spacy\n\n# Path to google news vectors\ngoogle_news_path = \"path\\to\\google\\news\\\\GoogleNews-vectors-negative300.bin.gz\"\n\n# Load google news vecs in gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format(gn_path, binary=True)\n\n# Init blank english spacy nlp object\nnlp = spacy.blank('en')\n\n# Loop through range of all indexes, get words associated with each index.\n# The words in the keys list will correspond to the order of the google embed matrix\nkeys = []\nfor idx in range(3000000):\n    keys.append(model.index2word[idx])\n\n# Set the vectors for our nlp object to the google news vectors\nnlp.vocab.vectors = spacy.vocab.Vectors(data=model.syn0, keys=keys)\n\n&gt;&gt;&gt; nlp.vocab.vectors.shape\n(3000000, 300)\n</code></pre>\n", "abstract": "I know that this question has already been answered, but I am going to offer a simpler solution. This solution will load google news vectors into a blank spacy nlp object."}, {"id": 50112345, "score": 2, "vote": 0, "content": "<p>I am using spaCy v2.0.10.</p>\n<blockquote>\n<p>Create a SpaCy compatible binary file:</p>\n<pre>\nspacy.vocab.write_binary_vectors('googlenews.txt.bz2','googlenews.bin')\n</pre>\n</blockquote>\n<p>I want to highlight that the specific code in the accepted answer is not working now. I encountered \"AttributeError: ...\" when I run the code.</p>\n<p>This has changed in spaCy v2. <code>write_binary_vectors</code> was removed in v2. From <a href=\"https://spacy.io/usage/vectors-similarity#custom\" rel=\"nofollow noreferrer\">spaCy documentations</a>, the current way to do this is as follows:</p>\n<pre><code class=\"python\">$ python -m spacy init-model en /path/to/output -v /path/to/vectors.bin.tar.gz\n</code></pre>\n", "abstract": "I am using spaCy v2.0.10. Create a SpaCy compatible binary file: I want to highlight that the specific code in the accepted answer is not working now. I encountered \"AttributeError: ...\" when I run the code. This has changed in spaCy v2. write_binary_vectors was removed in v2. From spaCy documentations, the current way to do this is as follows:"}, {"id": 52628972, "score": 1, "vote": 0, "content": "<p>it is much easier to use the gensim api for dowloading the word2vec compressed model by google, it will be stored in <code>/home/\"your_username\"/gensim-data/word2vec-google-news-300/</code> . Load the vectors and play ball. I have 16GB of RAM which is more than enough to handle the model</p>\n<pre><code class=\"python\">import gensim.downloader as api\n\nmodel = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\nword_vectors = model.wv #load the vectors from the model\n</code></pre>\n", "abstract": "it is much easier to use the gensim api for dowloading the word2vec compressed model by google, it will be stored in /home/\"your_username\"/gensim-data/word2vec-google-news-300/ . Load the vectors and play ball. I have 16GB of RAM which is more than enough to handle the model"}]}, {"link": "https://stackoverflow.com/questions/4543008/efficient-context-free-grammar-parser-preferably-python-friendly", "question": {"id": "4543008", "title": "Efficient Context-Free Grammar parser, preferably Python-friendly", "content": "<p>I am in need of parsing a small subset of English for one of my project, described as a context-free grammar with (1-level) feature structures (<a href=\"http://code.google.com/p/nltk/source/browse/trunk/nltk/examples/grammars/book_grammars/feat0.fcfg?r=8260\">example</a>) and I need to do it efficiently .</p>\n<p>Right now I'm using <a href=\"http://www.nltk.org/\">NLTK</a>'s parser which produces the right output but is very slow. For my grammar of ~450 fairly ambiguous non-lexicon rules and half a million lexical entries, parsing simple sentences can take anywhere from 2 to 30 seconds, depending it seems on the number of resulting trees. Lexical entries have little to no effect on performance.</p>\n<p>Another problem is that loading the (25MB) grammar+lexicon at the beginning can take up to a minute.</p>\n<p>From what I can find in literature, the running time of the algorithm used to parse such a grammar (Earley or CKY) should be linear to the size of the grammar and cubic to the size of the input token list. My experience with NLTK indicates that ambiguity is what hurts the performance most, not the absolute size of the grammar.</p>\n<p>So now I'm looking for a CFG parser to replace NLTK. I've been considering <a href=\"http://www.dabeaz.com/ply/\">PLY</a> but I can't tell whether it supports feature structures in CFGs, which are required in my case, and the examples I've seen seem to be doing a lot of procedural parsing rather than just specifying a grammar. Can anybody show me an example of PLY both supporting feature structs and using a declarative grammar?</p>\n<p>I'm also fine with any other parser that can do what I need efficiently. A Python interface is preferable but not absolutely necessary.</p>\n", "abstract": "I am in need of parsing a small subset of English for one of my project, described as a context-free grammar with (1-level) feature structures (example) and I need to do it efficiently . Right now I'm using NLTK's parser which produces the right output but is very slow. For my grammar of ~450 fairly ambiguous non-lexicon rules and half a million lexical entries, parsing simple sentences can take anywhere from 2 to 30 seconds, depending it seems on the number of resulting trees. Lexical entries have little to no effect on performance. Another problem is that loading the (25MB) grammar+lexicon at the beginning can take up to a minute. From what I can find in literature, the running time of the algorithm used to parse such a grammar (Earley or CKY) should be linear to the size of the grammar and cubic to the size of the input token list. My experience with NLTK indicates that ambiguity is what hurts the performance most, not the absolute size of the grammar. So now I'm looking for a CFG parser to replace NLTK. I've been considering PLY but I can't tell whether it supports feature structures in CFGs, which are required in my case, and the examples I've seen seem to be doing a lot of procedural parsing rather than just specifying a grammar. Can anybody show me an example of PLY both supporting feature structs and using a declarative grammar? I'm also fine with any other parser that can do what I need efficiently. A Python interface is preferable but not absolutely necessary."}, "answers": [{"id": 4543154, "score": 15, "vote": 0, "content": "<p>By all means take a look at <a href=\"https://github.com/pyparsing/pyparsing\" rel=\"nofollow noreferrer\">Pyparsing</a>. It's the most pythonic implementations of parsing I've come across, and it's a great design from a purely academic standpoint.</p>\n<p>I used both <a href=\"http://www.antlr.org/\" rel=\"nofollow noreferrer\">ANTLR</a> and <a href=\"http://en.wikipedia.org/wiki/JavaCC\" rel=\"nofollow noreferrer\">JavaCC</a> to teach translator and compiler theory at a local university. They're both good and mature, but I wouldn't use them in a Python project.</p>\n<p>That said, unlike programming languages, natural languages are much more about the semantics than about the syntax, so you could be much better off skipping the learning curves of existing parsing tools, going with a home-brewed (top-down, backtracking, unlimited lookahead) lexical analyzer and parser, and spending the bulk of your time writing the code that figures out what a parsed, but ambiguous, natural-language sentence means.</p>\n", "abstract": "By all means take a look at Pyparsing. It's the most pythonic implementations of parsing I've come across, and it's a great design from a purely academic standpoint. I used both ANTLR and JavaCC to teach translator and compiler theory at a local university. They're both good and mature, but I wouldn't use them in a Python project. That said, unlike programming languages, natural languages are much more about the semantics than about the syntax, so you could be much better off skipping the learning curves of existing parsing tools, going with a home-brewed (top-down, backtracking, unlimited lookahead) lexical analyzer and parser, and spending the bulk of your time writing the code that figures out what a parsed, but ambiguous, natural-language sentence means."}, {"id": 4545406, "score": 2, "vote": 0, "content": "<p>I've used pyparsing for limited vocabulary command parsing, but here is a little framework on top of pyparsing that addresses your posted example:</p>\n<pre><code class=\"python\">from pyparsing import *\n\ntransVerb, transVerbPlural, transVerbPast, transVerbProg = (Forward() for i in range(4))\nintransVerb, intransVerbPlural, intransVerbPast, intransVerbProg = (Forward() for i in range(4))\nsingNoun,pluralNoun,properNoun = (Forward() for i in range(3))\nsingArticle,pluralArticle = (Forward() for i in range(2))\nverbProg = transVerbProg | intransVerbProg\nverbPlural = transVerbPlural | intransVerbPlural\n\nfor expr in (transVerb, transVerbPlural, transVerbPast, transVerbProg,\n            intransVerb, intransVerbPlural, intransVerbPast, intransVerbProg,\n            singNoun, pluralNoun, properNoun, singArticle, pluralArticle):\n    expr &lt;&lt; MatchFirst([])\n\ndef appendExpr(e1, s):\n    c1 = s[0]\n    e2 = Regex(r\"[%s%s]%s\\b\" % (c1.upper(), c1.lower(), s[1:]))\n    e1.expr.exprs.append(e2)\n\ndef makeVerb(s, transitive):\n    v_pl, v_sg, v_past, v_prog = s.split()\n    if transitive:\n        appendExpr(transVerb, v_sg)\n        appendExpr(transVerbPlural, v_pl)\n        appendExpr(transVerbPast, v_past)\n        appendExpr(transVerbProg, v_prog)\n    else:\n        appendExpr(intransVerb, v_sg)\n        appendExpr(intransVerbPlural, v_pl)\n        appendExpr(intransVerbPast, v_past)\n        appendExpr(intransVerbProg, v_prog)\n\ndef makeNoun(s, proper=False):\n    if proper:\n        appendExpr(properNoun, s)\n    else:\n        n_sg,n_pl = (s.split() + [s+\"s\"])[:2]\n        appendExpr(singNoun, n_sg)\n        appendExpr(pluralNoun, n_pl)\n\ndef makeArticle(s, plural=False):\n    for ss in s.split():\n        if not plural:\n            appendExpr(singArticle, ss)\n        else:\n            appendExpr(pluralArticle, ss)\n\nmakeVerb(\"disappear disappears disappeared disappearing\", transitive=False)\nmakeVerb(\"walk walks walked walking\", transitive=False)\nmakeVerb(\"see sees saw seeing\", transitive=True)\nmakeVerb(\"like likes liked liking\", transitive=True)\n\nmakeNoun(\"dog\")\nmakeNoun(\"girl\")\nmakeNoun(\"car\")\nmakeNoun(\"child children\")\nmakeNoun(\"Kim\", proper=True)\nmakeNoun(\"Jody\", proper=True)\n\nmakeArticle(\"a the\")\nmakeArticle(\"this every\")\nmakeArticle(\"the these all some several\", plural=True)\n\ntransObject = (singArticle + singNoun | properNoun | Optional(pluralArticle) + pluralNoun | verbProg | \"to\" + verbPlural)\nsgSentence = (singArticle + singNoun | properNoun) + (intransVerb | intransVerbPast | (transVerb | transVerbPast) + transObject)\nplSentence = (Optional(pluralArticle) + pluralNoun) + (intransVerbPlural | intransVerbPast | (transVerbPlural |transVerbPast) + transObject)\n\nsentence = sgSentence | plSentence\n\n\ndef test(s):\n    print s\n    try:\n        print sentence.parseString(s).asList()\n    except ParseException, pe:\n        print pe\n\ntest(\"Kim likes cars\")\ntest(\"The girl saw the dog\")\ntest(\"The dog saw Jody\")\ntest(\"Kim likes walking\")\ntest(\"Every girl likes dogs\")\ntest(\"All dogs like children\")\ntest(\"Jody likes to walk\")\ntest(\"Dogs like walking\")\ntest(\"All dogs like walking\")\ntest(\"Every child likes Jody\")\n</code></pre>\n<p>Prints:</p>\n<pre><code class=\"python\">Kim likes cars\n['Kim', 'likes', 'cars']\nThe girl saw the dog\n['The', 'girl', 'saw', 'the', 'dog']\nThe dog saw Jody\n['The', 'dog', 'saw', 'Jody']\nKim likes walking\n['Kim', 'likes', 'walking']\nEvery girl likes dogs\n['Every', 'girl', 'likes', 'dogs']\nAll dogs like children\n['All', 'dogs', 'like', 'children']\nJody likes to walk\n['Jody', 'likes', 'to', 'walk']\nDogs like walking\n['Dogs', 'like', 'walking']\nAll dogs like walking\n['All', 'dogs', 'like', 'walking']\nEvery child likes Jody\n['Every', 'child', 'likes', 'Jody']\n</code></pre>\n<p>This is likely to get slow as you expand the vocabulary. Half a million entries?  I thought that a reasonable functional vocabulary was on the order of 5-6 thousand words. And you will be pretty limited in the sentence structures that you can handle - natural language is what NLTK is for.</p>\n", "abstract": "I've used pyparsing for limited vocabulary command parsing, but here is a little framework on top of pyparsing that addresses your posted example: Prints: This is likely to get slow as you expand the vocabulary. Half a million entries?  I thought that a reasonable functional vocabulary was on the order of 5-6 thousand words. And you will be pretty limited in the sentence structures that you can handle - natural language is what NLTK is for."}, {"id": 4546821, "score": 2, "vote": 0, "content": "<p>Tooling aside...</p>\n<p>You may remember from theory that there are infinite grammars that define the same language. There are criteria for categorizing grammars and determining which is the \"canonical\" or \"minimal\" one for a given language, but in the end, the \"best\" grammar is the one that's more convenient for the task and tools at hand (remember the transformations of CFGs into LL and LR grammars?).</p>\n<p>Then, you probably don't need a huge lexicon to parse an sentence in English. There's a lot to be known about a word in languages like German or Latin (or even Spanish), but not in the many times arbitrary and ambiguos English. You should be able to get away with a small lexicon that contains only the key words necessary to arrive to the structure of a sentence. At any rate, the grammar you choose, no matter its size, can be cached in a way that thee tooling can directly use it (i.e., you can skip parsing the grammar).</p>\n<p>Given that, it could be a good idea to take a look at a simpler parser already worked on by someone else. There must be thousands of those in the literature. Studying different approaches will let you evaluate your own, and may lead you to adopt someone else's.</p>\n<p>Finally, as I already mentioned, interpreting natural languages is much more about artificial intelligence than about parsing. Because structure determines meaning and meaning determines structure you have to play with both at the same time. An approach I've seen in the literature since the '80s is to let different specialized agents take shots at solving the problem against a \"<a href=\"http://en.wikipedia.org/wiki/Blackboard_system\" rel=\"nofollow\">blackboard</a>\". With that approach syntatic and semantic analysis happen concurrently.</p>\n", "abstract": "Tooling aside... You may remember from theory that there are infinite grammars that define the same language. There are criteria for categorizing grammars and determining which is the \"canonical\" or \"minimal\" one for a given language, but in the end, the \"best\" grammar is the one that's more convenient for the task and tools at hand (remember the transformations of CFGs into LL and LR grammars?). Then, you probably don't need a huge lexicon to parse an sentence in English. There's a lot to be known about a word in languages like German or Latin (or even Spanish), but not in the many times arbitrary and ambiguos English. You should be able to get away with a small lexicon that contains only the key words necessary to arrive to the structure of a sentence. At any rate, the grammar you choose, no matter its size, can be cached in a way that thee tooling can directly use it (i.e., you can skip parsing the grammar). Given that, it could be a good idea to take a look at a simpler parser already worked on by someone else. There must be thousands of those in the literature. Studying different approaches will let you evaluate your own, and may lead you to adopt someone else's. Finally, as I already mentioned, interpreting natural languages is much more about artificial intelligence than about parsing. Because structure determines meaning and meaning determines structure you have to play with both at the same time. An approach I've seen in the literature since the '80s is to let different specialized agents take shots at solving the problem against a \"blackboard\". With that approach syntatic and semantic analysis happen concurrently."}, {"id": 5379678, "score": 2, "vote": 0, "content": "<p>I would recommend using bitpar, a very efficient PCFG parser written in C++. I've written a shell-based Python wrapper for it, see <a href=\"https://github.com/andreasvc/eodop/blob/master/bitpar.py\" rel=\"nofollow\">https://github.com/andreasvc/eodop/blob/master/bitpar.py</a></p>\n", "abstract": "I would recommend using bitpar, a very efficient PCFG parser written in C++. I've written a shell-based Python wrapper for it, see https://github.com/andreasvc/eodop/blob/master/bitpar.py"}, {"id": 4543020, "score": 1, "vote": 0, "content": "<p>I think ANTLR is the best parser-generator that I know of for Java.  I don't know if Jython would provide you a good way for Python and Java to interact.</p>\n", "abstract": "I think ANTLR is the best parser-generator that I know of for Java.  I don't know if Jython would provide you a good way for Python and Java to interact."}, {"id": 4580110, "score": 1, "vote": 0, "content": "<p>Somewhat late on this, but here are two more options for you:</p>\n<p><a href=\"http://pages.cpsc.ucalgary.ca/~aycock/spark/\" rel=\"nofollow\">Spark</a> is a Earley parser written in Python. </p>\n<p><a href=\"http://scottmcpeak.com/elkhound/\" rel=\"nofollow\">Elkhound</a> is a GLR parser written in C++  Elkhound uses a Bison like syntax </p>\n", "abstract": "Somewhat late on this, but here are two more options for you: Spark is a Earley parser written in Python.  Elkhound is a GLR parser written in C++  Elkhound uses a Bison like syntax "}, {"id": 4543298, "score": 0, "vote": 0, "content": "<p>If it can be expressed as a <a href=\"http://pdos.csail.mit.edu/papers/parsing%3apopl04.pdf\" rel=\"nofollow\">PEG</a> language (I don't think all CFGs can, but supposedly many can), then you might use <a href=\"http://fdik.org/pyPEG/\" rel=\"nofollow\">pyPEG</a>, which is supposed to be linear-time when using a packrat parsing implementation (although potentially prohibitive on memory usage).</p>\n<p>I don't have any experience with it as I am just starting to research parsing and compilation again after a long time away from it, but I am reading some good buzz about this relatively up-to-date technique.  YMMV.</p>\n", "abstract": "If it can be expressed as a PEG language (I don't think all CFGs can, but supposedly many can), then you might use pyPEG, which is supposed to be linear-time when using a packrat parsing implementation (although potentially prohibitive on memory usage). I don't have any experience with it as I am just starting to research parsing and compilation again after a long time away from it, but I am reading some good buzz about this relatively up-to-date technique.  YMMV."}, {"id": 4543312, "score": 0, "vote": 0, "content": "<p>Try running it on PyPy, it might be a lot faster.</p>\n", "abstract": "Try running it on PyPy, it might be a lot faster."}]}, {"link": "https://stackoverflow.com/questions/63705803/merge-related-words-in-nlp", "question": {"id": "63705803", "title": "Merge related words in NLP", "content": "<p>I'd like to define a new word which includes count values from two (or more) different words. For example:</p>\n<pre><code class=\"python\">Words Frequency\n0   mom 250\n1   2020    151\n2   the 124\n3   19  82\n4   mother  81\n... ... ...\n10  London  6\n11  life    6\n12  something   6\n</code></pre>\n<p>I would like to define mother as <code>mom + mother</code>:</p>\n<pre><code class=\"python\">Words Frequency\n0   mother  331\n1   2020    151\n2   the 124\n3   19  82\n... ... ...\n9   London  6\n10  life    6\n11  something   6\n</code></pre>\n<p>This is a way to alternative define group of words having some meaning (at least for my purpose).</p>\n<p>Any suggestion would be appreciated.</p>\n", "abstract": "I'd like to define a new word which includes count values from two (or more) different words. For example: I would like to define mother as mom + mother: This is a way to alternative define group of words having some meaning (at least for my purpose). Any suggestion would be appreciated."}, "answers": [{"id": 63771196, "score": 12, "vote": 0, "content": "<p><strong>UPDATE 10-21-2020</strong></p>\n<p><strong>I decided to build a Python module to handle the tasks that I outlined in this answer. The module is called <em>wordhoard</em> and can be downloaded from <a href=\"https://pypi.org/project/wordhoard/\" rel=\"noreferrer\">pypi</a></strong></p>\n<hr/>\n<p>I have attempted to use Word2vec and WordNet in projects where I needed to determine the frequency of a keyword (e.g. healthcare) and the keyword's synonyms (e.g., wellness program, preventive medicine).  I found that most NLP libraries didn't produce the results that I needed, so I decided to build my own dictionary with custom keywords and synonyms.  This approached has worked for both analyzing and classification text in multiple projects.</p>\n<p>I'm sure that someone that is versed in NLP technology might have a more robust solution, but the one below is similar ones that have worked for me time and time again.</p>\n<p>I coded my answer to match the Words Frequency data you had in your question, but it can be modified to use any keyword and synonyms dataset.</p>\n<pre><code class=\"python\">import string\n\n# Python Dictionary\n# I manually created these word relationship - primary_word:synonyms\nword_relationship = {\"father\": ['dad', 'daddy', 'old man', 'pa', 'pappy', 'papa', 'pop'],\n          \"mother\": [\"mamma\", \"momma\", \"mama\", \"mammy\", \"mummy\", \"mommy\", \"mom\", \"mum\"]}\n\n# This input text is from various poems about mothers and fathers\ninput_text = 'The hand that rocks the cradle also makes the house a home. It is the prayers of the mother ' \\\n         'that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of ' \\\n         'her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She ' \\\n         'has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the ' \\\n         'greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend, ' \\\n         'This to me you have always been. Through the good times and the bad, Your understanding I have had.'\n\n# converts the input text to lowercase and splits the words based on empty space.\nwordlist = input_text.lower().split()\n\n# remove all punctuation from the wordlist\nremove_punctuation = [''.join(ch for ch in s if ch not in string.punctuation) \nfor s in wordlist]\n\n# list for word frequencies\nwordfreq = []\n\n# count the frequencies of a word\nfor w in remove_punctuation:\nwordfreq.append(remove_punctuation.count(w))\n\nword_frequencies = (dict(zip(remove_punctuation, wordfreq)))\n\nword_matches = []\n\n# loop through the dictionaries\nfor word, frequency in word_frequencies.items():\n   for keyword, synonym in word_relationship.items():\n      match = [x for x in synonym if word == x]\n      if word == keyword or match:\n        match = ' '.join(map(str, match))\n        # append the keywords (mother), synonyms(mom) and frequencies to a list\n        word_matches.append([keyword, match, frequency])\n\n# used to hold the final keyword and frequencies\nfinal_results = {}\n\n# list comprehension to obtain the primary keyword and its frequencies\nsynonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]\n\n# iterate synonym_matches and output total frequency count for a specific keyword\nfor item in synonym_matches:\n  if item[0] not in final_results.keys():\n    frequency_count = 0\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n  else:\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n\n \nprint(final_results)\n# output\n{'mother': 3, 'father': 2}\n</code></pre>\n<h2>Other Methods</h2>\n<p>Below are some other methods and their out-of-box output.</p>\n<hr/>\n<p><strong>NLTK WORDNET</strong></p>\n<p>In this example, I looked up the synonyms for the word 'mother.' Note that WordNet does not have the synonyms 'mom' or 'mum' linked to the word mother.  These two words are within my sample text above.  Also note that the word 'father' is listed as a synonym for 'mother.'</p>\n<pre><code class=\"python\">from nltk.corpus import wordnet\n\nsynonyms = []\nword = 'mother'\nfor synonym in wordnet.synsets(word):\n   for item in synonym.lemmas():\n      if word != synonym.name() and len(synonym.lemma_names()) &gt; 1:\n        synonyms.append(item.name())\n\nprint(synonyms)\n['mother', 'female_parent', 'mother', 'fuss', 'overprotect', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']\n</code></pre>\n<p><strong>PyDictionary</strong></p>\n<p>In this example, I looked up the synonyms for the word 'mother' using PyDictionary, which queries <a href=\"https://synonym.com\" rel=\"noreferrer\">synonym.com</a>. The synonyms in this example include the words 'mom' and 'mum.' This example also includes additional synonyms that WordNet did not generate.</p>\n<p>BUT, PyDictionary also produced a synonym list for 'mum.' Which has nothing to do with the word 'mother.'  It seems that PyDictionary pulled this list from the <a href=\"https://www.synonym.com/synonyms/mum\" rel=\"noreferrer\">adjective section</a> of the page instead of the noun section.  It's hard for a computer to distinguish between the adjective mum and the noun mum.</p>\n<pre><code class=\"python\">from PyDictionary import PyDictionary\ndictionary_mother = PyDictionary('mother')\n\nprint(dictionary_mother.getSynonyms())\n# output \n[{'mother': ['mother-in-law', 'female parent', 'supermom', 'mum', 'parent', 'mom', 'momma', 'para I', 'mama', 'mummy', 'quadripara', 'mommy', 'quintipara', 'ma', 'puerpera', 'surrogate mother', 'mater', 'primipara', 'mammy', 'mamma']}]\n\ndictionary_mum = PyDictionary('mum')\n\nprint(dictionary_mum.getSynonyms())\n# output \n[{'mum': ['incommunicative', 'silent', 'uncommunicative']}]\n</code></pre>\n<p>Some of the other possible approaches are using the Oxford Dictionary API or querying thesaurus.com. Both these methods also have pitfalls. For instance the Oxford Dictionary API requires an API key and a paid subscription based on query numbers. And thesaurus.com is missing potential synonyms that could be useful in grouping words.</p>\n<pre><code class=\"python\">https://www.thesaurus.com/browse/mother\nsynonyms: mom, parent, ancestor, creator, mommy, origin, predecessor, progenitor, source, child-bearer, forebearer, procreator\n</code></pre>\n<h2>UPDATE</h2>\n<p>Producing a precise synonym lists for each potential word in your corpus is hard and will require a multiple prong approach.  The code below using\nWordNet and PyDictionary to create a superset of synonyms.  Like all the other answers, this combine methods also leads to some over counting of word frequencies. I've been trying to reduce this over-counting by combining key and value pairs within my final dictionary of synonyms.  The latter problem is much harder than I anticipated and might require me to open my own question to solve.  In the end, I think that based on your use case you need to determine, which approach works best and will likely need to combine several approaches.</p>\n<p>Thanks for posting this question, because it allowed me to look at other methods for solving a complex problem.</p>\n<pre><code class=\"python\">from string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom PyDictionary import PyDictionary\n\ninput_text = \"\"\"The hand that rocks the cradle also makes the house a home. It is the prayers of the mother\n         that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of\n         her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She\n         has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the\n         greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend,\n         This to me you have always been. Through the good times and the bad, Your understanding I have had.\"\"\"\n\n\ndef normalize_textual_information(text):\n   # split text into tokens by white space\n   token = text.split()\n\n   # remove punctuation from each token\n   table = str.maketrans('', '', punctuation)\n   token = [word.translate(table) for word in token]\n\n   # remove any tokens that are not alphabetic\n   token = [word.lower() for word in token if word.isalpha()]\n\n   # filter out English stop words\n   stop_words = set(stopwords.words('english'))\n\n   # you could add additional stops like this\n   stop_words.add('cannot')\n   stop_words.add('could')\n   stop_words.add('would')\n\n   token = [word for word in token if word not in stop_words]\n\n   # filter out any short tokens\n   token = [word for word in token if len(word) &gt; 1]\n   return token\n\n\ndef generate_word_frequencies(words):\n   # list to hold word frequencies\n   word_frequencies = []\n\n   # loop through the tokens and generate a word count for each token\n   for word in words:\n      word_frequencies.append(words.count(word))\n\n   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary\n   word_frequencies = (dict(zip(words, word_frequencies)))\n\n   # sort the frequency of the words from low to high\n   sorted_frequencies = {key: value for key, value in \n   sorted(word_frequencies.items(), key=lambda item: item[1])}\n\n return sorted_frequencies\n\n\ndef get_synonyms_internet(word):\n   dictionary = PyDictionary(word)\n   synonym = dictionary.getSynonyms()\n   return synonym\n\n \nwords = normalize_textual_information(input_text)\n\nall_synsets_1 = {}\nfor word in words:\n  for synonym in wordnet.synsets(word):\n    if word != synonym.name() and len(synonym.lemma_names()) &gt; 1:\n      for item in synonym.lemmas():\n        if word != item.name():\n          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())\n\nall_synsets_2 = {}\nfor word in words:\n  word_synonyms = get_synonyms_internet(word)\n  for synonym in word_synonyms:\n    if word != synonym and synonym is not None:\n      all_synsets_2.update(synonym)\n\n word_relationship = {**all_synsets_1, **all_synsets_2}\n\n frequencies = generate_word_frequencies(words)\n word_matches = []\n word_set = {}\n duplication_check = set()\n\n for word, frequency in frequencies.items():\n    for keyword, synonym in word_relationship.items():\n       match = [x for x in synonym if word == x]\n       if word == keyword or match:\n         match = ' '.join(map(str, match))\n         if match not in word_set or match not in duplication_check or word not in duplication_check:\n            duplication_check.add(word)\n            duplication_check.add(match)\n            word_matches.append([keyword, match, frequency])\n\n # used to hold the final keyword and frequencies\n final_results = {}\n\n # list comprehension to obtain the primary keyword and its frequencies\n synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]\n\n # iterate synonym_matches and output total frequency count for a specific keyword\n for item in synonym_matches:\n    if item[0] not in final_results.keys():\n      frequency_count = 0\n      frequency_count = frequency_count + item[1]\n      final_results[item[0]] = frequency_count\n else:\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n\n# do something with the final results\n</code></pre>\n", "abstract": "UPDATE 10-21-2020 I decided to build a Python module to handle the tasks that I outlined in this answer. The module is called wordhoard and can be downloaded from pypi I have attempted to use Word2vec and WordNet in projects where I needed to determine the frequency of a keyword (e.g. healthcare) and the keyword's synonyms (e.g., wellness program, preventive medicine).  I found that most NLP libraries didn't produce the results that I needed, so I decided to build my own dictionary with custom keywords and synonyms.  This approached has worked for both analyzing and classification text in multiple projects. I'm sure that someone that is versed in NLP technology might have a more robust solution, but the one below is similar ones that have worked for me time and time again. I coded my answer to match the Words Frequency data you had in your question, but it can be modified to use any keyword and synonyms dataset. Below are some other methods and their out-of-box output. NLTK WORDNET In this example, I looked up the synonyms for the word 'mother.' Note that WordNet does not have the synonyms 'mom' or 'mum' linked to the word mother.  These two words are within my sample text above.  Also note that the word 'father' is listed as a synonym for 'mother.' PyDictionary In this example, I looked up the synonyms for the word 'mother' using PyDictionary, which queries synonym.com. The synonyms in this example include the words 'mom' and 'mum.' This example also includes additional synonyms that WordNet did not generate. BUT, PyDictionary also produced a synonym list for 'mum.' Which has nothing to do with the word 'mother.'  It seems that PyDictionary pulled this list from the adjective section of the page instead of the noun section.  It's hard for a computer to distinguish between the adjective mum and the noun mum. Some of the other possible approaches are using the Oxford Dictionary API or querying thesaurus.com. Both these methods also have pitfalls. For instance the Oxford Dictionary API requires an API key and a paid subscription based on query numbers. And thesaurus.com is missing potential synonyms that could be useful in grouping words. Producing a precise synonym lists for each potential word in your corpus is hard and will require a multiple prong approach.  The code below using\nWordNet and PyDictionary to create a superset of synonyms.  Like all the other answers, this combine methods also leads to some over counting of word frequencies. I've been trying to reduce this over-counting by combining key and value pairs within my final dictionary of synonyms.  The latter problem is much harder than I anticipated and might require me to open my own question to solve.  In the end, I think that based on your use case you need to determine, which approach works best and will likely need to combine several approaches. Thanks for posting this question, because it allowed me to look at other methods for solving a complex problem."}, {"id": 63779328, "score": 4, "vote": 0, "content": "<p>It is a hard problem and the best solution depends on the usecase you are trying to solve. It is a hard problem because to combine words you need to understand the semantics of the word. You can combine <code>mom</code> and  <code>mother</code> together because they are semantically related.</p>\n<p>One way to identfy if two words are semantically realted is by reling the distributed word embeddings (vectors) like word2vec, Glove, fasttext et. You can find teh cosine similarity between the vectors of all the words with respect to a word and may be pick up the top 5 close words and create new words.</p>\n<p>Example using word2vec</p>\n<pre><code class=\"python\"># Load a pretrained word2vec model\nimport gensim.downloader as api\nmodel = api.load('word2vec-google-news-300')\n\nvectors = [model.get_vector(w) for w in words]\nfor i, w in enumerate(vectors):\n   first_best_match = model.cosine_similarities(vectors[i], vectors).argsort()[::-1][1]\n   second_best_match = model.cosine_similarities(vectors[i], vectors).argsort()[::-1][2]\n   \n   print (f\"{words[i]} + {words[first_best_match]}\")\n   print (f\"{words[i]} + {words[second_best_match]}\")  \n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">mom + mother\nmom + teacher\nmother + mom\nmother + teacher\nlondon + mom\nlondon + life\nlife + mother\nlife + mom\nteach + teacher\nteach + mom\nteacher + teach\nteacher + mother\n</code></pre>\n<p>You can try putting the threshold on the cosine similarity and only select those which have cosine similarity greater then this threshold.</p>\n<p>One problem with semantic similarity is the they can be semantically opposite and so they are similar (man - woman), on the other hand (man-king) are semantically similar because they are same.</p>\n", "abstract": "It is a hard problem and the best solution depends on the usecase you are trying to solve. It is a hard problem because to combine words you need to understand the semantics of the word. You can combine mom and  mother together because they are semantically related. One way to identfy if two words are semantically realted is by reling the distributed word embeddings (vectors) like word2vec, Glove, fasttext et. You can find teh cosine similarity between the vectors of all the words with respect to a word and may be pick up the top 5 close words and create new words. Example using word2vec Output: You can try putting the threshold on the cosine similarity and only select those which have cosine similarity greater then this threshold. One problem with semantic similarity is the they can be semantically opposite and so they are similar (man - woman), on the other hand (man-king) are semantically similar because they are same."}, {"id": 63772736, "score": 2, "vote": 0, "content": "<p>One other wacky way to address this to use the good old PyDictionary lib. You can use the</p>\n<pre><code class=\"python\">dictionary.getSynonyms()\n</code></pre>\n<p>function to loop through all the words in your list and group them. All available synonyms listed will be covered and mapped to one group. There by allowing you to assign the final variable and summing up the synonyms. In your example. You choose the final word as Mother which displays the final count of synonyms.</p>\n", "abstract": "One other wacky way to address this to use the good old PyDictionary lib. You can use the function to loop through all the words in your list and group them. All available synonyms listed will be covered and mapped to one group. There by allowing you to assign the final variable and summing up the synonyms. In your example. You choose the final word as Mother which displays the final count of synonyms."}, {"id": 63780359, "score": 2, "vote": 0, "content": "<p>What you trying to achieve is <strong>Semantic Textual Similarity</strong>.</p>\n<p>I want to recommend on <a href=\"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder#similarity_visualized\" rel=\"nofollow noreferrer\">Tensorflow Universal Sentence Encoder</a></p>\n<p>for example :</p>\n<pre><code class=\"python\">#@title Load the Universal Sentence Encoder's TF Hub module\nfrom absl import logging\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\n\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\nmodel = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)\ndef embed(input):\n  return model(input)\n\ndef plot_similarity(labels, features, rotation):\n  corr = np.inner(features, features)\n  sns.set(font_scale=1.2)\n  g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n  g.set_xticklabels(labels, rotation=rotation)\n  g.set_title(\"Semantic Textual Similarity\")\n\ndef run_and_plot(messages_):\n  message_embeddings_ = embed(messages_)\n  plot_similarity(messages_, message_embeddings_, 90)\n\nmessages = [\n    \"Mother\",\n    \"Mom\",\n    \"Mama\",\n    \"Dog\",\n    \"Cat\"\n]\n\nrun_and_plot(messages)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/0098s.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/0098s.png\"/></a></p>\n<p>the example is written in python but I also created an example of loading the model in to JVM based languages</p>\n<p><a href=\"https://github.com/ntedgi/universal-sentence-encoder\" rel=\"nofollow noreferrer\">https://github.com/ntedgi/universal-sentence-encoder</a></p>\n", "abstract": "What you trying to achieve is Semantic Textual Similarity. I want to recommend on Tensorflow Universal Sentence Encoder for example :  the example is written in python but I also created an example of loading the model in to JVM based languages https://github.com/ntedgi/universal-sentence-encoder"}, {"id": 63826323, "score": 1, "vote": 0, "content": "<p>You can generate word embedding vectors and use some clustering algorithm. In the end, you need to tune the algorithm's hyperparameters to achieve the result with high accuracy.</p>\n<pre><code class=\"python\">from sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\n\nimport spacy\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load the large english model\nnlp = spacy.load(\"en_core_web_lg\")\n\ntokens = nlp(\"dog cat banana apple teaching teacher mom mother mama mommy berlin paris\")\n\n# Generate word embedding vectors\nvectors = np.array([token.vector for token in tokens])\nvectors.shape\n# (12, 300)\n</code></pre>\n<p>Let's use Principal Component Analysis algorithm to visualize our embeddings in 3-dimensional space:</p>\n<pre><code class=\"python\">pca_vecs = PCA(n_components=3).fit_transform(vectors)\npca_vecs.shape\n# (12, 3)\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection='3d')\nxs, ys, zs = pca_vecs[:, 0], pca_vecs[:, 1], pca_vecs[:, 2]\n_ = ax.scatter(xs, ys, zs)\n\nfor x, y, z, lable in zip(xs, ys, zs, tokens):\n    ax.text(x+0.3, y, z, str(lable))\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/pRm85.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/pRm85.png\"/></a></p>\n<p>Let's use DBSCAN algorithm to cluster words:</p>\n<pre><code class=\"python\">model = DBSCAN(eps=5, min_samples=1)\nmodel.fit(vectors)\n\nfor word, cluster in zip(tokens, model.labels_):\n    print(word, '-&gt;', cluster)\n</code></pre>\n<p>Output:</p>\n<pre><code class=\"python\">dog -&gt; 0\ncat -&gt; 0\nbanana -&gt; 1\napple -&gt; 2\nteaching -&gt; 3\nteacher -&gt; 3\nmom -&gt; 4\nmother -&gt; 4\nmama -&gt; 4\nmommy -&gt; 4\nberlin -&gt; 5\nparis -&gt; 6\n</code></pre>\n", "abstract": "You can generate word embedding vectors and use some clustering algorithm. In the end, you need to tune the algorithm's hyperparameters to achieve the result with high accuracy. Let's use Principal Component Analysis algorithm to visualize our embeddings in 3-dimensional space:  Let's use DBSCAN algorithm to cluster words: Output:"}, {"id": 63757585, "score": -1, "vote": 0, "content": "<p><a href=\"https://github.com/matthewreagan/WebstersEnglishDictionary\" rel=\"nofollow noreferrer\">matthewreagan/WebstersEnglishDictionary</a></p>\n<p>the idea is use this dictonary to identify similar words.</p>\n<p>in short: run some knowledge discovery algorithm which extracts knowledge according to english grammar</p>\n<p><a href=\"https://raw.githubusercontent.com/zaibacu/thesaurus/master/en_thesaurus.jsonl\" rel=\"nofollow noreferrer\">Here is a thesaurus</a>: its 18MB .</p>\n<p>HERE is an excerpt from thesaurus, you may try to match the word alternates via some algorithm.</p>\n<pre><code class=\"python\">{\"word\": \"ma\", \"key\": \"ma_1\", \"pos\": \"noun\", \"synonyms\": [\"mamma\", \"momma\", \"mama\", \"mammy\", \"mummy\", \"mommy\", \"mom\", \"mum\"]}\n</code></pre>\n<p>FOR a quick fix using external api here is the link: they allow to do much more with api like getting synonyms, finding multiple definitions, finding rhyming words and more.</p>\n<p><a href=\"https://www.wordsapi.com/\" rel=\"nofollow noreferrer\">WORDAPI</a></p>\n", "abstract": "matthewreagan/WebstersEnglishDictionary the idea is use this dictonary to identify similar words. in short: run some knowledge discovery algorithm which extracts knowledge according to english grammar Here is a thesaurus: its 18MB . HERE is an excerpt from thesaurus, you may try to match the word alternates via some algorithm. FOR a quick fix using external api here is the link: they allow to do much more with api like getting synonyms, finding multiple definitions, finding rhyming words and more. WORDAPI"}]}, {"link": "https://stackoverflow.com/questions/48432300/using-keras-tokenizer-for-new-words-not-in-training-set", "question": {"id": "48432300", "title": "Using keras tokenizer for new words not in training set", "content": "<p>I'm currently using the Keras Tokenizer to create a word index and then matching that word index to the the imported GloVe dictionary to create an embedding matrix.  However, the problem I have is that this seems to defeat one of the advantages of using a word vector embedding since when using the trained model for predictions if it runs into a new word that's not in the tokenizer's word index it removes it from the sequence.  </p>\n<pre><code class=\"python\">#fit the tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index\n\n#load glove embedding into a dict\nembeddings_index = {}\ndims = 100\nglove_data = 'glove.6B.'+str(dims)+'d.txt'\nf = open(glove_data)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    value = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = value\nf.close()\n\n#create embedding matrix\nembedding_matrix = np.zeros((len(word_index) + 1, dims))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector[:dims]\n\n#Embedding layer:\nembedding_layer = Embedding(embedding_matrix.shape[0],\n                        embedding_matrix.shape[1],\n                        weights=[embedding_matrix],\n                        input_length=12)\n\n#then to make a prediction\nsequence = tokenizer.texts_to_sequences([\"Test sentence\"])\nmodel.predict(sequence)\n</code></pre>\n<p>So is there a way I can still use the tokenizer to transform sentences into an array and still use as much of the words GloVe dictionary as I can instead of only the ones that show up in my training text?  </p>\n<p>Edit: Upon further contemplation, I guess one option would be to add a text or texts to the texts that the tokenizer is fit on that includes a list of the keys in the glove dictionary. Though that might mess with some of the statistics if I want to use tf-idf. Is there either a preferable way to doing this or a different better approach?</p>\n", "abstract": "I'm currently using the Keras Tokenizer to create a word index and then matching that word index to the the imported GloVe dictionary to create an embedding matrix.  However, the problem I have is that this seems to defeat one of the advantages of using a word vector embedding since when using the trained model for predictions if it runs into a new word that's not in the tokenizer's word index it removes it from the sequence.   So is there a way I can still use the tokenizer to transform sentences into an array and still use as much of the words GloVe dictionary as I can instead of only the ones that show up in my training text?   Edit: Upon further contemplation, I guess one option would be to add a text or texts to the texts that the tokenizer is fit on that includes a list of the keys in the glove dictionary. Though that might mess with some of the statistics if I want to use tf-idf. Is there either a preferable way to doing this or a different better approach?"}, "answers": [{"id": 57636651, "score": 11, "vote": 0, "content": "<p>In Keras Tokenizer you have the <strong>oov_token</strong> parameter. Just select your token and unknown words will have that one.</p>\n<pre><code class=\"python\">tokenizer_a = Tokenizer(oov_token=1)\ntokenizer_b = Tokenizer()\ntokenizer_a.fit_on_texts([\"Hello world\"])\ntokenizer_b.fit_on_texts([\"Hello world\"])\n</code></pre>\n<p>Outputs</p>\n<pre><code class=\"python\">In [26]: tokenizer_a.texts_to_sequences([\"Hello cruel world\"])\nOut[26]: [[2, 1, 3]]\n\nIn [27]: tokenizer_b.texts_to_sequences([\"Hello cruel world\"])\nOut[27]: [[1, 2]]\n</code></pre>\n", "abstract": "In Keras Tokenizer you have the oov_token parameter. Just select your token and unknown words will have that one. Outputs"}, {"id": 55870600, "score": 7, "vote": 0, "content": "<p>I would try a different approach. The main problem is that your <code>word_index</code> is based on your training data. Try this:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">#load glove embedding into a dict\nembeddings_index = {}\ndims = 100\nglove_data = 'glove.6B.'+str(dims)+'d.txt'\nf = open(glove_data)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    value = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = value\nf.close()\n\nword_index = {w: i for i, w in enumerate(embeddings_index.keys(), 1)}\n\n#create embedding matrix\nembedding_matrix = np.zeros((len(word_index) + 1, dims))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector[:dims]\n</code></pre>\n<p>Now your <code>embedding_matrix</code> contains all the GloVe works.</p>\n<p>To tokenize your texts you can use something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from keras.preprocessing.text import text_to_word_sequence\n\ndef texts_to_sequences(texts, word_index):\n    for text in texts:\n        tokens = text_to_word_sequence(text)\n        yield [word_index.get(w) for w in tokens if w in word_index]\n\nsequence = texts_to_sequences(['Test sentence'], word_index)\n</code></pre>\n", "abstract": "I would try a different approach. The main problem is that your word_index is based on your training data. Try this: Now your embedding_matrix contains all the GloVe works. To tokenize your texts you can use something like this:"}, {"id": 62464182, "score": 0, "vote": 0, "content": "<p>I had the same problem. In fact, Gloved covered about <strong><em>90 percent</em></strong> of my data before it was tokenized.</p>\n<p>what I did was that I created a list of the words from my text column in pandas dataframe and then created a dictionary of them with <code>enumerate</code>.</p>\n<p>(just like what tokenizer in Keras does but without changing the words and listing them by their frequency).</p>\n<p>Then I checked for words in Glove and added the vector in Glove to my initial weights matrix, whenever my word was in the Glove dictionary.</p>\n<p>I hope the explanation was clear. This is the code for further explanation:</p>\n<pre><code class=\"python\"># creating a vocab of my data\nvocab_of_text = set(\" \".join(df_concat.text).lower().split())\n\n# creating a dictionary of vocab with index\nvocab_of_text = list(enumerate(vocab_of_text, 1))\n\n# putting the index first\nindexed_vocab = {k:v for v,k in dict(vocab_of_text).items()}\n</code></pre>\n<p>Then we use Glove for our weights matrix:</p>\n<pre><code class=\"python\"># creating a matrix for initial weights\nvocab_matrix = np.zeros((len(indexed_vocab)+1,100))\n\n\n\n# searching for vactors in Glove\nfor i, word in indexed_vocab.items():\n    vector = embedding_index.get(word)\n    # embedding index is a dictionary of Glove\n    # with the shape of 'word': vecor\n\n    if vector is not None:\n        vocab_matrix[i] = vector\n</code></pre>\n<p>and then for making it ready for embedding:</p>\n<pre><code class=\"python\">def text_to_sequence(text, word_index):\n    tokens = text.lower().split()\n    return [word_index.get(token) for token in tokens if word_index.get(token) is not None]\n\n# giving ids\ndf_concat['sequences'] = df_concat.text.apply(lambda x : text_to_sequence(x, indexed_vocab))\n\nmax_len_seq = 34\n\n# padding\npadded = pad_sequences(df_concat['sequences'] ,\n              maxlen = max_len_seq, padding = 'post', \n              truncating = 'post')\n</code></pre>\n<p>also thanks to <a href=\"https://stackoverflow.com/users/2060205/spadarian\">@spadarian</a> for his answer. I could come up with this after reading and implementing his idea.part.</p>\n", "abstract": "I had the same problem. In fact, Gloved covered about 90 percent of my data before it was tokenized. what I did was that I created a list of the words from my text column in pandas dataframe and then created a dictionary of them with enumerate. (just like what tokenizer in Keras does but without changing the words and listing them by their frequency). Then I checked for words in Glove and added the vector in Glove to my initial weights matrix, whenever my word was in the Glove dictionary. I hope the explanation was clear. This is the code for further explanation: Then we use Glove for our weights matrix: and then for making it ready for embedding: also thanks to @spadarian for his answer. I could come up with this after reading and implementing his idea.part."}]}, {"link": "https://stackoverflow.com/questions/42269313/interpreting-the-sum-of-tf-idf-scores-of-words-across-documents", "question": {"id": "42269313", "title": "Interpreting the sum of TF-IDF scores of words across documents", "content": "<p>First let's extract the TF-IDF scores per term per document:</p>\n<pre><code class=\"python\">from gensim import corpora, models, similarities\ndocuments = [\"Human machine interface for lab abc computer applications\",\n              \"A survey of user opinion of computer system response time\",\n              \"The EPS user interface management system\",\n              \"System and human system engineering testing of EPS\",\n              \"Relation of user perceived response time to error measurement\",\n              \"The generation of random binary unordered trees\",\n              \"The intersection graph of paths in trees\",\n              \"Graph minors IV Widths of trees and well quasi ordering\",\n              \"Graph minors A survey\"]\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n</code></pre>\n<p>Printing it out:</p>\n<pre><code class=\"python\">for doc in corpus_tfidf:\n    print doc\n</code></pre>\n<p>[out]:</p>\n<pre><code class=\"python\">[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.4301019571350565), (3, 0.4301019571350565), (4, 0.2944198962221451), (5, 0.2944198962221451), (6, 0.2944198962221451)]\n[(4, 0.3726494271826947), (7, 0.27219160459794917), (8, 0.3726494271826947), (9, 0.27219160459794917), (10, 0.3726494271826947), (11, 0.5443832091958983), (12, 0.3726494271826947)]\n[(6, 0.438482464916089), (7, 0.32027755044706185), (9, 0.32027755044706185), (13, 0.6405551008941237), (14, 0.438482464916089)]\n[(5, 0.3449874408519962), (7, 0.5039733231394895), (14, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n[(9, 0.21953536176370683), (10, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n[(25, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]\n[(25, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n[(8, 0.6282580468670046), (26, 0.45889394536615247), (29, 0.6282580468670046)]\n</code></pre>\n<p>If we want to find the \"saliency\" or \"importance\" of the words within this corpus, <strong>can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents?</strong> I.e. </p>\n<pre><code class=\"python\">&gt;&gt;&gt; tfidf_saliency = Counter()\n&gt;&gt;&gt; for doc in corpus_tfidf:\n...     for word, score in doc:\n...         tfidf_saliency[word] += score / len(corpus_tfidf)\n... \n&gt;&gt;&gt; tfidf_saliency\nCounter({7: 0.12182694202050007, 8: 0.11121194156107769, 26: 0.10886469856464989, 29: 0.10093919463036093, 9: 0.09022272408985754, 14: 0.08705221175200946, 25: 0.08482488519466996, 6: 0.08143359568202602, 10: 0.07480097322359022, 12: 0.07480097322359022, 4: 0.07411881371164887, 13: 0.07117278898823597, 5: 0.07104525967490458, 27: 0.07027283689263066, 28: 0.07027283689263066, 11: 0.060487023243988705, 15: 0.055997035904387725, 16: 0.055997035904387725, 21: 0.05389680556362955, 22: 0.05389680556362955, 23: 0.05389680556362955, 24: 0.05389680556362955, 17: 0.048785635947490406, 18: 0.048785635947490406, 19: 0.048785635947490406, 20: 0.048785635947490406, 0: 0.04778910634833961, 1: 0.04778910634833961, 2: 0.04778910634833961, 3: 0.04778910634833961, 30: 0.045480127933079706, 31: 0.045480127933079706, 32: 0.045480127933079706, 33: 0.045480127933079706, 34: 0.045480127933079706})\n</code></pre>\n<p>Looking at the output, could we assume that the most \"prominent\" word in the corpus is:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; dictionary[7]\nu'system'\n&gt;&gt;&gt; dictionary[8]\nu'survey'\n&gt;&gt;&gt; dictionary[26]\nu'graph'\n</code></pre>\n<p>If so, <strong>what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?</strong></p>\n", "abstract": "First let's extract the TF-IDF scores per term per document: Printing it out: [out]: If we want to find the \"saliency\" or \"importance\" of the words within this corpus, can we simple do the sum of the tf-idf scores across all documents and divide it by the number of documents? I.e.  Looking at the output, could we assume that the most \"prominent\" word in the corpus is: If so, what is the mathematical interpretation of the sum of TF-IDF scores of words across documents?"}, "answers": [{"id": 42369493, "score": 6, "vote": 0, "content": "<p>The interpretation of TF-IDF in corpus is the highest TF-IDF in corpus for a given term.  </p>\n<p><strong>Find the Top Words in corpus_tfidf.</strong></p>\n<pre><code class=\"python\">    topWords = {}\n    for doc in corpus_tfidf:\n        for iWord, tf_idf in doc:\n            if iWord not in topWords:\n                topWords[iWord] = 0\n\n            if tf_idf &gt; topWords[iWord]:\n                topWords[iWord] = tf_idf\n\n    for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n        print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n        if i == 6: break\n</code></pre>\n<p>Output comparison cart:<br/>\n<strong>NOTE</strong>: Could'n use <code>gensim</code>, to create a matching <code>dictionary</code> with <code>corpus_tfidf</code>.<br/>\n Can only display Word Indizies.  </p>\n<pre><code class=\"python\">Question tfidf_saliency   topWords(corpus_tfidf)  Other TF-IDF implentation  \n---------------------------------------------------------------------------  \n1: Word(7)   0.121        1: Word(13)    0.640    1: paths         0.376019  \n2: Word(8)   0.111        2: Word(27)    0.632    2: intersection  0.376019  \n3: Word(26)  0.108        3: Word(28)    0.632    3: survey        0.366204  \n4: Word(29)  0.100        4: Word(8)     0.628    4: minors        0.366204  \n5: Word(9)   0.090        5: Word(29)    0.628    5: binary        0.300815  \n6: Word(14)  0.087        6: Word(11)    0.544    6: generation    0.300815  \n</code></pre>\n<p>The calculation of TF-IDF takes always the corpus in account.   </p>\n<p><strong><em>Tested with Python:3.4.2</em></strong></p>\n", "abstract": "The interpretation of TF-IDF in corpus is the highest TF-IDF in corpus for a given term.   Find the Top Words in corpus_tfidf. Output comparison cart:\nNOTE: Could'n use gensim, to create a matching dictionary with corpus_tfidf.\n Can only display Word Indizies.   The calculation of TF-IDF takes always the corpus in account.    Tested with Python:3.4.2"}, {"id": 48048345, "score": 3, "vote": 0, "content": "<p>This is a great discussion. Thanks for starting this thread. The idea of including document length by @avip seems interesting. Will have to experiment and check on the results. In the meantime let me try asking the question a little differently. What are we trying to interpret when querying for TF-IDF relevance scores ?</p>\n<ol>\n<li>Possibly trying to understand the word relevance at the document level </li>\n<li>Possibly trying to understand the word relevance per Class</li>\n<li><p>Possibly trying to understand the word relevance overall ( in the whole \ncorpus )</p>\n<pre><code class=\"python\"> # # features, corpus = 6 documents of length 3\n counts = [[3, 0, 1],\n           [2, 0, 0],\n           [3, 0, 0],\n           [4, 0, 0],\n           [3, 2, 0],\n           [3, 0, 2]]\n from sklearn.feature_extraction.text import TfidfTransformer\n transformer = TfidfTransformer(smooth_idf=False)\n tfidf = transformer.fit_transform(counts)\n print(tfidf.toarray())\n\n # lambda for basic stat computation\n summarizer_default = lambda x: np.sum(x, axis=0)\n summarizer_mean = lambda x: np.mean(x, axis=0)\n\n print(summarizer_default(tfidf))\n print(summarizer_mean(tfidf))\n</code></pre></li>\n</ol>\n<p>Result:</p>\n<pre><code class=\"python\"># Result post computing TF-IDF relevance scores\narray([[ 0.81940995,  0.        ,  0.57320793],\n           [ 1.        ,  0.        ,  0.        ],\n           [ 1.        ,  0.        ,  0.        ],\n           [ 1.        ,  0.        ,  0.        ],\n           [ 0.47330339,  0.88089948,  0.        ],\n           [ 0.58149261,  0.        ,  0.81355169]])\n\n# Result post aggregation (Sum, Mean) \n[[ 4.87420595  0.88089948  1.38675962]]\n[[ 0.81236766  0.14681658  0.2311266 ]]\n</code></pre>\n<p>If we observe closely, we realize the the feature1 witch occurred in all the document is not ignored completely because the sklearn implementation of idf = log [ n / df(d, t) ] + 1. +1 is added so that the important word which just so happens to occur in all document is not ignored. E.g. the word 'bike' occurring very frequently in classifying a particular document as 'motorcyle' (20_newsgroup dataset).</p>\n<ol>\n<li><p>Now in-regards to the first 2 questions, one is trying to interpret and understand the top common features that might be occurring in the document. In that case, aggregating in some form including all possible occurrence of the word in a doc is not taking anything away even mathematically. IMO such a query is very useful exploring the dataset and helping understanding what the dataset is about. The logic might be applied to vectorizing using Hashing as well.</p>\n<p>relevance_score = mean(tf(t,d) * idf(t,d)) = mean( (bias + \n            inital_wt * F(t,d) / max{F(t',d)}) * log(N/df(d, t)) + 1 ))</p></li>\n<li><p>Question3 is very important as it might as well be contributing to \nfeatures being selected for building a predictive model. Just using TF-IDF scores independently for feature selection might be misleading at multiple level. Adopting a more theoretical statistical test such as 'chi2' couple with TF-IDF relevance scores might be a better approach. Such statistical test also evaluates the importance of the feature in relation to the respective target class.</p></li>\n</ol>\n<p>And of-course combining such interpretation with the model's learned feature weights would be very helpful in understanding the importance of text derived features completely.</p>\n<p>** The problem is a little more complex to cover in detail here. But, hoping the above helps. What do others feel ? </p>\n<p>Reference: <a href=\"https://arxiv.org/abs/1707.05261\" rel=\"nofollow noreferrer\">https://arxiv.org/abs/1707.05261</a></p>\n", "abstract": "This is a great discussion. Thanks for starting this thread. The idea of including document length by @avip seems interesting. Will have to experiment and check on the results. In the meantime let me try asking the question a little differently. What are we trying to interpret when querying for TF-IDF relevance scores ? Possibly trying to understand the word relevance overall ( in the whole \ncorpus ) Result: If we observe closely, we realize the the feature1 witch occurred in all the document is not ignored completely because the sklearn implementation of idf = log [ n / df(d, t) ] + 1. +1 is added so that the important word which just so happens to occur in all document is not ignored. E.g. the word 'bike' occurring very frequently in classifying a particular document as 'motorcyle' (20_newsgroup dataset). Now in-regards to the first 2 questions, one is trying to interpret and understand the top common features that might be occurring in the document. In that case, aggregating in some form including all possible occurrence of the word in a doc is not taking anything away even mathematically. IMO such a query is very useful exploring the dataset and helping understanding what the dataset is about. The logic might be applied to vectorizing using Hashing as well. relevance_score = mean(tf(t,d) * idf(t,d)) = mean( (bias + \n            inital_wt * F(t,d) / max{F(t',d)}) * log(N/df(d, t)) + 1 )) Question3 is very important as it might as well be contributing to \nfeatures being selected for building a predictive model. Just using TF-IDF scores independently for feature selection might be misleading at multiple level. Adopting a more theoretical statistical test such as 'chi2' couple with TF-IDF relevance scores might be a better approach. Such statistical test also evaluates the importance of the feature in relation to the respective target class. And of-course combining such interpretation with the model's learned feature weights would be very helpful in understanding the importance of text derived features completely. ** The problem is a little more complex to cover in detail here. But, hoping the above helps. What do others feel ?  Reference: https://arxiv.org/abs/1707.05261"}, {"id": 42343244, "score": 2, "vote": 0, "content": "<p>There is two context that saliency can be calculated in them.</p>\n<ol>\n<li>saliency in the corpus </li>\n<li>saliency in a single document</li>\n</ol>\n<p>saliency in the corpus simply can be calculated by counting the appearances of particular word in corpus or by inverse of the counting of the documents that word appears in (IDF=Inverted Document Frequency). Because the words that hold the specific meaning does not appear in everywhere.</p>\n<p>saliency in the document is calculated by tf_idf. Because that is composed of two kinds of information. Global information (corpus-based) and local information (document-based).Claiming that \"the word with larger in-document frequency is more important in current document\" is not completely true or false because it depends on the global saliency of word. In a particular document you have many words like \"it, is, am, are ,...\" with large frequencies. But these word is not important in any document and you can take them as stop words!</p>\n<p><strong>---- edit ---</strong></p>\n<p>The denominator (=len(corpus_tfidf)) is a constant value and could be ignored if you want to deal with ordinality rather than cardinality of measurement. On the other hand we know that IDF means Inverted Document Freqeuncy so IDF can be reoresented by 1/DF. We know that the DF is a corpus level value and TF is document level-value. TF-IDF Summation turns document-level TF into Corpus-level TF. Indeed the summation is equal to this formula:</p>\n<p>count ( <strong>word</strong> ) / count ( documents contain <strong>word</strong>)</p>\n<p>This measurement can be called <strong>inverse-scattering</strong> value. When the value goes up means the words is gathered into smaller subset of documents and vice versa.</p>\n<p>I believe that this formula is not so useful.</p>\n", "abstract": "There is two context that saliency can be calculated in them. saliency in the corpus simply can be calculated by counting the appearances of particular word in corpus or by inverse of the counting of the documents that word appears in (IDF=Inverted Document Frequency). Because the words that hold the specific meaning does not appear in everywhere. saliency in the document is calculated by tf_idf. Because that is composed of two kinds of information. Global information (corpus-based) and local information (document-based).Claiming that \"the word with larger in-document frequency is more important in current document\" is not completely true or false because it depends on the global saliency of word. In a particular document you have many words like \"it, is, am, are ,...\" with large frequencies. But these word is not important in any document and you can take them as stop words! ---- edit --- The denominator (=len(corpus_tfidf)) is a constant value and could be ignored if you want to deal with ordinality rather than cardinality of measurement. On the other hand we know that IDF means Inverted Document Freqeuncy so IDF can be reoresented by 1/DF. We know that the DF is a corpus level value and TF is document level-value. TF-IDF Summation turns document-level TF into Corpus-level TF. Indeed the summation is equal to this formula: count ( word ) / count ( documents contain word) This measurement can be called inverse-scattering value. When the value goes up means the words is gathered into smaller subset of documents and vice versa. I believe that this formula is not so useful."}, {"id": 55577390, "score": 0, "vote": 0, "content": "<p>I stumbled across the same problem somehow. I will share my solution here but don't really know how effective it is. </p>\n<p>After calculating tf-idf basically what we have is like a matrix of terms vs documents. </p>\n<pre><code class=\"python\">[terms/docs : doc1  ,  doc2 , doc3..... docn\n term1      : tf(doc1)-idf, tf(doc2)-idf , tf(doc3)-idf.....\n .\n .\n .\n termn ........ ]\n</code></pre>\n<p>We can think of columns doc1,doc2...docn as scores given to every term according n different metrics. If we sum across the columns we are simply averaging the scores which is a naive way and does not completely represent the information captured. We can do something better as this is a top-k retrieval problem. One efficient algorithm is Fagin's algorithm and works on this idea : </p>\n<p>The sorted lists are scanned until k data items are found which have been\nseen in all the lists, then the algorithm can stop and it is guaranteed that among\nall the data items seen so far, even those which were not present in all the lists,\nthe top-k data items can be found. </p>\n<p>Sorted lists here simply mean a single column of a particular doc becomes a list and we have n such lists. So sort each one of them and then do fagins on it. </p>\n<p>Read about it more <a href=\"http://www.inf.unibz.it/dis/teaching/SDB/reports/report_mitterer.pdf\" rel=\"nofollow noreferrer\">here</a></p>\n", "abstract": "I stumbled across the same problem somehow. I will share my solution here but don't really know how effective it is.  After calculating tf-idf basically what we have is like a matrix of terms vs documents.  We can think of columns doc1,doc2...docn as scores given to every term according n different metrics. If we sum across the columns we are simply averaging the scores which is a naive way and does not completely represent the information captured. We can do something better as this is a top-k retrieval problem. One efficient algorithm is Fagin's algorithm and works on this idea :  The sorted lists are scanned until k data items are found which have been\nseen in all the lists, then the algorithm can stop and it is guaranteed that among\nall the data items seen so far, even those which were not present in all the lists,\nthe top-k data items can be found.  Sorted lists here simply mean a single column of a particular doc becomes a list and we have n such lists. So sort each one of them and then do fagins on it.  Read about it more here"}, {"id": 42359892, "score": -1, "vote": 0, "content": "<blockquote>\n<p>If we want to find the \"saliency\" or \"importance\" of the words within\n  this corpus, can we simple do the sum of the tf-idf scores across all\n  documents and divide it by the number of documents? If so, what is the\n  mathematical interpretation of the sum of TF-IDF scores of words\n  across documents?</p>\n</blockquote>\n<p>If you summed td-idf scores across documents, terms that would otherwise have low scores might get a boost and terms with higher scores might have their scores depressed.</p>\n<p>I don't think simply dividing by the total number of documents will be sufficient normalization to address this. Maybe incorporating document length into the normalization factor? Either way, I think all such methods would still need to be adjusted per domain. </p>\n<p>So, generally speaking, mathematically I expect you would get an undesirable averaging effect.</p>\n", "abstract": "If we want to find the \"saliency\" or \"importance\" of the words within\n  this corpus, can we simple do the sum of the tf-idf scores across all\n  documents and divide it by the number of documents? If so, what is the\n  mathematical interpretation of the sum of TF-IDF scores of words\n  across documents? If you summed td-idf scores across documents, terms that would otherwise have low scores might get a boost and terms with higher scores might have their scores depressed. I don't think simply dividing by the total number of documents will be sufficient normalization to address this. Maybe incorporating document length into the normalization factor? Either way, I think all such methods would still need to be adjusted per domain.  So, generally speaking, mathematically I expect you would get an undesirable averaging effect."}]}, {"link": "https://stackoverflow.com/questions/7386856/python-arabic-nlp", "question": {"id": "7386856", "title": "Python Arabic NLP", "content": "<p>I'm in the process of assessing the capabilities of the NLTK in processing Arabic text in a research to analyze and extract sentiments.</p>\n<p>Question is as follows:</p>\n<ol>\n<li>Is the NTLK capable of handling and allows the analysis of Arabic text?</li>\n<li>Is python capable of manipulating\\tokenizing Arabic text?</li>\n<li>Will I be able to parse and store Arabic text using Python?</li>\n</ol>\n<p>If python and NTLK aren't the tools for this job, what tools would you recommend (if existent)?</p>\n<p>Thank you.</p>\n<hr/>\n<h3>EDIT</h3>\n<p>Based on research:</p>\n<ol>\n<li>NTLK is only capable of stemming Arabic text: <a href=\"http://text-processing.com/demo/\" rel=\"noreferrer\">Link</a></li>\n<li>Python is capable of handling Arabic text since it supports UTF-8 unicode: <a href=\"http://www.spencegreen.com/2008/12/19/python-arabic-unicode/\" rel=\"noreferrer\">Link</a></li>\n<li>Parsing and Lemmatization of Arabic text can be done using: \nSNLPG (The Stanford Natural Language Processing Group) Statistical Parser: <a href=\"http://nlp.stanford.edu/software/lex-parser.shtml\" rel=\"noreferrer\">Link</a></li>\n</ol>\n", "abstract": "I'm in the process of assessing the capabilities of the NLTK in processing Arabic text in a research to analyze and extract sentiments. Question is as follows: If python and NTLK aren't the tools for this job, what tools would you recommend (if existent)? Thank you. Based on research:"}, "answers": [{"id": 7389720, "score": 6, "vote": 0, "content": "<p>A simple google search lead to these links:</p>\n<p><a href=\"https://sites.google.com/site/dyaafayedsite/\" rel=\"noreferrer\">Arabic Natural Language Processing</a></p>\n<p><a href=\"http://www.opensubscriber.com/message/comp-quran@comp.leeds.ac.uk/13493220.html\" rel=\"noreferrer\">Using Python with the Quranic Arabic Corpus</a></p>\n<p><a href=\"http://www.spencegreen.com/2008/12/19/python-arabic-unicode/\" rel=\"noreferrer\">HOWTO: Working with Python, Unicode, and Arabic</a></p>\n<p>Are any of these useful?</p>\n", "abstract": "A simple google search lead to these links: Arabic Natural Language Processing Using Python with the Quranic Arabic Corpus HOWTO: Working with Python, Unicode, and Arabic Are any of these useful?"}]}, {"link": "https://stackoverflow.com/questions/43377265/determine-if-text-is-in-english", "question": {"id": "43377265", "title": "Determine if text is in English?", "content": "<p>I am using both <a href=\"http://www.nltk.org/\" rel=\"noreferrer\">Nltk</a> and <a href=\"http://scikit-learn.org/stable/\" rel=\"noreferrer\">Scikit Learn</a> to do some text processing. However, within my list of documents I have some documents that are not in English. For example, the following could be true:</p>\n<pre><code class=\"python\">[ \"this is some text written in English\", \n  \"this is some more text written in English\", \n  \"Ce n'est pas en anglais\" ] \n</code></pre>\n<p>For the purposes of my analysis, I want all sentences that are not in English to be removed as part of pre-processing. However, is there a good way to do this? I have been Googling, but cannot find anything specific that will let me recognize if strings are in English or not. Is this something that is not offered as functionality in either <code>Nltk</code> or <code>Scikit learn</code>? <b>EDIT</b> I've seen questions both like <a href=\"https://stackoverflow.com/questions/29099621/how-to-find-out-wether-a-word-exists-in-english-using-nltk\">this</a> and <a href=\"https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\">this</a> but both are for individual words... Not a \"document\". Would I have to loop through every word in a sentence to check if the whole sentence is in English?</p>\n<p>I'm using Python, so libraries that are in Python would be preferable, but I can switch languages if needed, just thought that Python would be the best for this.</p>\n", "abstract": "I am using both Nltk and Scikit Learn to do some text processing. However, within my list of documents I have some documents that are not in English. For example, the following could be true: For the purposes of my analysis, I want all sentences that are not in English to be removed as part of pre-processing. However, is there a good way to do this? I have been Googling, but cannot find anything specific that will let me recognize if strings are in English or not. Is this something that is not offered as functionality in either Nltk or Scikit learn? EDIT I've seen questions both like this and this but both are for individual words... Not a \"document\". Would I have to loop through every word in a sentence to check if the whole sentence is in English? I'm using Python, so libraries that are in Python would be preferable, but I can switch languages if needed, just thought that Python would be the best for this."}, "answers": [{"id": 43377356, "score": 24, "vote": 0, "content": "<p>There is a library called langdetect. It is ported from Google's language-detection available here:</p>\n<p><a href=\"https://pypi.python.org/pypi/langdetect\" rel=\"noreferrer\">https://pypi.python.org/pypi/langdetect</a></p>\n<p>It supports 55 languages out of the box.</p>\n", "abstract": "There is a library called langdetect. It is ported from Google's language-detection available here: https://pypi.python.org/pypi/langdetect It supports 55 languages out of the box."}, {"id": 48436520, "score": 22, "vote": 0, "content": "<p>You might be interested in my paper <a href=\"https://arxiv.org/pdf/1801.07779.pdf\" rel=\"noreferrer\">The WiLI benchmark dataset for written\nlanguage identification</a>. I also benchmarked a couple of tools.</p>\n<p>TL;DR:</p>\n<ul>\n<li>CLD-2 is pretty good and extremely fast</li>\n<li><a href=\"https://pypi.python.org/pypi/langdetect?\" rel=\"noreferrer\">lang-detect</a> is a tiny bit better, but much slower</li>\n<li>langid is good, but CLD-2 and lang-detect are much better</li>\n<li>NLTK's Textcat is neither efficient nor effective.</li>\n</ul>\n<p>You can install <a href=\"https://github.com/MartinThoma/lidtk\" rel=\"noreferrer\"><code>lidtk</code></a> and classify languages:</p>\n<pre><code class=\"python\">$ lidtk cld2 predict --text \"this is some text written in English\"\neng\n$ lidtk cld2 predict --text \"this is some more text written in English\"\neng\n$ lidtk cld2 predict --text \"Ce n'est pas en anglais\"                  \nfra\n</code></pre>\n", "abstract": "You might be interested in my paper The WiLI benchmark dataset for written\nlanguage identification. I also benchmarked a couple of tools. TL;DR: You can install lidtk and classify languages:"}, {"id": 58861912, "score": 5, "vote": 0, "content": "<h2>Pretrained Fast Text Model Worked Best For My Similar Needs</h2>\n<p>I arrived at your question with a very similar need. I appreciated Martin Thoma's answer. However, I found the most help from Rabash's answer part 7 <a href=\"https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language/47106810\">HERE</a>. </p>\n<p>After experimenting to find what worked best for my needs, which were making sure text files were in English in 60,000+ text files, I found that <a href=\"https://www.tutorialkart.com/fasttext/make-model-learn-word-representations-using-fasttext-python/\" rel=\"noreferrer\">fasttext</a> was an excellent tool. </p>\n<p>With a little work, I had a tool that worked very fast over many files. Below is the code with comments. I believe that you and others will be able to modify this code for your more specific needs. </p>\n<pre><code class=\"python\">class English_Check:\n    def __init__(self):\n        # Don't need to train a model to detect languages. A model exists\n        #    that is very good. Let's use it.\n        pretrained_model_path = 'location of your lid.176.ftz file from fasttext'\n        self.model = fasttext.load_model(pretrained_model_path)\n\n    def predictionict_languages(self, text_file):\n        this_D = {}\n        with open(text_file, 'r') as f:\n            fla = f.readlines()  # fla = file line array.\n            # fasttext doesn't like newline characters, but it can take\n            #    an array of lines from a file. The two list comprehensions\n            #    below, just clean up the lines in fla\n            fla = [line.rstrip('\\n').strip(' ') for line in fla]\n            fla = [line for line in fla if len(line) &gt; 0]\n\n            for line in fla:  # Language predict each line of the file\n                language_tuple = self.model.predictionict(line)\n                # The next two lines simply get at the top language prediction\n                #    string AND the confidence value for that prediction.\n                prediction = language_tuple[0][0].replace('__label__', '')\n                value = language_tuple[1][0]\n\n                # Each top language prediction for the lines in the file\n                #    becomes a unique key for the this_D dictionary.\n                #    Everytime that language is found, add the confidence\n                #    score to the running tally for that language.\n                if prediction not in this_D.keys():\n                    this_D[prediction] = 0\n                this_D[prediction] += value\n\n        self.this_D = this_D\n\n    def determine_if_file_is_english(self, text_file):\n        self.predictionict_languages(text_file)\n\n        # Find the max tallied confidence and the sum of all confidences.\n        max_value = max(self.this_D.values())\n        sum_of_values = sum(self.this_D.values())\n        # calculate a relative confidence of the max confidence to all\n        #    confidence scores. Then find the key with the max confidence.\n        confidence = max_value / sum_of_values\n        max_key = [key for key in self.this_D.keys()\n                   if self.this_D[key] == max_value][0]\n\n        # Only want to know if this is english or not.\n        return max_key == 'en'\n</code></pre>\n<p>Below is the application / instantiation and use of the above class for my needs.</p>\n<pre><code class=\"python\">file_list = # some tool to get my specific list of files to check for English\n\nen_checker = English_Check()\nfor file in file_list:\n    check = en_checker.determine_if_file_is_english(file)\n    if not check:\n        print(file)\n</code></pre>\n", "abstract": "I arrived at your question with a very similar need. I appreciated Martin Thoma's answer. However, I found the most help from Rabash's answer part 7 HERE.  After experimenting to find what worked best for my needs, which were making sure text files were in English in 60,000+ text files, I found that fasttext was an excellent tool.  With a little work, I had a tool that worked very fast over many files. Below is the code with comments. I believe that you and others will be able to modify this code for your more specific needs.  Below is the application / instantiation and use of the above class for my needs."}, {"id": 57349398, "score": 4, "vote": 0, "content": "<p>This is what I've used some time ago.\nIt works for texts longer than 3 words and with less than 3 non-recognized words.\nOf course, you can play with the settings, but for my use case (website <strong>scraping</strong>) those worked pretty well.</p>\n<pre><code class=\"python\">from enchant.checker import SpellChecker\n\nmax_error_count = 4\nmin_text_length = 3\n\ndef is_in_english(quote):\n  d = SpellChecker(\"en_US\")\n  d.set_text(quote)\n  errors = [err.word for err in d]\n  return False if ((len(errors) &gt; max_error_count) or len(quote.split()) &lt; min_text_length) else True\n\nprint(is_in_english('\u201c\u4e2d\u6587\u201d'))\nprint(is_in_english('\u201cTwo things are infinite: the universe and human stupidity; and I\\'m not sure about the universe.\u201d'))\n\n&gt; False\n&gt; True\n</code></pre>\n", "abstract": "This is what I've used some time ago.\nIt works for texts longer than 3 words and with less than 3 non-recognized words.\nOf course, you can play with the settings, but for my use case (website scraping) those worked pretty well."}, {"id": 43377472, "score": 2, "vote": 0, "content": "<p>Use the enchant library</p>\n<pre><code class=\"python\">import enchant\n\ndictionary = enchant.Dict(\"en_US\") #also available are en_GB, fr_FR, etc\n\ndictionary.check(\"Hello\") # prints True\ndictionary.check(\"Helo\") #prints False\n</code></pre>\n<p>This example is taken directly from their <a href=\"https://pythonhosted.org/pyenchant/tutorial.html\" rel=\"nofollow noreferrer\">website</a></p>\n", "abstract": "Use the enchant library This example is taken directly from their website"}, {"id": 43380167, "score": 1, "vote": 0, "content": "<p>If you want something lightweight, letter trigrams are a popular approach. Every language has a different \"profile\" of common and uncommon trigrams. You can google around for it, or code your own. Here's a sample implementation I came across, which uses \"cosine similarity\" as a measure of distance between the sample text and the reference data: </p>\n<p><a href=\"http://code.activestate.com/recipes/326576-language-detection-using-character-trigrams/\" rel=\"nofollow noreferrer\">http://code.activestate.com/recipes/326576-language-detection-using-character-trigrams/</a></p>\n<p>If you know the common non-English languages in your corpus, it's pretty easy to turn this into a yes/no test. If you don't, you need to anticipate sentences from languages for which you don't have trigram statistics. I would do some testing to see the normal range of similarity scores for single-sentence texts in your documents, and choose a suitable threshold for the English cosine score.</p>\n", "abstract": "If you want something lightweight, letter trigrams are a popular approach. Every language has a different \"profile\" of common and uncommon trigrams. You can google around for it, or code your own. Here's a sample implementation I came across, which uses \"cosine similarity\" as a measure of distance between the sample text and the reference data:  http://code.activestate.com/recipes/326576-language-detection-using-character-trigrams/ If you know the common non-English languages in your corpus, it's pretty easy to turn this into a yes/no test. If you don't, you need to anticipate sentences from languages for which you don't have trigram statistics. I would do some testing to see the normal range of similarity scores for single-sentence texts in your documents, and choose a suitable threshold for the English cosine score."}, {"id": 71865565, "score": 0, "vote": 0, "content": "<pre><code class=\"python\">import enchant\ndef check(text):\n    text=text.split()\n    dictionary = enchant.Dict(\"en_US\") #also available are en_GB, fr_FR, etc\n    for i in range(len(text)):\n        if(dictionary.check(text[i])==False):\n            o = \"False\"\n            break\n        else:\n            o = (\"True\")\n        return o\n</code></pre>\n", "abstract": ""}]}]