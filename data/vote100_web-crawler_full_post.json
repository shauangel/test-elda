[{"link": "https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python", "question": {"id": "10606133", "title": "Sending &quot;User-agent&quot; using Requests library in Python", "content": "<p>I want to send a value for <code>\"User-agent\"</code> while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below:</p>\n<pre><code class=\"python\">debug = {'verbose': sys.stderr}\nuser_agent = {'User-agent': 'Mozilla/5.0'}\nresponse  = requests.get(url, headers = user_agent, config=debug)\n</code></pre>\n<p>The debug information isn't showing the headers being sent during the request.</p>\n<p>Is it acceptable to send this information in the header?  If not, how can I send it?</p>\n", "abstract": "I want to send a value for \"User-agent\" while requesting a webpage using Python Requests.  I am not sure is if it is okay to send this as a part of the header, as in the code below: The debug information isn't showing the headers being sent during the request. Is it acceptable to send this information in the header?  If not, how can I send it?"}, "answers": [{"id": 10606260, "score": 438, "vote": 0, "content": "<p>The <code>user-agent</code> should be specified as a field in the header.</p>\n<p>Here is a <a href=\"https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\" rel=\"noreferrer\">list of HTTP header fields</a>, and you'd probably be interested in <a href=\"https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Request_fields\" rel=\"noreferrer\">request-specific fields</a>, which includes <code>User-Agent</code>.</p>\n<h3>If you're using requests v2.13 and newer</h3>\n<p>The simplest way to do what you want is to create a dictionary and specify your headers directly, like so:</p>\n<pre><code class=\"python\">import requests\n\nurl = 'SOME URL'\n\nheaders = {\n    'User-Agent': 'My User Agent 1.0',\n    'From': 'youremail@domain.example'  # This is another valid field\n}\n\nresponse = requests.get(url, headers=headers)\n</code></pre>\n<h3>If you're using requests v2.12.x and older</h3>\n<p>Older versions of <code>requests</code> clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them.</p>\n<pre><code class=\"python\">import requests\n\nurl = 'SOME URL'\n\n# Get a copy of the default headers that requests would use\nheaders = requests.utils.default_headers()\n\n# Update the headers with your custom ones\n# You don't have to worry about case-sensitivity with\n# the dictionary keys, because default_headers uses a custom\n# CaseInsensitiveDict implementation within requests' source code.\nheaders.update(\n    {\n        'User-Agent': 'My User Agent 1.0',\n    }\n)\n\nresponse = requests.get(url, headers=headers)\n</code></pre>\n", "abstract": "The user-agent should be specified as a field in the header. Here is a list of HTTP header fields, and you'd probably be interested in request-specific fields, which includes User-Agent. The simplest way to do what you want is to create a dictionary and specify your headers directly, like so: Older versions of requests clobbered default headers, so you'd want to do the following to preserve default headers and then add your own to them."}, {"id": 44535317, "score": 90, "vote": 0, "content": "<p>It's more convenient to use a <a href=\"https://2.python-requests.org/en/master/user/advanced/#id1\" rel=\"noreferrer\">session</a>, this way you don't have to remember to set headers each time:</p>\n<pre><code class=\"python\">session = requests.Session()\nsession.headers.update({'User-Agent': 'Custom user agent'})\n\nsession.get('https://httpbin.org/headers')\n</code></pre>\n<p>By default, session also manages cookies for you. In case you want to disable that, see <a href=\"https://stackoverflow.com/questions/17037668/how-to-disable-cookie-handling-with-the-python-requests-library\">this question</a>.</p>\n", "abstract": "It's more convenient to use a session, this way you don't have to remember to set headers each time: By default, session also manages cookies for you. In case you want to disable that, see this question."}, {"id": 72687611, "score": 4, "vote": 0, "content": "<p>It will send the request like browser</p>\n<pre><code class=\"python\">import requests\n\nurl = 'https://Your-url'\nheaders={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n\nresponse= requests.get(url.strip(), headers=headers, timeout=10)\n</code></pre>\n", "abstract": "It will send the request like browser"}, {"id": 69149183, "score": -6, "vote": 0, "content": "<p><strong>simply you can do it like below:</strong></p>\n<pre><code class=\"python\">import requests\n\nurl = requests.post(\"URL\", headers={\"FUser\":\"your username\",\"FPass\":\"your password\",\"user-agent\": \"your custom text for the user agent \"})\n</code></pre>\n", "abstract": "simply you can do it like below:"}]}, {"link": "https://stackoverflow.com/questions/31019854/typeerror-cant-use-a-string-pattern-on-a-bytes-like-object-in-re-findall", "question": {"id": "31019854", "title": "TypeError: can&#39;t use a string pattern on a bytes-like object in re.findall()", "content": "<p>I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:</p>\n<pre><code class=\"python\">import urllib.request\nimport re\n\nurl = \"http://www.google.com\"\nregex = r'&lt;title&gt;(,+?)&lt;/title&gt;'\npattern  = re.compile(regex)\n\nwith urllib.request.urlopen(url) as response:\n   html = response.read()\n\ntitle = re.findall(pattern, html)\nprint(title)\n</code></pre>\n<p>And I get this unexpected error:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"path\\to\\file\\Crawler.py\", line 11, in &lt;module&gt;\n    title = re.findall(pattern, html)\n  File \"C:\\Python33\\lib\\re.py\", line 201, in findall\n    return _compile(pattern, flags).findall(string)\nTypeError: can't use a string pattern on a bytes-like object\n</code></pre>\n<p>What am I doing wrong?</p>\n", "abstract": "I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage: And I get this unexpected error: What am I doing wrong?"}, "answers": [{"id": 31019855, "score": 237, "vote": 0, "content": "<p>You want to convert html (a byte-like object) into a string using <code>.decode</code>, e.g.  <code>html = response.read().decode('utf-8')</code>. </p>\n<p>See <a href=\"https://stackoverflow.com/questions/606191/convert-bytes-to-a-python-string\">Convert bytes to a Python String</a></p>\n", "abstract": "You want to convert html (a byte-like object) into a string using .decode, e.g.  html = response.read().decode('utf-8').  See Convert bytes to a Python String"}, {"id": 50457549, "score": 53, "vote": 0, "content": "<p>The problem is that your regex is a string, but <code>html</code> is <a href=\"https://docs.python.org/3/library/functions.html#func-bytes\" rel=\"noreferrer\">bytes</a>:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; type(html)\n&lt;class 'bytes'&gt;\n</code></pre>\n<p>Since python doesn't know how those bytes are encoded, it throws an exception when you try to use a string regex on them.</p>\n<p>You can either <a href=\"https://docs.python.org/3/library/stdtypes.html#bytes.decode\" rel=\"noreferrer\"><code>decode</code></a> the bytes to a string:</p>\n<pre><code class=\"python\">html = html.decode('ISO-8859-1')  # encoding may vary!\ntitle = re.findall(pattern, html)  # no more error\n</code></pre>\n<p>Or use a bytes regex:</p>\n<pre><code class=\"python\">regex = rb'&lt;title&gt;(,+?)&lt;/title&gt;'\n#        ^\n</code></pre>\n<hr/>\n<p>In this particular context, you can get the encoding from the response headers:</p>\n<pre><code class=\"python\">with urllib.request.urlopen(url) as response:\n    encoding = response.info().get_param('charset', 'utf8')\n    html = response.read().decode(encoding)\n</code></pre>\n<p>See the <a href=\"https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen\" rel=\"noreferrer\"><code>urlopen</code> documentation</a> for more details.</p>\n", "abstract": "The problem is that your regex is a string, but html is bytes: Since python doesn't know how those bytes are encoded, it throws an exception when you try to use a string regex on them. You can either decode the bytes to a string: Or use a bytes regex: In this particular context, you can get the encoding from the response headers: See the urlopen documentation for more details."}, {"id": 73539945, "score": 0, "vote": 0, "content": "<p>Based upon last one, this was smimple to do when pdf read was done .</p>\n<pre><code class=\"python\">text = text.decode('ISO-8859-1') \n</code></pre>\n<p>Thanks @Aran-fey</p>\n", "abstract": "Based upon last one, this was smimple to do when pdf read was done . Thanks @Aran-fey"}, {"id": 73760071, "score": 0, "vote": 0, "content": "<p>Use <strong>str</strong> function in your code to convert html like this :</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">...\ntitle = re.findall(pattern, str(html))\n...\n</code></pre>\n", "abstract": "Use str function in your code to convert html like this :"}]}, {"link": "https://stackoverflow.com/questions/19687421/difference-between-beautifulsoup-and-scrapy-crawler", "question": {"id": "19687421", "title": "Difference between BeautifulSoup and Scrapy crawler?", "content": "<p>I want to make a website that shows the comparison between amazon and e-bay product price.\nWhich of these will work better and why? I am somewhat familiar with <strong>BeautifulSoup</strong> but not so much with <strong>Scrapy crawler</strong>.</p>\n", "abstract": "I want to make a website that shows the comparison between amazon and e-bay product price.\nWhich of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler."}, "answers": [{"id": 19734213, "score": 267, "vote": 0, "content": "<p><strong>Scrapy</strong> is a Web-spider or <strong>web scraper framework</strong>, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or <strong>crawling</strong>.</p>\n<p>While</p>\n<p><strong>BeautifulSoup</strong> is a <strong>parsing library</strong> which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria.</p>\n<p>In simple words, with Beautiful Soup you can build something similar to Scrapy.\nBeautiful Soup is a <strong>library</strong> while Scrapy is a <strong>complete framework</strong>.</p>\n<p><a href=\"http://www.quora.com/Python-programming-language-1/How-is-BeautifulSoup-different-from-Scrapy\" rel=\"noreferrer\">Source</a></p>\n", "abstract": "Scrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling. While BeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria. In simple words, with Beautiful Soup you can build something similar to Scrapy.\nBeautiful Soup is a library while Scrapy is a complete framework. Source"}, {"id": 19687572, "score": 21, "vote": 0, "content": "<p>I think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page.\nAfter that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags.</p>\n<p>If you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop.</p>\n<p>Take a look at the scrapy documentation, it's very simple to use.</p>\n", "abstract": "I think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page.\nAfter that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags. If you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop. Take a look at the scrapy documentation, it's very simple to use."}, {"id": 57474826, "score": 13, "vote": 0, "content": "<p><strong><a href=\"https://scrapy.readthedocs.io/en/latest/\" rel=\"noreferrer\">Scrapy</a></strong>\nIt is a <strong>web scraping framework</strong> which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below.</p>\n<ul>\n<li><a href=\"https://scrapy.readthedocs.io/en/latest/topics/feed-exports.html\" rel=\"noreferrer\">Feed exports</a>: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. </li>\n<li>Asynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request).</li>\n<li><a href=\"https://scrapy.readthedocs.io/en/latest/topics/selectors.html\" rel=\"noreferrer\">Selectors</a>: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup.</li>\n<li><p>Setting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically.</p></li>\n<li><p><a href=\"https://scrapy.readthedocs.io/en/latest/topics/item-pipeline.html\" rel=\"noreferrer\">Item Pipelines</a>: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server.</p></li>\n<li><p>Cookies: scrapy automatically handles cookies for us.</p></li>\n</ul>\n<p>etc.</p>\n<blockquote>\n<p>TLDR: scrapy is a framework that provides everything that one might\n  need to build large scale crawls. It provides various features that\n  hide complexity of crawling the webs. one can simply start writing web\n  crawlers without worrying about the setup burden.</p>\n</blockquote>\n<p><strong><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" rel=\"noreferrer\">Beautiful soup</a></strong>\nBeautiful Soup is a Python package for <strong>parsing HTML and XML documents</strong>. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,<strong>You cannot use beautiful soup only to make crawlers</strong>. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like <a href=\"https://docs.python.org/2/library/multiprocessing.htmlhttps://\" rel=\"noreferrer\">multiprocessing</a>.</p>\n<p>To sum up.</p>\n<ul>\n<li><p>Scrapy is a rich framework that you can use to start writing crawlers\nwithout any hassale.</p></li>\n<li><p>Beautiful soup is a library that you can use to parse a webpage. It\ncannot be used alone to scrape web.</p></li>\n</ul>\n<p><strong>You should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.</strong></p>\n", "abstract": "Scrapy\nIt is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below. Setting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically. Item Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server. Cookies: scrapy automatically handles cookies for us. etc. TLDR: scrapy is a framework that provides everything that one might\n  need to build large scale crawls. It provides various features that\n  hide complexity of crawling the webs. one can simply start writing web\n  crawlers without worrying about the setup burden. Beautiful soup\nBeautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing. To sum up. Scrapy is a rich framework that you can use to start writing crawlers\nwithout any hassale. Beautiful soup is a library that you can use to parse a webpage. It\ncannot be used alone to scrape web. You should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components."}, {"id": 46601960, "score": 3, "vote": 0, "content": "<p>Both are using to parse data.</p>\n<p><strong>Scrapy</strong>:</p>\n<ul>\n<li>Scrapy is a fast high-level web crawling and web scraping framework,\nused to crawl websites and extract structured data from their pages.</li>\n<li>But it has some limitations when data comes from java script or\nloading dynamicaly, we can over come it by using packages like splash, \nselenium etc.</li>\n</ul>\n<p><strong>BeautifulSoup</strong>:</p>\n<ul>\n<li><p>Beautiful Soup is a Python library for pulling data out of HTML and\nXML files.</p></li>\n<li><p>we can use this package for getting data from java script or \ndynamically loading pages.</p></li>\n</ul>\n<p>Scrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents </p>\n", "abstract": "Both are using to parse data. Scrapy: BeautifulSoup: Beautiful Soup is a Python library for pulling data out of HTML and\nXML files. we can use this package for getting data from java script or \ndynamically loading pages. Scrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents "}, {"id": 24040613, "score": 2, "vote": 0, "content": "<p>The way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup.</p>\n<p>The APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.</p>\n", "abstract": "The way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup. The APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc."}, {"id": 58106477, "score": 2, "vote": 0, "content": "<p><strong>BeautifulSoup</strong> is a library that lets you extract information from a web page.</p>\n<p><strong>Scrapy</strong> on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data.</p>\n<p>You can check this blog to get started with Scrapy\n<a href=\"https://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/\" rel=\"nofollow noreferrer\">https://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/</a></p>\n", "abstract": "BeautifulSoup is a library that lets you extract information from a web page. Scrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data. You can check this blog to get started with Scrapy\nhttps://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/"}, {"id": 49187707, "score": 1, "vote": 0, "content": "<p>Using <strong>scrapy</strong> you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then <strong>BeautifulSoup</strong> can be used in the place of scrapy method.\nBig project takes both advantages.</p>\n", "abstract": "Using scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method.\nBig project takes both advantages."}, {"id": 66479925, "score": 1, "vote": 0, "content": "<p>Beautifulsoup is web scraping small library. it does your job but sometime it does not satisfy your needs.i mean if you scrape  websites in large amount of data  so here in this case beautifulsoup fails.</p>\n<p>In this case  you should use Scrapy which is  a complete  scraping framework  which will do you job.\nAlso scrapy has support for databases(all kind of databases) so it is a huge\nof scrapy over other web  scraping libraries.</p>\n", "abstract": "Beautifulsoup is web scraping small library. it does your job but sometime it does not satisfy your needs.i mean if you scrape  websites in large amount of data  so here in this case beautifulsoup fails. In this case  you should use Scrapy which is  a complete  scraping framework  which will do you job.\nAlso scrapy has support for databases(all kind of databases) so it is a huge\nof scrapy over other web  scraping libraries."}, {"id": 54838886, "score": 0, "vote": 0, "content": "<p>The differences are many and selection of any tool/technology depends on individual needs.</p>\n<p>Few major differences are:</p>\n<ol>\n<li>BeautifulSoup is comparatively is <strong>easy to learn</strong> than Scrapy. </li>\n<li>The extensions, support, community is larger for Scrapy than for BeautifulSoup.</li>\n<li>Scrapy should be considered as a <strong>Spider</strong> while BeautifulSoup is a <strong>Parser</strong>.</li>\n</ol>\n", "abstract": "The differences are many and selection of any tool/technology depends on individual needs. Few major differences are:"}, {"id": 73927892, "score": 0, "vote": 0, "content": "<p>Long story short:</p>\n<blockquote>\n<p>Scrapy is a multitool. BS4 is a penknife.</p>\n</blockquote>\n<p>Now a list of peculiarities for each one from personal experience:</p>\n<p><strong><a href=\"https://scrapy.org/\" rel=\"nofollow noreferrer\">Scrapy:</a></strong></p>\n<ol>\n<li>heavy</li>\n<li>issues with installing dependencies might occur</li>\n<li>takes time to master</li>\n<li>well supported and documented, always up to date with a large and active community</li>\n<li>fast to extract</li>\n<li>good for large jobs</li>\n<li>has a native <a href=\"https://www.zyte.com/scrapy-cloud/\" rel=\"nofollow noreferrer\">cloud</a> (can deploy code into the cloud and forget about it until done)</li>\n<li>has a native <a href=\"https://docs.zyte.com/scrapy-cloud.html\" rel=\"nofollow noreferrer\">API</a></li>\n<li>variety of settings and add-ons (middlewares), allows you to fine-tune your code to the slightest details.</li>\n<li>highly  structured code</li>\n<li>easily integrates with <a href=\"https://brightdata.com/proxy-types/residential-proxies\" rel=\"nofollow noreferrer\">residential proxies</a>, and offers <a href=\"https://github.com/TeamHG-Memex/scrapy-rotating-proxies\" rel=\"nofollow noreferrer\">middleware</a> for IP rotation.</li>\n<li>handy informative(cloud) interface, handy for debugging<img alt=\"interface\" src=\"https://i.imgur.com/NNCpAXD.png\"/></li>\n</ol>\n<p><strong><a href=\"https://pypi.org/project/beautifulsoup4/\" rel=\"nofollow noreferrer\">bs4:</a></strong></p>\n<ol>\n<li>lightweight</li>\n<li>fast to install</li>\n<li>fast to learn</li>\n<li>fast and dirty to code</li>\n<li>is suitable for simple tasks</li>\n<li>is suitable for testing sites and hypothesis</li>\n<li>can use curl from chrome dev tools and convert curl to requests and use the result directly in your code for cookie-dependent sites or complex post requests.</li>\n</ol>\n<p><strong>Summary:</strong>\nUse <strong>bs4</strong> If you are just starting or using scraping once in a while for small projects.</p>\n<p>Use <strong>Scrapy</strong> if you are a professional web scraper that has to deal with large-scale data collection, and have to run the scraper for a long time.</p>\n", "abstract": "Long story short: Scrapy is a multitool. BS4 is a penknife. Now a list of peculiarities for each one from personal experience: Scrapy: bs4: Summary:\nUse bs4 If you are just starting or using scraping once in a while for small projects. Use Scrapy if you are a professional web scraper that has to deal with large-scale data collection, and have to run the scraper for a long time."}]}, {"link": "https://stackoverflow.com/questions/15611605/how-to-pass-a-user-defined-argument-in-scrapy-spider", "question": {"id": "15611605", "title": "How to pass a user defined argument in scrapy spider", "content": "<p>I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?</p>\n<p>I read about a parameter <code>-a</code> somewhere but have no idea how to use it.</p>\n", "abstract": "I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that? I read about a parameter -a somewhere but have no idea how to use it."}, "answers": [{"id": 15618520, "score": 231, "vote": 0, "content": "<p>Spider arguments are passed in the <code>crawl</code> command using the <code>-a</code> option. For example:</p>\n<pre><code class=\"python\">scrapy crawl myspider -a category=electronics -a domain=system\n</code></pre>\n<p>Spiders can access arguments as attributes:</p>\n<pre><code class=\"python\">class MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    def __init__(self, category='', **kwargs):\n        self.start_urls = [f'http://www.example.com/{category}']  # py36\n        super().__init__(**kwargs)  # python3\n\n    def parse(self, response)\n        self.log(self.domain)  # system\n</code></pre>\n<p>Taken from the Scrapy doc: <a href=\"http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\" rel=\"noreferrer\">http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments</a></p>\n<p><strong>Update 2013</strong>: Add second argument</p>\n<p><strong>Update 2015</strong>: Adjust wording</p>\n<p><strong>Update 2016</strong>: Use newer base class and add super, thanks @Birla</p>\n<p><strong>Update 2017</strong>: Use Python3 super</p>\n<pre><code class=\"python\"># previously\nsuper(MySpider, self).__init__(**kwargs)  # python2\n</code></pre>\n<p><strong>Update 2018</strong>: <a href=\"https://stackoverflow.com/a/41123138/858913\">As @eLRuLL points out</a>, spiders can access arguments as attributes</p>\n", "abstract": "Spider arguments are passed in the crawl command using the -a option. For example: Spiders can access arguments as attributes: Taken from the Scrapy doc: http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments Update 2013: Add second argument Update 2015: Adjust wording Update 2016: Use newer base class and add super, thanks @Birla Update 2017: Use Python3 super Update 2018: As @eLRuLL points out, spiders can access arguments as attributes"}, {"id": 41123138, "score": 52, "vote": 0, "content": "<p>Previous answers were correct, but you don't have to declare the constructor (<code>__init__</code>) every time you want to code a scrapy's spider, you could just specify the parameters as before:</p>\n<pre><code class=\"python\">scrapy crawl myspider -a parameter1=value1 -a parameter2=value2\n</code></pre>\n<p>and in your spider code you can just use them as spider arguments:</p>\n<pre><code class=\"python\">class MySpider(Spider):\n    name = 'myspider'\n    ...\n    def parse(self, response):\n        ...\n        if self.parameter1 == value1:\n            # this is True\n\n        # or also\n        if getattr(self, parameter2) == value2:\n            # this is also True\n</code></pre>\n<p>And it just works.</p>\n", "abstract": "Previous answers were correct, but you don't have to declare the constructor (__init__) every time you want to code a scrapy's spider, you could just specify the parameters as before: and in your spider code you can just use them as spider arguments: And it just works."}, {"id": 32456731, "score": 21, "vote": 0, "content": "<p>To pass arguments with crawl command</p>\n<blockquote>\n<p>scrapy crawl myspider -a category='mycategory' -a domain='example.com'</p>\n</blockquote>\n<p>To pass arguments to run on scrapyd replace <strong>-a</strong> with <strong>-d</strong></p>\n<blockquote>\n<p>curl <a href=\"http://your.ip.address.here:port/schedule.json\" rel=\"noreferrer\">http://your.ip.address.here:port/schedule.json</a> -d \n   spider=myspider -d category='mycategory' -d domain='example.com'</p>\n</blockquote>\n<p>The spider will receive arguments in its constructor.</p>\n<pre><code class=\"python\">\nclass MySpider(Spider):\n    name=\"myspider\"\n    def __init__(self,category='',domain='', *args,**kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.category = category\n        self.domain = domain</code></pre>\n<p>Scrapy puts all the arguments as spider attributes and you can skip the <strong>init</strong> method completely. Beware use <strong>getattr</strong> method for getting those attributes so your code does not break.</p>\n<pre><code class=\"python\">\nclass MySpider(Spider):\n    name=\"myspider\"\n    start_urls = ('https://httpbin.org/ip',)\n\n    def parse(self,response):\n        print getattr(self,'category','')\n        print getattr(self,'domain','')\n\n</code></pre>\n", "abstract": "To pass arguments with crawl command scrapy crawl myspider -a category='mycategory' -a domain='example.com' To pass arguments to run on scrapyd replace -a with -d curl http://your.ip.address.here:port/schedule.json -d \n   spider=myspider -d category='mycategory' -d domain='example.com' The spider will receive arguments in its constructor. Scrapy puts all the arguments as spider attributes and you can skip the init method completely. Beware use getattr method for getting those attributes so your code does not break."}, {"id": 32541281, "score": 11, "vote": 0, "content": "<p>Spider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this-</p>\n<blockquote>\n<p>scrapy crawl myspider -a domain=\"http://www.example.com\"</p>\n</blockquote>\n<p>And receive arguments in spider's constructors:</p>\n<pre><code class=\"python\">class MySpider(BaseSpider):\n    name = 'myspider'\n    def __init__(self, domain='', *args, **kwargs):\n        super(MySpider, self).__init__(*args, **kwargs)\n        self.start_urls = [domain]\n        #\n</code></pre>\n<p>...</p>\n<p>it will work :)</p>\n", "abstract": "Spider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this- scrapy crawl myspider -a domain=\"http://www.example.com\" And receive arguments in spider's constructors: ... it will work :)"}, {"id": 59950403, "score": 1, "vote": 0, "content": "<p>Alternatively we can use <a href=\"https://scrapyd.readthedocs.io/en/stable/\" rel=\"nofollow noreferrer\">ScrapyD</a> which expose an API where we can pass the start_url and spider name. ScrapyD has api's to stop/start/status/list the spiders.</p>\n<pre><code class=\"python\">pip install scrapyd scrapyd-deploy\nscrapyd\nscrapyd-deploy local -p default\n</code></pre>\n<p><code>scrapyd-deploy</code> will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use.</p>\n<pre><code class=\"python\">class MySpider(CrawlSpider):\n\n    def __init__(self, start_urls, *args, **kwargs):\n        self.start_urls = start_urls.split('|')\n        super().__init__(*args, **kwargs)\n    name = testspider\n</code></pre>\n<p><code>curl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls=\"https://www.anyurl...|https://www.anyurl2\"</code></p>\n<p>Added advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule API</p>\n<p>Refer <a href=\"https://scrapyd.readthedocs.io/en/stable/api.html#schedule-json\" rel=\"nofollow noreferrer\">scrapyd API documentation</a> for more details</p>\n", "abstract": "Alternatively we can use ScrapyD which expose an API where we can pass the start_url and spider name. ScrapyD has api's to stop/start/status/list the spiders. scrapyd-deploy will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use. curl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls=\"https://www.anyurl...|https://www.anyurl2\" Added advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule API Refer scrapyd API documentation for more details"}]}, {"link": "https://stackoverflow.com/questions/8372703/how-can-i-use-different-pipelines-for-different-spiders-in-a-single-scrapy-proje", "question": {"id": "8372703", "title": "How can I use different pipelines for different spiders in a single Scrapy project", "content": "<p>I have a scrapy project which contains multiple spiders.\nIs there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider.</p>\n<p>Thanks</p>\n", "abstract": "I have a scrapy project which contains multiple spiders.\nIs there any way I can define which pipelines to use for which spider? Not all the pipelines i have defined are applicable for every spider. Thanks"}, "answers": [{"id": 34647090, "score": 168, "vote": 0, "content": "<p>Just remove all pipelines from main settings and use this inside spider.</p>\n<p>This will define the pipeline to user per spider</p>\n<pre><code class=\"python\">class testSpider(InitSpider):\n    name = 'test'\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'app.MyPipeline': 400\n        }\n    }\n</code></pre>\n", "abstract": "Just remove all pipelines from main settings and use this inside spider. This will define the pipeline to user per spider"}, {"id": 14165844, "score": 39, "vote": 0, "content": "<p>Building on <a href=\"http://groups.google.com/group/scrapy-users/browse_thread/thread/9ac290ed469887f1\" rel=\"noreferrer\">the solution from Pablo Hoffman</a>, you can use the following decorator on the <code>process_item</code> method of a Pipeline object so that it checks the <code>pipeline</code> attribute of your spider for whether or not it should be executed. For example:</p>\n<pre><code class=\"python\">def check_spider_pipeline(process_item_method):\n\n    @functools.wraps(process_item_method)\n    def wrapper(self, item, spider):\n\n        # message template for debugging\n        msg = '%%s %s pipeline step' % (self.__class__.__name__,)\n\n        # if class is in the spider's pipeline, then use the\n        # process_item method normally.\n        if self.__class__ in spider.pipeline:\n            spider.log(msg % 'executing', level=log.DEBUG)\n            return process_item_method(self, item, spider)\n\n        # otherwise, just return the untouched item (skip this step in\n        # the pipeline)\n        else:\n            spider.log(msg % 'skipping', level=log.DEBUG)\n            return item\n\n    return wrapper\n</code></pre>\n<p>For this decorator to work correctly, the spider must have a pipeline attribute with a container of the Pipeline objects that you want to use to process the item, for example:</p>\n<pre><code class=\"python\">class MySpider(BaseSpider):\n\n    pipeline = set([\n        pipelines.Save,\n        pipelines.Validate,\n    ])\n\n    def parse(self, response):\n        # insert scrapy goodness here\n        return item\n</code></pre>\n<p>And then in a <code>pipelines.py</code> file:</p>\n<pre><code class=\"python\">class Save(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do saving here\n        return item\n\nclass Validate(object):\n\n    @check_spider_pipeline\n    def process_item(self, item, spider):\n        # do validating here\n        return item\n</code></pre>\n<p>All Pipeline objects should still be defined in ITEM_PIPELINES in settings (in the correct order -- would be nice to change so that the order could be specified on the Spider, too).</p>\n", "abstract": "Building on the solution from Pablo Hoffman, you can use the following decorator on the process_item method of a Pipeline object so that it checks the pipeline attribute of your spider for whether or not it should be executed. For example: For this decorator to work correctly, the spider must have a pipeline attribute with a container of the Pipeline objects that you want to use to process the item, for example: And then in a pipelines.py file: All Pipeline objects should still be defined in ITEM_PIPELINES in settings (in the correct order -- would be nice to change so that the order could be specified on the Spider, too)."}, {"id": 33445943, "score": 17, "vote": 0, "content": "<p>The other solutions given here are good, but I think they could be slow, because we are not really <strong>not</strong> using the pipeline per spider, instead we are checking if a pipeline exists every time an item is returned (and in some cases this could reach millions).</p>\n<p>A good way to completely disable (or enable) a feature per spider is using <code>custom_setting</code> and <code>from_crawler</code> for all extensions like this:</p>\n<p><strong>pipelines.py</strong></p>\n<pre><code class=\"python\">from scrapy.exceptions import NotConfigured\n\nclass SomePipeline(object):\n    def __init__(self):\n        pass\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('SOMEPIPELINE_ENABLED'):\n            # if this isn't specified in settings, the pipeline will be completely disabled\n            raise NotConfigured\n        return cls()\n\n    def process_item(self, item, spider):\n        # change my item\n        return item\n</code></pre>\n<p><strong>settings.py</strong></p>\n<pre><code class=\"python\">ITEM_PIPELINES = {\n   'myproject.pipelines.SomePipeline': 300,\n}\nSOMEPIPELINE_ENABLED = True # you could have the pipeline enabled by default\n</code></pre>\n<p><strong>spider1.py</strong></p>\n<pre><code class=\"python\">class Spider1(Spider):\n\n    name = 'spider1'\n\n    start_urls = [\"http://example.com\"]\n\n    custom_settings = {\n        'SOMEPIPELINE_ENABLED': False\n    }\n</code></pre>\n<p>As you check, we have specified <code>custom_settings</code> that will override the things specified in <code>settings.py</code>, and we are disabling <code>SOMEPIPELINE_ENABLED</code> for this spider.</p>\n<p>Now when you run this spider, check for something like:</p>\n<pre><code class=\"python\">[scrapy] INFO: Enabled item pipelines: []\n</code></pre>\n<p>Now scrapy has completely disabled the pipeline, not bothering of its existence for the whole run. Check that this also works for scrapy <code>extensions</code> and <code>middlewares</code>.</p>\n", "abstract": "The other solutions given here are good, but I think they could be slow, because we are not really not using the pipeline per spider, instead we are checking if a pipeline exists every time an item is returned (and in some cases this could reach millions). A good way to completely disable (or enable) a feature per spider is using custom_setting and from_crawler for all extensions like this: pipelines.py settings.py spider1.py As you check, we have specified custom_settings that will override the things specified in settings.py, and we are disabling SOMEPIPELINE_ENABLED for this spider. Now when you run this spider, check for something like: Now scrapy has completely disabled the pipeline, not bothering of its existence for the whole run. Check that this also works for scrapy extensions and middlewares."}, {"id": 27668863, "score": 15, "vote": 0, "content": "<p>You can use the <code>name</code> attribute of the spider in your pipeline</p>\n<pre><code class=\"python\">class CustomPipeline(object)\n\n    def process_item(self, item, spider)\n         if spider.name == 'spider1':\n             # do something\n             return item\n         return item\n</code></pre>\n<p>Defining all pipelines this way can accomplish what you want.</p>\n", "abstract": "You can use the name attribute of the spider in your pipeline Defining all pipelines this way can accomplish what you want."}, {"id": 8372894, "score": 12, "vote": 0, "content": "<p>I can think of at least four approaches:</p>\n<ol>\n<li>Use a different scrapy project per set of spiders+pipelines (might be appropriate if your spiders are different enough warrant being in different projects)</li>\n<li>On the scrapy tool command line, change the pipeline setting with <code>scrapy settings</code> in between each invocation of your spider</li>\n<li>Isolate your spiders into their own <a href=\"http://readthedocs.org/docs/scrapy/en/latest/topics/commands.html\">scrapy tool commands</a>, and define the <code>default_settings['ITEM_PIPELINES']</code> on your command class to the pipeline list you want for that command. See <a href=\"https://github.com/scrapy/scrapy/blob/master/scrapy/commands/settings.py\">line 6 of this example</a>.</li>\n<li>In the pipeline classes themselves, have <code>process_item()</code> check what spider it's running against, and do nothing if it should be ignored for that spider. See the <a href=\"http://readthedocs.org/docs/scrapy/en/latest/topics/item-pipeline.html#item-pipeline-example-with-resources-per-spider\">example using resources per spider</a> to get you started. (This seems like an ugly solution because it tightly couples spiders and item pipelines. You probably shouldn't use this one.)</li>\n</ol>\n", "abstract": "I can think of at least four approaches:"}, {"id": 63753043, "score": 10, "vote": 0, "content": "<p>The most simple and effective solution is to set custom settings in each spider itself.</p>\n<pre><code class=\"python\">custom_settings = {'ITEM_PIPELINES': {'project_name.pipelines.SecondPipeline': 300}}\n</code></pre>\n<p>After that you need to set them in the settings.py file</p>\n<pre><code class=\"python\">ITEM_PIPELINES = {\n   'project_name.pipelines.FistPipeline': 300,\n   'project_name.pipelines.SecondPipeline': 400\n}\n</code></pre>\n<p>in that way each spider will use the respective pipeline.</p>\n", "abstract": "The most simple and effective solution is to set custom settings in each spider itself. After that you need to set them in the settings.py file in that way each spider will use the respective pipeline."}, {"id": 54491496, "score": 6, "vote": 0, "content": "<p>You can just set the item pipelines settings inside of the spider like this:</p>\n<pre><code class=\"python\">class CustomSpider(Spider):\n    name = 'custom_spider'\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            '__main__.PagePipeline': 400,\n            '__main__.ProductPipeline': 300,\n        },\n        'CONCURRENT_REQUESTS_PER_DOMAIN': 2\n    }\n</code></pre>\n<p>I can then split up a pipeline (or even use multiple pipelines) by adding a value to the loader/returned item that identifies which part of the spider sent items over. This way I won\u2019t get any KeyError exceptions and I know which items should be available. </p>\n<pre><code class=\"python\">    ...\n    def scrape_stuff(self, response):\n        pageloader = PageLoader(\n                PageItem(), response=response)\n\n        pageloader.add_xpath('entire_page', '/html//text()')\n        pageloader.add_value('item_type', 'page')\n        yield pageloader.load_item()\n\n        productloader = ProductLoader(\n                ProductItem(), response=response)\n\n        productloader.add_xpath('product_name', '//span[contains(text(), \"Example\")]')\n        productloader.add_value('item_type', 'product')\n        yield productloader.load_item()\n\nclass PagePipeline:\n    def process_item(self, item, spider):\n        if item['item_type'] == 'product':\n            # do product stuff\n\n        if item['item_type'] == 'page':\n            # do page stuff\n</code></pre>\n", "abstract": "You can just set the item pipelines settings inside of the spider like this: I can then split up a pipeline (or even use multiple pipelines) by adding a value to the loader/returned item that identifies which part of the spider sent items over. This way I won\u2019t get any KeyError exceptions and I know which items should be available. "}, {"id": 38124696, "score": 1, "vote": 0, "content": "<p>I am using two pipelines, one for image download (MyImagesPipeline) and second for save data in mongodb (MongoPipeline).</p>\n<p>suppose we have many spiders(spider1,spider2,...........),in my example spider1 and spider5 can not use MyImagesPipeline</p>\n<p>settings.py</p>\n<pre><code class=\"python\">ITEM_PIPELINES = {'scrapycrawler.pipelines.MyImagesPipeline' : 1,'scrapycrawler.pipelines.MongoPipeline' : 2}\nIMAGES_STORE = '/var/www/scrapycrawler/dowload'\n</code></pre>\n<p>And bellow complete code of pipeline</p>\n<pre><code class=\"python\">import scrapy\nimport string\nimport pymongo\nfrom scrapy.pipelines.images import ImagesPipeline\n\nclass MyImagesPipeline(ImagesPipeline):\n    def process_item(self, item, spider):\n        if spider.name not in ['spider1', 'spider5']:\n            return super(ImagesPipeline, self).process_item(item, spider)\n        else:\n           return item \n\n    def file_path(self, request, response=None, info=None):\n        image_name = string.split(request.url, '/')[-1]\n        dir1 = image_name[0]\n        dir2 = image_name[1]\n        return dir1 + '/' + dir2 + '/' +image_name\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n    collection_url='snapdeal_urls'\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'scraping')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        #self.db[self.collection_name].insert(dict(item))\n        collection_name=item.get( 'collection_name', self.collection_name )\n        self.db[collection_name].insert(dict(item))\n        data = {}\n        data['base_id'] = item['base_id']\n        self.db[self.collection_url].update({\n            'base_id': item['base_id']\n        }, {\n            '$set': {\n            'image_download': 1\n            }\n        }, upsert=False, multi=True)\n        return item\n</code></pre>\n", "abstract": "I am using two pipelines, one for image download (MyImagesPipeline) and second for save data in mongodb (MongoPipeline). suppose we have many spiders(spider1,spider2,...........),in my example spider1 and spider5 can not use MyImagesPipeline settings.py And bellow complete code of pipeline"}, {"id": 52945919, "score": 1, "vote": 0, "content": "<p>we can use some conditions in pipeline as this</p>\n<pre><code class=\"python\">    # -*- coding: utf-8 -*-\nfrom scrapy_app.items import x\n\nclass SaveItemPipeline(object):\n    def process_item(self, item, spider):\n        if isinstance(item, x,):\n            item.save()\n        return item\n</code></pre>\n", "abstract": "we can use some conditions in pipeline as this"}, {"id": 56703989, "score": 0, "vote": 0, "content": "<p>Simple but still useful solution.</p>\n<p>Spider code</p>\n<pre><code class=\"python\">    def parse(self, response):\n        item = {}\n        ... do parse stuff\n        item['info'] = {'spider': 'Spider2'}\n</code></pre>\n<p>pipeline code</p>\n<pre><code class=\"python\">    def process_item(self, item, spider):\n        if item['info']['spider'] == 'Spider1':\n            logging.error('Spider1 pipeline works')\n        elif item['info']['spider'] == 'Spider2':\n            logging.error('Spider2 pipeline works')\n        elif item['info']['spider'] == 'Spider3':\n            logging.error('Spider3 pipeline works')\n</code></pre>\n<p>Hope this save some time for somebody!</p>\n", "abstract": "Simple but still useful solution. Spider code pipeline code Hope this save some time for somebody!"}]}, {"link": "https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script", "question": {"id": "13437402", "title": "How to run Scrapy from within a Python script", "content": "<p>I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:</p>\n<p><a href=\"http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/\">http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/</a></p>\n<p><a href=\"http://snipplr.com/view/67006/using-scrapy-from-a-script/\">http://snipplr.com/view/67006/using-scrapy-from-a-script/</a></p>\n<p>I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code:</p>\n<pre><code class=\"python\"># This snippet can be used to run scrapy spiders independent of scrapyd or the scrapy command line tool and use it from a script. \n# \n# The multiprocessing library is used in order to work around a bug in Twisted, in which you cannot restart an already running reactor or in this case a scrapy instance.\n# \n# [Here](http://groups.google.com/group/scrapy-users/browse_thread/thread/f332fc5b749d401a) is the mailing-list discussion for this snippet. \n\n#!/usr/bin/python\nimport os\nos.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'project.settings') #Must be at the top before other imports\n\nfrom scrapy import log, signals, project\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy.conf import settings\nfrom scrapy.crawler import CrawlerProcess\nfrom multiprocessing import Process, Queue\n\nclass CrawlerScript():\n\n    def __init__(self):\n        self.crawler = CrawlerProcess(settings)\n        if not hasattr(project, 'crawler'):\n            self.crawler.install()\n        self.crawler.configure()\n        self.items = []\n        dispatcher.connect(self._item_passed, signals.item_passed)\n\n    def _item_passed(self, item):\n        self.items.append(item)\n\n    def _crawl(self, queue, spider_name):\n        spider = self.crawler.spiders.create(spider_name)\n        if spider:\n            self.crawler.queue.append_spider(spider)\n        self.crawler.start()\n        self.crawler.stop()\n        queue.put(self.items)\n\n    def crawl(self, spider):\n        queue = Queue()\n        p = Process(target=self._crawl, args=(queue, spider,))\n        p.start()\n        p.join()\n        return queue.get(True)\n\n# Usage\nif __name__ == \"__main__\":\n    log.start()\n\n    \"\"\"\n    This example runs spider1 and then spider2 three times. \n    \"\"\"\n    items = list()\n    crawler = CrawlerScript()\n    items.append(crawler.crawl('spider1'))\n    for i in range(3):\n        items.append(crawler.crawl('spider2'))\n    print items\n\n# Snippet imported from snippets.scrapy.org (which no longer works)\n# author: joehillen\n# date  : Oct 24, 2010\n</code></pre>\n<p>Thank you.</p>\n", "abstract": "I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this: http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/ http://snipplr.com/view/67006/using-scrapy-from-a-script/ I can't figure out where I should put my spider code and how to call it from the main function. Please help. This is the example code: Thank you."}, "answers": [{"id": 31374345, "score": 96, "vote": 0, "content": "<p>All other answers reference Scrapy v0.x. According to <a href=\"http://doc.scrapy.org/en/1.0/topics/practices.html\">the updated docs</a>, Scrapy 1.0 demands:</p>\n<pre><code class=\"python\">import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    # Your spider definition\n    ...\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start() # the script will block here until the crawling is finished\n</code></pre>\n", "abstract": "All other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:"}, {"id": 56391201, "score": 19, "vote": 0, "content": "<p>Simply we can use</p>\n<pre><code class=\"python\">from scrapy.crawler import CrawlerProcess\nfrom project.spiders.test_spider import SpiderName\n\nprocess = CrawlerProcess()\nprocess.crawl(SpiderName, arg1=val1,arg2=val2)\nprocess.start()\n</code></pre>\n<p>Use these arguments inside spider <code>__init__</code> function with the global scope.</p>\n", "abstract": "Simply we can use Use these arguments inside spider __init__ function with the global scope."}, {"id": 14267403, "score": 16, "vote": 0, "content": "<p>Though I haven't tried it I think the answer can be found within the <a href=\"http://scrapy.readthedocs.org/en/0.16/topics/practices.html\">scrapy documentation</a>. To quote directly from it:</p>\n<pre><code class=\"python\">from twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy.settings import Settings\nfrom scrapy import log\nfrom testspiders.spiders.followall import FollowAllSpider\n\nspider = FollowAllSpider(domain='scrapinghub.com')\ncrawler = Crawler(Settings())\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here\n</code></pre>\n<p>From what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete.</p>\n", "abstract": "Though I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it: From what I gather this is a new development in the library which renders some of the earlier approaches online (such as that in the question) obsolete."}, {"id": 19060578, "score": 13, "vote": 0, "content": "<p>In scrapy 0.19.x you should do this:</p>\n<pre><code class=\"python\">from twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom testspiders.spiders.followall import FollowAllSpider\nfrom scrapy.utils.project import get_project_settings\n\nspider = FollowAllSpider(domain='scrapinghub.com')\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.signals.connect(reactor.stop, signal=signals.spider_closed)\ncrawler.configure()\ncrawler.crawl(spider)\ncrawler.start()\nlog.start()\nreactor.run() # the script will block here until the spider_closed signal was sent\n</code></pre>\n<p>Note these lines     </p>\n<pre><code class=\"python\">settings = get_project_settings()\ncrawler = Crawler(settings)\n</code></pre>\n<p>Without it your spider won't use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn't saving my items. I sent a pull request to fix the doc example.</p>\n<p>One more to do so is just call command directly from you script</p>\n<pre><code class=\"python\">from scrapy import cmdline\ncmdline.execute(\"scrapy crawl followall\".split())  #followall is the spider's name\n</code></pre>\n<p>Copied this answer from my first answer in here:\n<a href=\"https://stackoverflow.com/a/19060485/1402286\">https://stackoverflow.com/a/19060485/1402286</a></p>\n", "abstract": "In scrapy 0.19.x you should do this: Note these lines      Without it your spider won't use your settings and will not save the items.\nTook me a while to figure out why the example in documentation wasn't saving my items. I sent a pull request to fix the doc example. One more to do so is just call command directly from you script Copied this answer from my first answer in here:\nhttps://stackoverflow.com/a/19060485/1402286"}, {"id": 27242187, "score": 11, "vote": 0, "content": "<p>When there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. </p>\n<p>However, I found while doing my project that using </p>\n<pre><code class=\"python\">os.system(\"scrapy crawl yourspider\")\n</code></pre>\n<p>is the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders.</p>\n<p>If Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:</p>\n<pre><code class=\"python\">def _crawl(spider_name=None):\n    if spider_name:\n        os.system('scrapy crawl %s' % spider_name)\n    return None\n\ndef run_crawler():\n\n    spider_names = ['spider1', 'spider2', 'spider2']\n\n    pool = Pool(processes=len(spider_names))\n    pool.map(_crawl, spider_names)\n</code></pre>\n", "abstract": "When there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted.  However, I found while doing my project that using  is the easiest. This will save me from handling all sorts of signals especially when I have multiple spiders. If Performance is a concern, you can use multiprocessing to run your spiders in parallel, something like:"}, {"id": 64476716, "score": 1, "vote": 0, "content": "<p>it  is an improvement of\n<a href=\"https://stackoverflow.com/questions/53033791/scrapy-throws-an-error-when-run-using-crawlerprocess\">Scrapy throws an error when run using crawlerprocess</a></p>\n<p>and <a href=\"https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087\" rel=\"nofollow noreferrer\">https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087</a></p>\n<p>First create your usual spider for successful command line running. it is very very important that it should run and export data or image or file</p>\n<p>Once it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings.</p>\n<p>it will get necessary settings which \"from scrapy.utils.project import get_project_settings\" failed to do which is recommended by many</p>\n<p>both above and below portions should be there together. only one don't run.\nSpider will run in scrapy.cfg folder not any other folder</p>\n<p>tree  diagram may be displayed by the moderators for reference</p>\n<pre><code class=\"python\">#Tree\n[enter image description here][1]\n\n#spider.py\nimport sys\nsys.path.append(r'D:\\ivana\\flow') #folder where scrapy.cfg is located\n\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.settings import Settings\nfrom flow import settings as my_settings\n\n#----------------Typical Spider Program starts here-----------------------------\n\n          spider class definition here\n\n#----------------Typical Spider Program ends here-------------------------------\n\nif __name__ == \"__main__\":\n\n    crawler_settings = Settings()\n    crawler_settings.setmodule(my_settings)\n\n    process = CrawlerProcess(settings=crawler_settings)\n    process.crawl(FlowSpider) # it is for class FlowSpider(scrapy.Spider):\n    process.start(stop_after_crawl=True)\n</code></pre>\n", "abstract": "it  is an improvement of\nScrapy throws an error when run using crawlerprocess and https://github.com/scrapy/scrapy/issues/1904#issuecomment-205331087 First create your usual spider for successful command line running. it is very very important that it should run and export data or image or file Once it is over, do just like pasted in my program above spider class definition and below __name __ to invoke settings. it will get necessary settings which \"from scrapy.utils.project import get_project_settings\" failed to do which is recommended by many both above and below portions should be there together. only one don't run.\nSpider will run in scrapy.cfg folder not any other folder tree  diagram may be displayed by the moderators for reference"}, {"id": 38242163, "score": -3, "vote": 0, "content": "<pre><code class=\"python\"># -*- coding: utf-8 -*-\nimport sys\nfrom scrapy.cmdline import execute\n\n\ndef gen_argv(s):\n    sys.argv = s.split()\n\n\nif __name__ == '__main__':\n    gen_argv('scrapy crawl abc_spider')\n    execute()\n</code></pre>\n<p>Put this code to the path you can run <code>scrapy crawl abc_spider</code> from command line. (Tested with Scrapy==0.24.6)</p>\n", "abstract": "Put this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)"}, {"id": 47989623, "score": -6, "vote": 0, "content": "<p>If you want to run a simple crawling, It's easy by just running command: </p>\n<p>scrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv. </p>\n<p>scrapy crawl  -o result.csv or result.json or result.xml. </p>\n<p>you may want to try it</p>\n", "abstract": "If you want to run a simple crawling, It's easy by just running command:  scrapy crawl . \nThere is another options to export your results to store in some formats like: \nJson, xml, csv.  scrapy crawl  -o result.csv or result.json or result.xml.  you may want to try it"}]}, {"link": "https://stackoverflow.com/questions/9648015/pypi-download-counts-seem-unrealistic", "question": {"id": "9648015", "title": "PyPi download counts seem unrealistic", "content": "<p>I put <a href=\"http://pypi.python.org/pypi/powerlaw\" rel=\"noreferrer\">a package on PyPi</a> for the first time ~2 months ago, and have made some version updates since then. I noticed this week the download count recording, and was surprised to see it had been downloaded hundreds of times. Over the next few days, I was more surprised to see the download count increasing by sometimes hundreds <em>per day</em>, even though this is a niche statistical test toolbox. In particular, older versions of package are continuing to be downloaded, sometimes at higher rates than the newest version.</p>\n<p>What is going on here?</p>\n<p>Is there a bug in PyPi's downloaded counting, or is there an abundance of crawlers grabbing open source code (as mine is)?</p>\n", "abstract": "I put a package on PyPi for the first time ~2 months ago, and have made some version updates since then. I noticed this week the download count recording, and was surprised to see it had been downloaded hundreds of times. Over the next few days, I was more surprised to see the download count increasing by sometimes hundreds per day, even though this is a niche statistical test toolbox. In particular, older versions of package are continuing to be downloaded, sometimes at higher rates than the newest version. What is going on here? Is there a bug in PyPi's downloaded counting, or is there an abundance of crawlers grabbing open source code (as mine is)?"}, "answers": [{"id": 14726265, "score": 79, "vote": 0, "content": "<p>This is kind of an old question at this point, but I noticed the same thing about a package I have on PyPI and investigated further. It turns out PyPI keeps reasonably detailed <a href=\"http://pypi.python.org/stats/\">download statistics</a>, including (apparently slightly anonymised) user agents. From that, it was apparent that most people downloading my package were things like \"z3c.pypimirror/1.0.15.1\" and \"pep381client/1.5\". (PEP 381 describes a mirroring infrastructure for PyPI.)</p>\n<p>I wrote <a href=\"https://gist.github.com/Cairnarvon/4715057\">a quick script</a> to tally everything up, first including all of them and then leaving out the most obvious bots, and it turns out that <b>literally 99%</b> of the download activity for my package was caused by mirrorbots: 14,335 downloads total, compared to only 146 downloads with the bots filtered. And that's just leaving out the very obvious ones, so it's probably still an overestimate.</p>\n<p>It looks like the main reason PyPI needs mirrors is because it has them.</p>\n", "abstract": "This is kind of an old question at this point, but I noticed the same thing about a package I have on PyPI and investigated further. It turns out PyPI keeps reasonably detailed download statistics, including (apparently slightly anonymised) user agents. From that, it was apparent that most people downloading my package were things like \"z3c.pypimirror/1.0.15.1\" and \"pep381client/1.5\". (PEP 381 describes a mirroring infrastructure for PyPI.) I wrote a quick script to tally everything up, first including all of them and then leaving out the most obvious bots, and it turns out that literally 99% of the download activity for my package was caused by mirrorbots: 14,335 downloads total, compared to only 146 downloads with the bots filtered. And that's just leaving out the very obvious ones, so it's probably still an overestimate. It looks like the main reason PyPI needs mirrors is because it has them."}, {"id": 15584542, "score": 11, "vote": 0, "content": "<p>Starting with Cairnarvon's summarizing statement: </p>\n<blockquote>\n<p>\"It looks like the main reason PyPI needs mirrors is because it has them.\"</p>\n</blockquote>\n<p>I would slightly modify this: </p>\n<blockquote>\n<p>It might be more the <strong>way</strong> PyPI actually works and thus has to be mirrored, that might contribute an additional bit (or two :-) to the <em>real</em> traffic. </p>\n</blockquote>\n<p>At the moment I think you MUST interact with the main index to know what to update in your repository. State is not simply accesible through timestamps on some publicly accessible folder hierarchy. So, the bad thing is, rsync is out of the equation. The good thing is, you MAY talk to the index through JSON, OAuth, XML-RPC or HTTP interfaces.</p>\n<p>For XML-RPC:</p>\n<pre><code class=\"python\">$&gt; python\n&gt;&gt;&gt; import xmlrpclib\n&gt;&gt;&gt; import pprint\n&gt;&gt;&gt; client = xmlrpclib.ServerProxy('http://pypi.python.org/pypi')\n&gt;&gt;&gt; client.package_releases('PartitionSets')\n['0.1.1']\n</code></pre>\n<p>For JSON eg.:</p>\n<pre><code class=\"python\">$&gt; curl https://pypi.python.org/pypi/PartitionSets/0.1.1/json\n</code></pre>\n<p>If there are approx. 30.000 packages hosted [<a href=\"https://pypi.python.org/pypi\" rel=\"noreferrer\">1</a>] with some being downloaded 50.000 to 300.000 times a week [<a href=\"http://pypi-ranking.info/week\" rel=\"noreferrer\">2</a>] (like distribute, pip, requests, paramiko, lxml, boto, paramike, redis and others) you really need mirrors at least from an accessibilty perspective. Just imagine what a user does when <code>pip install NeedThisPackage</code> fails: <em>Wait</em>? Also company wide PyPI mirrors are quite common acting as proxies for otherwise unrouteable networks. Finally not to forget the wonderful multi version checking enabled through virtualenv and friends. These all are IMO legitimate and potentially wonderful uses of packages ...</p>\n<p>In the end, you never know what an agent <em>really</em> does with a downloaded package: Have N users really use it or just overwrite it next time ... and after all, IMHO package authors should care more for <strong>number and nature of uses</strong>, than the pure <em>number of potential users</em> ;-) </p>\n<hr/>\n<p>Refs: The guestimated numbers are from <a href=\"https://pypi.python.org/pypi\" rel=\"noreferrer\">https://pypi.python.org/pypi</a> (29303 packages) and <a href=\"http://pypi-ranking.info/week\" rel=\"noreferrer\">http://pypi-ranking.info/week</a> (for the weekly numbers, requested 2013-03-23). </p>\n", "abstract": "Starting with Cairnarvon's summarizing statement:  \"It looks like the main reason PyPI needs mirrors is because it has them.\" I would slightly modify this:  It might be more the way PyPI actually works and thus has to be mirrored, that might contribute an additional bit (or two :-) to the real traffic.  At the moment I think you MUST interact with the main index to know what to update in your repository. State is not simply accesible through timestamps on some publicly accessible folder hierarchy. So, the bad thing is, rsync is out of the equation. The good thing is, you MAY talk to the index through JSON, OAuth, XML-RPC or HTTP interfaces. For XML-RPC: For JSON eg.: If there are approx. 30.000 packages hosted [1] with some being downloaded 50.000 to 300.000 times a week [2] (like distribute, pip, requests, paramiko, lxml, boto, paramike, redis and others) you really need mirrors at least from an accessibilty perspective. Just imagine what a user does when pip install NeedThisPackage fails: Wait? Also company wide PyPI mirrors are quite common acting as proxies for otherwise unrouteable networks. Finally not to forget the wonderful multi version checking enabled through virtualenv and friends. These all are IMO legitimate and potentially wonderful uses of packages ... In the end, you never know what an agent really does with a downloaded package: Have N users really use it or just overwrite it next time ... and after all, IMHO package authors should care more for number and nature of uses, than the pure number of potential users ;-)  Refs: The guestimated numbers are from https://pypi.python.org/pypi (29303 packages) and http://pypi-ranking.info/week (for the weekly numbers, requested 2013-03-23). "}, {"id": 9649144, "score": 10, "vote": 0, "content": "<p>You also have to take into account that virtualenv is getting more popular. If your package is something like a core library that people use in many of their projects, they will usually download it multiple times.</p>\n<p>Consider a single user has 5 projects where he uses your package and each lives in its own virtualenv. Using pip to meet the requirements, your package is already downloaded 5 times this way. Then these projects might be set up on different machines, like work, home and laptop computers, in addition there might be a staging and a live server in case of a web application. Summing this up, you end up with many downloads by a single person.</p>\n<p>Just a thought... perhaps your package is simply good. ;)</p>\n", "abstract": "You also have to take into account that virtualenv is getting more popular. If your package is something like a core library that people use in many of their projects, they will usually download it multiple times. Consider a single user has 5 projects where he uses your package and each lives in its own virtualenv. Using pip to meet the requirements, your package is already downloaded 5 times this way. Then these projects might be set up on different machines, like work, home and laptop computers, in addition there might be a staging and a live server in case of a web application. Summing this up, you end up with many downloads by a single person. Just a thought... perhaps your package is simply good. ;)"}, {"id": 36624483, "score": 4, "vote": 0, "content": "<p>Hypothesis: CI tools like Travis CI and Appveyor also contribute quite a bit. It might mean that each commit/push leads to a build of a package and the installation of everything in requirements.txt</p>\n", "abstract": "Hypothesis: CI tools like Travis CI and Appveyor also contribute quite a bit. It might mean that each commit/push leads to a build of a package and the installation of everything in requirements.txt"}]}, {"link": "https://stackoverflow.com/questions/37274835/getting-forbidden-by-robots-txt-scrapy", "question": {"id": "37274835", "title": "getting Forbidden by robots.txt: scrapy", "content": "<p>while crawling website like <a href=\"https://www.netflix.com\">https://www.netflix.com</a>, getting Forbidden by robots.txt: https://www.netflix.com/&gt;</p>\n<p>ERROR: No response downloaded for: <a href=\"https://www.netflix.com/\">https://www.netflix.com/</a></p>\n", "abstract": "while crawling website like https://www.netflix.com, getting Forbidden by robots.txt: https://www.netflix.com/> ERROR: No response downloaded for: https://www.netflix.com/"}, "answers": [{"id": 37278895, "score": 180, "vote": 0, "content": "<p>In the new version (scrapy 1.1) launched 2016-05-11 the crawl first downloads robots.txt before crawling. To change this behavior change in your <code>settings.py</code> with <a href=\"http://doc.scrapy.org/en/1.1/topics/settings.html#robotstxt-obey\" rel=\"noreferrer\">ROBOTSTXT_OBEY</a></p>\n<pre><code class=\"python\">ROBOTSTXT_OBEY = False\n</code></pre>\n<p>Here are the <a href=\"https://doc.scrapy.org/en/1.1/news.html#id7\" rel=\"noreferrer\">release notes</a></p>\n", "abstract": "In the new version (scrapy 1.1) launched 2016-05-11 the crawl first downloads robots.txt before crawling. To change this behavior change in your settings.py with ROBOTSTXT_OBEY Here are the release notes"}, {"id": 61967829, "score": 3, "vote": 0, "content": "<p>Netflix's Terms of Use state:</p>\n<blockquote>\n<p>You also agree not to circumvent, remove, alter, deactivate, degrade or thwart any of the content protections in the Netflix service; use any robot, spider, scraper or other automated means to access the Netflix service;</p>\n</blockquote>\n<p>They have their robots.txt set up to block web scrapers. If you override the setting in <code>settings.py</code> to <code>ROBOTSTXT_OBEY=False</code> then you are violating their terms of use which can result in a law suit.</p>\n", "abstract": "Netflix's Terms of Use state: You also agree not to circumvent, remove, alter, deactivate, degrade or thwart any of the content protections in the Netflix service; use any robot, spider, scraper or other automated means to access the Netflix service; They have their robots.txt set up to block web scrapers. If you override the setting in settings.py to ROBOTSTXT_OBEY=False then you are violating their terms of use which can result in a law suit."}, {"id": 37277482, "score": 2, "vote": 0, "content": "<p>First thing you need to ensure is that you change your user agent in the request, otherwise default user agent will be blocked for sure.</p>\n", "abstract": "First thing you need to ensure is that you change your user agent in the request, otherwise default user agent will be blocked for sure."}]}, {"link": "https://stackoverflow.com/questions/419235/anyone-know-of-a-good-python-based-web-crawler-that-i-could-use", "question": {"id": "419235", "title": "Anyone know of a good Python based web crawler that I could use?", "content": "<p>I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of <a href=\"http://en.wikipedia.org/wiki/Web_crawler#Open-source_crawlers\" rel=\"nofollow noreferrer\">open source crawlers</a> but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project.</p>\n<p>Thanks in advance.  Also, this is my first SO question!</p>\n", "abstract": "I'm half-tempted to write my own, but I don't really have enough time right now.  I've seen the Wikipedia list of open source crawlers but I'd prefer something written in Python.  I realize that I could probably just use one of the tools on the Wikipedia page and wrap it in Python.  I might end up doing that - if anyone has any advice about any of those tools, I'm open to hearing about them.  I've used Heritrix via its web interface and I found it to be quite cumbersome.  I definitely won't be using a browser API for my upcoming project. Thanks in advance.  Also, this is my first SO question!"}, "answers": [{"id": 419259, "score": 56, "vote": 0, "content": "<ul>\n<li><a href=\"http://wwwsearch.sourceforge.net/mechanize/\" rel=\"nofollow noreferrer\">Mechanize</a> is my favorite; great high-level browsing capabilities (super-simple form filling and submission).</li>\n<li><a href=\"http://twill.idyll.org/\" rel=\"nofollow noreferrer\">Twill</a> is a simple scripting language built on top of Mechanize</li>\n<li><a href=\"http://www.crummy.com/software/BeautifulSoup/\" rel=\"nofollow noreferrer\">BeautifulSoup</a> + <a href=\"http://docs.python.org/library/urllib2.html\" rel=\"nofollow noreferrer\">urllib2</a> also works quite nicely.</li>\n<li><a href=\"http://scrapy.org/\" rel=\"nofollow noreferrer\">Scrapy</a> looks like an extremely promising project; it's new.</li>\n</ul>\n", "abstract": ""}, {"id": 421645, "score": 44, "vote": 0, "content": "<p>Use <a href=\"http://scrapy.org/\" rel=\"nofollow noreferrer\">Scrapy</a>.</p>\n<p>It is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies:</p>\n<ul>\n<li>Built-in support for parsing HTML, XML, CSV, and Javascript</li>\n<li>A media pipeline for scraping items with images (or any other media) and download the image files as well</li>\n<li>Support for extending Scrapy by plugging your own functionality using middlewares, extensions, and pipelines</li>\n<li>Wide range of built-in middlewares and extensions for handling of compression, cache, cookies, authentication, user-agent spoofing, robots.txt handling, statistics, crawl depth restriction, etc</li>\n<li>Interactive scraping shell console, very useful for developing and debugging</li>\n<li>Web management console for monitoring and controlling your bot</li>\n<li>Telnet console for low-level access to the Scrapy process</li>\n</ul>\n<p>Example code to extract information about all torrent files added today in the <a href=\"http://www.mininova.org/\" rel=\"nofollow noreferrer\">mininova</a> torrent site, by using a XPath selector on the HTML returned:</p>\n<pre><code class=\"python\">class Torrent(ScrapedItem):\n    pass\n\nclass MininovaSpider(CrawlSpider):\n    domain_name = 'mininova.org'\n    start_urls = ['http://www.mininova.org/today']\n    rules = [Rule(RegexLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    def parse_torrent(self, response):\n        x = HtmlXPathSelector(response)\n        torrent = Torrent()\n\n        torrent.url = response.url\n        torrent.name = x.x(\"//h1/text()\").extract()\n        torrent.description = x.x(\"//div[@id='description']\").extract()\n        torrent.size = x.x(\"//div[@id='info-left']/p[2]/text()[2]\").extract()\n        return [torrent]\n</code></pre>\n", "abstract": "Use Scrapy. It is a twisted-based web crawler framework. Still under heavy development but it works already. Has many goodies: Example code to extract information about all torrent files added today in the mininova torrent site, by using a XPath selector on the HTML returned:"}, {"id": 419255, "score": 6, "vote": 0, "content": "<p>Check the <a href=\"http://bulba.sdsu.edu/docwiki/HarvestMan\" rel=\"noreferrer\">HarvestMan</a>, a multi-threaded web-crawler written in Python, also give a look to the <a href=\"http://pypi.python.org/pypi/spider.py/0.5\" rel=\"noreferrer\">spider.py</a> module.</p>\n<p>And <a href=\"http://www.example-code.com/python/pythonspider.asp\" rel=\"noreferrer\">here</a> you can find code samples to build a simple web-crawler.</p>\n", "abstract": "Check the HarvestMan, a multi-threaded web-crawler written in Python, also give a look to the spider.py module. And here you can find code samples to build a simple web-crawler."}, {"id": 419253, "score": 3, "vote": 0, "content": "<p>I've used <a href=\"http://ruya.sourceforge.net/\" rel=\"nofollow noreferrer\">Ruya</a> and found it pretty good.</p>\n", "abstract": "I've used Ruya and found it pretty good."}, {"id": 4153223, "score": 3, "vote": 0, "content": "<p>I hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there.</p>\n<pre><code class=\"python\">#!/usr/bin/python\n\nimport httplib2\nimport urllib\nimport urllib2\nfrom cookielib import CookieJar\nimport sys\nimport re\nfrom HTMLParser import HTMLParser\n\nclass miniHTMLParser( HTMLParser ):\n\n  viewedQueue = []\n  instQueue = []\n  headers = {}\n  opener = \"\"\n\n  def get_next_link( self ):\n    if self.instQueue == []:\n      return ''\n    else:\n      return self.instQueue.pop(0)\n\n\n  def gethtmlfile( self, site, page ):\n    try:\n        url = 'http://'+site+''+page\n        response = self.opener.open(url)\n        return response.read()\n    except Exception, err:\n        print \" Error retrieving: \"+page\n        sys.stderr.write('ERROR: %s\\n' % str(err))\n    return \"\" \n\n    return resppage\n\n  def loginSite( self, site_url ):\n    try:\n    cj = CookieJar()\n    self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n\n    url = 'http://'+site_url \n        params = {'name': 'customer_admin', 'pass': 'customer_admin123', 'opt': 'Log in', 'form_build_id': 'form-3560fb42948a06b01d063de48aa216ab', 'form_id':'user_login_block'}\n    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n    self.headers = { 'User-Agent' : user_agent }\n\n    data = urllib.urlencode(params)\n    response = self.opener.open(url, data)\n    print \"Logged in\"\n    return response.read() \n\n    except Exception, err:\n    print \" Error logging in\"\n    sys.stderr.write('ERROR: %s\\n' % str(err))\n\n    return 1\n\n  def handle_starttag( self, tag, attrs ):\n    if tag == 'a':\n      newstr = str(attrs[0][1])\n      print newstr\n      if re.search('http', newstr) == None:\n        if re.search('mailto', newstr) == None:\n          if re.search('#', newstr) == None:\n            if (newstr in self.viewedQueue) == False:\n              print \"  adding\", newstr\n              self.instQueue.append( newstr )\n              self.viewedQueue.append( newstr )\n          else:\n            print \"  ignoring\", newstr\n        else:\n          print \"  ignoring\", newstr\n      else:\n        print \"  ignoring\", newstr\n\n\ndef main():\n\n  if len(sys.argv)!=3:\n    print \"usage is ./minispider.py site link\"\n    sys.exit(2)\n\n  mySpider = miniHTMLParser()\n\n  site = sys.argv[1]\n  link = sys.argv[2]\n\n  url_login_link = site+\"/node?destination=node\"\n  print \"\\nLogging in\", url_login_link\n  x = mySpider.loginSite( url_login_link )\n\n  while link != '':\n\n    print \"\\nChecking link \", link\n\n    # Get the file from the site and link\n    retfile = mySpider.gethtmlfile( site, link )\n\n    # Feed the file into the HTML parser\n    mySpider.feed(retfile)\n\n    # Search the retfile here\n\n    # Get the next link in level traversal order\n    link = mySpider.get_next_link()\n\n  mySpider.close()\n\n  print \"\\ndone\\n\"\n\nif __name__ == \"__main__\":\n  main()\n</code></pre>\n", "abstract": "I hacked the above script to include a login page as I needed it to access a drupal site. Not pretty but may help someone out there."}, {"id": 8310728, "score": 3, "vote": 0, "content": "<p>Trust me nothing is better than curl.. . the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2</p>\n<p><strong>CAUTION:</strong> <em>Don't hit the same domain at such a high speed.. .</em></p>\n<pre><code class=\"python\">#! /usr/bin/env python\n# -*- coding: iso-8859-1 -*-\n# vi:ts=4:et\n# $Id: retriever-multi.py,v 1.29 2005/07/28 11:04:13 mfx Exp $\n\n#\n# Usage: python retriever-multi.py &lt;file with URLs to fetch&gt; [&lt;# of\n#          concurrent connections&gt;]\n#\n\nimport sys\nimport pycurl\n\n# We should ignore SIGPIPE when using pycurl.NOSIGNAL - see\n# the libcurl tutorial for more info.\ntry:\n    import signal\n    from signal import SIGPIPE, SIG_IGN\n    signal.signal(signal.SIGPIPE, signal.SIG_IGN)\nexcept ImportError:\n    pass\n\n\n# Get args\nnum_conn = 10\ntry:\n    if sys.argv[1] == \"-\":\n        urls = sys.stdin.readlines()\n    else:\n        urls = open(sys.argv[1]).readlines()\n    if len(sys.argv) &gt;= 3:\n        num_conn = int(sys.argv[2])\nexcept:\n    print \"Usage: %s &lt;file with URLs to fetch&gt; [&lt;# of concurrent connections&gt;]\" % sys.argv[0]\n    raise SystemExit\n\n\n# Make a queue with (url, filename) tuples\nqueue = []\nfor url in urls:\n    url = url.strip()\n    if not url or url[0] == \"#\":\n        continue\n    filename = \"doc_%03d.dat\" % (len(queue) + 1)\n    queue.append((url, filename))\n\n\n# Check args\nassert queue, \"no URLs given\"\nnum_urls = len(queue)\nnum_conn = min(num_conn, num_urls)\nassert 1 &lt;= num_conn &lt;= 10000, \"invalid number of concurrent connections\"\nprint \"PycURL %s (compiled against 0x%x)\" % (pycurl.version, pycurl.COMPILE_LIBCURL_VERSION_NUM)\nprint \"----- Getting\", num_urls, \"URLs using\", num_conn, \"connections -----\"\n\n\n# Pre-allocate a list of curl objects\nm = pycurl.CurlMulti()\nm.handles = []\nfor i in range(num_conn):\n    c = pycurl.Curl()\n    c.fp = None\n    c.setopt(pycurl.FOLLOWLOCATION, 1)\n    c.setopt(pycurl.MAXREDIRS, 5)\n    c.setopt(pycurl.CONNECTTIMEOUT, 30)\n    c.setopt(pycurl.TIMEOUT, 300)\n    c.setopt(pycurl.NOSIGNAL, 1)\n    m.handles.append(c)\n\n\n# Main loop\nfreelist = m.handles[:]\nnum_processed = 0\nwhile num_processed &lt; num_urls:\n    # If there is an url to process and a free curl object, add to multi stack\n    while queue and freelist:\n        url, filename = queue.pop(0)\n        c = freelist.pop()\n        c.fp = open(filename, \"wb\")\n        c.setopt(pycurl.URL, url)\n        c.setopt(pycurl.WRITEDATA, c.fp)\n        m.add_handle(c)\n        # store some info\n        c.filename = filename\n        c.url = url\n    # Run the internal curl state machine for the multi stack\n    while 1:\n        ret, num_handles = m.perform()\n        if ret != pycurl.E_CALL_MULTI_PERFORM:\n            break\n    # Check for curl objects which have terminated, and add them to the freelist\n    while 1:\n        num_q, ok_list, err_list = m.info_read()\n        for c in ok_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Success:\", c.filename, c.url, c.getinfo(pycurl.EFFECTIVE_URL)\n            freelist.append(c)\n        for c, errno, errmsg in err_list:\n            c.fp.close()\n            c.fp = None\n            m.remove_handle(c)\n            print \"Failed: \", c.filename, c.url, errno, errmsg\n            freelist.append(c)\n        num_processed = num_processed + len(ok_list) + len(err_list)\n        if num_q == 0:\n            break\n    # Currently no more I/O is pending, could do something in the meantime\n    # (display a progress bar, etc.).\n    # We just call select() to sleep until some more data is available.\n    m.select(1.0)\n\n\n# Cleanup\nfor c in m.handles:\n    if c.fp is not None:\n        c.fp.close()\n        c.fp = None\n    c.close()\nm.close()\n</code></pre>\n", "abstract": "Trust me nothing is better than curl.. . the following code can crawl 10,000 urls in parallel in less than 300 secs on Amazon EC2 CAUTION: Don't hit the same domain at such a high speed.. ."}, {"id": 1906601, "score": 2, "vote": 0, "content": "<p>Another <a href=\"http://www.grenadepod.com/2009/12/13/python-web-crawler/\" rel=\"nofollow noreferrer\">simple spider</a> \nUses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it.</p>\n", "abstract": "Another simple spider \nUses BeautifulSoup and urllib2. Nothing too sophisticated, just reads all a href's builds a list and goes though it."}, {"id": 3427062, "score": 0, "vote": 0, "content": "<p><a href=\"http://bauerdata.bauerhost.dk/python-program-eksempler/pyspider\" rel=\"nofollow noreferrer\">pyspider.py</a></p>\n", "abstract": "pyspider.py"}]}, {"link": "https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy", "question": {"id": "6682503", "title": "Click a Button in Scrapy", "content": "<p>I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).</p>\n<p>I found out that Scrapy can handle forms (like logins) as shown <a href=\"http://doc.scrapy.org/topics/request-response.html?highlight=formrequest#scrapy.http.FormRequest\">here</a>. But the problem is that there is no form to fill out, so it's not exactly what I need.</p>\n<p>How can I simply click a button, which then shows the information I need?</p>\n<p>Do I have to use an external library like mechanize or lxml?</p>\n", "abstract": "I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking). I found out that Scrapy can handle forms (like logins) as shown here. But the problem is that there is no form to fill out, so it's not exactly what I need. How can I simply click a button, which then shows the information I need? Do I have to use an external library like mechanize or lxml?"}, "answers": [{"id": 6682557, "score": 77, "vote": 0, "content": "<p>Scrapy cannot interpret javascript.</p>\n<p>If you absolutely must interact with the javascript on the page, you want to be using Selenium.</p>\n<p>If using Scrapy, the solution to the problem depends on what the button is doing.</p>\n<p>If it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there.</p>\n<p>If it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL.</p>\n<blockquote>\n<p>Do I have to use an external library like mechanize or lxml?</p>\n</blockquote>\n<p>If you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go.</p>\n<p>If you can give the URL of the page you're working on scraping I can take a look.</p>\n", "abstract": "Scrapy cannot interpret javascript. If you absolutely must interact with the javascript on the page, you want to be using Selenium. If using Scrapy, the solution to the problem depends on what the button is doing. If it's just showing content that was previously hidden, you can scrape the data without a problem, it doesn't matter that it wouldn't appear in the browser, the HTML is still there. If it's fetching the content dynamically via AJAX when the button is pressed, the best thing to do is to view the HTTP request that goes out when you press the button using a tool like Firebug. You can then just request the data directly from that URL. Do I have to use an external library like mechanize or lxml? If you want to interpret javascript, yes you need to use a different library, although neither of those two fit the bill. Neither of them know anything about javascript. Selenium is the way to go. If you can give the URL of the page you're working on scraping I can take a look."}, {"id": 27065771, "score": 22, "vote": 0, "content": "<p><code>Selenium</code> browser provide very nice solution. Here is an example (<code>pip install -U selenium</code>):</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nclass northshoreSpider(Spider):\n    name = 'xxx'\n    allowed_domains = ['www.example.org']\n    start_urls = ['https://www.example.org']\n\n    def __init__(self):\n        self.driver = webdriver.Firefox()\n\n    def parse(self,response):\n            self.driver.get('https://www.example.org/abc')\n\n            while True:\n                try:\n                    next = self.driver.find_element_by_xpath('//*[@id=\"BTN_NEXT\"]')\n                    url = 'http://www.example.org/abcd'\n                    yield Request(url,callback=self.parse2)\n                    next.click()\n                except:\n                    break\n\n            self.driver.close()\n\n    def parse2(self,response):\n        print 'you are here!'\n</code></pre>\n", "abstract": "Selenium browser provide very nice solution. Here is an example (pip install -U selenium):"}, {"id": 6683191, "score": 0, "vote": 0, "content": "<p>To properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.</p>\n", "abstract": "To properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc."}, {"id": 70554784, "score": 0, "vote": 0, "content": "<p>Although it's an old thread I've found quite useful to use <a href=\"https://selenium-python-helium.readthedocs.io/en/latest/api.html\" rel=\"nofollow noreferrer\">Helium</a> (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from helium import *\n\nstart_firefox('your_url')\ns = S('path_to_your_button')\nclick(s)\n...\n\n</code></pre>\n", "abstract": "Although it's an old thread I've found quite useful to use Helium (built on top of Selenium) for this purpose and far more easier/simpler than using Selenium. It will be something like the following:"}]}, {"link": "https://stackoverflow.com/questions/8814802/python-errno-10054-an-existing-connection-was-forcibly-closed-by-the-remote-h", "question": {"id": "8814802", "title": "python: [Errno 10054] An existing connection was forcibly closed by the remote host", "content": "<p>I am writing python to crawl Twitter space using Twitter-py. I have set the crawler to sleep for a while (2 seconds) between each request to api.twitter.com. However, after some times of running (around 1), when the Twitter's rate limit not exceeded yet, I got this error.</p>\n<pre><code class=\"python\">[Errno 10054] An existing connection was forcibly closed by the remote host.\n</code></pre>\n<p>What are possible causes of this problem and how to solve this?</p>\n<p>I have searched through and found that the Twitter server itself may force to close the connection due to many requests.</p>\n<p>Thank you very much in advance.</p>\n", "abstract": "I am writing python to crawl Twitter space using Twitter-py. I have set the crawler to sleep for a while (2 seconds) between each request to api.twitter.com. However, after some times of running (around 1), when the Twitter's rate limit not exceeded yet, I got this error. What are possible causes of this problem and how to solve this? I have searched through and found that the Twitter server itself may force to close the connection due to many requests. Thank you very much in advance."}, "answers": [{"id": 8814832, "score": 24, "vote": 0, "content": "<p>This can be caused by the two sides of the connection disagreeing over whether the connection timed out or not during a keepalive. (Your code tries to reused the connection just as the server is closing it because it has been idle for too long.) You should basically just retry the operation over a new connection. (I'm surprised your library doesn't do this automatically.)</p>\n", "abstract": "This can be caused by the two sides of the connection disagreeing over whether the connection timed out or not during a keepalive. (Your code tries to reused the connection just as the server is closing it because it has been idle for too long.) You should basically just retry the operation over a new connection. (I'm surprised your library doesn't do this automatically.)"}, {"id": 63038475, "score": 14, "vote": 0, "content": "<p>I know this is a very old question but it may be that you need to set the request headers. This solved it for me.</p>\n<p>For example 'user-agent', 'accept' etc. here is an example with user-agent:</p>\n<pre><code class=\"python\">url = 'your-url-here'\nheaders = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\nr = requests.get(url, headers=headers)\n</code></pre>\n", "abstract": "I know this is a very old question but it may be that you need to set the request headers. This solved it for me. For example 'user-agent', 'accept' etc. here is an example with user-agent:"}, {"id": 8826322, "score": 13, "vote": 0, "content": "<p>there are many causes such as </p>\n<ul>\n<li>The network link between server and client may be temporarily going down.</li>\n<li>running out of system resources.</li>\n<li>sending malformed data.</li>\n</ul>\n<p>To examine the problem in detail, you can use Wireshark.</p>\n<p>or you can just re-request or re-connect again.</p>\n", "abstract": "there are many causes such as  To examine the problem in detail, you can use Wireshark. or you can just re-request or re-connect again."}, {"id": 46157429, "score": 2, "vote": 0, "content": "<p>For me this problem arised while trying to connect to the SAP Hana database. When I got this error, </p>\n<pre>OperationalError: Lost connection to HANA server (ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))</pre>\n<p>I tried to run the code for connection(mentioned below), which created that error, again and it worked. </p>\n<pre>\n\n    import pyhdb\n    connection = pyhdb.connect(host=\"example.com\",port=30015,user=\"user\",password=\"secret\")\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT 'Hello Python World' FROM DUMMY\")\n    cursor.fetchone()\n    connection.close()\n\n</pre>\n<p>It was because the server refused to connect. It might require you to wait for a while and try again. Try closing the Hana Studio by logging off and then logging in again. Keep running the code for a number of times.</p>\n", "abstract": "For me this problem arised while trying to connect to the SAP Hana database. When I got this error,  I tried to run the code for connection(mentioned below), which created that error, again and it worked.  It was because the server refused to connect. It might require you to wait for a while and try again. Try closing the Hana Studio by logging off and then logging in again. Keep running the code for a number of times."}, {"id": 70786206, "score": 2, "vote": 0, "content": "<p>I got the same error <em><strong>([WinError 10054] An existing connection was forcibly closed by the remote host)</strong></em> with <a href=\"https://websocket-client.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">websocket-client</a> after setting <strong><code>ping_interval = 2</code></strong> in <code>websocket.run_forever()</code>. (I had multiple threads connecting to the same host.)</p>\n<p>Setting <code>ping_interval = 10</code> and <code>ping_timeout = 9</code> solved the issue. May be you need to reduce the amount of requests and <strong>stop making host busy</strong> otherwise it will forcibly disconnect you.</p>\n", "abstract": "I got the same error ([WinError 10054] An existing connection was forcibly closed by the remote host) with websocket-client after setting ping_interval = 2 in websocket.run_forever(). (I had multiple threads connecting to the same host.) Setting ping_interval = 10 and ping_timeout = 9 solved the issue. May be you need to reduce the amount of requests and stop making host busy otherwise it will forcibly disconnect you."}]}, {"link": "https://stackoverflow.com/questions/6809402/python-maximum-recursion-depth-exceeded-while-calling-a-python-object", "question": {"id": "6809402", "title": "Python: maximum recursion depth exceeded while calling a Python object", "content": "<p>I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need.</p>\n<p>after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url.</p>\n<p>you can see the subtrahends quite fast (a little ex' of the few first \"good IDs\") -</p>\n<pre><code class=\"python\">510000011 # +8\n510000029 # +18\n510000037 # +8\n510000045 # +8\n510000052 # +7\n510000060 # +8\n510000078 # +18\n510000086 # +8\n510000094 # +8\n510000102 # +8\n510000110 # etc'\n510000128\n510000136\n510000144\n510000151\n510000169\n510000177\n510000185\n510000193\n510000201\n</code></pre>\n<p>after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\\18\\17\\8 (top returning subtrahends ) etc'.</p>\n<p>this is the function - </p>\n<pre><code class=\"python\">def checkNextID(ID):\n    global numOfRuns, curRes, lastResult\n    while ID &lt; lastResult:\n        try:\n            numOfRuns += 1\n            if numOfRuns % 10 == 0:\n                time.sleep(3) # sleep every 10 iterations\n            if isValid(ID + 8):\n                parseHTML(curRes)\n                checkNextID(ID + 8)\n                return 0\n            if isValid(ID + 18):\n                parseHTML(curRes)\n                checkNextID(ID + 18)\n                return 0\n            if isValid(ID + 7):\n                parseHTML(curRes)\n                checkNextID(ID + 7)\n                return 0\n            if isValid(ID + 17):\n                parseHTML(curRes)\n                checkNextID(ID + 17)\n                return 0\n            if isValid(ID+6):\n                parseHTML(curRes)\n                checkNextID(ID + 6)\n                return 0\n            if isValid(ID + 16):\n                parseHTML(curRes)\n                checkNextID(ID + 16)\n                return 0\n            else:\n                checkNextID(ID + 1)\n                return 0\n        except Exception, e:\n            print \"somethin went wrong: \" + str(e)\n</code></pre>\n<p>what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first \"if isValid\" clause (isValid(ID + 8) will return True).</p>\n<p><strong>lastResult</strong> is a variable which saves the last known url id, so we'll run until numOfRuns is</p>\n<p><strong>isValid()</strong> is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - '<strong>curRes</strong>', it returns False if the url doesn't contain the data I need.</p>\n<p><strong>parseHTML</strong> is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True.</p>\n<p>if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url.</p>\n<p>you can see the rest of the code <a href=\"http://pastebin.com/Vq9Zwpc9\">here</a></p>\n<p>after running the code I got about 950~ good results and suddenly an exception had raised -</p>\n<blockquote>\n<p>\"somethin went wrong: maximum recursion depth exceeded while calling a\n  Python object\"</p>\n</blockquote>\n<p>I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it.</p>\n<p>I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes.</p>\n<p>I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old <em>\"algorithm\"</em>, it will never end).</p>\n<p>Thanks!</p>\n", "abstract": "I've built a crawler that had to run on about 5M pages (by increasing the url ID) and then parses the pages which contain the info' I need. after using an algorithm which run on the urls (200K) and saved the good and bad results I found that the I'm wasting a lot of time. I could see that there are a a few returning subtrahends which I can use to check the next valid url. you can see the subtrahends quite fast (a little ex' of the few first \"good IDs\") - after crawling about 200K urls which gave me only 14K good results I knew I was wasting my time and need to optimize it, so I run some statistics and built a function that will check the urls while increasing the id with 8\\18\\17\\8 (top returning subtrahends ) etc'. this is the function -  what is basically does is -checkNextID(ID) is getting the first id I know that contain the data minus 8 so the first iteration will match the first \"if isValid\" clause (isValid(ID + 8) will return True). lastResult is a variable which saves the last known url id, so we'll run until numOfRuns is isValid() is a function that gets an ID + one of the subtrahends and returns True if the url contains what I need and saves a soup object of the url to a global varibale named - 'curRes', it returns False if the url doesn't contain the data I need. parseHTML is a function that gets the soup object (curRes), parses the data I need and then saves the data to a csv, then returns True. if isValid() returns True, we'll call parseHTML() and then try to check the next ID+the subtrahends (by calling checkNextID(ID + subtrahends), if none of them will return what I'm looking for I'll increase it with 1 and check again until I'll find the next valid url. you can see the rest of the code here after running the code I got about 950~ good results and suddenly an exception had raised - \"somethin went wrong: maximum recursion depth exceeded while calling a\n  Python object\" I could see on WireShark that the scipt stuck on id - 510009541 (I started my script with 510000003), the script tried getting the url with that ID a few times before I noticed the error and stopped it. I was really exciting to see that I got the same results but 25x-40x times faster then my old script, with fewer HTTP requests, it's very precise, I have missed only 1 result for 1000 good results, which is find by me, it's impossible to rum 5M times, I had my old script running for 30 hours and got 14-15K results when my new script gave me 960~ results in 5-10 minutes. I read about stack limitations, but there must be a solution for the algorithm I'm trying to implement in Python (I can't go back to my old \"algorithm\", it will never end). Thanks!"}, "answers": [{"id": 6809586, "score": 58, "vote": 0, "content": "<p>Python don't have a great support for recursion because of it's lack of TRE (<a href=\"http://neopythonic.blogspot.com/2009/04/tail-recursion-elimination.html\" rel=\"noreferrer\">Tail Recursion Elimination</a>).</p>\n<p>This means that each call to your recursive function will create a function call stack and because there is a limit of stack depth (by default is 1000) that you can check out by <a href=\"http://docs.python.org/library/sys.html#sys.getrecursionlimit\" rel=\"noreferrer\"><code>sys.getrecursionlimit</code></a> (of course you can change it using <a href=\"http://docs.python.org/library/sys.html#sys.setrecursionlimit\" rel=\"noreferrer\">sys.setrecursionlimit</a> but it's not recommended) your program will end up by crashing when it hits this limit.</p>\n<p>As other answer has already give you a much nicer way for how to solve this in your case (which is to replace recursion by simple loop) there is another solution if you still want to use recursion which is to use one of the many recipes of implementing TRE in python like this <a href=\"http://code.activestate.com/recipes/474088-tail-call-optimization-decorator/\" rel=\"noreferrer\">one</a>.</p>\n<p><strong>N.B:</strong> My answer is meant to give you more insight on why you get the error, and I'm not advising you to use the TRE as i already explained because in your case a loop will be much better and easy to read.</p>\n", "abstract": "Python don't have a great support for recursion because of it's lack of TRE (Tail Recursion Elimination). This means that each call to your recursive function will create a function call stack and because there is a limit of stack depth (by default is 1000) that you can check out by sys.getrecursionlimit (of course you can change it using sys.setrecursionlimit but it's not recommended) your program will end up by crashing when it hits this limit. As other answer has already give you a much nicer way for how to solve this in your case (which is to replace recursion by simple loop) there is another solution if you still want to use recursion which is to use one of the many recipes of implementing TRE in python like this one. N.B: My answer is meant to give you more insight on why you get the error, and I'm not advising you to use the TRE as i already explained because in your case a loop will be much better and easy to read."}, {"id": 15241424, "score": 51, "vote": 0, "content": "<p>You can increase the capacity of the stack by the following :</p>\n<pre><code class=\"python\">import sys\nsys.setrecursionlimit(10000)\n</code></pre>\n", "abstract": "You can increase the capacity of the stack by the following :"}, {"id": 6809450, "score": 19, "vote": 0, "content": "<p>this turns the recursion in to a loop:</p>\n<pre><code class=\"python\">def checkNextID(ID):\n    global numOfRuns, curRes, lastResult\n    while ID &lt; lastResult:\n        try:\n            numOfRuns += 1\n            if numOfRuns % 10 == 0:\n                time.sleep(3) # sleep every 10 iterations\n            if isValid(ID + 8):\n                parseHTML(curRes)\n                ID = ID + 8\n            elif isValid(ID + 18):\n                parseHTML(curRes)\n                ID = ID + 18\n            elif isValid(ID + 7):\n                parseHTML(curRes)\n                ID = ID + 7\n            elif isValid(ID + 17):\n                parseHTML(curRes)\n                ID = ID + 17\n            elif isValid(ID+6):\n                parseHTML(curRes)\n                ID = ID + 6\n            elif isValid(ID + 16):\n                parseHTML(curRes)\n                ID = ID + 16\n            else:\n                ID = ID + 1\n        except Exception, e:\n            print \"somethin went wrong: \" + str(e)\n</code></pre>\n", "abstract": "this turns the recursion in to a loop:"}, {"id": 67044574, "score": 6, "vote": 0, "content": "<p>You can increase the recursion depth and thread stack size.</p>\n<pre><code class=\"python\">import sys, threading\nsys.setrecursionlimit(10**7) # max depth of recursion\nthreading.stack_size(2**27)  # new thread will get stack of such size\n</code></pre>\n", "abstract": "You can increase the recursion depth and thread stack size."}, {"id": 6809456, "score": 2, "vote": 0, "content": "<p>Instead of doing recursion, the parts of the code with <code>checkNextID(ID + 18)</code> and similar could be replaced with <code>ID+=18</code>, and then if you remove all instances of <code>return 0</code>, then it should do the same thing but as a simple loop. You should then put a <code>return 0</code> at the end and make your variables non-global.</p>\n", "abstract": "Instead of doing recursion, the parts of the code with checkNextID(ID + 18) and similar could be replaced with ID+=18, and then if you remove all instances of return 0, then it should do the same thing but as a simple loop. You should then put a return 0 at the end and make your variables non-global."}, {"id": 70957763, "score": -1, "vote": 0, "content": "<p>\nuse try and except but don't print your error in except just run your function again in except statement\n</p>\n", "abstract": "\nuse try and except but don't print your error in except just run your function again in except statement\n"}]}, {"link": "https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver", "question": {"id": "28070315", "title": "Python: Disable images in Selenium Google ChromeDriver", "content": "<p>I spend a lot of time searching about this.\nAt the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.</p>\n<p>1- The answer in <a href=\"https://stackoverflow.com/questions/18657976/disable-images-in-selenium-google-chromedriver\">Disable images in Selenium Google ChromeDriver</a> works in Java. So we should do the same thing in Python:</p>\n<pre><code class=\"python\">opt = webdriver.ChromeOptions()\nopt.add_extension(\"Block-image_v1.1.crx\")\nbrowser = webdriver.Chrome(chrome_options=opt)\n</code></pre>\n<p>2- But downloading \"Block-image_v1.1.crx\" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: <a href=\"https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj\" rel=\"noreferrer\">https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj</a></p>\n<p>you can go to <a href=\"http://chrome-extension-downloader.com/\" rel=\"noreferrer\">http://chrome-extension-downloader.com/</a>\nand paste the extension url there to be able to download the extension file.</p>\n<p>3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded.</p>\n", "abstract": "I spend a lot of time searching about this.\nAt the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this. 1- The answer in Disable images in Selenium Google ChromeDriver works in Java. So we should do the same thing in Python: 2- But downloading \"Block-image_v1.1.crx\" is a little bit tricky, because there is no direct way to do that. For this purpose, instead of going to: https://chrome.google.com/webstore/detail/block-image/pehaalcefcjfccdpbckoablngfkfgfgj you can go to http://chrome-extension-downloader.com/\nand paste the extension url there to be able to download the extension file. 3- Then you will be able to use the above mentioned code with the path to the extension file that you have downloaded."}, "answers": [{"id": 31581387, "score": 119, "vote": 0, "content": "<p>Here is another way to disable images:</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nchrome_options = webdriver.ChromeOptions()\nprefs = {\"profile.managed_default_content_settings.images\": 2}\nchrome_options.add_experimental_option(\"prefs\", prefs)\ndriver = webdriver.Chrome(chrome_options=chrome_options)\n</code></pre>\n<p>I found it below:</p>\n<p><a href=\"http://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option\" rel=\"noreferrer\">http://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option</a></p>\n", "abstract": "Here is another way to disable images: I found it below: http://nullege.com/codes/show/src@o@s@osintstalker-HEAD@fbstalker1.py/56/selenium.webdriver.ChromeOptions.add_experimental_option"}, {"id": 53135560, "score": 3, "vote": 0, "content": "<p><strong>Java:</strong>\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.</p>\n<pre><code class=\"python\">    chromeOptions = new ChromeOptions();\n    HashMap&lt;String, Object&gt; images = new HashMap&lt;String, Object&gt;();\n    images.put(\"images\", 2);\n    HashMap&lt;String, Object&gt; prefs = new HashMap&lt;String, Object&gt;();\n    prefs.put(\"profile.default_content_setting_values\", images);\n    chromeOptions.setExperimentalOption(\"prefs\", prefs);\n    driver=new ChromeDriver(chromeOptions);\n\n    firefoxOpt = new FirefoxOptions();\n    FirefoxProfile profile = new FirefoxProfile();\n    profile.setPreference(\"permissions.default.image\", 2);\n    firefoxOpt.setProfile(profile);\n</code></pre>\n", "abstract": "Java:\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same."}, {"id": 48777373, "score": 2, "vote": 0, "content": "<p>There is another way that comes probably to mind to everyone to access <code>chrome://settings</code> and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, <strong>although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge.</strong>.  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  </p>\n<pre><code class=\"python\">driver = webdriver.Chrome()\n\n\ndef expand_shadow_element(element):\n  shadow_root = driver.execute_script('return arguments[0].shadowRoot', element)\n  return shadow_root\n\ndriver.get(\"chrome://settings\")\nroot1 = driver.find_element_by_tag_name('settings-ui')\nshadow_root1 = expand_shadow_element(root1)\n\nroot2 = shadow_root1.find_element_by_css_selector('[page-name=\"Settings\"]')\nshadow_root2 = expand_shadow_element(root2)\n\nroot3 = shadow_root2.find_element_by_id('search')\nshadow_root3 = expand_shadow_element(root3)\n\nsearch_button = shadow_root3.find_element_by_id(\"searchTerm\")\nsearch_button.click()\n\ntext_area = shadow_root3.find_element_by_id('searchInput')\ntext_area.send_keys(\"content settings\")\n\nroot0 = shadow_root1.find_element_by_id('main')\nshadow_root0_s = expand_shadow_element(root0)\n\n\nroot1_p = shadow_root0_s.find_element_by_css_selector('settings-basic-page')\nshadow_root1_p = expand_shadow_element(root1_p)\n\n\nroot1_s = shadow_root1_p.find_element_by_css_selector('settings-privacy-page')\nshadow_root1_s = expand_shadow_element(root1_s)\n\ncontent_settings_div = shadow_root1_s.find_element_by_css_selector('#site-settings-subpage-trigger')\ncontent_settings = content_settings_div.find_element_by_css_selector(\"button\")\ncontent_settings.click()\n</code></pre>\n", "abstract": "There is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  "}]}, {"link": "https://stackoverflow.com/questions/12553117/how-to-filter-duplicate-requests-based-on-url-in-scrapy", "question": {"id": "12553117", "title": "how to filter duplicate requests based on url in scrapy", "content": "<p>I am writing a crawler for a website using scrapy with CrawlSpider.</p>\n<p>Scrapy provides an in-built duplicate-request filter which filters duplicate requests based on urls. Also, I can filter requests using <em>rules</em> member of CrawlSpider. </p>\n<p>What I want to do is to filter requests like:</p>\n<pre><code class=\"python\">http:://www.abc.com/p/xyz.html?id=1234&amp;refer=5678\n</code></pre>\n<p>If I have already visited</p>\n<pre><code class=\"python\">http:://www.abc.com/p/xyz.html?id=1234&amp;refer=4567\n</code></pre>\n<blockquote>\n<p><strong>NOTE:</strong> refer is a parameter that doesn't affect the response I get, so I don't care if the value of that parameter changes.</p>\n</blockquote>\n<p>Now, if I have a set which accumulates all <em>ids</em> I could ignore it in my callback function <em>parse_item</em> (that's my callback function) to achieve this functionality.</p>\n<p>But that would mean I am still at least fetching that page, when I don't need to.</p>\n<p>So what is the way in which I can tell scrapy that it shouldn't send a particular request based on the url?</p>\n", "abstract": "I am writing a crawler for a website using scrapy with CrawlSpider. Scrapy provides an in-built duplicate-request filter which filters duplicate requests based on urls. Also, I can filter requests using rules member of CrawlSpider.  What I want to do is to filter requests like: If I have already visited NOTE: refer is a parameter that doesn't affect the response I get, so I don't care if the value of that parameter changes. Now, if I have a set which accumulates all ids I could ignore it in my callback function parse_item (that's my callback function) to achieve this functionality. But that would mean I am still at least fetching that page, when I don't need to. So what is the way in which I can tell scrapy that it shouldn't send a particular request based on the url?"}, "answers": [{"id": 13605919, "score": 41, "vote": 0, "content": "<p>You can write custom middleware for duplicate removal and add it in settings</p>\n<pre><code class=\"python\">import os\n\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass CustomFilter(RFPDupeFilter):\n\"\"\"A dupe filter that considers specific ids in the url\"\"\"\n\n    def __getid(self, url):\n        mm = url.split(\"&amp;refer\")[0] #or something like that\n        return mm\n\n    def request_seen(self, request):\n        fp = self.__getid(request.url)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)\n</code></pre>\n<p>Then you need to set the correct DUPFILTER_CLASS in settings.py</p>\n<pre><code class=\"python\">DUPEFILTER_CLASS = 'scraper.duplicate_filter.CustomFilter'\n</code></pre>\n<p>It should work after that</p>\n", "abstract": "You can write custom middleware for duplicate removal and add it in settings Then you need to set the correct DUPFILTER_CLASS in settings.py It should work after that"}, {"id": 18809057, "score": 10, "vote": 0, "content": "<p>Following ytomar's lead, I wrote this filter that filters based purely on URLs that have already been seen by checking an in-memory set.  I'm a Python noob so let me know if I screwed something up, but it seems to work all right:</p>\n<pre><code class=\"python\">from scrapy.dupefilter import RFPDupeFilter\n\nclass SeenURLFilter(RFPDupeFilter):\n    \"\"\"A dupe filter that considers the URL\"\"\"\n\n    def __init__(self, path=None):\n        self.urls_seen = set()\n        RFPDupeFilter.__init__(self, path)\n\n    def request_seen(self, request):\n        if request.url in self.urls_seen:\n            return True\n        else:\n            self.urls_seen.add(request.url)\n</code></pre>\n<p>As ytomar mentioned, be sure to add the <code>DUPEFILTER_CLASS</code> constant to <code>settings.py</code>:</p>\n<pre><code class=\"python\">DUPEFILTER_CLASS = 'scraper.custom_filters.SeenURLFilter'\n</code></pre>\n", "abstract": "Following ytomar's lead, I wrote this filter that filters based purely on URLs that have already been seen by checking an in-memory set.  I'm a Python noob so let me know if I screwed something up, but it seems to work all right: As ytomar mentioned, be sure to add the DUPEFILTER_CLASS constant to settings.py:"}, {"id": 28606785, "score": 3, "vote": 0, "content": "<p><a href=\"https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py\" rel=\"nofollow\">https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py</a></p>\n<p>This file might help you. This file creates a database of unique delta fetch key from the url ,a user pass in a scrapy.Reqeust(meta={'deltafetch_key':uniqe_url_key}).\nThis this let you avoid duplicate requests you already have visited in the past.</p>\n<p>A sample mongodb implementation using deltafetch.py </p>\n<pre><code class=\"python\">        if isinstance(r, Request):\n            key = self._get_key(r)\n            key = key+spider.name\n\n            if self.db['your_collection_to_store_deltafetch_key'].find_one({\"_id\":key}):\n                spider.log(\"Ignoring already visited: %s\" % r, level=log.INFO)\n                continue\n        elif isinstance(r, BaseItem):\n\n            key = self._get_key(response.request)\n            key = key+spider.name\n            try:\n                self.db['your_collection_to_store_deltafetch_key'].insert({\"_id\":key,\"time\":datetime.now()})\n            except:\n                spider.log(\"Ignoring already visited: %s\" % key, level=log.ERROR)\n        yield r\n</code></pre>\n<p>eg. id = 345\nscrapy.Request(url,meta={deltafetch_key:345},callback=parse)</p>\n", "abstract": "https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py This file might help you. This file creates a database of unique delta fetch key from the url ,a user pass in a scrapy.Reqeust(meta={'deltafetch_key':uniqe_url_key}).\nThis this let you avoid duplicate requests you already have visited in the past. A sample mongodb implementation using deltafetch.py  eg. id = 345\nscrapy.Request(url,meta={deltafetch_key:345},callback=parse)"}, {"id": 30540760, "score": 1, "vote": 0, "content": "<p>Here is my custom filter base on scrapy 0.24.6.</p>\n<p>In this filter, it only cares id in the url. for example</p>\n<p><code>http://www.example.com/products/cat1/1000.html?p=1</code>\n<code>http://www.example.com/products/cat2/1000.html?p=2</code></p>\n<p>are treated as same url. But</p>\n<p><code>http://www.example.com/products/cat2/all.html</code></p>\n<p>will not.</p>\n<pre><code class=\"python\">import re\nimport os\nfrom scrapy.dupefilter import RFPDupeFilter\n\n\nclass MyCustomURLFilter(RFPDupeFilter):\n\n    def _get_id(self, url):\n        m = re.search(r'(\\d+)\\.html', url)\n        return None if m is None else m.group(1)\n\n    def request_fingerprint(self, request):\n        style_id = self._get_id(request.url)\n        return style_id\n</code></pre>\n", "abstract": "Here is my custom filter base on scrapy 0.24.6. In this filter, it only cares id in the url. for example http://www.example.com/products/cat1/1000.html?p=1\nhttp://www.example.com/products/cat2/1000.html?p=2 are treated as same url. But http://www.example.com/products/cat2/all.html will not."}, {"id": 59950449, "score": 0, "vote": 0, "content": "<p>In the latest scrapy, we can use the default duplication filter or extend and have custom one.</p>\n<p>define the below config in spider settings</p>\n<p><code>DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'</code></p>\n", "abstract": "In the latest scrapy, we can use the default duplication filter or extend and have custom one. define the below config in spider settings DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'"}]}, {"link": "https://stackoverflow.com/questions/18920930/scrapy-python-set-up-user-agent", "question": {"id": "18920930", "title": "Scrapy Python Set up User Agent", "content": "<p>I tried to override the user-agent of my crawlspider by adding an extra line to the project <a href=\"http://doc.scrapy.org/en/latest/topics/settings.html#project-settings-module\">configuration file</a>. Here is the code:</p>\n<pre><code class=\"python\">[settings]\ndefault = myproject.settings\nUSER_AGENT = \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"\n\n\n[deploy]\n#url = http://localhost:6800/\nproject = myproject\n</code></pre>\n<p>But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one \"Scrapy/0.18.2 (+<a href=\"http://scrapy.org\">http://scrapy.org</a>)\". \nCan any one explain what I have done wrong. </p>\n<p>Note:</p>\n<p>(1). It works when I tried to override the <a href=\"http://doc.scrapy.org/en/latest/topics/settings.html#global-overrides\">user agent globally</a>: </p>\n<pre><code class=\"python\">scrapy crawl myproject.com -o output.csv -t csv -s USER_AGENT=\"Mozilla....\"\n</code></pre>\n<p>(2). When I remove the line \"default = myproject.setting\" from the configuration file, and run scrapy crawl myproject.com, it says \"cannot find spider..\", so I feel like the default setting should not be removed in this case.</p>\n<p>Thanks a lot for the help in advance.                            </p>\n", "abstract": "I tried to override the user-agent of my crawlspider by adding an extra line to the project configuration file. Here is the code: But when I run the crawler against my own web, I notice the spider did not pick up my customized user agent but the default one \"Scrapy/0.18.2 (+http://scrapy.org)\". \nCan any one explain what I have done wrong.  Note: (1). It works when I tried to override the user agent globally:  (2). When I remove the line \"default = myproject.setting\" from the configuration file, and run scrapy crawl myproject.com, it says \"cannot find spider..\", so I feel like the default setting should not be removed in this case. Thanks a lot for the help in advance.                            "}, "answers": [{"id": 18922842, "score": 48, "vote": 0, "content": "<p>Move your USER_AGENT line to the <code>settings.py</code> file, and not in your <code>scrapy.cfg</code> file. <code>settings.py</code> should be at same level as <code>items.py</code> if you use <code>scrapy startproject</code> command, in your case  it should be something like <code>myproject/settings.py</code></p>\n", "abstract": "Move your USER_AGENT line to the settings.py file, and not in your scrapy.cfg file. settings.py should be at same level as items.py if you use scrapy startproject command, in your case  it should be something like myproject/settings.py"}, {"id": 45420166, "score": 3, "vote": 0, "content": "<p>Just in case anyone lands here that manually controls the scrapy crawl. i.e. you do <strong>not</strong> use the scrapy crawl process from the shell...</p>\n<pre><code class=\"python\">$ scrapy crawl myproject\n</code></pre>\n<p>But insted you use <code>CrawlerProcess()</code> or <code>CrawlerRunner()</code>...</p>\n<pre><code class=\"python\">process = CrawlerProcess()\n</code></pre>\n<p>or </p>\n<pre><code class=\"python\">process = CrawlerRunner()\n</code></pre>\n<p>then the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables. </p>\n<p>Like this...</p>\n<pre><code class=\"python\">    process = CrawlerProcess(\n            {\n                'USER_AGENT': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n            }\n    )\n</code></pre>\n", "abstract": "Just in case anyone lands here that manually controls the scrapy crawl. i.e. you do not use the scrapy crawl process from the shell... But insted you use CrawlerProcess() or CrawlerRunner()... or  then the user agent, along with other settings, can be passed to the crawler in a dictionary of configuration variables.  Like this..."}]}, {"link": "https://stackoverflow.com/questions/10123104/unknown-command-crawl-error", "question": {"id": "10123104", "title": "unknown command: crawl error", "content": "<p>I am a newbie to python. I am running python 2.7.3 version 32 bit on 64 bit OS. (I tried 64 bit but it didn't workout).</p>\n<p>I followed the tutorial and installed scrapy on my machine. I have created one project, demoz. But when I enter <code>scrapy crawl demoz</code> it shows an error. I came across this thing when i hit scrapy command under (C:\\python27\\scripts) it shows:</p>\n<pre><code class=\"python\">C:\\Python27\\Scripts&gt;scrapy\nScrapy 0.14.2 - no active project\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  fetch         Fetch a URL using the Scrapy downloader\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\nC:\\Python27\\Scripts&gt;\n</code></pre>\n<p>I guess their is something missing in installation can anybody help please .. Thanks in advance..</p>\n", "abstract": "I am a newbie to python. I am running python 2.7.3 version 32 bit on 64 bit OS. (I tried 64 bit but it didn't workout). I followed the tutorial and installed scrapy on my machine. I have created one project, demoz. But when I enter scrapy crawl demoz it shows an error. I came across this thing when i hit scrapy command under (C:\\python27\\scripts) it shows: I guess their is something missing in installation can anybody help please .. Thanks in advance.."}, "answers": [{"id": 10123615, "score": 79, "vote": 0, "content": "<p>You should run <code>scrapy crawl spider_name</code> command being in a scrapy project folder, where <code>scrapy.cfg</code> file resides.</p>\n<p>From the <a href=\"http://doc.scrapy.org/en/latest/intro/tutorial.html#crawling\" rel=\"noreferrer\">docs</a>:</p>\n<blockquote>\n<p>Crawling</p>\n<p>To put our spider to work, go to the project\u2019s top level directory and run:</p>\n<p><code>scrapy crawl dmoz</code></p>\n</blockquote>\n", "abstract": "You should run scrapy crawl spider_name command being in a scrapy project folder, where scrapy.cfg file resides. From the docs: Crawling To put our spider to work, go to the project\u2019s top level directory and run: scrapy crawl dmoz"}, {"id": 37341500, "score": 13, "vote": 0, "content": "<p>You can run <code>scrapy crawl demoz</code> code from your scrapy project folder which you have created using following command</p>\n<pre><code class=\"python\">scrapy startproject tutorials\n</code></pre>\n<p>For example, if you have started scrapy project of name <code>tutorials</code>, then go to tutorials folder first and run <code>crawl</code> command from there</p>\n<pre><code class=\"python\">scrapy crawl demoz\n</code></pre>\n", "abstract": "You can run scrapy crawl demoz code from your scrapy project folder which you have created using following command For example, if you have started scrapy project of name tutorials, then go to tutorials folder first and run crawl command from there"}, {"id": 60716126, "score": 0, "vote": 0, "content": "<p>If you have already an existing project and suddenly this error \"Scrapy 1.3.3 - no active project\" came, in my case when I open my project, I cant see the scrapy.cfg file. What I did was, I just made some little changes on my project like removing some extra line and deployed it from github to instance and it worked!</p>\n", "abstract": "If you have already an existing project and suddenly this error \"Scrapy 1.3.3 - no active project\" came, in my case when I open my project, I cant see the scrapy.cfg file. What I did was, I just made some little changes on my project like removing some extra line and deployed it from github to instance and it worked!"}]}, {"link": "https://stackoverflow.com/questions/34382356/passing-arguments-to-process-crawl-in-scrapy-python", "question": {"id": "34382356", "title": "Passing arguments to process.crawl in Scrapy python", "content": "<p>I would like to get the same result as this command line :\nscrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json</p>\n<p>My script is as follows :</p>\n<pre><code class=\"python\">import scrapy\nfrom linkedin_anonymous_spider import LinkedInAnonymousSpider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nspider = LinkedInAnonymousSpider(None, \"James\", \"Bond\")\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl(spider) ## &lt;-------------- (1)\nprocess.start()\n</code></pre>\n<p>I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()?</p>\n<p>linkedin_anonymous :</p>\n<pre><code class=\"python\">from logging import INFO\n\nimport scrapy\n\nclass LinkedInAnonymousSpider(scrapy.Spider):\n    name = \"linkedin_anonymous\"\n    allowed_domains = [\"linkedin.com\"]\n    start_urls = []\n\n    base_url = \"https://www.linkedin.com/pub/dir/?first=%s&amp;last=%s&amp;search=Search\"\n\n    def __init__(self, input = None, first= None, last=None):\n        self.input = input  # source file name\n        self.first = first\n        self.last = last\n\n    def start_requests(self):\n        print self.first ## &lt;------------- (2)\n        if self.first and self.last: # taking input from command line parameters\n                url = self.base_url % (self.first, self.last)\n                yield self.make_requests_from_url(url)\n\n    def parse(self, response): . . .\n</code></pre>\n", "abstract": "I would like to get the same result as this command line :\nscrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json My script is as follows : I found out that process.crawl() in (1) is creating another LinkedInAnonymousSpider where first and last are None (printed in (2)), if so, then there is no point of creating the object spider and how is it possible to pass the arguments first and last to process.crawl()? linkedin_anonymous :"}, "answers": [{"id": 34383962, "score": 67, "vote": 0, "content": "<p>pass the spider arguments on the <code>process.crawl</code> method:</p>\n<pre><code class=\"python\">process.crawl(spider, input='inputargument', first='James', last='Bond')\n</code></pre>\n", "abstract": "pass the spider arguments on the process.crawl method:"}, {"id": 58583448, "score": 6, "vote": 0, "content": "<p>You can do it the easy way:</p>\n<pre><code class=\"python\">from scrapy import cmdline\n\ncmdline.execute(\"scrapy crawl linkedin_anonymous -a first=James -a last=Bond -o output.json\".split())\n</code></pre>\n", "abstract": "You can do it the easy way:"}, {"id": 64765345, "score": 3, "vote": 0, "content": "<p>if you have Scrapyd and you want to schedule the spider, do this</p>\n<p><code>curl http://localhost:6800/schedule.json -d project=projectname -d spider=spidername -d first='James' -d last='Bond'</code></p>\n", "abstract": "if you have Scrapyd and you want to schedule the spider, do this curl http://localhost:6800/schedule.json -d project=projectname -d spider=spidername -d first='James' -d last='Bond'"}]}, {"link": "https://stackoverflow.com/questions/30342243/send-post-request-in-scrapy", "question": {"id": "30342243", "title": "Send Post Request in Scrapy", "content": "<p>I am trying to crawl the latest reviews from google play store and to get that I need to make a post request.</p>\n<p>With the Postman, it works and I get desired response.</p>\n<p><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/zjLvu.png\"/></p>\n<p>but a post request in terminal gives me a server error</p>\n<p>For ex: this page <a href=\"https://play.google.com/store/apps/details?id=com.supercell.boombeach\" rel=\"noreferrer\">https://play.google.com/store/apps/details?id=com.supercell.boombeach</a></p>\n<pre><code class=\"python\">curl -H \"Content-Type: application/json\" -X POST -d '{\"id\": \"com.supercell.boombeach\", \"reviewType\": '0', \"reviewSortOrder\": '0', \"pageNum\":'0'}' https://play.google.com/store/getreviews\n</code></pre>\n<p>gives a server error and</p>\n<p>Scrapy just ignores this line:</p>\n<pre><code class=\"python\">frmdata = {\"id\": \"com.supercell.boombeach\", \"reviewType\": 0, \"reviewSortOrder\": 0, \"pageNum\":0}\n        url = \"https://play.google.com/store/getreviews\"\n        yield Request(url, callback=self.parse, method=\"POST\", body=urllib.urlencode(frmdata))\n</code></pre>\n", "abstract": "I am trying to crawl the latest reviews from google play store and to get that I need to make a post request. With the Postman, it works and I get desired response.  but a post request in terminal gives me a server error For ex: this page https://play.google.com/store/apps/details?id=com.supercell.boombeach gives a server error and Scrapy just ignores this line:"}, "answers": [{"id": 41747616, "score": 46, "vote": 0, "content": "<p>The answer above do not really solved the problem. They are sending the data as paramters instead of JSON data as the body of the request.</p>\n<p>From <a href=\"http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json\" rel=\"noreferrer\">http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json</a>:</p>\n<pre><code class=\"python\">my_data = {'field1': 'value1', 'field2': 'value2'}\nrequest = scrapy.Request( url, method='POST', \n                          body=json.dumps(my_data), \n                          headers={'Content-Type':'application/json'} )\n</code></pre>\n", "abstract": "The answer above do not really solved the problem. They are sending the data as paramters instead of JSON data as the body of the request. From http://bajiecc.cc/questions/1135255/scrapy-formrequest-sending-json:"}, {"id": 30345296, "score": 32, "vote": 0, "content": "<p>Make sure that each element in your <code>formdata</code> is of type string/unicode</p>\n<pre><code class=\"python\">frmdata = {\"id\": \"com.supercell.boombeach\", \"reviewType\": '0', \"reviewSortOrder\": '0', \"pageNum\":'0'}\nurl = \"https://play.google.com/store/getreviews\"\nyield FormRequest(url, callback=self.parse, formdata=frmdata)\n</code></pre>\n<p>I think this will do</p>\n<pre><code class=\"python\">In [1]: from scrapy.http import FormRequest\n\nIn [2]: frmdata = {\"id\": \"com.supercell.boombeach\", \"reviewType\": '0', \"reviewSortOrder\": '0', \"pageNum\":'0'}\n\nIn [3]: url = \"https://play.google.com/store/getreviews\"\n\nIn [4]: r = FormRequest(url, formdata=frmdata)\n\nIn [5]: fetch(r)\n 2015-05-20 14:40:09+0530 [default] DEBUG: Crawled (200) &lt;POST      https://play.google.com/store/getreviews&gt; (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7f3ea4258890&gt;\n[s]   item       {}\n[s]   r          &lt;POST https://play.google.com/store/getreviews&gt;\n[s]   request    &lt;POST https://play.google.com/store/getreviews&gt;\n[s]   response   &lt;200 https://play.google.com/store/getreviews&gt;\n[s]   settings   &lt;scrapy.settings.Settings object at 0x7f3eaa205450&gt;\n[s]   spider     &lt;Spider 'default' at 0x7f3ea3449cd0&gt;\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n</code></pre>\n", "abstract": "Make sure that each element in your formdata is of type string/unicode I think this will do"}, {"id": 35340544, "score": 0, "vote": 0, "content": "<p>Sample Page Traversing using Post in Scrapy:</p>\n<pre><code class=\"python\">def directory_page(self,response):\n    if response:\n        profiles = response.xpath(\"//div[@class='heading-h']/h3/a/@href\").extract()\n        for profile in profiles:\n            yield Request(urljoin(response.url,profile),callback=self.profile_collector)\n\n        page = response.meta['page'] + 1\n        if page :\n            yield FormRequest('https://rotmanconnect.com/AlumniDirectory/getmorerecentjoineduser',\n                                        formdata={'isSortByName':'false','pageNumber':str(page)},\n                                        callback= self.directory_page,\n                                        meta={'page':page})\n    else:\n         print \"No more page available\"\n</code></pre>\n", "abstract": "Sample Page Traversing using Post in Scrapy:"}]}, {"link": "https://stackoverflow.com/questions/23131283/how-to-force-scrapy-to-crawl-duplicate-url", "question": {"id": "23131283", "title": "How to force scrapy to crawl duplicate url?", "content": "<p>I am learning <a href=\"http://scrapy.org/\" rel=\"noreferrer\">Scrapy</a> a web crawling framework.<br/>\nby default it does not crawl duplicate urls or urls which scrapy have already crawled.  </p>\n<p>How to make Scrapy to crawl duplicate urls or urls which have already crawled?<br/>\nI tried to find out on internet but could not find relevant help.  </p>\n<p>I found <code>DUPEFILTER_CLASS = RFPDupeFilter</code> and <code>SgmlLinkExtractor</code> from <a href=\"https://stackoverflow.com/questions/15104831/scrapy-spider-crawls-duplicate-urls\">Scrapy - Spider crawls duplicate urls</a> but this question is opposite of what I am looking</p>\n", "abstract": "I am learning Scrapy a web crawling framework.\nby default it does not crawl duplicate urls or urls which scrapy have already crawled.   How to make Scrapy to crawl duplicate urls or urls which have already crawled?\nI tried to find out on internet but could not find relevant help.   I found DUPEFILTER_CLASS = RFPDupeFilter and SgmlLinkExtractor from Scrapy - Spider crawls duplicate urls but this question is opposite of what I am looking"}, "answers": [{"id": 23131785, "score": 56, "vote": 0, "content": "<p>You're probably looking for the <code>dont_filter=True</code> argument on <code>Request()</code>.\nSee <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\">http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects</a></p>\n", "abstract": "You're probably looking for the dont_filter=True argument on Request().\nSee http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects"}, {"id": 48947411, "score": 30, "vote": 0, "content": "<p>A more elegant solution is to disable the duplicate filter altogether:</p>\n<pre><code class=\"python\"># settings.py\nDUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'\n</code></pre>\n<p>This way you don't have to clutter all your Request creation code with <code>dont_filter=True</code>. Another side effect: this only disables duplicate filtering and not any other filters like offsite filtering.</p>\n<p>If you want to use this setting selectively for only one or some of multiple spiders in your project, you can set it via <a href=\"https://doc.scrapy.org/en/latest/topics/settings.html#settings-per-spider\" rel=\"noreferrer\"><code>custom_settings</code></a> in the spider implementation:</p>\n<pre><code class=\"python\">class MySpider(scrapy.Spider):\n    name = 'myspider'\n\n    custom_settings = {\n        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n    }\n</code></pre>\n", "abstract": "A more elegant solution is to disable the duplicate filter altogether: This way you don't have to clutter all your Request creation code with dont_filter=True. Another side effect: this only disables duplicate filtering and not any other filters like offsite filtering. If you want to use this setting selectively for only one or some of multiple spiders in your project, you can set it via custom_settings in the spider implementation:"}]}, {"link": "https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable", "question": {"id": "41495052", "title": "Scrapy - Reactor not Restartable", "content": "<p>with:</p>\n<pre><code class=\"python\">from twisted.internet import reactor\nfrom scrapy.crawler import CrawlerProcess\n</code></pre>\n<p>I've always ran this process sucessfully:</p>\n<pre><code class=\"python\">process = CrawlerProcess(get_project_settings())\nprocess.crawl(*args)\n# the script will block here until the crawling is finished\nprocess.start() \n</code></pre>\n<p>but since I've moved this code into a <code>web_crawler(self)</code> function, like so:</p>\n<pre><code class=\"python\">def web_crawler(self):\n    # set up a crawler\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(*args)\n    # the script will block here until the crawling is finished\n    process.start() \n\n    # (...)\n\n    return (result1, result2) \n</code></pre>\n<p>and started calling the method using class instantiation, like:</p>\n<pre><code class=\"python\">def __call__(self):\n    results1 = test.web_crawler()[1]\n    results2 = test.web_crawler()[0]\n</code></pre>\n<p>and running:</p>\n<pre><code class=\"python\">test()\n</code></pre>\n<p>I am getting the following error:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"test.py\", line 573, in &lt;module&gt;\n    print (test())\n  File \"test.py\", line 530, in __call__\n    artists = test.web_crawler()\n  File \"test.py\", line 438, in web_crawler\n    process.start() \n  File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 280, in start\n    reactor.run(installSignalHandlers=False)  # blocking call\n  File \"/Library/Python/2.7/site-packages/twisted/internet/base.py\", line 1194, in run\n    self.startRunning(installSignalHandlers=installSignalHandlers)\n  File \"/Library/Python/2.7/site-packages/twisted/internet/base.py\", line 1174, in startRunning\n    ReactorBase.startRunning(self)\n  File \"/Library/Python/2.7/site-packages/twisted/internet/base.py\", line 684, in startRunning\n    raise error.ReactorNotRestartable()\ntwisted.internet.error.ReactorNotRestartable\n</code></pre>\n<p>what is wrong?</p>\n", "abstract": "with: I've always ran this process sucessfully: but since I've moved this code into a web_crawler(self) function, like so: and started calling the method using class instantiation, like: and running: I am getting the following error: what is wrong?"}, "answers": [{"id": 43661172, "score": 59, "vote": 0, "content": "<p>You cannot restart the reactor, but you should be able to run it more times by forking a separate process:</p>\n<pre><code class=\"python\">import scrapy\nimport scrapy.crawler as crawler\nfrom scrapy.utils.log import configure_logging\nfrom multiprocessing import Process, Queue\nfrom twisted.internet import reactor\n\n# your spider\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = ['http://quotes.toscrape.com/tag/humor/']\n\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            print(quote.css('span.text::text').extract_first())\n\n\n# the wrapper to make it run more times\ndef run_spider(spider):\n    def f(q):\n        try:\n            runner = crawler.CrawlerRunner()\n            deferred = runner.crawl(spider)\n            deferred.addBoth(lambda _: reactor.stop())\n            reactor.run()\n            q.put(None)\n        except Exception as e:\n            q.put(e)\n\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    result = q.get()\n    p.join()\n\n    if result is not None:\n        raise result\n</code></pre>\n<p>Run it twice:</p>\n<pre><code class=\"python\">configure_logging()\n\nprint('first run:')\nrun_spider(QuotesSpider)\n\nprint('\\nsecond run:')\nrun_spider(QuotesSpider)\n</code></pre>\n<p>Result:</p>\n<pre class=\"lang-none prettyprint-override\"><code class=\"python\">first run:\n\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d\n\u201cA day without sunshine is like, you know, night.\u201d\n...\n\nsecond run:\n\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d\n\u201cA day without sunshine is like, you know, night.\u201d\n...\n</code></pre>\n", "abstract": "You cannot restart the reactor, but you should be able to run it more times by forking a separate process: Run it twice: Result:"}, {"id": 47581199, "score": 17, "vote": 0, "content": "<p>This is what helped for me to win the battle against ReactorNotRestartable error: <a href=\"https://stackoverflow.com/questions/44788051/reactornotrestartable-twisted-and-scrapy\">last answer from the author of the question</a><br/>\n0) <code>pip install crochet</code><br/>\n1) <code>import from crochet import setup</code><br/>\n2) <code>setup()</code> - at the top of the file<br/>\n3) remove 2 lines:<br/>\na) <code>d.addBoth(lambda _: reactor.stop())</code><br/>\nb) <code>reactor.run()</code><br/>\n<br/>\nI had the same problem with this error, and spend 4+ hours to solve this problem, read all questions here about it. Finally found that one - and share it. That is how i solved this. The only meaningful lines from <a href=\"https://doc.scrapy.org/en/latest/topics/practices.html\" rel=\"noreferrer\">Scrapy docs</a> left are 2 last lines in this my code:</p>\n<pre><code class=\"python\">#some more imports\nfrom crochet import setup\nsetup()\n\ndef run_spider(spiderName):\n    module_name=\"first_scrapy.spiders.{}\".format(spiderName)\n    scrapy_var = import_module(module_name)   #do some dynamic import of selected spider   \n    spiderObj=scrapy_var.mySpider()           #get mySpider-object from spider module\n    crawler = CrawlerRunner(get_project_settings())   #from Scrapy docs\n    crawler.crawl(spiderObj)                          #from Scrapy docs\n</code></pre>\n<p>This code allows me to select what spider to run just with its name passed to <code>run_spider</code> function and after scrapping finishes - select another spider and run it again.<br/> \nHope this will help somebody, as it helped for me :)</p>\n", "abstract": "This is what helped for me to win the battle against ReactorNotRestartable error: last answer from the author of the question\n0) pip install crochet\n1) import from crochet import setup\n2) setup() - at the top of the file\n3) remove 2 lines:\na) d.addBoth(lambda _: reactor.stop())\nb) reactor.run()\n\nI had the same problem with this error, and spend 4+ hours to solve this problem, read all questions here about it. Finally found that one - and share it. That is how i solved this. The only meaningful lines from Scrapy docs left are 2 last lines in this my code: This code allows me to select what spider to run just with its name passed to run_spider function and after scrapping finishes - select another spider and run it again. \nHope this will help somebody, as it helped for me :)"}, {"id": 41496323, "score": 2, "vote": 0, "content": "<p>As per the <a href=\"http://scrapy.readthedocs.io/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess.start\" rel=\"nofollow noreferrer\">Scrapy documentation</a>, the <code>start()</code> method of the <code>CrawlerProcess</code> class does the following:</p>\n<blockquote>\n<p>\"[...] starts a Twisted reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE.\"</p>\n</blockquote>\n<p>The error you are receiving is being thrown by <code>Twisted</code>, because a Twisted reactor cannot be restarted.  It uses a ton of globals, and even if you do jimmy-rig some sort of code to restart it (I've seen it done), there's no guarantee it will work.  </p>\n<p>Honestly, if you think you need to restart the reactor, you're likely doing something wrong.</p>\n<p>Depending on what you want to do, I would also review the <a href=\"http://scrapy.readthedocs.io/en/latest/topics/practices.html#run-scrapy-from-a-script\" rel=\"nofollow noreferrer\">Running Scrapy from a Script</a> portion of the documentation, too.</p>\n", "abstract": "As per the Scrapy documentation, the start() method of the CrawlerProcess class does the following: \"[...] starts a Twisted reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE.\" The error you are receiving is being thrown by Twisted, because a Twisted reactor cannot be restarted.  It uses a ton of globals, and even if you do jimmy-rig some sort of code to restart it (I've seen it done), there's no guarantee it will work.   Honestly, if you think you need to restart the reactor, you're likely doing something wrong. Depending on what you want to do, I would also review the Running Scrapy from a Script portion of the documentation, too."}, {"id": 54719360, "score": 2, "vote": 0, "content": "<p>As some people pointed out already: You shouldn't need to restart the reactor.</p>\n<p>Ideally if you want to chain your processes (crawl1 then crawl2 then crawl3) you simply add callbacks.</p>\n<p>For example, I've been using this loop spider that follows this pattern:</p>\n<pre><code class=\"python\">1. Crawl A\n2. Sleep N\n3. goto 1\n</code></pre>\n<p>And this is how it looks in scrapy:</p>\n<pre><code class=\"python\">import time\n\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nfrom twisted.internet import reactor\n\nclass HttpbinSpider(scrapy.Spider):\n    name = 'httpbin'\n    allowed_domains = ['httpbin.org']\n    start_urls = ['http://httpbin.org/ip']\n\n    def parse(self, response):\n        print(response.body)\n\ndef sleep(_, duration=5):\n    print(f'sleeping for: {duration}')\n    time.sleep(duration)  # block here\n\n\ndef crawl(runner):\n    d = runner.crawl(HttpbinSpider)\n    d.addBoth(sleep)\n    d.addBoth(lambda _: crawl(runner))\n    return d\n\n\ndef loop_crawl():\n    runner = CrawlerRunner(get_project_settings())\n    crawl(runner)\n    reactor.run()\n\n\nif __name__ == '__main__':\n    loop_crawl()\n</code></pre>\n<p>To explain the process more the <code>crawl</code> function schedules a crawl and adds two extra callbacks that are being called when crawling is over: blocking sleep and recursive call to itself (schedule another crawl).</p>\n<pre><code class=\"python\">$ python endless_crawl.py \nb'{\\n  \"origin\": \"000.000.000.000\"\\n}\\n'\nsleeping for: 5\nb'{\\n  \"origin\": \"000.000.000.000\"\\n}\\n'\nsleeping for: 5\nb'{\\n  \"origin\": \"000.000.000.000\"\\n}\\n'\nsleeping for: 5\nb'{\\n  \"origin\": \"000.000.000.000\"\\n}\\n'\nsleeping for: 5\n</code></pre>\n", "abstract": "As some people pointed out already: You shouldn't need to restart the reactor. Ideally if you want to chain your processes (crawl1 then crawl2 then crawl3) you simply add callbacks. For example, I've been using this loop spider that follows this pattern: And this is how it looks in scrapy: To explain the process more the crawl function schedules a crawl and adds two extra callbacks that are being called when crawling is over: blocking sleep and recursive call to itself (schedule another crawl)."}, {"id": 41497241, "score": 1, "vote": 0, "content": "<p>The mistake is in this code:</p>\n<pre><code class=\"python\">def __call__(self):\n    result1 = test.web_crawler()[1]\n    result2 = test.web_crawler()[0] # here\n</code></pre>\n<p><code>web_crawler()</code> returns two results, and for that purpose it is trying to start the process twice, restarting the Reactor, as pointed by @Rejected.</p>\n<p>obtaining results running one single process, and storing both results in a tuple, is the way to go here:</p>\n<pre><code class=\"python\">def __call__(self):\n    result1, result2 = test.web_crawler()\n</code></pre>\n", "abstract": "The mistake is in this code: web_crawler() returns two results, and for that purpose it is trying to start the process twice, restarting the Reactor, as pointed by @Rejected. obtaining results running one single process, and storing both results in a tuple, is the way to go here:"}, {"id": 47127561, "score": 0, "vote": 0, "content": "<p>This solved my problem,put below code after <code>reactor.run()</code> or <code>process.start()</code>:</p>\n<pre><code class=\"python\">time.sleep(0.5)\n\nos.execl(sys.executable, sys.executable, *sys.argv)\n</code></pre>\n", "abstract": "This solved my problem,put below code after reactor.run() or process.start():"}]}, {"link": "https://stackoverflow.com/questions/27243246/can-scrapy-be-replaced-by-pyspider", "question": {"id": "27243246", "title": "Can Scrapy be replaced by pyspider?", "content": "<p>I've been using <code>Scrapy</code> web-scraping framework pretty extensively, but, recently I've discovered that there is another framework/system called <a href=\"https://github.com/binux/pyspider\" rel=\"noreferrer\"><code>pyspider</code></a>, which, according to it's github page, is fresh, actively developed and popular.</p>\n<p><code>pyspider</code>'s home page lists several things being supported out-of-the-box:</p>\n<blockquote>\n<ul>\n<li><p>Powerful WebUI with script editor, task monitor, project manager and result viewer</p></li>\n<li><p>Javascript pages supported!  </p></li>\n<li><p>Task priority, retry, periodical and\n  recrawl by age or marks in index page (like update time)  </p></li>\n<li><p>Distributed architecture</p></li>\n</ul>\n</blockquote>\n<p>These are the things that <code>Scrapy</code> itself doesn't provide, but, it is possible with the help of <a href=\"https://github.com/scrapinghub/portia\" rel=\"noreferrer\"><code>portia</code></a> (for Web UI), <a href=\"https://github.com/scrapinghub/scrapyjs\" rel=\"noreferrer\"><code>scrapyjs</code></a> (for js pages) and <a href=\"http://scrapyd.readthedocs.org/en/latest\" rel=\"noreferrer\"><code>scrapyd</code></a> (deploying and distributing through API).</p>\n<p>Is it true that <code>pyspider</code> alone can replace all of these tools? In other words, is <code>pyspider</code> a direct alternative to Scrapy? If not, then which use cases does it cover?</p>\n<p><sup>I hope I'm not crossing \"too broad\" or \"opinion-based\" line.</sup></p>\n", "abstract": "I've been using Scrapy web-scraping framework pretty extensively, but, recently I've discovered that there is another framework/system called pyspider, which, according to it's github page, is fresh, actively developed and popular. pyspider's home page lists several things being supported out-of-the-box: Powerful WebUI with script editor, task monitor, project manager and result viewer Javascript pages supported!   Task priority, retry, periodical and\n  recrawl by age or marks in index page (like update time)   Distributed architecture These are the things that Scrapy itself doesn't provide, but, it is possible with the help of portia (for Web UI), scrapyjs (for js pages) and scrapyd (deploying and distributing through API). Is it true that pyspider alone can replace all of these tools? In other words, is pyspider a direct alternative to Scrapy? If not, then which use cases does it cover? I hope I'm not crossing \"too broad\" or \"opinion-based\" line."}, "answers": [{"id": 27246549, "score": 29, "vote": 0, "content": "<p>pyspider and Scrapy have the same purpose, web scraping, but a different view about doing that.</p>\n<ul>\n<li><p>spider should never stop till WWW dead. (information is changing, data is updating in websites, spider should have the ability and responsibility to scrape latest data. That's why pyspider has URL database, powerful scheduler, <code>@every</code>, <code>age</code>, etc..)</p></li>\n<li><p>pyspider is a service more than a framework. (Components are running in isolated process, lite - <code>all</code> version is running as service too, you needn't have a Python environment but a browser, everything about fetch or schedule is controlled by script via API not startup parameters or global configs, resources/projects is managed by pyspider, etc...)</p></li>\n<li><p>pyspider is a spider system. (Any components can been replaced, even developed in C/C++/Java or any language, for better performance or larger capacity)</p></li>\n</ul>\n<p>and </p>\n<ul>\n<li><code>on_start</code> vs <code>start_url</code></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Token_bucket\" rel=\"noreferrer\">token bucket</a> traffic control vs <code>download_delay</code></li>\n<li><code>return json</code> vs <code>class Item</code></li>\n<li>message queue vs <code>Pipeline</code></li>\n<li>built-in url database vs <code>set</code></li>\n<li>Persistence vs In-memory</li>\n<li><a href=\"https://pythonhosted.org/pyquery/\" rel=\"noreferrer\">PyQuery</a> + any third package you like vs built-in CSS/Xpath support</li>\n</ul>\n<p>In fact, I have not referred much from Scrapy. pyspider is really different from Scrapy.</p>\n<p>But, why not <a href=\"http://demo.pyspider.org\" rel=\"noreferrer\">try it yourself</a>? pyspider is also <a href=\"https://gist.github.com/binux/67b276c51e988f8e2c31#comment-1339242\" rel=\"noreferrer\">fast</a>, has easy-to-use API and you can try it without install.</p>\n", "abstract": "pyspider and Scrapy have the same purpose, web scraping, but a different view about doing that. spider should never stop till WWW dead. (information is changing, data is updating in websites, spider should have the ability and responsibility to scrape latest data. That's why pyspider has URL database, powerful scheduler, @every, age, etc..) pyspider is a service more than a framework. (Components are running in isolated process, lite - all version is running as service too, you needn't have a Python environment but a browser, everything about fetch or schedule is controlled by script via API not startup parameters or global configs, resources/projects is managed by pyspider, etc...) pyspider is a spider system. (Any components can been replaced, even developed in C/C++/Java or any language, for better performance or larger capacity) and  In fact, I have not referred much from Scrapy. pyspider is really different from Scrapy. But, why not try it yourself? pyspider is also fast, has easy-to-use API and you can try it without install."}, {"id": 56284633, "score": 9, "vote": 0, "content": "<p>Since I use both scrapy and pyspider, I would like to suggest the following:</p>\n<p>If the website is really small / simple, try pyspider first since it has almost everything you need</p>\n<ul>\n<li>Use webui to setup project</li>\n<li>Try the online code editor and view parse result instantly</li>\n<li>View the result easily in browser</li>\n<li>Run/Pause the project</li>\n<li>Setup the expiration date so it can re-process the url</li>\n</ul>\n<p>However, if you tried pyspider and found it can't fit your needs, it's time to use scrapy. \n - migrate on_start to start_request \n - migrate index_page to parse\n - migrate detail_age to detail_age \n - change self.crawl to response.follow</p>\n<p>Then you are almost done.\nNow you can play with scrapy's advanced features like middleware, items, pipline etc. </p>\n", "abstract": "Since I use both scrapy and pyspider, I would like to suggest the following: If the website is really small / simple, try pyspider first since it has almost everything you need However, if you tried pyspider and found it can't fit your needs, it's time to use scrapy. \n - migrate on_start to start_request \n - migrate index_page to parse\n - migrate detail_age to detail_age \n - change self.crawl to response.follow Then you are almost done.\nNow you can play with scrapy's advanced features like middleware, items, pipline etc. "}]}, {"link": "https://stackoverflow.com/questions/53729201/save-complete-web-page-incl-css-images-using-python-selenium", "question": {"id": "53729201", "title": "Save complete web page (incl css, images) using python/selenium", "content": "<p>I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nURL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&amp;PAGE_TYPE=BlastSearch&amp;LINK_LOC=blasthome'\nSEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'\nCHROME_WEBDRIVER_LOCATION = '/home/max/Downloads/chromedriver' # update this for your machine\n\n# open page with selenium\n# (first need to download Chrome webdriver, or a firefox webdriver, etc)\ndriver = webdriver.Chrome(executable_path=CHROME_WEBDRIVER_LOCATION)\ndriver.get(URL)\ntime.sleep(5)\n\n# enter sequence into the query field and hit 'blast' button to search\nseq_query_field = driver.find_element_by_id(\"seq\")\nseq_query_field.send_keys(SEQUENCE)\n\nblast_button = driver.find_element_by_id(\"b1\")\nblast_button.click()\ntime.sleep(60)\n</code></pre>\n<p>At that point I have a page that I can manually click \"save as,\" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc.</p>\n<pre><code class=\"python\">content = driver.page_source\nwith open('webpage.html', 'w') as f:\n    f.write(content)\n</code></pre>\n<p>I've also found <a href=\"https://stackoverflow.com/a/14517464/1870832\">this question/answer on SO</a>, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out)</p>\n<p>Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality?</p>\n<p>UPDATE - Follow up question for James' answer\nSo I ran James' code to generate a <code>page.html</code> (and associated files) and compared it to the html file I got from manually clicking save-as. The <code>page.html</code> saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). \n<a href=\"https://i.stack.imgur.com/k4poh.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/k4poh.png\"/></a></p>\n<p>This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script:</p>\n<pre><code class=\"python\">&lt;p class=\"helpbox ui-ncbitoggler-slave ui-ncbitoggler\" id=\"hlp1\" aria-hidden=\"true\"&gt;\nThese options control formatting of alignments in results pages. The\ndefault is HTML, but other formats (including plain text) are available.\nPSSM and PssmWithParameters are representations of Position Specific Scoring Matrices and are only available for PSI-BLAST. \nThe Advanced view option allows the database descriptions to be sorted by various indices in a table.\n&lt;/p&gt;\n</code></pre>\n<p>Any idea why this is happening?</p>\n", "abstract": "I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want: At that point I have a page that I can manually click \"save as,\" and get a local file (with a corresponding folder of image/js assets) that lets me view the whole returned page locally (minus content which is generated dynamically from scrolling down the page, which is fine). I assumed there would be a simple way to mimic this 'save as' function in python/selenium but haven't found one. The code to save the page below just saves html, and does not leave me with a local file that looks like it does in the web browser, with images, etc. I've also found this question/answer on SO, but the accepted answer just brings up the 'save as' box, and does not provide a way to click it (as two commenters point out) Is there a simple way to 'save [full page] as' using python? Ideally I'd prefer an answer using selenium since selenium makes the crawling part so straightforward, but I'm open to using another library if there's a better tool for this job. Or maybe I just need to specify all of the images/tables I want to download in code, and there is no shortcut to emulating the right-click 'save as' functionality? UPDATE - Follow up question for James' answer\nSo I ran James' code to generate a page.html (and associated files) and compared it to the html file I got from manually clicking save-as. The page.html saved via James' script is great and has everything I need, but when opened in a browser it also shows a lot of extra formatting text that's hidden in the manually save'd page. See attached screenshot (manually saved page on the left, script-saved page with extra formatting text shown on right). \n This is especially surprising to me because the raw html of the page saved by James' script seems to indicate those fields should still be hidden. See e.g. the html below, which appears the same in both files, but the text at issue only appears in the browser-rendered page on the one saved by James' script: Any idea why this is happening?"}, "answers": [{"id": 53966809, "score": 13, "vote": 0, "content": "<p>As you noted, Selenium cannot interact with the browser's context menu to use <code>Save as...</code>, so instead to do so, you could use an external automation library like <a href=\"https://pyautogui.readthedocs.io/en/latest/\" rel=\"noreferrer\"><code>pyautogui</code></a>.</p>\n<pre><code class=\"python\">pyautogui.hotkey('ctrl', 's')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + '.html')\npyautogui.hotkey('enter')\n</code></pre>\n<p>This code opens the <code>Save as...</code> window through its keyboard shortcut <code>CTRL+S</code> and then saves the webpage and its assets into the default downloads location by pressing enter. This code also names the file as the sequence in order to give it a unique name, though you could change this for your use case. If needed, you could additionally change the download location through some extra work with the tab and arrow keys.</p>\n<p>Tested on Ubuntu 18.10; depending on your OS you may need to modify the key combination sent.</p>\n<hr/>\n<p>Full code, in which I also added conditional waits to improve speed:</p>\n<pre><code class=\"python\">import time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.expected_conditions import visibility_of_element_located\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport pyautogui\n\nURL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&amp;PAGE_TYPE=BlastSearch&amp;LINK_LOC=blasthome'\nSEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' #'GAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGAGAAGA'\n\n# open page with selenium\n# (first need to download Chrome webdriver, or a firefox webdriver, etc)\ndriver = webdriver.Chrome()\ndriver.get(URL)\n\n# enter sequence into the query field and hit 'blast' button to search\nseq_query_field = driver.find_element_by_id(\"seq\")\nseq_query_field.send_keys(SEQUENCE)\n\nblast_button = driver.find_element_by_id(\"b1\")\nblast_button.click()\n\n# wait until results are loaded\nWebDriverWait(driver, 60).until(visibility_of_element_located((By.ID, 'grView')))\n\n# open 'Save as...' to save html and assets\npyautogui.hotkey('ctrl', 's')\ntime.sleep(1)\npyautogui.typewrite(SEQUENCE + '.html')\npyautogui.hotkey('enter')\n</code></pre>\n", "abstract": "As you noted, Selenium cannot interact with the browser's context menu to use Save as..., so instead to do so, you could use an external automation library like pyautogui. This code opens the Save as... window through its keyboard shortcut CTRL+S and then saves the webpage and its assets into the default downloads location by pressing enter. This code also names the file as the sequence in order to give it a unique name, though you could change this for your use case. If needed, you could additionally change the download location through some extra work with the tab and arrow keys. Tested on Ubuntu 18.10; depending on your OS you may need to modify the key combination sent. Full code, in which I also added conditional waits to improve speed:"}, {"id": 53919316, "score": 5, "vote": 0, "content": "<p>This is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of \"save as full web page (complete)\" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.  </p>\n<p>Most of the javascript won't work due to cross origin request blocking.  But the content will look (mostly) the same.</p>\n<p>This uses <code>requests</code> to save the loaded files, <code>lxml</code> to parse the html, and <code>os</code> for the path legwork.</p>\n<pre><code class=\"python\">from selenium import webdriver\nimport chromedriver_binary\nfrom lxml import html\nimport requests\nimport os\n\ndriver = webdriver.Chrome()\nURL = 'https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastx&amp;PAGE_TYPE=BlastSearch&amp;LINK_LOC=blasthome'\nSEQUENCE = 'CCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACAGCTCAAACACAAAGTTACCTAAACTATAGAAGGACA' \nbase = 'https://blast.ncbi.nlm.nih.gov/'\n\ndriver.get(URL)\nseq_query_field = driver.find_element_by_id(\"seq\")\nseq_query_field.send_keys(SEQUENCE)\nblast_button = driver.find_element_by_id(\"b1\")\nblast_button.click()\n\ncontent = driver.page_source\n# write the page content\nos.mkdir('page')\nwith open('page/page.html', 'w') as fp:\n    fp.write(content)\n\n# download the referenced files to the same path as in the html\nsess = requests.Session()\nsess.get(base)            # sets cookies\n\n# parse html\nh = html.fromstring(content)\n# get css/js files loaded in the head\nfor hr in h.xpath('head//@href'):\n    if not hr.startswith('http'):\n        local_path = 'page/' + hr\n        hr = base + hr\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, 'wb') as fp:\n        fp.write(res.content)\n\n# get image/js files from the body.  skip anything loaded from outside sources\nfor src in h.xpath('//@src'):\n    if not src or src.startswith('http'):\n        continue\n    local_path = 'page/' + src\n    print(local_path)\n    src = base + src\n    res = sess.get(hr)\n    if not os.path.exists(os.path.dirname(local_path)):\n        os.makedirs(os.path.dirname(local_path))\n    with open(local_path, 'wb') as fp:\n        fp.write(res.content)  \n</code></pre>\n<p>You should have a folder called <code>page</code> with a file called <code>page.html</code> in it with the content you are after.</p>\n", "abstract": "This is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of \"save as full web page (complete)\" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.   Most of the javascript won't work due to cross origin request blocking.  But the content will look (mostly) the same. This uses requests to save the loaded files, lxml to parse the html, and os for the path legwork. You should have a folder called page with a file called page.html in it with the content you are after."}, {"id": 63573072, "score": 2, "vote": 0, "content": "<p>Inspired by FThompson's answer above, I came up with the following tool that can download full/complete html for a given page url (see: <a href=\"https://github.com/markfront/SinglePageFullHtml\" rel=\"nofollow noreferrer\">https://github.com/markfront/SinglePageFullHtml</a>)</p>\n<p>UPDATE - follow up with Max's suggestion, below are steps to use the tool:</p>\n<ol>\n<li>Clone the project, then run maven to build:</li>\n</ol>\n<pre><code class=\"python\">$&gt; git clone https://github.com/markfront/SinglePageFullHtml.git\n\n$&gt; cd ~/git/SinglePageFullHtml\n$&gt; mvn clean compile package\n</code></pre>\n<ol start=\"2\">\n<li><p>Find the generated jar file in target folder: SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar</p>\n</li>\n<li><p>Run the jar in command line like:</p>\n</li>\n</ol>\n<pre><code class=\"python\">$&gt; java -jar .target/SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar &lt;page_url&gt;\n</code></pre>\n<ol start=\"4\">\n<li><p>The result file name will have a prefix \"FP, followed by the hashcode of the page url, with file extension \".html\". It will be found in either folder \"/tmp\" (which you can get by System.getProperty(\"java.io.tmp\"). If not, try find it in your home dir or System.getProperty(\"user.home\") in Java).</p>\n</li>\n<li><p>The result file will be a big fat self-contained html file that includes everything (css, javascript, images, etc.) referred to by the original html source.</p>\n</li>\n</ol>\n", "abstract": "Inspired by FThompson's answer above, I came up with the following tool that can download full/complete html for a given page url (see: https://github.com/markfront/SinglePageFullHtml) UPDATE - follow up with Max's suggestion, below are steps to use the tool: Find the generated jar file in target folder: SinglePageFullHtml-1.0-SNAPSHOT-jar-with-dependencies.jar Run the jar in command line like: The result file name will have a prefix \"FP, followed by the hashcode of the page url, with file extension \".html\". It will be found in either folder \"/tmp\" (which you can get by System.getProperty(\"java.io.tmp\"). If not, try find it in your home dir or System.getProperty(\"user.home\") in Java). The result file will be a big fat self-contained html file that includes everything (css, javascript, images, etc.) referred to by the original html source."}, {"id": 53938905, "score": -2, "vote": 0, "content": "<p>I'll advise u to have a try on <a href=\"http://sikulix.com/\" rel=\"nofollow noreferrer\">sikulix</a> which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(\"xxxx\") or subprocess...).</p>\n", "abstract": "I'll advise u to have a try on sikulix which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(\"xxxx\") or subprocess...)."}]}, {"link": "https://stackoverflow.com/questions/9561020/how-do-i-use-the-python-scrapy-module-to-list-all-the-urls-from-my-website", "question": {"id": "9561020", "title": "How do I use the Python Scrapy module to list all the URLs from my website?", "content": "<p>I want to use the Python <a href=\"http://scrapy.org/\" rel=\"noreferrer\">Scrapy module</a> to scrape all the URLs from my website and write the list to a file. I looked in the examples but didn't see any simple example to do this.</p>\n", "abstract": "I want to use the Python Scrapy module to scrape all the URLs from my website and write the list to a file. I looked in the examples but didn't see any simple example to do this."}, "answers": [{"id": 9570320, "score": 52, "vote": 0, "content": "<p>Here's the python program that worked for me:</p>\n<pre><code class=\"python\">from scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\n\nDOMAIN = 'example.com'\nURL = 'http://%s' % DOMAIN\n\nclass MySpider(BaseSpider):\n    name = DOMAIN\n    allowed_domains = [DOMAIN]\n    start_urls = [\n        URL\n    ]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        for url in hxs.select('//a/@href').extract():\n            if not ( url.startswith('http://') or url.startswith('https://') ):\n                url= URL + url \n            print url\n            yield Request(url, callback=self.parse)\n</code></pre>\n<p>Save this in a file called <code>spider.py</code>.</p>\n<p>You can then use a shell pipeline to post process this text:</p>\n<pre><code class=\"python\">bash$ scrapy runspider spider.py &gt; urls.out\nbash$ cat urls.out| grep 'example.com' |sort |uniq |grep -v '#' |grep -v 'mailto' &gt; example.urls\n</code></pre>\n<p>This gives me a list of all the unique urls in my site.</p>\n", "abstract": "Here's the python program that worked for me: Save this in a file called spider.py. You can then use a shell pipeline to post process this text: This gives me a list of all the unique urls in my site."}, {"id": 33481828, "score": 15, "vote": 0, "content": "<p>something cleaner (and maybe more useful) would be using <a href=\"http://doc.scrapy.org/en/latest/topics/link-extractors.html#link-extractors\">LinkExtractor</a></p>\n<pre><code class=\"python\">from scrapy.linkextractors import LinkExtractor\n\n    def parse(self, response):\n        le = LinkExtractor() # empty for getting everything, check different options on documentation\n        for link in le.extract_links(response):\n            yield Request(link.url, callback=self.parse)\n</code></pre>\n", "abstract": "something cleaner (and maybe more useful) would be using LinkExtractor"}]}, {"link": "https://stackoverflow.com/questions/3533528/python-web-crawlers-and-getting-html-source-code", "question": {"id": "3533528", "title": "Python Web Crawlers and &quot;getting&quot; html source code", "content": "<p>So my brother wanted me to write a web crawler in Python (self-taught) and I know C++, Java, and a bit of html.  I'm using version 2.7 and reading the python library, but I have a few problems\n1. <code>httplib.HTTPConnection</code> and <code>request</code> concept to me is new and I don't understand if it downloads an html script like cookie or an instance.  If you do both of those, do you get the source for a website page? And what are some words that I would need to know to modify the page and return the modified page.</p>\n<p>Just for background, I need to download a page and replace any img with ones I have</p>\n<p>And it would be nice if you guys could tell me your opinion of 2.7 and 3.1</p>\n", "abstract": "So my brother wanted me to write a web crawler in Python (self-taught) and I know C++, Java, and a bit of html.  I'm using version 2.7 and reading the python library, but I have a few problems\n1. httplib.HTTPConnection and request concept to me is new and I don't understand if it downloads an html script like cookie or an instance.  If you do both of those, do you get the source for a website page? And what are some words that I would need to know to modify the page and return the modified page. Just for background, I need to download a page and replace any img with ones I have And it would be nice if you guys could tell me your opinion of 2.7 and 3.1"}, "answers": [{"id": 3533678, "score": 46, "vote": 0, "content": "<p><s>Use Python 2.7, is has more 3rd party libs at the moment.</s> (<strong>Edit:</strong> see below).</p>\n<p>I recommend you using the stdlib module <code>urllib2</code>, it will allow you to comfortably get web resources.\nExample:</p>\n<pre><code class=\"python\">import urllib2\n\nresponse = urllib2.urlopen(\"http://google.de\")\npage_source = response.read()\n</code></pre>\n<p>For parsing the code, have a look at <code>BeautifulSoup</code>.</p>\n<p>BTW: what exactly do you want to do:</p>\n<blockquote>\n<p>Just for background, I need to download a page and replace any img with ones I have</p>\n</blockquote>\n<p><strong>Edit:</strong> It's 2014 now, most of the important libraries have been ported, and you should definitely use Python 3 if you can. <a href=\"http://docs.python-requests.org/en/latest/\" rel=\"noreferrer\"><code>python-requests</code></a> is a very nice high-level library which is easier to use than <code>urllib2</code>.</p>\n", "abstract": "Use Python 2.7, is has more 3rd party libs at the moment. (Edit: see below). I recommend you using the stdlib module urllib2, it will allow you to comfortably get web resources.\nExample: For parsing the code, have a look at BeautifulSoup. BTW: what exactly do you want to do: Just for background, I need to download a page and replace any img with ones I have Edit: It's 2014 now, most of the important libraries have been ported, and you should definitely use Python 3 if you can. python-requests is a very nice high-level library which is easier to use than urllib2."}, {"id": 49294529, "score": 11, "vote": 0, "content": "<p>An Example with <code>python3</code> and the <code>requests</code> library as mentioned by @leoluk:</p>\n<pre><code class=\"python\">pip install requests\n</code></pre>\n<p>Script req.py:</p>\n<pre><code class=\"python\">import requests\n\nurl='http://localhost'\n\n# in case you need a session\ncd = { 'sessionid': '123..'}\n\nr = requests.get(url, cookies=cd)\n# or without a session: r = requests.get(url)\nr.content\n</code></pre>\n<p>Now,execute it and you will get the html source of localhost!</p>\n<p><code>python3 req.py</code></p>\n", "abstract": "An Example with python3 and the requests library as mentioned by @leoluk: Script req.py: Now,execute it and you will get the html source of localhost! python3 req.py"}, {"id": 62045102, "score": 3, "vote": 0, "content": "<p>If you are using <code>Python &gt; 3.x</code> you don't need to install any libraries, this is directly built in the python framework. The old <code>urllib2</code> package has been renamed to <code>urllib</code>:</p>\n<pre><code class=\"python\">from urllib import request\n\nresponse = request.urlopen(\"https://www.google.com\")\n# set the correct charset below\npage_source = response.read().decode('utf-8')\nprint(page_source)\n</code></pre>\n", "abstract": "If you are using Python > 3.x you don't need to install any libraries, this is directly built in the python framework. The old urllib2 package has been renamed to urllib:"}, {"id": 3533666, "score": 0, "vote": 0, "content": "<p>The first thing you need to do is read the <a href=\"http://www.w3.org/Protocols/rfc2616/rfc2616.html\" rel=\"nofollow noreferrer\">HTTP spec</a> which will explain what you can expect to receive over the wire.  The data returned inside the content will be the \"rendered\" web page, not the source.  The source could be a JSP, a servlet, a CGI script, in short, just about anything, and you have no access to that.  You only get the HTML that the server sent you.  In the case of a static HTML page, then yes, you will be seeing the \"source\".  But for anything else you see the generated HTML, not the source.</p>\n<p>When you say <code>modify the page and return the modified page</code> what do you mean?</p>\n", "abstract": "The first thing you need to do is read the HTTP spec which will explain what you can expect to receive over the wire.  The data returned inside the content will be the \"rendered\" web page, not the source.  The source could be a JSP, a servlet, a CGI script, in short, just about anything, and you have no access to that.  You only get the HTML that the server sent you.  In the case of a static HTML page, then yes, you will be seeing the \"source\".  But for anything else you see the generated HTML, not the source. When you say modify the page and return the modified page what do you mean?"}]}, {"link": "https://stackoverflow.com/questions/26901882/what-is-the-easiest-way-to-run-python-scripts-in-a-cloud-server", "question": {"id": "26901882", "title": "What is the easiest way to run python scripts in a cloud server?", "content": "<p>I have a web crawling python script that takes hours to complete, and is infeasible to run in its entirety on my local machine. Is there a convenient way to deploy this to a simple web server? The script basically downloads webpages into text files. How would this be best accomplished?\nThanks!</p>\n", "abstract": "I have a web crawling python script that takes hours to complete, and is infeasible to run in its entirety on my local machine. Is there a convenient way to deploy this to a simple web server? The script basically downloads webpages into text files. How would this be best accomplished?\nThanks!"}, "answers": [{"id": 26902071, "score": 9, "vote": 0, "content": "<p>Since you said that performance is a problem and you are doing web-scraping, first thing to try is a <a href=\"http://scrapy.org/\" rel=\"noreferrer\"><code>Scrapy</code></a> framework - it is a very fast and easy to use web-scraping framework. <a href=\"http://scrapyd.readthedocs.org/en/latest/\" rel=\"noreferrer\"><code>scrapyd</code></a> tool would allow you to distribute the crawling - you can have multiple <code>scrapyd</code> services running on different servers and split the load between each. See:</p>\n<ul>\n<li><a href=\"http://doc.scrapy.org/en/latest/topics/practices.html#distributed-crawls\" rel=\"noreferrer\">Distributed crawls</a></li>\n<li><a href=\"http://seminar.io/2013/03/26/running-scrapy-on-amazon-ec2/\" rel=\"noreferrer\">Running Scrapy on Amazon EC2</a></li>\n</ul>\n<p>There is also a <a href=\"http://scrapinghub.com/scrapy-cloud\" rel=\"noreferrer\"><code>Scrapy Cloud</code></a> service out there:</p>\n<blockquote>\n<p>Scrapy Cloud bridges the highly efficient Scrapy development\n  environment with a robust, fully-featured production environment to\n  deploy and run your crawls. It's like a Heroku for Scrapy, although\n  other technologies will be supported in the near future. It runs on\n  top of the Scrapinghub platform, which means your project can scale on\n  demand, as needed.</p>\n</blockquote>\n", "abstract": "Since you said that performance is a problem and you are doing web-scraping, first thing to try is a Scrapy framework - it is a very fast and easy to use web-scraping framework. scrapyd tool would allow you to distribute the crawling - you can have multiple scrapyd services running on different servers and split the load between each. See: There is also a Scrapy Cloud service out there: Scrapy Cloud bridges the highly efficient Scrapy development\n  environment with a robust, fully-featured production environment to\n  deploy and run your crawls. It's like a Heroku for Scrapy, although\n  other technologies will be supported in the near future. It runs on\n  top of the Scrapinghub platform, which means your project can scale on\n  demand, as needed."}, {"id": 41726193, "score": 6, "vote": 0, "content": "<p>As an alternative to the solutions already given, I would suggest <strong>Heroku</strong>. You can not only deploy easily a website, but also scripts for bots to run.</p>\n<p>Basic account is free and is pretty flexible.</p>\n<p><a href=\"http://briancaffey.github.io/2016/04/05/twitter-bot-tutorial.html\" rel=\"noreferrer\">This blog entry</a>, <a href=\"https://bigishdata.com/2016/12/15/running-python-background-jobs-with-heroku/\" rel=\"noreferrer\">this one</a> and <a href=\"https://www.youtube.com/watch?v=DwWPunpypNA\" rel=\"noreferrer\">this video</a> contain practical examples of how to make it work.</p>\n", "abstract": "As an alternative to the solutions already given, I would suggest Heroku. You can not only deploy easily a website, but also scripts for bots to run. Basic account is free and is pretty flexible. This blog entry, this one and this video contain practical examples of how to make it work."}, {"id": 26902234, "score": 4, "vote": 0, "content": "<p>There are multiple places where you can do that. Just google for \"python in the cloud\", you will come up with a few, for example <a href=\"https://www.pythonanywhere.com/\" rel=\"nofollow\">https://www.pythonanywhere.com/</a>.</p>\n<p>In addition, there are also several cloud IDEs that essentially give you a small VM for free where you can develop your code in a web-based IDE and also run it in the VM, one example is <a href=\"http://www.c9.io\" rel=\"nofollow\">http://www.c9.io</a>.</p>\n", "abstract": "There are multiple places where you can do that. Just google for \"python in the cloud\", you will come up with a few, for example https://www.pythonanywhere.com/. In addition, there are also several cloud IDEs that essentially give you a small VM for free where you can develop your code in a web-based IDE and also run it in the VM, one example is http://www.c9.io."}, {"id": 67290539, "score": 2, "vote": 0, "content": "<p>In 2021, Replit.com makes it very easy to write and run Python in the cloud.</p>\n", "abstract": "In 2021, Replit.com makes it very easy to write and run Python in the cloud."}, {"id": 65263186, "score": -1, "vote": 0, "content": "<p>If you have a google e-mail account you have an access to google drive and utilities. Choose for colaboratory (or find it in more... options first). This \"CoLab\" is essentially your python notebook on google drive with full access to your files on your drive, also with access to your GitHub. So, in addition to your local stuff you can edit your GitHub scripts as well.</p>\n", "abstract": "If you have a google e-mail account you have an access to google drive and utilities. Choose for colaboratory (or find it in more... options first). This \"CoLab\" is essentially your python notebook on google drive with full access to your files on your drive, also with access to your GitHub. So, in addition to your local stuff you can edit your GitHub scripts as well."}]}, {"link": "https://stackoverflow.com/questions/22082938/how-do-scrapy-rules-work-with-crawl-spider", "question": {"id": "22082938", "title": "How do Scrapy rules work with crawl spider", "content": "<p>I have hard time to understand scrapy crawl spider rules. I have example that doesn't work as I would like it did, so it can be two things:</p>\n<ol>\n<li>I don't understand how rules work.</li>\n<li>I formed incorrect regex that prevents me to get results that I need.</li>\n</ol>\n<p>OK here it is what I want to do:</p>\n<p>I want to write crawl spider that will get all available statistics information from <a href=\"http://www.euroleague.net\" rel=\"noreferrer\">http://www.euroleague.net</a> website.\nThe website page that hosts all the information that I need for the start is <a href=\"http://www.euroleague.net/main/results/by-date\" rel=\"noreferrer\">here</a>.</p>\n<p><strong>Step 1</strong></p>\n<p>First step what I am thinking is extract \"Seasons\" link(s) and fallow it.\nHere it is HTML/href that I am intending to match (I want to match all links in the \"Seasons\" section one by one, but I think that it will be easer to have one link as an example):</p>\n<pre><code class=\"python\">href=\"/main/results/by-date?seasoncode=E2001\"\n</code></pre>\n<p>And here is a rule/regex that I created for it:</p>\n<pre><code class=\"python\">Rule(SgmlLinkExtractor(allow=('by-date\\?seasoncode\\=E\\d+',)),follow=True),\n</code></pre>\n<p><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/AhJ2M.png\"/></p>\n<p><strong>Step 2</strong></p>\n<p>When I am brought by spider to the web page <a href=\"http://www.euroleague.net/main/results/by-date?seasoncode=E2001\" rel=\"noreferrer\">http://www.euroleague.net/main/results/by-date?seasoncode=E2001</a> for the second step I want that spider extracted link(s) from section \"Regular season\". At this case lets say it should be \"Round 1\". The HTML/href that I am looking for is:</p>\n<pre><code class=\"python\">&lt;a href=\"/main/results/by-date?seasoncode=E2001&amp;gamenumber=1&amp;phasetypecode=RS\"\n</code></pre>\n<p>And rule/regex that I constructed would be:</p>\n<pre><code class=\"python\">Rule(SgmlLinkExtractor(allow=('seasoncode\\=E\\d+\\&amp;gamenumber\\=\\d+\\&amp;phasetypecode\\=\\w+',)),follow=True),\n</code></pre>\n<p><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/6pLhL.png\"/></p>\n<p><strong>Step 3</strong></p>\n<p>Now I reached page (<a href=\"http://www.euroleague.net/main/results/by-date?seasoncode=E2001&amp;gamenumber=1&amp;phasetypecode=RS\" rel=\"noreferrer\">http://www.euroleague.net/main/results/by-date?seasoncode=E2001&amp;gamenumber=1&amp;phasetypecode=RS</a>) I am ready to extract links that leads to the pages that has all the information that I need:\nI am looking for HTML/href:</p>\n<pre><code class=\"python\">href=\"/main/results/showgame?gamenumber=1&amp;phasetypecode=RS&amp;gamecode=4&amp;seasoncode=E2001#!boxscore\"\n</code></pre>\n<p>And my regex that has to follow would be:</p>\n<pre><code class=\"python\">Rule(SgmlLinkExtractor(allow=('gamenumber\\=\\d+\\&amp;phasetypecode\\=\\w+\\&amp;gamecode\\=\\d+\\&amp;seasoncode\\=E\\d+',)),callback='parse_item'),\n</code></pre>\n<p><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/1qF7o.png\"/></p>\n<p><strong>The problem</strong></p>\n<p>I think that crawler should work something like this:\nThat rules crawler is something like a loop. When first link is matched the crawler will follow to the \"Step 2\" page, than to \"step 3\" and after that it will extract data. After doing that it will return to \"step 1\" to match second link and start loop again to the point when there is no links in first step.</p>\n<p>What I see from terminal it seems that crawler loops in \"Step 1\". It loops through all \"Step 1\" links, but doesn't involves \"step 2\"/\"step 3\" rules. </p>\n<pre><code class=\"python\">2014-02-28 00:20:31+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2000&gt; (referer: http://  www.euroleague.net/main/results/by-date)\n2014-02-28 00:20:31+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2001&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 00:20:31+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2002&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 00:20:32+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2003&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 00:20:33+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2004&gt; (referer: http://www.euroleague.net/main/results/by-date)\n</code></pre>\n<p>After it loops through all the \"Seasons\" links it starts with links that I don't see, in any of three steps that I mentioned:</p>\n<pre><code class=\"python\">http://www.euroleague.net/main/results/by-date?gamenumber=23&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013\n</code></pre>\n<p>And such link structure you can find only if you loop through all the links in \"Step 2\" without returning to the \"Step 1\" starting point. </p>\n<p><strong>The question would be:</strong>\nHow rules work? Is it working step by step like I am intending it should work with this example or every rule has it's own loop and goes from rule to rule only after it's finished looping through the first rule?</p>\n<p>That is how I see it. Of course it could be something wrong with my rules/regex and it is very possible.</p>\n<p>And here is all what I am getting from the terminal:</p>\n<pre><code class=\"python\">scrapy crawl basketsp_test -o item6.xml -t xml\n2014-02-28 01:09:20+0200 [scrapy] INFO: Scrapy 0.20.0 started (bot: basketbase)\n2014-02-28 01:09:20+0200 [scrapy] DEBUG: Optional features available: ssl, http11, boto, django\n2014-02-28 01:09:20+0200 [scrapy] DEBUG: Overridden settings: {'NEWSPIDER_MODULE': 'basketbase.spiders', 'FEED_FORMAT': 'xml', 'SPIDER_MODULES': ['basketbase.spiders'], 'FEED_URI': 'item6.xml', 'BOT_NAME': 'basketbase'}\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Enabled extensions: FeedExporter, LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Enabled item pipelines: Basketpipeline3, Basketpipeline1db\n2014-02-28 01:09:21+0200 [basketsp_test] INFO: Spider opened\n2014-02-28 01:09:21+0200 [basketsp_test] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2014-02-28 01:09:21+0200 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2014-02-28 01:09:21+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date&gt; (referer: None)\n2014-02-28 01:09:22+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:22+0200 [basketsp_test] DEBUG: Filtered duplicate request: &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2013&gt; - no more duplicates will be shown (see DUPEFILTER_CLASS)\n2014-02-28 01:09:22+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2000&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:23+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2001&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:23+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2002&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:24+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2003&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:24+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2004&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:25+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2005&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:26+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2006&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:26+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2007&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:27+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2008&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:27+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2009&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:28+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2010&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:29+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2011&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:29+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?seasoncode=E2012&gt; (referer: http://www.euroleague.net/main/results/by-date)\n2014-02-28 01:09:30+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=24&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:30+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=23&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:31+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=22&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:32+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=21&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:32+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=20&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:33+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=19&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:34+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=18&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:34+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=17&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:35+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=16&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:35+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=15&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:36+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=14&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:37+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=13&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:37+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=12&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:38+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=11&amp;phasetypecode=TS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:39+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=10&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:39+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=9&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:40+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=8&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:40+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=7&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:41+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=6&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:42+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=5&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:42+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=4&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:43+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=3&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:44+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=2&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:44+0200 [basketsp_test] DEBUG: Crawled (200) &lt;GET http://www.euroleague.net/main/results/by-date?gamenumber=1&amp;phasetypecode=RS++++++++&amp;seasoncode=E2013&gt; (referer: http://www.euroleague.net/main/results/by-date?seasoncode=E2013)\n2014-02-28 01:09:44+0200 [basketsp_test] INFO: Closing spider (finished)\n2014-02-28 01:09:44+0200 [basketsp_test] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 13663,\n     'downloader/request_count': 39,\n     'downloader/request_method_count/GET': 39,\n     'downloader/response_bytes': 527838,\n     'downloader/response_count': 39,\n     'downloader/response_status_count/200': 39,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 2, 27, 23, 9, 44, 569579),\n     'log_count/DEBUG': 46,\n     'log_count/INFO': 3,\n     'request_depth_max': 2,\n     'response_received_count': 39,\n     'scheduler/dequeued': 39,\n     'scheduler/dequeued/memory': 39,\n     'scheduler/enqueued': 39,\n     'scheduler/enqueued/memory': 39,\n     'start_time': datetime.datetime(2014, 2, 27, 23, 9, 21, 111255)}\n2014-02-28 01:09:44+0200 [basketsp_test] INFO: Spider closed (finished)\n</code></pre>\n<p>And here is a rules part from the crawler:</p>\n<pre><code class=\"python\">class Basketspider(CrawlSpider):\n    name = \"basketsp_test\"\n    download_delay = 0.5\n\n    allowed_domains = [\"www.euroleague.net\"]\n    start_urls = [\"http://www.euroleague.net/main/results/by-date\"]\n    rules = (\n        Rule(SgmlLinkExtractor(allow=('by-date\\?seasoncode\\=E\\d+',)),follow=True),\n        Rule(SgmlLinkExtractor(allow=('seasoncode\\=E\\d+\\&amp;gamenumber\\=\\d+\\&amp;phasetypecode\\=\\w+',)),follow=True),\n        Rule(SgmlLinkExtractor(allow=('gamenumber\\=\\d+\\&amp;phasetypecode\\=\\w+\\&amp;gamecode\\=\\d+\\&amp;seasoncode\\=E\\d+',)),callback='parse_item'),\n\n\n\n)  \n</code></pre>\n", "abstract": "I have hard time to understand scrapy crawl spider rules. I have example that doesn't work as I would like it did, so it can be two things: OK here it is what I want to do: I want to write crawl spider that will get all available statistics information from http://www.euroleague.net website.\nThe website page that hosts all the information that I need for the start is here. Step 1 First step what I am thinking is extract \"Seasons\" link(s) and fallow it.\nHere it is HTML/href that I am intending to match (I want to match all links in the \"Seasons\" section one by one, but I think that it will be easer to have one link as an example): And here is a rule/regex that I created for it:  Step 2 When I am brought by spider to the web page http://www.euroleague.net/main/results/by-date?seasoncode=E2001 for the second step I want that spider extracted link(s) from section \"Regular season\". At this case lets say it should be \"Round 1\". The HTML/href that I am looking for is: And rule/regex that I constructed would be:  Step 3 Now I reached page (http://www.euroleague.net/main/results/by-date?seasoncode=E2001&gamenumber=1&phasetypecode=RS) I am ready to extract links that leads to the pages that has all the information that I need:\nI am looking for HTML/href: And my regex that has to follow would be:  The problem I think that crawler should work something like this:\nThat rules crawler is something like a loop. When first link is matched the crawler will follow to the \"Step 2\" page, than to \"step 3\" and after that it will extract data. After doing that it will return to \"step 1\" to match second link and start loop again to the point when there is no links in first step. What I see from terminal it seems that crawler loops in \"Step 1\". It loops through all \"Step 1\" links, but doesn't involves \"step 2\"/\"step 3\" rules.  After it loops through all the \"Seasons\" links it starts with links that I don't see, in any of three steps that I mentioned: And such link structure you can find only if you loop through all the links in \"Step 2\" without returning to the \"Step 1\" starting point.  The question would be:\nHow rules work? Is it working step by step like I am intending it should work with this example or every rule has it's own loop and goes from rule to rule only after it's finished looping through the first rule? That is how I see it. Of course it could be something wrong with my rules/regex and it is very possible. And here is all what I am getting from the terminal: And here is a rules part from the crawler:"}, "answers": [{"id": 22094696, "score": 15, "vote": 0, "content": "<p>You are right, according to the <a href=\"https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spiders/crawl.py#L51-54\" rel=\"noreferrer\">source code</a> before returning each response to the callback function, the crawler loops over the Rules, starting, from the first. You should have it in mind, when you write the rules. For example the following rules:</p>\n<pre><code class=\"python\">rules(\n        Rule(SgmlLinkExtractor(allow=(r'/items',)), callback='parse_item',follow=True),\n        Rule(SgmlLinkExtractor(allow=(r'/items/electronics',)), callback='parse_electronic_item',follow=True),\n     )\n</code></pre>\n<p>The second rule will never be applied since all the links will be extracted by the first rule with <strong>parse_item</strong> callback. The matches for the second rule will be filtered out as duplicates by the <code>scrapy.dupefilter.RFPDupeFilter</code>. You should use deny for correct matching of links:</p>\n<pre><code class=\"python\">rules(\n        Rule(SgmlLinkExtractor(allow=(r'/items',)), deny=(r'/items/electronics',), callback='parse_item',follow=True),\n        Rule(SgmlLinkExtractor(allow=(r'/items/electronics',)), callback='parse_electronic_item',follow=True),\n     )\n</code></pre>\n", "abstract": "You are right, according to the source code before returning each response to the callback function, the crawler loops over the Rules, starting, from the first. You should have it in mind, when you write the rules. For example the following rules: The second rule will never be applied since all the links will be extracted by the first rule with parse_item callback. The matches for the second rule will be filtered out as duplicates by the scrapy.dupefilter.RFPDupeFilter. You should use deny for correct matching of links:"}, {"id": 42685097, "score": 9, "vote": 0, "content": "<p>If you are from china, I have a chinese blog post about this:</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/25650763\" rel=\"noreferrer\">\u522b\u518d\u6ee5\u7528scrapy CrawlSpider\u4e2d\u7684follow=True</a></p>\n<hr/>\n<p>Let's check out how the rules work under the hood:</p>\n<pre><code class=\"python\">def _requests_to_follow(self, response):\n    seen = set()\n    for n, rule in enumerate(self._rules):\n        links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                 if lnk not in seen]\n        for link in links:\n            seen.add(link)\n            r = Request(url=link.url, callback=self._response_downloaded)\n            yield r\n</code></pre>\n<p>as you can see, when we follow a link, the link in the response is extracted by all the rule using a for loop then add them to a set object.</p>\n<p>and all the response will be handled by <code>self._response_downloaded</code>:</p>\n<pre><code class=\"python\">def _response_downloaded(self, response):\n    rule = self._rules[response.meta['rule']]\n    return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)\n\ndef _parse_response(self, response, callback, cb_kwargs, follow=True):\n\n    if callback:\n        cb_res = callback(response, **cb_kwargs) or ()\n        cb_res = self.process_results(response, cb_res)\n        for requests_or_item in iterate_spider_output(cb_res):\n            yield requests_or_item\n\n    # follow will go back to the rules again\n    if follow and self._follow_links:\n        for request_or_item in self._requests_to_follow(response):\n            yield request_or_item\n</code></pre>\n<p>and it goes back to the <code>self._requests_to_follow(response)</code> again and again.</p>\n<p>In summary:<a href=\"https://zhuanlan.zhihu.com/p/25650763\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/5Qd8m.jpg\"/></a></p>\n", "abstract": "If you are from china, I have a chinese blog post about this: \u522b\u518d\u6ee5\u7528scrapy CrawlSpider\u4e2d\u7684follow=True Let's check out how the rules work under the hood: as you can see, when we follow a link, the link in the response is extracted by all the rule using a for loop then add them to a set object. and all the response will be handled by self._response_downloaded: and it goes back to the self._requests_to_follow(response) again and again. In summary:"}, {"id": 22094662, "score": 7, "vote": 0, "content": "<p>I would be tempted to use a BaseSpider scraper instead of a crawler. Using a basespider you can have more of a flow of intended request routes instead of finding ALL hrefs on the page and visiting them based on global rules. Use yield Requests() to continue looping through the parent sets of links and callbacks to pass the output object all the way to the end.</p>\n<p>From your description:</p>\n<blockquote>\n<p>I think that crawler should work something like this: That rules crawler is something like a loop. When first link is matched the crawler will follow to the \"Step 2\" page, than to \"step 3\" and after that it will extract data. After doing that it will return to \"step 1\" to match second link and start loop again to the point when there is no links in first step.</p>\n</blockquote>\n<p>A request callback stack like this would suit you very well. Since you know the order of the pages and which pages you need to scrape. This also has the added benefit of being able to collect information over multiple pages before returning the output object to be processed.</p>\n<pre><code class=\"python\">class Basketspider(BaseSpider, errorLog):\n    name = \"basketsp_test\"\n    download_delay = 0.5\n\n    def start_requests(self):\n\n        item = WhateverYourOutputItemIs()\n        yield Request(\"http://www.euroleague.net/main/results/by-date\", callback=self.parseSeasonsLinks, meta={'item':item})\n\n    def parseSeaseonsLinks(self, response):\n\n        item = response.meta['item'] \n\n        hxs = HtmlXPathSelector(response)\n\n        html = hxs.extract()\n        roundLinkList = list()\n\n        roundLinkPttern = re.compile(r'http://www\\.euroleague\\.net/main/results/by-date\\?gamenumber=\\d+&amp;phasetypecode=RS')\n\n        for (roundLink) in re.findall(roundLinkPttern, html):\n            if roundLink not in roundLinkList:\n                roundLinkList.append(roundLink)        \n\n        for i in range(len(roundLinkList)):\n\n            #if you wanna output this info in the final item\n            item['RoundLink'] = roundLinkList[i]\n\n            # Generate new request for round page\n            yield Request(stockpageUrl, callback=self.parseStockItem, meta={'item':item})\n\n\n    def parseRoundPAge(self, response):\n\n        item = response.meta['item'] \n        #Do whatever you need to do in here call more requests if needed or return item here\n\n        item['Thing'] = 'infoOnPage'\n        #....\n        #....\n        #....\n\n        return  item\n</code></pre>\n", "abstract": "I would be tempted to use a BaseSpider scraper instead of a crawler. Using a basespider you can have more of a flow of intended request routes instead of finding ALL hrefs on the page and visiting them based on global rules. Use yield Requests() to continue looping through the parent sets of links and callbacks to pass the output object all the way to the end. From your description: I think that crawler should work something like this: That rules crawler is something like a loop. When first link is matched the crawler will follow to the \"Step 2\" page, than to \"step 3\" and after that it will extract data. After doing that it will return to \"step 1\" to match second link and start loop again to the point when there is no links in first step. A request callback stack like this would suit you very well. Since you know the order of the pages and which pages you need to scrape. This also has the added benefit of being able to collect information over multiple pages before returning the output object to be processed."}]}, {"link": "https://stackoverflow.com/questions/8532252/scrapy-logging-to-file-and-stdout-simultaneously-with-spider-names", "question": {"id": "8532252", "title": "Scrapy - logging to file and stdout simultaneously, with spider names", "content": "<p>I've decided to use the Python logging module because the messages generated by Twisted on std error is too long, and I want to <code>INFO</code> level meaningful messages such as those generated by the <code>StatsCollector</code> to be written on a separate log file while maintaining the on screen messages.</p>\n<pre><code class=\"python\"> from twisted.python import log\n     import logging\n     logging.basicConfig(level=logging.INFO, filemode='w', filename='buyerlog.txt')\n     observer = log.PythonLoggingObserver()\n     observer.start()\n</code></pre>\n<p>Well, this is fine, I've got my messages, but the downside is that I do not know the messages are generated by which spider! This is my log file, with \"twisted\" being displayed by <code>%(name)s</code>:</p>\n<pre><code class=\"python\"> INFO:twisted:Log opened.\n  2 INFO:twisted:Scrapy 0.12.0.2543 started (bot: property)\n  3 INFO:twisted:scrapy.telnet.TelnetConsole starting on 6023\n  4 INFO:twisted:scrapy.webservice.WebService starting on 6080\n  5 INFO:twisted:Spider opened\n  6 INFO:twisted:Spider opened\n  7 INFO:twisted:Received SIGINT, shutting down gracefully. Send again to force unclean shutdown\n  8 INFO:twisted:Closing spider (shutdown)\n  9 INFO:twisted:Closing spider (shutdown)\n 10 INFO:twisted:Dumping spider stats:\n 11 {'downloader/exception_count': 3,\n 12  'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 3,\n 13  'downloader/request_bytes': 9973,\n</code></pre>\n<p>As compared to the messages generated from twisted on standard error:</p>\n<pre><code class=\"python\">2011-12-16 17:34:56+0800 [expats] DEBUG: number of rules: 4\n2011-12-16 17:34:56+0800 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2011-12-16 17:34:56+0800 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2011-12-16 17:34:56+0800 [iproperty] INFO: Spider opened\n2011-12-16 17:34:56+0800 [iproperty] DEBUG: Redirecting (301) to &lt;GET http://www.iproperty.com.sg/&gt; from &lt;GET http://iproperty.com.sg&gt;\n2011-12-16 17:34:57+0800 [iproperty] DEBUG: Crawled (200) &lt;\n</code></pre>\n<p>I've tried %(name)s, %(module)s amongst others but I don't seem to be able to show the spider name. Does anyone knows the answer?</p>\n<p>EDIT:\nthe problem with using <code>LOG_FILE</code> and <code>LOG_LEVEL</code> in settings is that the lower level messages will not be shown on std error.</p>\n", "abstract": "I've decided to use the Python logging module because the messages generated by Twisted on std error is too long, and I want to INFO level meaningful messages such as those generated by the StatsCollector to be written on a separate log file while maintaining the on screen messages. Well, this is fine, I've got my messages, but the downside is that I do not know the messages are generated by which spider! This is my log file, with \"twisted\" being displayed by %(name)s: As compared to the messages generated from twisted on standard error: I've tried %(name)s, %(module)s amongst others but I don't seem to be able to show the spider name. Does anyone knows the answer? EDIT:\nthe problem with using LOG_FILE and LOG_LEVEL in settings is that the lower level messages will not be shown on std error."}, "answers": [{"id": 8532689, "score": 24, "vote": 0, "content": "<p>You want to use the <a href=\"https://github.com/scrapy/scrapy/blob/master/scrapy/log.py#L37\"><code>ScrapyFileLogObserver</code></a>.</p>\n<pre><code class=\"python\">import logging\nfrom scrapy.log import ScrapyFileLogObserver\n\nlogfile = open('testlog.log', 'w')\nlog_observer = ScrapyFileLogObserver(logfile, level=logging.DEBUG)\nlog_observer.start()\n</code></pre>\n<p>I'm glad you asked this question, I've been wanting to do this myself.</p>\n", "abstract": "You want to use the ScrapyFileLogObserver. I'm glad you asked this question, I've been wanting to do this myself."}, {"id": 9446470, "score": 18, "vote": 0, "content": "<p>It is very easy to redirect output using: <code>scrapy some-scrapy's-args 2&gt;&amp;1 | tee -a logname</code></p>\n<p>This way, all what scrapy ouputs into stdout and stderr, will be redirected to a logname file and also, prited to the screen.</p>\n", "abstract": "It is very easy to redirect output using: scrapy some-scrapy's-args 2>&1 | tee -a logname This way, all what scrapy ouputs into stdout and stderr, will be redirected to a logname file and also, prited to the screen."}, {"id": 34134373, "score": 9, "vote": 0, "content": "<p>For all those folks who came here before reading the current <a href=\"http://doc.scrapy.org/en/latest/topics/logging.html?highlight=logging#module-scrapy.utils.log\" rel=\"noreferrer\">documentation</a> version:</p>\n<pre><code class=\"python\">import logging\nfrom scrapy.utils.log import configure_logging\n\nconfigure_logging(install_root_handler=False)\nlogging.basicConfig(\n    filename='log.txt',\n    filemode = 'a',\n    format='%(levelname)s: %(message)s',\n    level=logging.DEBUG\n)\n</code></pre>\n", "abstract": "For all those folks who came here before reading the current documentation version:"}, {"id": 16449857, "score": 5, "vote": 0, "content": "<p>I know this is old but it was a really helpful post since the class still isn't properly documented in the Scrapy docs. Also, we can skip importing logging and use scrapy logs directly. Thanks All!</p>\n<pre><code class=\"python\">from scrapy import log\n\nlogfile = open('testlog.log', 'a')\nlog_observer = log.ScrapyFileLogObserver(logfile, level=log.DEBUG)\nlog_observer.start()\n</code></pre>\n", "abstract": "I know this is old but it was a really helpful post since the class still isn't properly documented in the Scrapy docs. Also, we can skip importing logging and use scrapy logs directly. Thanks All!"}, {"id": 52181367, "score": 5, "vote": 0, "content": "<p>As the Scrapy Official Doc said:</p>\n<blockquote>\n<p>Scrapy uses Python\u2019s builtin logging system for event logging.</p>\n</blockquote>\n<p>So you can config your logger just as a normal Python script.</p>\n<p>First, you have to import the logging module:</p>\n<pre><code class=\"python\">import logging\n</code></pre>\n<p>You can add this line to your spider:</p>\n<pre><code class=\"python\">logging.getLogger().addHandler(logging.StreamHandler())\n</code></pre>\n<p>It adds a stream handler to log to console.</p>\n<p>After that, you have to config logging file path.</p>\n<p>Add a dict named <code>custom_settings</code> which consists of your spider-specified settings:</p>\n<pre><code class=\"python\">custom_settings = {\n     'LOG_FILE': 'my_log.log',\n     'LOG_LEVEL': 'INFO',\n     ... # you can add more settings\n }\n</code></pre>\n<p>The whole class looks like:</p>\n<pre><code class=\"python\">import logging\n\nclass AbcSpider(scrapy.Spider):\n    name: str = 'abc_spider'\n    start_urls = ['you_url']\n    custom_settings = {\n         'LOG_FILE': 'my_log.log',\n         'LOG_LEVEL': 'INFO',\n         ... # you can add more settings\n     }\n     logging.getLogger().addHandler(logging.StreamHandler())\n\n     def parse(self, response):\n        pass\n</code></pre>\n", "abstract": "As the Scrapy Official Doc said: Scrapy uses Python\u2019s builtin logging system for event logging. So you can config your logger just as a normal Python script. First, you have to import the logging module: You can add this line to your spider: It adds a stream handler to log to console. After that, you have to config logging file path. Add a dict named custom_settings which consists of your spider-specified settings: The whole class looks like:"}, {"id": 53354012, "score": 2, "vote": 0, "content": "<p>ScrapyFileLogObserver is no longer supported. You may use standard python logging module. </p>\n<pre><code class=\"python\">import logging\nlogging.getLogger().addHandler(logging.StreamHandler())\n</code></pre>\n", "abstract": "ScrapyFileLogObserver is no longer supported. You may use standard python logging module. "}, {"id": 64617052, "score": 1, "vote": 0, "content": "<p>As of Scrapy 2.3, none of the answers mentioned above worked for me.\nIn addition, the solution found in the <a href=\"https://docs.scrapy.org/en/latest/topics/practices.html#run-from-script\" rel=\"nofollow noreferrer\">documentation</a> caused overwriting of the log file with every message, which is of course not what you want in a log.\nI couldn't find a built-in setting that changed the mode to \"a\" (append).\nI achieved logging to both file and stdout with the following configuration code:</p>\n<pre><code class=\"python\">configure_logging(settings={\n    \"LOG_STDOUT\": True\n})\nfile_handler = logging.FileHandler(filename, mode=\"a\")\nformatter = logging.Formatter(\n    fmt=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n    datefmt=\"%H:%M:%S\"\n)\nfile_handler.setFormatter(formatter)\nfile_handler.setLevel(\"DEBUG\")\nlogging.root.addHandler(file_handler) \n</code></pre>\n", "abstract": "As of Scrapy 2.3, none of the answers mentioned above worked for me.\nIn addition, the solution found in the documentation caused overwriting of the log file with every message, which is of course not what you want in a log.\nI couldn't find a built-in setting that changed the mode to \"a\" (append).\nI achieved logging to both file and stdout with the following configuration code:"}, {"id": 73208322, "score": 0, "vote": 0, "content": "<p>Another way is to disable Scrapy's log setting and use custom setting file.</p>\n<p>settings.py</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import logging\nimport yaml\n\nLOG_ENABLED = False\nlogging.config.dictConfig(yaml.load(open(\"logging.yml\").read(), Loader=yaml.SafeLoader))\n</code></pre>\n<p>logging.yml</p>\n<pre class=\"lang-yaml prettyprint-override\"><code class=\"python\">version: 1\nformatters:\n  simple:\n    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: INFO\n    formatter: simple\n    stream: ext://sys.stdout\n  file:\n    class : logging.FileHandler\n    level: INFO\n    formatter: simple\n    filename: scrapy.log\nroot:\n  level: INFO\n  handlers: [console, file]\ndisable_existing_loggers: False\n</code></pre>\n<p>example_spider.py</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import scrapy\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\"http://example.com/\"]\n\n    def parse(self, response):\n        self.logger.info(\"test\")\n        pass\n</code></pre>\n", "abstract": "Another way is to disable Scrapy's log setting and use custom setting file. settings.py logging.yml example_spider.py"}]}, {"link": "https://stackoverflow.com/questions/46746701/scrapy-http-status-code-is-not-handled-or-not-allowed", "question": {"id": "46746701", "title": "Scrapy: HTTP status code is not handled or not allowed?", "content": "<p>I want to get product title,link,price in category <a href=\"https://tiki.vn/dien-thoai-may-tinh-bang/c1789\" rel=\"noreferrer\">https://tiki.vn/dien-thoai-may-tinh-bang/c1789</a></p>\n<p>But it fails \"HTTP status code is not handled or not allowed\":</p>\n<p><a href=\"https://i.stack.imgur.com/KCFw2.jpg\" rel=\"noreferrer\"><img alt=\"error log\" src=\"https://i.stack.imgur.com/KCFw2.jpg\"/></a></p>\n<p>My file: spiders/tiki.py</p>\n<pre><code class=\"python\">import scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\nfrom stackdata.items import StackdataItem\n\n\nclass StackdataSpider(CrawlSpider):\n    name = \"tiki\"\n    allowed_domains = [\"tiki.vn\"]\n    start_urls = [\n        \"https://tiki.vn/dien-thoai-may-tinh-bang/c1789\",\n    ]\n\n    rules = (\n        Rule(LinkExtractor(allow=r\"\\?page=2\"),\n             callback=\"parse_item\", follow=True),\n    )\n\n    def parse_item(self, response):\n        questions = response.xpath('//div[@class=\"product-item\"]')\n\n        for question in questions:\n            question_location = question.xpath(\n                '//a/@href').extract()[0]\n            full_url = response.urljoin(question_location)\n            yield scrapy.Request(full_url, callback=self.parse_question)\n\n    def parse_question(self, response):\n        item = StackdataItem()\n        item[\"title\"] = response.css(\n            \".item-box h1::text\").extract()[0]\n        item[\"url\"] = response.url\n        item[\"content\"] = response.css(\n            \".price span::text\").extract()[0]\n        yield item\n</code></pre>\n<p>File: items.py</p>\n<pre><code class=\"python\">import scrapy\n\n\nclass StackdataItem(scrapy.Item):\n    title = scrapy.Field()\n    url = scrapy.Field()\n    price = scrapy.Field()\n</code></pre>\n<p>Please help me!!!! thanks!</p>\n", "abstract": "I want to get product title,link,price in category https://tiki.vn/dien-thoai-may-tinh-bang/c1789 But it fails \"HTTP status code is not handled or not allowed\":  My file: spiders/tiki.py File: items.py Please help me!!!! thanks!"}, "answers": [{"id": 46747093, "score": 44, "vote": 0, "content": "<h2>tl;dr</h2>\n<p>You are being blocked based on scrapy's user-agent.</p>\n<p>You have two options:</p>\n<ol>\n<li>Grant the wish of the website and do not scrape them, or</li>\n<li>Change your user-agent</li>\n</ol>\n<p>I assume you want to take option 2.</p>\n<p>Go to your <code>settings.py</code> in your scrapy project and set your user-agent to a non-default value. Either your own project name (it probably should not contain the word <code>scrapy</code>) or a standard browser's user-agent.</p>\n<pre><code class=\"python\">USER_AGENT='my-cool-project (http://example.com)'\n</code></pre>\n<h2>Detailed error analysis</h2>\n<p>We all want to learn, so here is an explanation of how I got to this result and what you can do if you see such behavior again.</p>\n<p>The website <em>tiki.vn</em> seems to return <a href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\" rel=\"noreferrer\">HTTP status 404</a> for all requests of your spider. You can see in your screenshot that you get a 404 for both your requests to <code>/robots.txt</code> and <code>/dien-thoai-may-tinh-bang/c1789</code>.</p>\n<p>404 means \"not found\" and web servers use this to show that a URL does not exist. However, if we check the same sites manually, we can see that both sites contain valid content. Now, it could technically be possible that these sites return both content and a 404 error code at the same time, but we can check this with the developer console of our browser (e.g. Chrome or Firefox).</p>\n<p><a href=\"https://i.stack.imgur.com/FZLiN.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/FZLiN.png\"/></a></p>\n<p>Here we can see that robots.txt returns a valid 200 status code.</p>\n<h2>Further investigations to be done</h2>\n<p>Many web sites try to restrict scraping, so they try to detect scraping behavior. So, they will look at some indicators and decide if they will serve content to you or block your request. I assume that exactly this is what's happening to you.</p>\n<p>I wanted to crawl one website, which worked totally fine from my home PC, but did not respond at all (not even 404) to any request from my server (scrapy, wget, curl, ...).</p>\n<p>Next steps you'll have to take to analyze the reason for this issue:</p>\n<ul>\n<li>Can you reach the website from your home PC (and do you get Status code 200)?</li>\n<li>What happens if you run scrapy from your home PC? Still 404?</li>\n<li>Try to load the website from the server, on which you run scrapy (e.g. with wget or curl)</li>\n</ul>\n<p>You can fetch it with wget like this:</p>\n<pre><code class=\"python\">wget https://tiki.vn/dien-thoai-may-tinh-bang/c1789\n</code></pre>\n<p>wget does send a custom user-agent, so you might want to set it to a <a href=\"https://techblog.willshouse.com/2012/01/03/most-common-user-agents/\" rel=\"noreferrer\">web browser's user-agent</a> if this command does not work (it does from my PC).</p>\n<pre><code class=\"python\">wget -U 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36' https://tiki.vn/dien-thoai-may-tinh-bang/c1789\n</code></pre>\n<p>This will help you to find out if the problem is with the server (e.g. they blocked the IP or a whole IP range) or if you need to make some modifications to your spider.</p>\n<h2>Checking the user-agent</h2>\n<p>If it works with wget for your server, I would suspect the user-agent of scrapy to be the problem. <a href=\"https://doc.scrapy.org/en/latest/topics/settings.html#user-agent\" rel=\"noreferrer\">According to the documentation</a>, scrapy does use <code>Scrapy/VERSION (+http://scrapy.org)</code> as the user-agent unless you set it yourself. It's quite possible that they block your spider based on the user-agent.</p>\n<p>So, you have to go to <code>settings.py</code> in your scrapy project and look for the settings <code>USER_AGENT</code> there. Now, set this to anything which does not contain the keyword <code>scrapy</code>. If you want to be nice, use your project name + domain, otherwise use a standard browser user-agent.</p>\n<p>Nice variant:</p>\n<pre><code class=\"python\">USER_AGENT='my-cool-project (http://example.com)'\n</code></pre>\n<p>Not so nice (but common in scraping) variant:</p>\n<pre><code class=\"python\">USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n</code></pre>\n<p>In fact, I was able to verify that they block on the user-agent with this wget command from my local PC:</p>\n<pre><code class=\"python\">wget -U 'Scrapy/1.3.0 (+http://scrapy.org)' https://tiki.vn/dien-thoai-may-tinh-bang/c1789\n</code></pre>\n<p>which results in</p>\n<pre><code class=\"python\">--2017-10-14 18:54:04--  https://tiki.vn/dien-thoai-may-tinh-bang/c1789\nLoaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\nResolving tiki.vn... 203.162.81.188\nConnecting to tiki.vn|203.162.81.188|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2017-10-14 18:54:06 ERROR 404: Not Found.\n</code></pre>\n", "abstract": "You are being blocked based on scrapy's user-agent. You have two options: I assume you want to take option 2. Go to your settings.py in your scrapy project and set your user-agent to a non-default value. Either your own project name (it probably should not contain the word scrapy) or a standard browser's user-agent. We all want to learn, so here is an explanation of how I got to this result and what you can do if you see such behavior again. The website tiki.vn seems to return HTTP status 404 for all requests of your spider. You can see in your screenshot that you get a 404 for both your requests to /robots.txt and /dien-thoai-may-tinh-bang/c1789. 404 means \"not found\" and web servers use this to show that a URL does not exist. However, if we check the same sites manually, we can see that both sites contain valid content. Now, it could technically be possible that these sites return both content and a 404 error code at the same time, but we can check this with the developer console of our browser (e.g. Chrome or Firefox).  Here we can see that robots.txt returns a valid 200 status code. Many web sites try to restrict scraping, so they try to detect scraping behavior. So, they will look at some indicators and decide if they will serve content to you or block your request. I assume that exactly this is what's happening to you. I wanted to crawl one website, which worked totally fine from my home PC, but did not respond at all (not even 404) to any request from my server (scrapy, wget, curl, ...). Next steps you'll have to take to analyze the reason for this issue: You can fetch it with wget like this: wget does send a custom user-agent, so you might want to set it to a web browser's user-agent if this command does not work (it does from my PC). This will help you to find out if the problem is with the server (e.g. they blocked the IP or a whole IP range) or if you need to make some modifications to your spider. If it works with wget for your server, I would suspect the user-agent of scrapy to be the problem. According to the documentation, scrapy does use Scrapy/VERSION (+http://scrapy.org) as the user-agent unless you set it yourself. It's quite possible that they block your spider based on the user-agent. So, you have to go to settings.py in your scrapy project and look for the settings USER_AGENT there. Now, set this to anything which does not contain the keyword scrapy. If you want to be nice, use your project name + domain, otherwise use a standard browser user-agent. Nice variant: Not so nice (but common in scraping) variant: In fact, I was able to verify that they block on the user-agent with this wget command from my local PC: which results in"}, {"id": 53623459, "score": 2, "vote": 0, "content": "<p>Apart from the <a href=\"https://stackoverflow.com/a/46747093/2869160\">Aufziehvogel</a> user agent change, please refer the http error code also. In your case http error code is 404, which indicates a CLIENT ERROR(<a href=\"https://httpstatuses.com/404\" rel=\"nofollow noreferrer\">NOT FOUND</a>). </p>\n<p>If the website requires an authenticated session to scrape the content then the http error code might be 401 which indicates a CLIENT ERROR(<a href=\"https://httpstatuses.com/401\" rel=\"nofollow noreferrer\">UNAUTHORIZED</a>)</p>\n", "abstract": "Apart from the Aufziehvogel user agent change, please refer the http error code also. In your case http error code is 404, which indicates a CLIENT ERROR(NOT FOUND).  If the website requires an authenticated session to scrape the content then the http error code might be 401 which indicates a CLIENT ERROR(UNAUTHORIZED)"}, {"id": 64084614, "score": 1, "vote": 0, "content": "<p>Try to change your user-agent using the following command:</p>\n<pre><code class=\"python\">scrapy shell -s USER_AGENT='custom user agent' 'http://www.example.com'\n</code></pre>\n<p>Sometimes, the websites block specific user-agents to prevent from crawling and sending too many requests. Then you can see whether it works or not using <code>view(response)</code> command.</p>\n<p>For the class-based implementation <a href=\"https://stackoverflow.com/a/46747093/7310077\">@Aufziehvogel</a> tells the story very complete!</p>\n", "abstract": "Try to change your user-agent using the following command: Sometimes, the websites block specific user-agents to prevent from crawling and sending too many requests. Then you can see whether it works or not using view(response) command. For the class-based implementation @Aufziehvogel tells the story very complete!"}, {"id": 63333846, "score": 0, "vote": 0, "content": "<p>In addition to the other two good answers, try the following:</p>\n<ol>\n<li>Set the value of <code>'Redirect_enabled':</code>  to <code>true</code></li>\n<li>Use <code>https</code> or <code>http</code> protocols.</li>\n<li>Use or remove www prefix.</li>\n</ol>\n", "abstract": "In addition to the other two good answers, try the following:"}]}, {"link": "https://stackoverflow.com/questions/24761074/is-scrapy-single-threaded-or-multi-threaded", "question": {"id": "24761074", "title": "Is Scrapy single-threaded or multi-threaded?", "content": "<p>There are few concurrency settings in Scrapy, like <a href=\"http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-requests\" rel=\"noreferrer\">CONCURRENT_REQUESTS</a>. Does it mean, that Scrapy crawler is multi-threaded? So if I run <code>scrapy crawl my_crawler</code> it will literally fire multiple simultaneous requests in parallel? \nIm asking because, I've read that Scrapy is single-threaded.</p>\n", "abstract": "There are few concurrency settings in Scrapy, like CONCURRENT_REQUESTS. Does it mean, that Scrapy crawler is multi-threaded? So if I run scrapy crawl my_crawler it will literally fire multiple simultaneous requests in parallel? \nIm asking because, I've read that Scrapy is single-threaded."}, "answers": [{"id": 24761824, "score": 15, "vote": 0, "content": "<p>Scrapy is single-threaded, except the interactive shell and some tests, see <a href=\"https://github.com/scrapy/scrapy/search?q=thread&amp;type=Code\" rel=\"noreferrer\">source</a>.</p>\n<p>It's built on top of <a href=\"https://twistedmatrix.com/trac/wiki/Documentation\" rel=\"noreferrer\">Twisted</a>, which is single-threaded too, and makes use of it's own asynchronous concurrency capabilities, such as <a href=\"http://twistedmatrix.com/documents/current/api/twisted.internet.interfaces.IReactorThreads.html#callFromThread\" rel=\"noreferrer\"><code>twisted.internet.interfaces.IReactorThreads.callFromThread</code></a>, see <a href=\"https://github.com/scrapy/scrapy/search?q=callFromThread&amp;type=Code\" rel=\"noreferrer\">source</a>.</p>\n", "abstract": "Scrapy is single-threaded, except the interactive shell and some tests, see source. It's built on top of Twisted, which is single-threaded too, and makes use of it's own asynchronous concurrency capabilities, such as twisted.internet.interfaces.IReactorThreads.callFromThread, see source."}, {"id": 24771189, "score": 7, "vote": 0, "content": "<p>Scrapy does most of it's work synchronously. However, the handling of requests is done asynchronously. </p>\n<p>I suggest this page if you haven't already seen it.</p>\n<p><a href=\"http://doc.scrapy.org/en/latest/topics/architecture.html\" rel=\"noreferrer\">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p>\n<p>edit:\nI realize now the question was about threading and not necessarily whether it's asynchronous or not. That link would still be a good read though :)</p>\n<p>regarding your question about CONCURRENT_REQUESTS. This setting changes the number of requests that twisted will defer at once. Once that many requests have been started it will wait for some of them to finish before starting more.</p>\n", "abstract": "Scrapy does most of it's work synchronously. However, the handling of requests is done asynchronously.  I suggest this page if you haven't already seen it. http://doc.scrapy.org/en/latest/topics/architecture.html edit:\nI realize now the question was about threading and not necessarily whether it's asynchronous or not. That link would still be a good read though :) regarding your question about CONCURRENT_REQUESTS. This setting changes the number of requests that twisted will defer at once. Once that many requests have been started it will wait for some of them to finish before starting more."}, {"id": 37548285, "score": 3, "vote": 0, "content": "<p>Scrapy is single-threaded framework, we cannot use multiple threads within a spider at the same time. However, we can create multiple spiders and piplines at the same time to make the process concurrent.\nScrapy does not support <code>multi-threading</code> because it is built on <code>Twisted</code>, which is an <code>Asynchronous http protocol framework</code>.</p>\n", "abstract": "Scrapy is single-threaded framework, we cannot use multiple threads within a spider at the same time. However, we can create multiple spiders and piplines at the same time to make the process concurrent.\nScrapy does not support multi-threading because it is built on Twisted, which is an Asynchronous http protocol framework."}, {"id": 73441309, "score": 3, "vote": 0, "content": "<p>Scrapy is a single-threaded framework, But we can use <strong>Multiple threads</strong> within a spider at the same time.</p>\n<p>please read this article.</p>\n<p><a href=\"https://levelup.gitconnected.com/how-to-run-scrapy-spiders-in-your-program-7db56792c1f7#:%7E:text=We%20use%20the%20CrawlerProcess%20class,custom%20settings%20for%20the%20Spider\" rel=\"nofollow noreferrer\">https://levelup.gitconnected.com/how-to-run-scrapy-spiders-in-your-program-7db56792c1f7#:~:text=We%20use%20the%20CrawlerProcess%20class,custom%20settings%20for%20the%20Spider</a></p>\n<p>We can use subprocess to run spiders.</p>\n<pre><code class=\"python\">import subprocess\nsubprocess.run([\"scrapy\", \"crawl\", \"quotes\", \"-o\", \"quotes_all.json\"])\n</code></pre>\n<p>or</p>\n<p>Use <strong>CrawlerProcess</strong> to run multiple spiders in the same process.</p>\n<p>If you want to run multiple spiders per process or want to fetch and use the scraped items directly in your program, you would need to use the internal API of Scrapy.</p>\n<pre><code class=\"python\">    # Run the spider with the internal API of Scrapy:\n    from scrapy.crawler import Crawler, CrawlerProcess\n    from scrapy.utils.project import get_project_settings\n\ndef crawler_func(spider, url):\n    crawler_process = CrawlerProcess(settings)\n    crawler_process.crawl(spider, url)\n    crawler_process.start()\n\ndef start_spider(spider, urls):\n      p = multiprocessing.Pool(100)\n      return p.map(partial(crawler_func, spider), urls)\n</code></pre>\n", "abstract": "Scrapy is a single-threaded framework, But we can use Multiple threads within a spider at the same time. please read this article. https://levelup.gitconnected.com/how-to-run-scrapy-spiders-in-your-program-7db56792c1f7#:~:text=We%20use%20the%20CrawlerProcess%20class,custom%20settings%20for%20the%20Spider We can use subprocess to run spiders. or Use CrawlerProcess to run multiple spiders in the same process. If you want to run multiple spiders per process or want to fetch and use the scraped items directly in your program, you would need to use the internal API of Scrapy."}]}, {"link": "https://stackoverflow.com/questions/17872753/what-is-the-difference-between-scrapys-spider-middleware-and-downloader-middlew", "question": {"id": "17872753", "title": "What is the difference between Scrapy&#39;s spider middleware and downloader middleware?", "content": "<p>Both middleware can process Request and Response. But what is the difference?</p>\n", "abstract": "Both middleware can process Request and Response. But what is the difference?"}, "answers": [{"id": 17872865, "score": 23, "vote": 0, "content": "<p>While they have almost identical interfaces, they serve different purposes:</p>\n<ul>\n<li><p>Downloader middlewares modify requests and responses or generate requests in response to responses. They don't directly interact with spiders. Some examples are middlewares that implement cookies, caching, proxies, redirects, setting user-agent headers, etc. They just add functionality to the downloader system.</p></li>\n<li><p>Spider middlewares modify things that pass in and out of spiders, like requests, items, exceptions, and <code>start_requests</code>. They do share some basic functionality with downloader middlewares, but they can't generate requests in response to responses. They stand between the spiders and the downloader. One example is filtering out responses with bad HTTP status codes.</p></li>\n</ul>\n<p>Some middlewares can function as either a downloader middleware or a spider middleware, but they're often trivial and will be forced into one category or the other once you add more complex functionality.</p>\n", "abstract": "While they have almost identical interfaces, they serve different purposes: Downloader middlewares modify requests and responses or generate requests in response to responses. They don't directly interact with spiders. Some examples are middlewares that implement cookies, caching, proxies, redirects, setting user-agent headers, etc. They just add functionality to the downloader system. Spider middlewares modify things that pass in and out of spiders, like requests, items, exceptions, and start_requests. They do share some basic functionality with downloader middlewares, but they can't generate requests in response to responses. They stand between the spiders and the downloader. One example is filtering out responses with bad HTTP status codes. Some middlewares can function as either a downloader middleware or a spider middleware, but they're often trivial and will be forced into one category or the other once you add more complex functionality."}]}, {"link": "https://stackoverflow.com/questions/15517483/how-to-extract-urls-from-an-html-page-in-python", "question": {"id": "15517483", "title": "How to extract URLs from an HTML page in Python", "content": "<p>I have to write a web crawler in Python. I don't know how to parse a page and extract the URLs from HTML. Where should I go and study to write such a program?</p>\n<p>In other words, is there a simple python program which can be used as a template for a generic web crawler? Ideally it should use modules which are relatively simple to use and it should include plenty of comments to describe what each line of code is doing.  </p>\n", "abstract": "I have to write a web crawler in Python. I don't know how to parse a page and extract the URLs from HTML. Where should I go and study to write such a program? In other words, is there a simple python program which can be used as a template for a generic web crawler? Ideally it should use modules which are relatively simple to use and it should include plenty of comments to describe what each line of code is doing.  "}, "answers": [{"id": 15517610, "score": 21, "vote": 0, "content": "<p>Look at example code below. The script extracts html code of a web page (here Python home page) and extracts all the links in that page. Hope this helps.</p>\n<pre><code class=\"python\">#!/usr/bin/env python\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = \"http://www.python.org\"\nresponse = requests.get(url)\n# parse html\npage = str(BeautifulSoup(response.content))\n\n\ndef getURL(page):\n    \"\"\"\n\n    :param page: html of web page (here: Python home page) \n    :return: urls in that page \n    \"\"\"\n    start_link = page.find(\"a href\")\n    if start_link == -1:\n        return None, 0\n    start_quote = page.find('\"', start_link)\n    end_quote = page.find('\"', start_quote + 1)\n    url = page[start_quote + 1: end_quote]\n    return url, end_quote\n\nwhile True:\n    url, n = getURL(page)\n    page = page[n:]\n    if url:\n        print(url)\n    else:\n        break\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code class=\"python\">/\n#left-hand-navigation\n#content-body\n/search\n/about/\n/news/\n/doc/\n/download/\n/getit/\n/community/\n/psf/\nhttp://docs.python.org/devguide/\n/about/help/\nhttp://pypi.python.org/pypi\n/download/releases/2.7.3/\nhttp://docs.python.org/2/\n/ftp/python/2.7.3/python-2.7.3.msi\n/ftp/python/2.7.3/Python-2.7.3.tar.bz2\n/download/releases/3.3.0/\nhttp://docs.python.org/3/\n/ftp/python/3.3.0/python-3.3.0.msi\n/ftp/python/3.3.0/Python-3.3.0.tar.bz2\n/community/jobs/\n/community/merchandise/\n/psf/donations/\nhttp://wiki.python.org/moin/Languages\nhttp://wiki.python.org/moin/Languages\nhttp://www.google.com/calendar/ical/b6v58qvojllt0i6ql654r1vh00%40group.calendar.google.com/public/basic.ics\nhttp://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\nhttp://pycon.org/#calendar\nhttp://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\nhttp://pycon.org/#calendar\nhttp://www.psfmember.org\n</code></pre>\n<p>...</p>\n", "abstract": "Look at example code below. The script extracts html code of a web page (here Python home page) and extracts all the links in that page. Hope this helps. Output: ..."}, {"id": 15518277, "score": 19, "vote": 0, "content": "<p>You can use <a href=\"http://www.crummy.com/software/BeautifulSoup/\" rel=\"noreferrer\">BeautifulSoup</a> as many have also stated. It can parse HTML,XML etc. To see some of it's features, see <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\" rel=\"noreferrer\">here</a>.</p>\n<p>Example:</p>\n<pre><code class=\"python\">import urllib2\nfrom bs4 import BeautifulSoup\nurl = 'http://www.google.co.in/'\n\nconn = urllib2.urlopen(url)\nhtml = conn.read()\n\nsoup = BeautifulSoup(html)\nlinks = soup.find_all('a')\n\nfor tag in links:\n    link = tag.get('href',None)\n    if link is not None:\n        print link\n</code></pre>\n", "abstract": "You can use BeautifulSoup as many have also stated. It can parse HTML,XML etc. To see some of it's features, see here. Example:"}, {"id": 15518253, "score": 6, "vote": 0, "content": "<pre><code class=\"python\">import sys\nimport re\nimport urllib2\nimport urlparse\ntocrawl = set([\"http://www.facebook.com/\"])\ncrawled = set([])\nkeywordregex = re.compile('&lt;meta\\sname=[\"\\']keywords[\"\\']\\scontent=[\"\\'](.*?)[\"\\']\\s/&gt;')\nlinkregex = re.compile('&lt;a\\s*href=[\\'|\"](.*?)[\\'\"].*?&gt;')\n\nwhile 1:\n    try:\n        crawling = tocrawl.pop()\n        print crawling\n    except KeyError:\n        raise StopIteration\n    url = urlparse.urlparse(crawling)\n    try:\n        response = urllib2.urlopen(crawling)\n    except:\n        continue\n    msg = response.read()\n    startPos = msg.find('&lt;title&gt;')\n    if startPos != -1:\n        endPos = msg.find('&lt;/title&gt;', startPos+7)\n        if endPos != -1:\n            title = msg[startPos+7:endPos]\n            print title\n    keywordlist = keywordregex.findall(msg)\n    if len(keywordlist) &gt; 0:\n        keywordlist = keywordlist[0]\n        keywordlist = keywordlist.split(\", \")\n        print keywordlist\n    links = linkregex.findall(msg)\n    crawled.add(crawling)\n    for link in (links.pop(0) for _ in xrange(len(links))):\n        if link.startswith('/'):\n            link = 'http://' + url[1] + link\n        elif link.startswith('#'):\n            link = 'http://' + url[1] + url[2] + link\n        elif not link.startswith('http'):\n            link = 'http://' + url[1] + '/' + link\n        if link not in crawled:\n            tocrawl.add(link)\n</code></pre>\n<p>Referenced to: <a href=\"http://ryanmerl.com/2009/02/14/python-web-crawler-in-less-than-50-lines/\" rel=\"noreferrer\">Python Web Crawler in Less Than 50 Lines</a> (Slow or no longer works, does not load for me)</p>\n", "abstract": "Referenced to: Python Web Crawler in Less Than 50 Lines (Slow or no longer works, does not load for me)"}, {"id": 15517686, "score": 5, "vote": 0, "content": "<p>You can use <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/\" rel=\"noreferrer\">beautifulsoup</a>. Follow the documentation and see what matches your requirements. The documentation contains code snippets for how to extract URL's as well.</p>\n<pre><code class=\"python\">from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html_doc)\n\nsoup.find_all('a') # Finds all hrefs from the html doc.\n</code></pre>\n", "abstract": "You can use beautifulsoup. Follow the documentation and see what matches your requirements. The documentation contains code snippets for how to extract URL's as well."}, {"id": 15517656, "score": 3, "vote": 0, "content": "<p>With parsing pages, check out the <a href=\"http://www.crummy.com/software/BeautifulSoup/\" rel=\"nofollow noreferrer\"><code>BeautifulSoup</code></a> module. It's simple to use and allows you to parse pages with HTML. You can extract URLs from the HTML simply by doing <code>str.find('a')</code></p>\n<p><a href=\"https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454\" title=\"t\">Don't use regular expressions for parsing HTML</a></p>\n", "abstract": "With parsing pages, check out the BeautifulSoup module. It's simple to use and allows you to parse pages with HTML. You can extract URLs from the HTML simply by doing str.find('a') Don't use regular expressions for parsing HTML"}]}, {"link": "https://stackoverflow.com/questions/17721782/is-it-possible-for-scrapy-to-get-plain-text-from-raw-html-data", "question": {"id": "17721782", "title": "Is it possible for Scrapy to get plain text from raw HTML data?", "content": "<p>For example:</p>\n<pre><code class=\"python\">scrapy shell http://scrapy.org/\ncontent = hxs.select('//*[@id=\"content\"]').extract()[0]\nprint content\n</code></pre>\n<p>Then, I get the following raw HTML code:</p>\n<pre class=\"lang-html prettyprint-override\"><code class=\"python\">&lt;div id=\"content\"&gt;\n\n\n  &lt;h2&gt;Welcome to Scrapy&lt;/h2&gt;\n\n  &lt;h3&gt;What is Scrapy?&lt;/h3&gt;\n\n  &lt;p&gt;Scrapy is a fast high-level screen scraping and web crawling\n    framework, used to crawl websites and extract structured data from their\n    pages. It can be used for a wide range of purposes, from data mining to\n    monitoring and automated testing.&lt;/p&gt;\n\n  &lt;h3&gt;Features&lt;/h3&gt;\n\n  &lt;dl&gt;\n\n    &lt;dt&gt;Simple&lt;/dt&gt;\n    &lt;dt&gt;\n    &lt;/dt&gt;\n    &lt;dd&gt;Scrapy was designed with simplicity in mind, by providing the features\n      you need without getting in your way\n    &lt;/dd&gt;\n\n    &lt;dt&gt;Productive&lt;/dt&gt;\n    &lt;dd&gt;Just write the rules to extract the data from web pages and let Scrapy\n      crawl the entire web site for you\n    &lt;/dd&gt;\n\n    &lt;dt&gt;Fast&lt;/dt&gt;\n    &lt;dd&gt;Scrapy is used in production crawlers to completely scrape more than\n      500 retailer sites daily, all in one server\n    &lt;/dd&gt;\n\n    &lt;dt&gt;Extensible&lt;/dt&gt;\n    &lt;dd&gt;Scrapy was designed with extensibility in mind and so it provides\n      several mechanisms to plug new code without having to touch the framework\n      core\n\n    &lt;/dd&gt;\n    &lt;dt&gt;Portable, open-source, 100% Python&lt;/dt&gt;\n    &lt;dd&gt;Scrapy is completely written in Python and runs on Linux, Windows, Mac and BSD&lt;/dd&gt;\n\n    &lt;dt&gt;Batteries included&lt;/dt&gt;\n    &lt;dd&gt;Scrapy comes with lots of functionality built in. Check &lt;a\n        href=\"http://doc.scrapy.org/en/latest/intro/overview.html#what-else\"&gt;this\n      section&lt;/a&gt; of the documentation for a list of them.\n    &lt;/dd&gt;\n\n    &lt;dt&gt;Well-documented &amp;amp; well-tested&lt;/dt&gt;\n    &lt;dd&gt;Scrapy is &lt;a href=\"/doc/\"&gt;extensively documented&lt;/a&gt; and has an comprehensive test suite\n      with &lt;a href=\"http://static.scrapy.org/coverage-report/\"&gt;very good code\n        coverage&lt;/a&gt;&lt;/dd&gt;\n\n    &lt;dt&gt;&lt;a href=\"/community\"&gt;Healthy community&lt;/a&gt;&lt;/dt&gt;\n    &lt;dd&gt;\n      1,500 watchers, 350 forks on Github (&lt;a href=\"https://github.com/scrapy/scrapy\"&gt;link&lt;/a&gt;)&lt;br&gt;\n      700 followers on Twitter (&lt;a href=\"http://twitter.com/ScrapyProject\"&gt;link&lt;/a&gt;)&lt;br&gt;\n      850 questions on StackOverflow (&lt;a href=\"http://stackoverflow.com/tags/scrapy/info\"&gt;link&lt;/a&gt;)&lt;br&gt;\n      200 messages per month on mailing list (&lt;a\n        href=\"https://groups.google.com/forum/?fromgroups#!aboutgroup/scrapy-users\"&gt;link&lt;/a&gt;)&lt;br&gt;\n      40-50 users always connected to IRC channel (&lt;a href=\"http://webchat.freenode.net/?channels=scrapy\"&gt;link&lt;/a&gt;)\n    &lt;/dd&gt;\n\n    &lt;dt&gt;&lt;a href=\"/support\"&gt;Commercial support&lt;/a&gt;&lt;/dt&gt;\n    &lt;dd&gt;A few companies provide Scrapy consulting and support&lt;/dd&gt;\n\n    &lt;p&gt;Still not sure if Scrapy is what you're looking for?. Check out &lt;a\n        href=\"http://doc.scrapy.org/en/latest/intro/overview.html\"&gt;Scrapy at a\n      glance&lt;/a&gt;.\n\n    &lt;/p&gt;\n    &lt;h3&gt;Companies using Scrapy&lt;/h3&gt;\n\n    &lt;p&gt;Scrapy is being used in large production environments, to crawl\n      thousands of sites daily. Here is a list of &lt;a href=\"/companies/\"&gt;Companies\n        using Scrapy&lt;/a&gt;.&lt;/p&gt;\n\n    &lt;h3&gt;Where to start?&lt;/h3&gt;\n\n    &lt;p&gt;Start by reading &lt;a href=\"http://doc.scrapy.org/en/latest/intro/overview.html\"&gt;Scrapy at a glance&lt;/a&gt;,\n      then &lt;a href=\"/download/\"&gt;download Scrapy&lt;/a&gt; and follow the &lt;a\n          href=\"http://doc.scrapy.org/en/latest/intro/tutorial.html\"&gt;Tutorial&lt;/a&gt;.\n\n\n    &lt;/p&gt;&lt;/dl&gt;\n&lt;/div&gt;\n</code></pre>\n<p>But I want to get <strong>plain text</strong> directly from scrapy.</p>\n<p>I do not want to use any xPath selectors to extract the <code>p</code>, <code>h2</code>, <code>h3</code>... tags, since I am crawling a website whose main content is embedded into a <code>table</code>, <code>tbody</code>; recursively. It can be a tedious task to find the xPath.</p>\n<p>Can this be implemented by a built in function in Scrapy? Or do I need external tools to convert it? I have read through all of Scrapy's docs, but have gained nothing.</p>\n<p>This is a sample site which can convert raw HTML into plain text: <a href=\"http://beaker.mailchimp.com/html-to-text\" rel=\"nofollow noreferrer\">http://beaker.mailchimp.com/html-to-text</a></p>\n", "abstract": "For example: Then, I get the following raw HTML code: But I want to get plain text directly from scrapy. I do not want to use any xPath selectors to extract the p, h2, h3... tags, since I am crawling a website whose main content is embedded into a table, tbody; recursively. It can be a tedious task to find the xPath. Can this be implemented by a built in function in Scrapy? Or do I need external tools to convert it? I have read through all of Scrapy's docs, but have gained nothing. This is a sample site which can convert raw HTML into plain text: http://beaker.mailchimp.com/html-to-text"}, "answers": [{"id": 17722635, "score": 22, "vote": 0, "content": "<p>Scrapy doesn't have such functionality built-in. <a href=\"https://github.com/aaronsw/html2text\" rel=\"noreferrer\">html2text</a> is what you are looking for.</p>\n<p>Here's a sample spider that scrapes <a href=\"http://en.wikipedia.org/wiki/Python_%28programming_language%29\" rel=\"noreferrer\">wikipedia's python page</a>, gets first paragraph using xpath and converts html into plain text using <code>html2text</code>:</p>\n<pre><code class=\"python\">from scrapy.selector import HtmlXPathSelector\nfrom scrapy.spider import BaseSpider\nimport html2text\n\n\nclass WikiSpider(BaseSpider):\n    name = \"wiki_spider\"\n    allowed_domains = [\"www.wikipedia.org\"]\n    start_urls = [\"http://en.wikipedia.org/wiki/Python_(programming_language)\"]\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n        sample = hxs.select(\"//div[@id='mw-content-text']/p[1]\").extract()[0]\n\n        converter = html2text.HTML2Text()\n        converter.ignore_links = True\n        print(converter.handle(sample)) #Python 3 print syntax\n</code></pre>\n<p>prints:</p>\n<blockquote>\n<p>**Python** is a widely used general-purpose, high-level programming language.[11][12][13] Its design philosophy emphasizes code\n  readability, and its syntax allows programmers to express concepts in\n  fewer lines of code than would be possible in languages such as\n  C.[14][15] The language provides constructs intended to enable clear\n  programs on both a small and large scale.[16]</p>\n</blockquote>\n", "abstract": "Scrapy doesn't have such functionality built-in. html2text is what you are looking for. Here's a sample spider that scrapes wikipedia's python page, gets first paragraph using xpath and converts html into plain text using html2text: prints: **Python** is a widely used general-purpose, high-level programming language.[11][12][13] Its design philosophy emphasizes code\n  readability, and its syntax allows programmers to express concepts in\n  fewer lines of code than would be possible in languages such as\n  C.[14][15] The language provides constructs intended to enable clear\n  programs on both a small and large scale.[16]"}, {"id": 17727287, "score": 19, "vote": 0, "content": "<p>Another solution using <code>lxml.html</code>'s <code>tostring()</code> with parameter <code>method=\"text\"</code>. <code>lxml</code> is used in Scrapy internally. (parameter <code>encoding=unicode</code> is usually what you want.)</p>\n<p>See <a href=\"http://lxml.de/api/lxml.html-module.html\">http://lxml.de/api/lxml.html-module.html</a> for details.</p>\n<pre><code class=\"python\">from scrapy.spider import BaseSpider\nimport lxml.etree\nimport lxml.html\n\nclass WikiSpider(BaseSpider):\n    name = \"wiki_spider\"\n    allowed_domains = [\"www.wikipedia.org\"]\n    start_urls = [\"http://en.wikipedia.org/wiki/Python_(programming_language)\"]\n\n    def parse(self, response):\n        root = lxml.html.fromstring(response.body)\n\n        # optionally remove tags that are not usually rendered in browsers\n        # javascript, HTML/HEAD, comments, add the tag names you dont want at the end\n        lxml.etree.strip_elements(root, lxml.etree.Comment, \"script\", \"head\")\n\n        # complete text\n        print lxml.html.tostring(root, method=\"text\", encoding=unicode)\n\n        # or same as in alecxe's example spider,\n        # pinpoint a part of the document using XPath\n        #for p in root.xpath(\"//div[@id='mw-content-text']/p[1]\"):\n        #   print lxml.html.tostring(p, method=\"text\")\n</code></pre>\n", "abstract": "Another solution using lxml.html's tostring() with parameter method=\"text\". lxml is used in Scrapy internally. (parameter encoding=unicode is usually what you want.) See http://lxml.de/api/lxml.html-module.html for details."}, {"id": 41579014, "score": 4, "vote": 0, "content": "<p>At this moment, I don't think you need to install any 3rd party library. scrapy <a href=\"https://doc.scrapy.org/en/latest/topics/selectors.html#using-text-nodes-in-a-condition\" rel=\"nofollow noreferrer\">provides</a> this functionality using selectors:<br/>\nAssume this complex selector:\n    </p>\n<pre><code class=\"python\">sel = Selector(text='&lt;a href=\"#\"&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;')\n</code></pre>\n<p>we can get the entire text using: \n    </p>\n<pre><code class=\"python\">text_content = sel.xpath(\"//a[1]//text()\").extract()\n# which results [u'Click here to go to the ', u'Next Page']\n</code></pre>\n<p>then you can join them together easily: \n    </p>\n<pre><code class=\"python\">   ' '.join(text_content)\n   # Click here to go to the Next Page\n</code></pre>\n", "abstract": "At this moment, I don't think you need to install any 3rd party library. scrapy provides this functionality using selectors:\nAssume this complex selector:\n     we can get the entire text using: \n     then you can join them together easily: \n    "}]}, {"link": "https://stackoverflow.com/questions/15836062/scrapy-crawlspider-doesnt-crawl-the-first-landing-page", "question": {"id": "15836062", "title": "Scrapy CrawlSpider doesn&#39;t crawl the first landing page", "content": "<p>I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.\nAlthough the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:</p>\n<pre><code class=\"python\">class DownloadSpider(CrawlSpider):\n    name = 'downloader'\n    allowed_domains = ['bnt-chemicals.de']\n    start_urls = [\n        \"http://www.bnt-chemicals.de\"        \n        ]\n    rules = (   \n        Rule(SgmlLinkExtractor(aloow='prod'), callback='parse_item', follow=True),\n        )\n    fname = 1\n\n    def parse_item(self, response):\n        open(str(self.fname)+ '.txt', 'a').write(response.url)\n        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))\n        open(str(self.fname)+ '.txt', 'a').write('\\n')\n        open(str(self.fname)+ '.txt', 'a').write(response.body)\n        open(str(self.fname)+ '.txt', 'a').write('\\n')\n        self.fname = self.fname + 1\n</code></pre>\n", "abstract": "I am new to Scrapy and I am working on a scraping exercise and I am using the CrawlSpider.\nAlthough the Scrapy framework works beautifully and it follows the relevant links, I can't seem to make the CrawlSpider to scrape the very first link (the home page / landing page). Instead it goes directly to scrape the links determined by the rule but doesn't scrape the landing page on which the links are. I don't know how to fix this since it is not recommended to overwrite the parse method for a CrawlSpider. Modifying follow=True/False also doesn't yield any good results. Here is the snippet of code:"}, "answers": [{"id": 15839428, "score": 22, "vote": 0, "content": "<p>Just change your callback to <code>parse_start_url</code> and override it:</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nclass DownloadSpider(CrawlSpider):\n    name = 'downloader'\n    allowed_domains = ['bnt-chemicals.de']\n    start_urls = [\n        \"http://www.bnt-chemicals.de\",\n    ]\n    rules = (\n        Rule(SgmlLinkExtractor(allow='prod'), callback='parse_start_url', follow=True),\n    )\n    fname = 0\n\n    def parse_start_url(self, response):\n        self.fname += 1\n        fname = '%s.txt' % self.fname\n\n        with open(fname, 'w') as f:\n            f.write('%s, %s\\n' % (response.url, response.meta.get('depth', 0)))\n            f.write('%s\\n' % response.body)\n</code></pre>\n", "abstract": "Just change your callback to parse_start_url and override it:"}, {"id": 15839394, "score": 17, "vote": 0, "content": "<p>There's a number of ways of doing this, but one of the simplest is to implement <code>parse_start_url</code> and then modify <code>start_urls</code></p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\n\nclass DownloadSpider(CrawlSpider):\n    name = 'downloader'\n    allowed_domains = ['bnt-chemicals.de']\n    start_urls = [\"http://www.bnt-chemicals.de/tunnel/index.htm\"]\n    rules = (\n        Rule(SgmlLinkExtractor(allow='prod'), callback='parse_item', follow=True),\n        )\n    fname = 1\n\n    def parse_start_url(self, response):\n        return self.parse_item(response)\n\n\n    def parse_item(self, response):\n        open(str(self.fname)+ '.txt', 'a').write(response.url)\n        open(str(self.fname)+ '.txt', 'a').write(','+ str(response.meta['depth']))\n        open(str(self.fname)+ '.txt', 'a').write('\\n')\n        open(str(self.fname)+ '.txt', 'a').write(response.body)\n        open(str(self.fname)+ '.txt', 'a').write('\\n')\n        self.fname = self.fname + 1\n</code></pre>\n", "abstract": "There's a number of ways of doing this, but one of the simplest is to implement parse_start_url and then modify start_urls"}]}, {"link": "https://stackoverflow.com/questions/21008953/python-requests-1-how-to-disable-keep-alive", "question": {"id": "21008953", "title": "Python-Requests (&gt;= 1.*): How to disable keep-alive?", "content": "<p>I'm trying to program a simple web-crawler using the Requests module, and I would like to know how to disable its -default- keep-alive feauture.</p>\n<p>I tried using:</p>\n<pre><code class=\"python\">s = requests.session()\ns.config['keep_alive'] = False\n</code></pre>\n<p>However, I get an error stating that session object has no attribute 'config', I think it was changed with the new version, but i cannot seem to find how to do it in the official documentation.</p>\n<p>The truth is when I run the crawler on a specific website, it only gets five pages at most, and then keeps looping around infinitely, so I thought it has something to do with the keep-alive feature!</p>\n<p>PS: is Requests a good module for a web-crawler? is there something more adapted?</p>\n<p>Thank you !</p>\n", "abstract": "I'm trying to program a simple web-crawler using the Requests module, and I would like to know how to disable its -default- keep-alive feauture. I tried using: However, I get an error stating that session object has no attribute 'config', I think it was changed with the new version, but i cannot seem to find how to do it in the official documentation. The truth is when I run the crawler on a specific website, it only gets five pages at most, and then keeps looping around infinitely, so I thought it has something to do with the keep-alive feature! PS: is Requests a good module for a web-crawler? is there something more adapted? Thank you !"}, "answers": [{"id": 21009511, "score": 21, "vote": 0, "content": "<p>This works</p>\n<pre><code class=\"python\">s = requests.session()\ns.keep_alive = False\n</code></pre>\n<p><a href=\"https://stackoverflow.com/questions/10115126/python-requests-close-http-connection#comment22579942_10115553\">Answered in the comments of a similar question.</a></p>\n", "abstract": "This works Answered in the comments of a similar question."}, {"id": 21009040, "score": 9, "vote": 0, "content": "<p>I am not sure but can you try passing {\"Connection\": \"close\"} as HTTP headers when sending a GET request using requests. This will close the connection as soon a server returns a response.</p>\n<pre><code class=\"python\">&gt;&gt;&gt; headers = {\"Connection\": \"close\"}\n&gt;&gt;&gt; r = requests.get('https://example.xcom', headers=headers)\n</code></pre>\n", "abstract": "I am not sure but can you try passing {\"Connection\": \"close\"} as HTTP headers when sending a GET request using requests. This will close the connection as soon a server returns a response."}, {"id": 31173262, "score": 4, "vote": 0, "content": "<p>As @praveen suggested it's expected from us to use <code>HTTP/1.1</code> header  <code>Connection: close</code> to notify the server that the connection should be closed after completion of the response.</p>\n<p>Here is how it's described in <a href=\"http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.10\" rel=\"nofollow\">RFC 2616</a>:</p>\n<blockquote>\n<p>HTTP/1.1 defines the \"close\" connection option for the sender to signal that  the connection will be closed after completion of the response. For example,</p>\n<pre><code class=\"python\">Connection: close\n</code></pre>\n<p>in either the request or the response header fields indicates that the connection SHOULD NOT be considered `persistent' (section 8.1) after the current request/response is complete.</p>\n<p>HTTP/1.1 applications that do not support persistent connections MUST include the \"close\" connection option in every message.</p>\n</blockquote>\n", "abstract": "As @praveen suggested it's expected from us to use HTTP/1.1 header  Connection: close to notify the server that the connection should be closed after completion of the response. Here is how it's described in RFC 2616: HTTP/1.1 defines the \"close\" connection option for the sender to signal that  the connection will be closed after completion of the response. For example, in either the request or the response header fields indicates that the connection SHOULD NOT be considered `persistent' (section 8.1) after the current request/response is complete. HTTP/1.1 applications that do not support persistent connections MUST include the \"close\" connection option in every message."}]}, {"link": "https://stackoverflow.com/questions/53973423/getting-value-after-button-click-with-beautifulsoup-python", "question": {"id": "53973423", "title": "Getting value after button click with BeautifulSoup Python", "content": "<p>I'm trying to get a value that is given by the website after a click on a button.</p>\n<p>Here is the website: <a href=\"https://www.4devs.com.br/gerador_de_cpf\" rel=\"noreferrer\">https://www.4devs.com.br/gerador_de_cpf</a></p>\n<p>You can see that there is a button called \"Gerar CPF\", this button provides a number that appears after the click.</p>\n<p>My current script opens the browser and get the value, but I'm getting the value from the page before the click, so the value is empty. I would like to know if it is possible to get the value after the click on the button.</p>\n<pre><code class=\"python\">from selenium import webdriver\nfrom bs4 import BeautifulSoup\nfrom requests import get\n\nurl = \"https://www.4devs.com.br/gerador_de_cpf\"\n\ndef open_browser():\n    driver = webdriver.Chrome(\"/home/felipe/Downloads/chromedriver\")\n    driver.get(url)\n    driver.find_element_by_id('bt_gerar_cpf').click()\n\ndef get_cpf():\n    response = get(url)\n\n    page_with_cpf = BeautifulSoup(response.text, 'html.parser')\n\n    cpf = page_with_cpf.find(\"div\", {\"id\": \"texto_cpf\"}).text\n\n    print(\"The value is: \" + cpf)\n\n\nopen_browser()\nget_cpf()\n</code></pre>\n", "abstract": "I'm trying to get a value that is given by the website after a click on a button. Here is the website: https://www.4devs.com.br/gerador_de_cpf You can see that there is a button called \"Gerar CPF\", this button provides a number that appears after the click. My current script opens the browser and get the value, but I'm getting the value from the page before the click, so the value is empty. I would like to know if it is possible to get the value after the click on the button."}, "answers": [{"id": 53973459, "score": 12, "vote": 0, "content": "<p><code>open_browser</code> and <code>get_cpf</code> are absolutely not related to each other...</p>\n<p>Actually you don't need <code>get_cpf</code> at all. Just wait for text after clicking the button:</p>\n<pre><code class=\"python\">from selenium.webdriver.support.ui import WebDriverWait as wait\n\ndef open_browser():\n    driver = webdriver.Chrome(\"/home/felipe/Downloads/chromedriver\")\n    driver.get(url)\n    driver.find_element_by_id('bt_gerar_cpf').click()\n    text_field = driver.find_element_by_id('texto_cpf')\n    text = wait(driver, 10).until(lambda driver: not text_field.text == 'Gerando...' and text_field.text)\n    return text\n\nprint(open_browser())\n</code></pre>\n<p><em>Update</em></p>\n<p>The same with requests:</p>\n<pre><code class=\"python\">import requests\n\nurl = 'https://www.4devs.com.br/ferramentas_online.php'\ndata = {'acao': 'gerar_cpf', 'pontuacao': 'S'}\nresponse = requests.post(url, data=data)\nprint(response.text)\n</code></pre>\n", "abstract": "open_browser and get_cpf are absolutely not related to each other... Actually you don't need get_cpf at all. Just wait for text after clicking the button: Update The same with requests:"}, {"id": 53973564, "score": 6, "vote": 0, "content": "<p>You don't need to use requests and BeautifulSoup.</p>\n<pre><code class=\"python\">from selenium import webdriver\nfrom time import sleep\n\nurl = \"https://www.4devs.com.br/gerador_de_cpf\"\n\ndef get_cpf():\n    driver = webdriver.Chrome(\"/home/felipe/Downloads/chromedriver\")\n    driver.get(url)\n    driver.find_element_by_id('bt_gerar_cpf').click()\n    sleep(10)\n    text=driver.find_element_by_id('texto_cpf').text\n    print(text)\nget_cpf()\n</code></pre>\n", "abstract": "You don't need to use requests and BeautifulSoup."}, {"id": 53973484, "score": 3, "vote": 0, "content": "<p>Can you use a While loop until text changes?</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nurl = \"https://www.4devs.com.br/gerador_de_cpf\"\n\ndef get_value():\n    driver = webdriver.Chrome()\n    driver.get(url)\n    driver.find_element_by_id('bt_gerar_cpf').click()\n    while driver.find_element_by_id('texto_cpf').text == 'Gerando...':\n        continue\n    val = driver.find_element_by_id('texto_cpf').text\n    driver.quit()\n    return val\n\nprint(get_value())\n</code></pre>\n", "abstract": "Can you use a While loop until text changes?"}, {"id": 69418316, "score": 0, "vote": 0, "content": "<p>I recommend this website that does exactly the same thing.</p>\n<p><a href=\"https://4devs.net.br/gerador-cpf\" rel=\"nofollow noreferrer\">https://4devs.net.br/gerador-cpf</a></p>\n<p>But to get the \"gerar cpf\" action with selenium, you can inspect the HTML source code with a browser and click on \"copy XPath for this element\".</p>\n<p>It is much simpler than manually searching for the elements in the page.</p>\n", "abstract": "I recommend this website that does exactly the same thing. https://4devs.net.br/gerador-cpf But to get the \"gerar cpf\" action with selenium, you can inspect the HTML source code with a browser and click on \"copy XPath for this element\". It is much simpler than manually searching for the elements in the page."}]}, {"link": "https://stackoverflow.com/questions/15564844/locally-run-all-of-the-spiders-in-scrapy", "question": {"id": "15564844", "title": "Locally run all of the spiders in Scrapy", "content": "<p>Is there a way to run all of the spiders in a Scrapy project without using the Scrapy daemon? There used to be a way to run multiple spiders with <code>scrapy crawl</code>, but that syntax was removed and Scrapy's code changed quite a bit.</p>\n<p>I tried creating my own command:</p>\n<pre><code class=\"python\">from scrapy.command import ScrapyCommand\nfrom scrapy.utils.misc import load_object\nfrom scrapy.conf import settings\n\nclass Command(ScrapyCommand):\n    requires_project = True\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])\n        spiders = spman_cls.from_settings(settings)\n\n        for spider_name in spiders.list():\n            spider = self.crawler.spiders.create(spider_name)\n            self.crawler.crawl(spider)\n\n        self.crawler.start()\n</code></pre>\n<p>But once a spider is registered with <code>self.crawler.crawl()</code>, I get assertion errors for all of the other spiders:</p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/scrapy/cmdline.py\", line 138, in _run_command\n    cmd.run(args, opts)\n  File \"/home/blender/Projects/scrapers/store_crawler/store_crawler/commands/crawlall.py\", line 22, in run\n    self.crawler.crawl(spider)\n  File \"/usr/lib/python2.7/site-packages/scrapy/crawler.py\", line 47, in crawl\n    return self.engine.open_spider(spider, requests)\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1214, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- &lt;exception caught here&gt; ---\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1071, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/lib/python2.7/site-packages/scrapy/core/engine.py\", line 215, in open_spider\n    spider.name\nexceptions.AssertionError: No free spider slots when opening 'spidername'\n</code></pre>\n<p>Is there any way to do this? I'd rather not start subclassing core Scrapy components just to run all of my spiders like this.</p>\n", "abstract": "Is there a way to run all of the spiders in a Scrapy project without using the Scrapy daemon? There used to be a way to run multiple spiders with scrapy crawl, but that syntax was removed and Scrapy's code changed quite a bit. I tried creating my own command: But once a spider is registered with self.crawler.crawl(), I get assertion errors for all of the other spiders: Is there any way to do this? I'd rather not start subclassing core Scrapy components just to run all of my spiders like this."}, "answers": [{"id": 27471668, "score": 30, "vote": 0, "content": "<p>Why didn't you just use something like:</p>\n<pre><code class=\"python\">scrapy list|xargs -n 1 scrapy crawl\n</code></pre>\n<p>?</p>\n", "abstract": "Why didn't you just use something like: ?"}, {"id": 15580406, "score": 23, "vote": 0, "content": "<p>Here is an example that does not run inside a custom command, but runs the Reactor manually and creates a new Crawler <a href=\"http://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\">for each spider</a>:</p>\n<pre><code class=\"python\">from twisted.internet import reactor\nfrom scrapy.crawler import Crawler\n# scrapy.conf.settings singlton was deprecated last year\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy import log\n\ndef setup_crawler(spider_name):\n    crawler = Crawler(settings)\n    crawler.configure()\n    spider = crawler.spiders.create(spider_name)\n    crawler.crawl(spider)\n    crawler.start()\n\nlog.start()\nsettings = get_project_settings()\ncrawler = Crawler(settings)\ncrawler.configure()\n\nfor spider_name in crawler.spiders.list():\n    setup_crawler(spider_name)\n\nreactor.run()\n</code></pre>\n<p>You will have to design <a href=\"http://doc.scrapy.org/en/latest/topics/signals.html\">some signal system</a> to stop the reactor when all spiders are finished.</p>\n<p>EDIT: And here is how you can run multiple spiders in a custom command:</p>\n<pre><code class=\"python\">from scrapy.command import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import Crawler\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n\n        for spider_name in self.crawler.spiders.list():\n            crawler = Crawler(settings)\n            crawler.configure()\n            spider = crawler.spiders.create(spider_name)\n            crawler.crawl(spider)\n            crawler.start()\n\n        self.crawler.start()\n</code></pre>\n", "abstract": "Here is an example that does not run inside a custom command, but runs the Reactor manually and creates a new Crawler for each spider: You will have to design some signal system to stop the reactor when all spiders are finished. EDIT: And here is how you can run multiple spiders in a custom command:"}, {"id": 32088756, "score": 7, "vote": 0, "content": "<p>the answer of @Steven Almeroth will be failed in Scrapy 1.0, and you should edit the script like this:</p>\n<pre><code class=\"python\">from scrapy.commands import ScrapyCommand\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nclass Command(ScrapyCommand):\n\n    requires_project = True\n    excludes = ['spider1']\n\n    def syntax(self):\n        return '[options]'\n\n    def short_desc(self):\n        return 'Runs all of the spiders'\n\n    def run(self, args, opts):\n        settings = get_project_settings()\n        crawler_process = CrawlerProcess(settings) \n\n        for spider_name in crawler_process.spider_loader.list():\n            if spider_name in self.excludes:\n                continue\n            spider_cls = crawler_process.spider_loader.load(spider_name) \n            crawler_process.crawl(spider_cls)\n        crawler_process.start()\n</code></pre>\n", "abstract": "the answer of @Steven Almeroth will be failed in Scrapy 1.0, and you should edit the script like this:"}, {"id": 43927097, "score": 5, "vote": 0, "content": "<p>this code is works on My scrapy version is 1.3.3 (save it in same directory in scrapy.cfg):</p>\n<pre><code class=\"python\">from scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (\"Running spider %s\" % (spider_name))\n    process.crawl(spider_name,query=\"dvh\") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n</code></pre>\n<p>for scrapy 1.5.x (so you don't get the deprecation warning)</p>\n<pre><code class=\"python\">from scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spider_loader.list():\n    print (\"Running spider %s\" % (spider_name))\n    process.crawl(spider_name,query=\"dvh\") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n</code></pre>\n", "abstract": "this code is works on My scrapy version is 1.3.3 (save it in same directory in scrapy.cfg): for scrapy 1.5.x (so you don't get the deprecation warning)"}, {"id": 69035122, "score": 0, "vote": 0, "content": "<p>Linux script</p>\n<pre class=\"lang-sh prettyprint-override\"><code class=\"python\">#!/bin/bash\nfor spider in $(scrapy list)\ndo\nscrapy crawl \"$spider\" -o \"$spider\".json\ndone\n</code></pre>\n", "abstract": "Linux script"}]}, {"link": "https://stackoverflow.com/questions/19385837/python-scrapy-on-offline-local-data", "question": {"id": "19385837", "title": "Python Scrapy on offline (local) data", "content": "<p>I have a 270MB dataset (10000 html files) on my computer. Can I use Scrapy to crawl this dataset locally? How?</p>\n", "abstract": "I have a 270MB dataset (10000 html files) on my computer. Can I use Scrapy to crawl this dataset locally? How?"}, "answers": [{"id": 19386118, "score": 34, "vote": 0, "content": "<h1>SimpleHTTP Server Hosting</h1>\n<p>If you truly want to host it locally and use scrapy, you could serve it by navigating to the directory it's stored in and run the SimpleHTTPServer (port 8000 shown below):</p>\n<pre><code class=\"python\">python -m SimpleHTTPServer 8000\n</code></pre>\n<p>Then just point scrapy at 127.0.0.1:8000</p>\n<pre><code class=\"python\">$ scrapy crawl 127.0.0.1:8000\n</code></pre>\n<h1>file://</h1>\n<p>An alternative is to just have scrapy point to the set of files directly:</p>\n<pre><code class=\"python\">$ scrapy crawl file:///home/sagi/html_files # Assuming you're on a *nix system\n</code></pre>\n<h1>Wrapping up</h1>\n<p>Once you've set up your scraper for scrapy (see <a href=\"https://github.com/scrapy/dirbot\">example dirbot</a>), just run the crawler:</p>\n<pre><code class=\"python\">$ scrapy crawl 127.0.0.1:8000\n</code></pre>\n<p>If links in the html files are absolute rather than relative though, these may not work well. You would need to adjust the files yourself.</p>\n", "abstract": "If you truly want to host it locally and use scrapy, you could serve it by navigating to the directory it's stored in and run the SimpleHTTPServer (port 8000 shown below): Then just point scrapy at 127.0.0.1:8000 An alternative is to just have scrapy point to the set of files directly: Once you've set up your scraper for scrapy (see example dirbot), just run the crawler: If links in the html files are absolute rather than relative though, these may not work well. You would need to adjust the files yourself."}, {"id": 19387287, "score": 9, "vote": 0, "content": "<p>Go to your Dataset folder :</p>\n<pre><code class=\"python\">import os\nfiles = os.listdir(os.getcwd())\nfor file in files:\n    with open(file,\"r\") as f:\n        page_content = f.read()\n        #do here watever you want to do with page_content. I guess parsing with lxml or Beautiful soup.\n</code></pre>\n<p>No need to go for Scrapy !</p>\n", "abstract": "Go to your Dataset folder : No need to go for Scrapy !"}]}, {"link": "https://stackoverflow.com/questions/7123387/should-i-create-pipeline-to-save-files-with-scrapy", "question": {"id": "7123387", "title": "Should I create pipeline to save files with scrapy?", "content": "<p>I need to save a file (.pdf) but I'm unsure how to do it. I need to save .pdfs and store them in such a way that they are organized in a directories much like they are stored on the site I'm scraping them off. </p>\n<p>From what I can gather I need to make a pipeline, but from what I understand pipelines save \"Items\" and \"items\" are just basic data like strings/numbers. Is saving files a proper use of pipelines, or should I save file in spider instead?</p>\n", "abstract": "I need to save a file (.pdf) but I'm unsure how to do it. I need to save .pdfs and store them in such a way that they are organized in a directories much like they are stored on the site I'm scraping them off.  From what I can gather I need to make a pipeline, but from what I understand pipelines save \"Items\" and \"items\" are just basic data like strings/numbers. Is saving files a proper use of pipelines, or should I save file in spider instead?"}, "answers": [{"id": 7169241, "score": 18, "vote": 0, "content": "<p>Yes and no[1]. If you fetch a pdf it will be stored in memory, but if the pdfs are not big enough to fill up your available memory so it is ok.</p>\n<p>You could save the pdf in the spider callback:</p>\n<pre><code class=\"python\">def parse_listing(self, response):\n    # ... extract pdf urls\n    for url in pdf_urls:\n        yield Request(url, callback=self.save_pdf)\n\ndef save_pdf(self, response):\n    path = self.get_path(response.url)\n    with open(path, \"wb\") as f:\n        f.write(response.body)\n</code></pre>\n<p>If you choose to do it in a pipeline:</p>\n<pre><code class=\"python\"># in the spider\ndef parse_pdf(self, response):\n    i = MyItem()\n    i['body'] = response.body\n    i['url'] = response.url\n    # you can add more metadata to the item\n    return i\n\n# in your pipeline\ndef process_item(self, item, spider):\n    path = self.get_path(item['url'])\n    with open(path, \"wb\") as f:\n        f.write(item['body'])\n    # remove body and add path as reference\n    del item['body']\n    item['path'] = path\n    # let item be processed by other pipelines. ie. db store\n    return item\n</code></pre>\n<p>[1] another approach could be only store pdfs' urls and use another process to fetch the documents without buffering into memory. (e.g. <code>wget</code>)</p>\n", "abstract": "Yes and no[1]. If you fetch a pdf it will be stored in memory, but if the pdfs are not big enough to fill up your available memory so it is ok. You could save the pdf in the spider callback: If you choose to do it in a pipeline: [1] another approach could be only store pdfs' urls and use another process to fetch the documents without buffering into memory. (e.g. wget)"}, {"id": 19795896, "score": 9, "vote": 0, "content": "<p>There is a FilesPipeline that you can use directly, assuming you already have the file url, the link shows how to use <a href=\"https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/files.py\" rel=\"noreferrer\">FilesPipeline</a>:</p>\n<p><a href=\"https://groups.google.com/forum/print/msg/scrapy-users/kzGHFjXywuY/O6PIhoT3thsJ\" rel=\"noreferrer\">https://groups.google.com/forum/print/msg/scrapy-users/kzGHFjXywuY/O6PIhoT3thsJ</a></p>\n", "abstract": "There is a FilesPipeline that you can use directly, assuming you already have the file url, the link shows how to use FilesPipeline: https://groups.google.com/forum/print/msg/scrapy-users/kzGHFjXywuY/O6PIhoT3thsJ"}, {"id": 7135102, "score": 4, "vote": 0, "content": "<p>It's a perfect tool for the job. The way Scrapy works is that you have spiders that transform web pages into structured data(items). Pipelines are postprocessors, but they use same asynchronous infrastructure as spiders so it's perfect for fetching media files.</p>\n<p>In your case, you'd first extract location of PDFs in spider, fetch them in pipeline and have another pipeline to save items.</p>\n", "abstract": "It's a perfect tool for the job. The way Scrapy works is that you have spiders that transform web pages into structured data(items). Pipelines are postprocessors, but they use same asynchronous infrastructure as spiders so it's perfect for fetching media files. In your case, you'd first extract location of PDFs in spider, fetch them in pipeline and have another pipeline to save items."}]}, {"link": "https://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider", "question": {"id": "9814827", "title": "Creating a generic scrapy spider", "content": "<p>My question is really how to do the same thing as a previous question, but in Scrapy 0.14.</p>\n<p><a href=\"https://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites\">Using one Scrapy spider for several websites</a></p>\n<p>Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance.</p>\n<p>Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it.</p>\n<pre><code class=\"python\">class MySpider(CrawlSpider):\n\nname = 'MySpider'\nallowed_domains = ['somedomain.com', 'sub.somedomain.com']\nstart_urls = ['http://www.somedomain.com']\n\nrules = (\n    Rule(SgmlLinkExtractor(allow=('/pages/', ), deny=('', ))),\n\n    Rule(SgmlLinkExtractor(allow=('/2012/03/')), callback='parse_item'),\n)\n\ndef parse_item(self, response):\n    contentTags = []\n\n    soup = BeautifulSoup(response.body)\n\n    contentTags = soup.findAll('p', itemprop=\"myProp\")\n\n    for contentTag in contentTags:\n        matchedResult = re.search('Keyword1|Keyword2', contentTag.text)\n        if matchedResult:\n            print('URL Found: ' + response.url)\n\n    pass\n</code></pre>\n", "abstract": "My question is really how to do the same thing as a previous question, but in Scrapy 0.14. Using one Scrapy spider for several websites Basically, I have GUI that takes parameters like domain, keywords, tag names, etc. and I want to create a generic spider to crawl those domains for those keywords in those tags.  I've read conflicting things, using older versions of scrapy, by either overriding the spider manager class or by dynamically creating a spider.  Which method is preferred and how do I implement and invoke the proper solution?  Thanks in advance. Here is the code that I want to make generic.  It also uses BeautifulSoup.  I paired it down so hopefully didn't remove anything crucial to understand it."}, "answers": [{"id": 13054768, "score": 2, "vote": 0, "content": "<p>You could create a run-time spider which is evaluated by the interpreter. This code piece could be evaluated at runtime like so:</p>\n<pre><code class=\"python\">a = open(\"test.py\")\nfrom compiler import compile\nd = compile(a.read(), 'spider.py', 'exec')\neval(d)\n\nMySpider\n&lt;class '__main__.MySpider'&gt;\nprint MySpider.start_urls\n['http://www.somedomain.com']\n</code></pre>\n", "abstract": "You could create a run-time spider which is evaluated by the interpreter. This code piece could be evaluated at runtime like so:"}, {"id": 26823500, "score": 2, "vote": 0, "content": "<p>I use the <a href=\"http://doc.scrapy.org/en/latest/topics/extensions.html\" rel=\"nofollow noreferrer\">Scrapy Extensions</a> approach to <strong>extend the Spider class</strong> to a class named Masterspider that includes a generic parser.</p>\n<p>Below is the very <em>\"short\" version</em> of my generic extended parser. Note that you'll need to implement a renderer with a Javascript engine (such as <a href=\"https://stackoverflow.com/a/17979285\">Selenium</a> or BeautifulSoup) a as soon as you start working on pages using AJAX. And a lot of additional code to manage differences between sites (scrap based on column title, handle relative vs long URL, manage different kind of data containers, etc...).</p>\n<p>What is interresting with the Scrapy Extension approach is that you can still override the generic parser method if something does not fit but I never had to. The Masterspider class checks if some methods have been created (eg. parser_start, next_url_parser...) under the site specific spider class to allow the management of specificies: send a form, construct the next_url request from elements in the page, etc.</p>\n<p>As I'm scraping very different sites, there's always specificities to manage. That's why I prefer to keep a class for each scraped site so that I can write some specific methods to handle it (pre-/post-processing except PipeLines, Request generators...).</p>\n<p>masterspider/sitespider/settings.py</p>\n<pre><code class=\"python\">EXTENSIONS = {\n    'masterspider.masterspider.MasterSpider': 500\n}\n</code></pre>\n<p>masterspider/masterspdier/masterspider.py</p>\n<pre><code class=\"python\"># -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\n\nclass MasterSpider(Spider):\n\n    def start_requests(self):\n        if hasattr(self,'parse_start'): # First page requiring a specific parser\n            fcallback = self.parse_start\n        else:\n            fcallback = self.parse\n        return [ Request(self.spd['start_url'],\n                     callback=fcallback,\n                     meta={'itemfields': {}}) ]\n\n    def parse(self, response):\n        sel = Selector(response)\n        lines = sel.xpath(self.spd['xlines'])\n        # ...\n        for line in lines:\n            item = genspiderItem(response.meta['itemfields'])               \n            # ...\n            # Get request_url of detailed page and scrap basic item info\n            # ... \n            yield  Request(request_url,\n                   callback=self.parse_item,\n                   meta={'item':item, 'itemfields':response.meta['itemfields']})\n\n        for next_url in sel.xpath(self.spd['xnext_url']).extract():\n            if hasattr(self,'next_url_parser'): # Need to process the next page URL before?\n                yield self.next_url_parser(next_url, response)\n            else:\n                yield Request(\n                    request_url,\n                    callback=self.parse,\n                    meta=response.meta)\n\n    def parse_item(self, response):\n        sel = Selector(response)\n        item = response.meta['item']\n        for itemname, xitemname in self.spd['x_ondetailpage'].iteritems():\n            item[itemname] = \"\\n\".join(sel.xpath(xitemname).extract())\n        return item\n</code></pre>\n<p>masterspider/sitespider/spiders/somesite_spider.py</p>\n<pre><code class=\"python\"># -*- coding: utf8 -*-\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom sitespider.items import genspiderItem\nfrom masterspider.masterspider import MasterSpider\n\nclass targetsiteSpider(MasterSpider):\n    name = \"targetsite\"\n    allowed_domains = [\"www.targetsite.com\"]\n    spd = {\n        'start_url' : \"http://www.targetsite.com/startpage\", # Start page\n        'xlines' : \"//td[something...]\",\n        'xnext_url' : \"//a[contains(@href,'something?page=')]/@href\", # Next pages\n        'x_ondetailpage' : {\n            \"itemprop123\" :      u\"id('someid')//text()\"\n            }\n    }\n\n#     def next_url_parser(self, next_url, response): # OPTIONAL next_url regexp pre-processor\n#          ...\n</code></pre>\n", "abstract": "I use the Scrapy Extensions approach to extend the Spider class to a class named Masterspider that includes a generic parser. Below is the very \"short\" version of my generic extended parser. Note that you'll need to implement a renderer with a Javascript engine (such as Selenium or BeautifulSoup) a as soon as you start working on pages using AJAX. And a lot of additional code to manage differences between sites (scrap based on column title, handle relative vs long URL, manage different kind of data containers, etc...). What is interresting with the Scrapy Extension approach is that you can still override the generic parser method if something does not fit but I never had to. The Masterspider class checks if some methods have been created (eg. parser_start, next_url_parser...) under the site specific spider class to allow the management of specificies: send a form, construct the next_url request from elements in the page, etc. As I'm scraping very different sites, there's always specificities to manage. That's why I prefer to keep a class for each scraped site so that I can write some specific methods to handle it (pre-/post-processing except PipeLines, Request generators...). masterspider/sitespider/settings.py masterspider/masterspdier/masterspider.py masterspider/sitespider/spiders/somesite_spider.py"}, {"id": 12617444, "score": 1, "vote": 0, "content": "<p>Instead of having the variables <code>name</code>,<code>allowed_domains</code>, <code>start_urls</code> and <code>rules</code> attached to the class, you should write a <code>MySpider.__init__</code>, call <code>CrawlSpider.__init__</code> from that passing the necessary arguments, and setting <code>name</code>, <code>allowed_domains</code> etc. per object. \n<code>MyProp</code> and keywords also should be set within your <code>__init__</code>. So in the end you should have something like below. You don't have to add <code>name</code> to the arguments, as <code>name</code> is set by <code>BaseSpider</code> itself from <code>kwargs</code>: </p>\n<pre><code class=\"python\">class MySpider(CrawlSpider):\n\n    def __init__(self, allowed_domains=[], start_urls=[], \n            rules=[], findtag='', finditemprop='', keywords='', **kwargs):\n        CrawlSpider.__init__(self, **kwargs)\n        self.allowed_domains = allowed_domains\n        self.start_urls = start_urls\n        self.rules = rules\n        self.findtag = findtag\n        self.finditemprop = finditemprop\n        self.keywords = keywords\n\n    def parse_item(self, response):\n        contentTags = []\n\n        soup = BeautifulSoup(response.body)\n\n        contentTags = soup.findAll(self.findtag, itemprop=self.finditemprop)\n\n        for contentTag in contentTags:\n            matchedResult = re.search(self.keywords, contentTag.text)\n            if matchedResult:\n                print('URL Found: ' + response.url)\n</code></pre>\n", "abstract": "Instead of having the variables name,allowed_domains, start_urls and rules attached to the class, you should write a MySpider.__init__, call CrawlSpider.__init__ from that passing the necessary arguments, and setting name, allowed_domains etc. per object. \nMyProp and keywords also should be set within your __init__. So in the end you should have something like below. You don't have to add name to the arguments, as name is set by BaseSpider itself from kwargs: "}, {"id": 11381067, "score": 0, "vote": 0, "content": "<p>I am not sure which way is preferred, but I will tell you what I have done in the past. I am in no way sure that this is the best (or correct) way of doing this and I would be interested to learn what other people think.</p>\n<p>I usually just override the parent class (<code>CrawlSpider</code>) and either pass in arguments and then initialize the parent class via <code>super(MySpider, self).__init__()</code> from within my own init-function <em>or</em> I pull in that data from a database where I have saved a list of links to be appended to <code>start_urls</code> earlier.</p>\n", "abstract": "I am not sure which way is preferred, but I will tell you what I have done in the past. I am in no way sure that this is the best (or correct) way of doing this and I would be interested to learn what other people think. I usually just override the parent class (CrawlSpider) and either pass in arguments and then initialize the parent class via super(MySpider, self).__init__() from within my own init-function or I pull in that data from a database where I have saved a list of links to be appended to start_urls earlier."}, {"id": 28547818, "score": 0, "vote": 0, "content": "<p>As far as crawling specific domains passed as arguments goes, I just override <code>Spider.__init__</code>:</p>\n<pre><code class=\"python\">class MySpider(scrapy.Spider):\n    \"\"\"\n    This spider will try to crawl whatever is passed in `start_urls` which\n    should be a comma-separated string of fully qualified URIs.\n\n    Example: start_urls=http://localhost,http://example.com\n    \"\"\"\n    def __init__(self, name=None, **kwargs):\n        if 'start_urls' in kwargs:\n            self.start_urls = kwargs.pop('start_urls').split(',')\n        super(Spider, self).__init__(name, **kwargs)\n</code></pre>\n", "abstract": "As far as crawling specific domains passed as arguments goes, I just override Spider.__init__:"}]}, {"link": "https://stackoverflow.com/questions/8567171/how-do-i-remove-a-query-from-a-url", "question": {"id": "8567171", "title": "How do I remove a query from a url?", "content": "<p>I am using scrapy to crawl a site which seems to be appending random values to the query string at the end of each URL. This is turning the crawl into a sort of an infinite loop.</p>\n<p>How do i make scrapy to neglect the query string part of the URL's?</p>\n", "abstract": "I am using scrapy to crawl a site which seems to be appending random values to the query string at the end of each URL. This is turning the crawl into a sort of an infinite loop. How do i make scrapy to neglect the query string part of the URL's?"}, "answers": [{"id": 8588800, "score": 29, "vote": 0, "content": "<p>See <a href=\"http://docs.python.org/release/3.1.3/library/urllib.parse.html\">urllib.urlparse</a></p>\n<p>Example code:</p>\n<pre><code class=\"python\">from urlparse import urlparse\no = urlparse('http://url.something.com/bla.html?querystring=stuff')\n\nurl_without_query_string = o.scheme + \"://\" + o.netloc + o.path\n</code></pre>\n<p>Example output:</p>\n<pre><code class=\"python\">Python 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) \n[GCC 4.2.1 (Apple Inc. build 5646)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from urlparse import urlparse\n&gt;&gt;&gt; o = urlparse('http://url.something.com/bla.html?querystring=stuff')\n&gt;&gt;&gt; url_without_query_string = o.scheme + \"://\" + o.netloc + o.path\n&gt;&gt;&gt; print url_without_query_string\nhttp://url.something.com/bla.html\n&gt;&gt;&gt; \n</code></pre>\n", "abstract": "See urllib.urlparse Example code: Example output:"}, {"id": 8620843, "score": 14, "vote": 0, "content": "<p>There is a function <code>url_query_cleaner</code> in <code>w3lib.url</code> module (used by scrapy itself) to clean urls keeping only a list of allowed arguments.</p>\n", "abstract": "There is a function url_query_cleaner in w3lib.url module (used by scrapy itself) to clean urls keeping only a list of allowed arguments."}, {"id": 8576950, "score": 6, "vote": 0, "content": "<p>Provide some code, so we can help you.</p>\n<p>If you are using <code>CrawlSpider</code> and <code>Rule</code>'s with <code>SgmlLinkExtractor</code>, provide custom function to <code>proccess_value</code> parameter of <code>SgmlLinkExtractor</code> constructor.</p>\n<p>See documentation for <a href=\"http://doc.scrapy.org/en/latest/topics/link-extractors.html#basesgmllinkextractor\" rel=\"noreferrer\">BaseSgmlLinkExtractor</a></p>\n<pre><code class=\"python\">def delete_random_garbage_from_url(url):\n    cleaned_url = ... # process url somehow\n    return cleaned_url\n\nRule(\n    SgmlLinkExtractor(\n         # ... your allow, deny parameters, etc\n         process_value=delete_random_garbage_from_url,\n    )\n)\n</code></pre>\n", "abstract": "Provide some code, so we can help you. If you are using CrawlSpider and Rule's with SgmlLinkExtractor, provide custom function to proccess_value parameter of SgmlLinkExtractor constructor. See documentation for BaseSgmlLinkExtractor"}, {"id": 51014174, "score": 6, "vote": 0, "content": "<p>You can use the <a href=\"https://docs.python.org/3/library/urllib.parse.html\" rel=\"nofollow noreferrer\"><code>urllib.parse.urlsplit()</code> function</a>. The result is a <a href=\"https://docs.python.org/3/library/urllib.parse.html#urlparse-result-object\" rel=\"nofollow noreferrer\"><em>structured parse result</em></a>, a named tuple with added functionality.</p>\n<p>Use the <code>namedtuple._replace()</code> method to alter the parsed result values, then use the <a href=\"https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urllib.parse.SplitResult.geturl\" rel=\"nofollow noreferrer\"><code>SplitResult.geturl()</code> method</a> to get a URL string again.</p>\n<p>To remove the query string, set the <code>query</code> value to <code>None</code>:</p>\n<pre><code class=\"python\">from urllib.parse import urlsplit\n\nupdated_url = urlsplit(url)._replace(query=None).geturl()\n</code></pre>\n<p>Demo:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; from urllib.parse import urlsplit\n&gt;&gt;&gt; url = 'https://example.com/example/path?query_string=everything+after+the+questionmark'\n&gt;&gt;&gt; urlparse.urlsplit(url)._replace(query=None).geturl()\n'https://example.com/example/path'\n</code></pre>\n<p>For Python 2, the same function is available under the <a href=\"https://docs.python.org/2/library/urlparse.html#urlparse.urlsplit\" rel=\"nofollow noreferrer\"><code>urlparse.urlsplit()</code> name</a>.</p>\n<p>You could also use the <a href=\"https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse\" rel=\"nofollow noreferrer\"><code>urllparse.parse.urlparse()</code> function</a>; for URLs without any <a href=\"https://www.rfc-editor.org/rfc/rfc2396.html#section-3.3\" rel=\"nofollow noreferrer\">path parameters</a>, the result would be the same. The two functions differ in how path parameters are handled; <code>urlparse()</code> only supports path parameters for the last segment of the path, while <code>urlsplit()</code> leaves path parameters in place in the path, leaving parsing of such parameters to other code. Since path parameters are rarely used these days [later URL RFCs have dropped the feature altogether), the difference is academical. <code>urlparse()</code> uses <code>urlsplit()</code> and without parameters, doesn't add anything other than extra overhead. It is better to just use <code>urlsplit()</code> directly.</p>\n", "abstract": "You can use the urllib.parse.urlsplit() function. The result is a structured parse result, a named tuple with added functionality. Use the namedtuple._replace() method to alter the parsed result values, then use the SplitResult.geturl() method to get a URL string again. To remove the query string, set the query value to None: Demo: For Python 2, the same function is available under the urlparse.urlsplit() name. You could also use the urllparse.parse.urlparse() function; for URLs without any path parameters, the result would be the same. The two functions differ in how path parameters are handled; urlparse() only supports path parameters for the last segment of the path, while urlsplit() leaves path parameters in place in the path, leaving parsing of such parameters to other code. Since path parameters are rarely used these days [later URL RFCs have dropped the feature altogether), the difference is academical. urlparse() uses urlsplit() and without parameters, doesn't add anything other than extra overhead. It is better to just use urlsplit() directly."}, {"id": 52906339, "score": 1, "vote": 0, "content": "<p>use this method to remove query string from url</p>\n<pre><code class=\"python\">urllink=\"http://url.something.com/bla.html?querystring=stuff\"\nurl_final=urllink.split('?')[0]\nprint(url_final)\n</code></pre>\n<p>output will be:\n<a href=\"http://url.something.com/bla.html\" rel=\"nofollow noreferrer\">http://url.something.com/bla.html</a></p>\n", "abstract": "use this method to remove query string from url output will be:\nhttp://url.something.com/bla.html"}, {"id": 8586276, "score": 0, "vote": 0, "content": "<p>If you are using BaseSpider, before yielding a new request, remove manually random values from the query part of the URL using <a href=\"http://docs.python.org/library/urlparse.html\" rel=\"nofollow\">urlparse</a>:</p>\n<pre><code class=\"python\">def parse(self, response):\n    hxs = HtmlXPathSelector(response)\n    item_urls = hxs.select(\".//a[@class='...']/@href\").extract()\n    for item_url in item_urls:\n        # remove the bad part of the query part of the URL here\n        item_url = urlparse.urljoin(response.url, item_url)\n        self.log('Found item URL: %s' % item_url)\n        yield Request(item_url, callback = self.parse_item)\n</code></pre>\n", "abstract": "If you are using BaseSpider, before yielding a new request, remove manually random values from the query part of the URL using urlparse:"}]}, {"link": "https://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results", "question": {"id": "8377055", "title": "Submit data via web form and extract the results", "content": "<p>My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site <code>http://bookblog.net/gender/genie.php</code> but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this.</p>\n<p>This is the form dom: </p>\n<pre><code class=\"python\">&lt;form action=\"analysis.php\" method=\"POST\"&gt;\n&lt;textarea cols=\"75\" rows=\"13\" name=\"text\"&gt;&lt;/textarea&gt;\n&lt;div class=\"copyright\"&gt;(NOTE: The genie works best on texts of more than 500 words.)&lt;/div&gt;\n&lt;p&gt;\n&lt;b&gt;Genre:&lt;/b&gt;\n&lt;input type=\"radio\" value=\"fiction\" name=\"genre\"&gt;\nfiction&amp;nbsp;&amp;nbsp;\n&lt;input type=\"radio\" value=\"nonfiction\" name=\"genre\"&gt;\nnonfiction&amp;nbsp;&amp;nbsp;\n&lt;input type=\"radio\" value=\"blog\" name=\"genre\"&gt;\nblog entry\n&lt;/p&gt;\n&lt;p&gt;\n&lt;/form&gt;\n</code></pre>\n<p>results page dom:</p>\n<pre><code class=\"python\">&lt;p&gt;\n&lt;b&gt;The Gender Genie thinks the author of this passage is:&lt;/b&gt;\nmale!\n&lt;/p&gt;\n</code></pre>\n", "abstract": "My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this. This is the form dom:  results page dom:"}, "answers": [{"id": 8377373, "score": 27, "vote": 0, "content": "<p>No need to use mechanize, just send the correct form data in a POST request.</p>\n<p>Also, using regular expression to parse HTML is a bad idea. You would be better off using a HTML parser like lxml.html.</p>\n<pre><code class=\"python\">import requests\nimport lxml.html as lh\n\n\ndef gender_genie(text, genre):\n    url = 'http://bookblog.net/gender/analysis.php'\n    caption = 'The Gender Genie thinks the author of this passage is:'\n\n    form_data = {\n        'text': text,\n        'genre': genre,\n        'submit': 'submit',\n    }\n\n    response = requests.post(url, data=form_data)\n\n    tree = lh.document_fromstring(response.content)\n\n    return tree.xpath(\"//b[text()=$caption]\", caption=caption)[0].tail.strip()\n\n\nif __name__ == '__main__':\n    print gender_genie('I have a beard!', 'blog')\n</code></pre>\n", "abstract": "No need to use mechanize, just send the correct form data in a POST request. Also, using regular expression to parse HTML is a bad idea. You would be better off using a HTML parser like lxml.html."}, {"id": 8377285, "score": 17, "vote": 0, "content": "<p>You can use <a href=\"http://wwwsearch.sourceforge.net/mechanize/\">mechanize</a> to submit and retrieve content, and the <a href=\"http://docs.python.org/library/re.html\">re</a> module for getting what you want. For example, the script below does it for the text of your own question:</p>\n<pre><code class=\"python\">import re\nfrom mechanize import Browser\n\ntext = \"\"\"\nMy python level is Novice. I have never written a web scraper \nor crawler. I have written a python code to connect to an api and \nextract the data that I want. But for some the extracted data I want to \nget the gender of the author. I found this web site \nhttp://bookblog.net/gender/genie.php but downside is there isn't an api \navailable. I was wondering how to write a python to submit data to the \nform in the page and extract the return data. It would be a great help \nif I could get some guidance on this.\"\"\"\n\nbrowser = Browser()\nbrowser.open(\"http://bookblog.net/gender/genie.php\")\n\nbrowser.select_form(nr=0)\nbrowser['text'] = text\nbrowser['genre'] = ['nonfiction']\n\nresponse = browser.submit()\n\ncontent = response.read()\n\nresult = re.findall(\n    r'&lt;b&gt;The Gender Genie thinks the author of this passage is:&lt;/b&gt; (\\w*)!', content)\n\nprint result[0]\n</code></pre>\n<p>What does it do? It creates a <code>mechanize.Browser</code> and goes to the given URL:</p>\n<pre><code class=\"python\">browser = Browser()\nbrowser.open(\"http://bookblog.net/gender/genie.php\")\n</code></pre>\n<p>Then it selects the form (since there is only one form to be filled, it will be the first):</p>\n<pre><code class=\"python\">browser.select_form(nr=0)\n</code></pre>\n<p>Also, it sets the entries of the form...</p>\n<pre><code class=\"python\">browser['text'] = text\nbrowser['genre'] = ['nonfiction']\n</code></pre>\n<p>... and submit it:</p>\n<pre><code class=\"python\">response = browser.submit()\n</code></pre>\n<p>Now, we get the result:</p>\n<pre><code class=\"python\">content = response.read()\n</code></pre>\n<p>We know that the result is in the form:</p>\n<pre><code class=\"python\">&lt;b&gt;The Gender Genie thinks the author of this passage is:&lt;/b&gt; male!\n</code></pre>\n<p>So we create a regex for matching and use <code>re.findall()</code>:</p>\n<pre><code class=\"python\">result = re.findall(\n    r'&lt;b&gt;The Gender Genie thinks the author of this passage is:&lt;/b&gt; (\\w*)!',\n    content)\n</code></pre>\n<p>Now the result is available for your use:</p>\n<pre><code class=\"python\">print result[0]\n</code></pre>\n", "abstract": "You can use mechanize to submit and retrieve content, and the re module for getting what you want. For example, the script below does it for the text of your own question: What does it do? It creates a mechanize.Browser and goes to the given URL: Then it selects the form (since there is only one form to be filled, it will be the first): Also, it sets the entries of the form... ... and submit it: Now, we get the result: We know that the result is in the form: So we create a regex for matching and use re.findall(): Now the result is available for your use:"}, {"id": 8377212, "score": 1, "vote": 0, "content": "<p>You can use <a href=\"http://wwwsearch.sourceforge.net/mechanize/\" rel=\"nofollow\">mechanize</a>, see <a href=\"http://wwwsearch.sourceforge.net/mechanize/forms.html\" rel=\"nofollow\">examples</a> for details.</p>\n<pre><code class=\"python\">from mechanize import ParseResponse, urlopen, urljoin\n\nuri = \"http://bookblog.net\"\n\nresponse = urlopen(urljoin(uri, \"/gender/genie.php\"))\nforms = ParseResponse(response, backwards_compat=False)\nform = forms[0]\n\n#print form\n\nform['text'] = 'cheese'\nform['genre'] = ['fiction']\n\nprint urlopen(form.click()).read()\n</code></pre>\n", "abstract": "You can use mechanize, see examples for details."}]}, {"link": "https://stackoverflow.com/questions/27964410/scrapy-only-follow-internal-urls-but-extract-all-links-found", "question": {"id": "27964410", "title": "Scrapy, only follow internal URLS but extract all links found", "content": "<p>I want to get all external links from a given website using Scrapy. Using the following code the spider crawls external links as well:</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors import LinkExtractor\nfrom myproject.items import someItem\n\nclass someSpider(CrawlSpider):\n  name = 'crawltest'\n  allowed_domains = ['someurl.com']\n  start_urls = ['http://www.someurl.com/']\n\n  rules = (Rule (LinkExtractor(), callback=\"parse_obj\", follow=True),\n  )\n\n  def parse_obj(self,response):\n    item = someItem()\n    item['url'] = response.url\n    return item\n</code></pre>\n<p>What am I missing? Doesn't \"allowed_domains\" prevent the external links to be crawled? If I set \"allow_domains\" for LinkExtractor it does not extract the external links. Just to clarify: I wan't to crawl internal links but extract external links. Any help appriciated!</p>\n", "abstract": "I want to get all external links from a given website using Scrapy. Using the following code the spider crawls external links as well: What am I missing? Doesn't \"allowed_domains\" prevent the external links to be crawled? If I set \"allow_domains\" for LinkExtractor it does not extract the external links. Just to clarify: I wan't to crawl internal links but extract external links. Any help appriciated!"}, "answers": [{"id": 27978534, "score": 16, "vote": 0, "content": "<p>You can also use the link extractor to pull all the links once you are parsing each page. </p>\n<p>The link extractor will filter the links for you. In this example the  link extractor will deny links in the allowed domain so it only gets outside links.</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors import LxmlLinkExtractor\nfrom myproject.items import someItem\n\nclass someSpider(CrawlSpider):\n  name = 'crawltest'\n  allowed_domains = ['someurl.com']\n  start_urls = ['http://www.someurl.com/']\n\n  rules = (Rule(LxmlLinkExtractor(allow=()), callback='parse_obj', follow=True),)\n\n\n  def parse_obj(self,response):\n    for link in LxmlLinkExtractor(allow=(),deny = self.allowed_domains).extract_links(response):\n        item = someItem()\n        item['url'] = link.url\n</code></pre>\n", "abstract": "You can also use the link extractor to pull all the links once you are parsing each page.  The link extractor will filter the links for you. In this example the  link extractor will deny links in the allowed domain so it only gets outside links."}, {"id": 31992992, "score": 6, "vote": 0, "content": "<p>An updated code based on 12Ryan12's answer,</p>\n<pre><code class=\"python\">from scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\nfrom scrapy.item import Item, Field\n\nclass MyItem(Item):\n    url= Field()\n\n\nclass someSpider(CrawlSpider):\n    name = 'crawltest'\n    allowed_domains = ['someurl.com']\n    start_urls = ['http://www.someurl.com/']\n    rules = (Rule(LxmlLinkExtractor(allow=()), callback='parse_obj', follow=True),)\n\n    def parse_obj(self,response):\n        item = MyItem()\n        item['url'] = []\n        for link in LxmlLinkExtractor(allow=(),deny = self.allowed_domains).extract_links(response):\n            item['url'].append(link.url)\n        return item\n</code></pre>\n", "abstract": "An updated code based on 12Ryan12's answer,"}, {"id": 27966347, "score": 4, "vote": 0, "content": "<p>A solution would be make usage a process_link function in the SgmlLinkExtractor\nDocumentation here <a href=\"http://doc.scrapy.org/en/latest/topics/link-extractors.html\" rel=\"nofollow noreferrer\">http://doc.scrapy.org/en/latest/topics/link-extractors.html</a> </p>\n<pre><code class=\"python\">class testSpider(CrawlSpider):\n    name = \"test\"\n    bot_name = 'test'\n    allowed_domains = [\"news.google.com\"]\n    start_urls = [\"https://news.google.com/\"]\n    rules = (\n    Rule(SgmlLinkExtractor(allow_domains=()), callback='parse_items',process_links=\"filter_links\",follow= True) ,\n     )\n\n    def filter_links(self, links):\n        for link in links:\n            if self.allowed_domains[0] not in link.url:\n                print link.url\n\n        return links\n\n    def parse_items(self, response):\n        ### ...\n</code></pre>\n", "abstract": "A solution would be make usage a process_link function in the SgmlLinkExtractor\nDocumentation here http://doc.scrapy.org/en/latest/topics/link-extractors.html "}]}, {"link": "https://stackoverflow.com/questions/30152261/make-scrapy-follow-links-and-collect-data", "question": {"id": "30152261", "title": "Make Scrapy follow links and collect data", "content": "<p>I am trying to write program in Scrapy to open links and collect data from this tag: <code>&lt;p class=\"attrgroup\"&gt;&lt;/p&gt;</code>.</p>\n<p>I've managed to make Scrapy collect all the links from given URL but not to follow them. Any help is very appreciated.</p>\n", "abstract": "I am trying to write program in Scrapy to open links and collect data from this tag: <p class=\"attrgroup\"></p>. I've managed to make Scrapy collect all the links from given URL but not to follow them. Any help is very appreciated."}, "answers": [{"id": 30152967, "score": 24, "vote": 0, "content": "<p>You need to yield <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request\" rel=\"noreferrer\"><code>Request</code></a> instances for the links to follow, assign a callback and extract the text of the desired <code>p</code> element in the callback:</p>\n<pre><code class=\"python\"># -*- coding: utf-8 -*-\nimport scrapy\n\n\n# item class included here \nclass DmozItem(scrapy.Item):\n    # define the fields for your item here like:\n    link = scrapy.Field()\n    attr = scrapy.Field()\n\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"craigslist.org\"]\n    start_urls = [\n    \"http://chicago.craigslist.org/search/emd?\"\n    ]\n\n    BASE_URL = 'http://chicago.craigslist.org/'\n\n    def parse(self, response):\n        links = response.xpath('//a[@class=\"hdrlnk\"]/@href').extract()\n        for link in links:\n            absolute_url = self.BASE_URL + link\n            yield scrapy.Request(absolute_url, callback=self.parse_attr)\n\n    def parse_attr(self, response):\n        item = DmozItem()\n        item[\"link\"] = response.url\n        item[\"attr\"] = \"\".join(response.xpath(\"//p[@class='attrgroup']//text()\").extract())\n        return item\n</code></pre>\n", "abstract": "You need to yield Request instances for the links to follow, assign a callback and extract the text of the desired p element in the callback:"}]}, {"link": "https://stackoverflow.com/questions/37207959/how-to-scrape-all-contents-from-infinite-scroll-website-scrapy", "question": {"id": "37207959", "title": "How to scrape all contents from infinite scroll website? scrapy", "content": "<p>I'm using scrapy.</p>\n<p>The website i'm using has infinite scroll.</p>\n<p>the website has loads of posts but i only scraped 13.</p>\n<p>How to scrape the rest of the posts?</p>\n<p>here's my code:</p>\n<pre><code class=\"python\">class exampleSpider(scrapy.Spider):\nname = \"example\"\n#from_date = datetime.date.today() - datetime.timedelta(6*365/12)\nallowed_domains = [\"example.com\"]\nstart_urls = [\n    \"http://www.example.com/somethinghere/\"\n]\n\ndef parse(self, response):\n  for href in response.xpath(\"//*[@id='page-wrap']/div/div/div/section[2]/div/div/div/div[3]/ul/li/div/h1/a/@href\"):\n    url = response.urljoin(href.extract())\n    yield scrapy.Request(url, callback=self.parse_dir_contents)\n\n\ndef parse_dir_contents(self, response):\n    #scrape contents code here\n</code></pre>\n", "abstract": "I'm using scrapy. The website i'm using has infinite scroll. the website has loads of posts but i only scraped 13. How to scrape the rest of the posts? here's my code:"}, "answers": [{"id": 43419784, "score": 10, "vote": 0, "content": "<p>Check the website code. </p>\n<p>If the infinite scroll is automatically triggering js action, you could proceed as follows using the Alioth proposal: <a href=\"http://0--key.github.io/emacs/spynner-installation.html\" rel=\"noreferrer\">spynner</a></p>\n<p>Following the spynner <a href=\"https://github.com/makinacorpus/spynner/blob/master/src/spynner/tests/spynner.rst\" rel=\"noreferrer\">docs</a>, you can find that can trigger jquery events.</p>\n<blockquote>\n<p>Look up the library code to see which kind of events you can fire.</p>\n</blockquote>\n<p>Try to generate a <em>scroll to bottom</em> event or <em>create a css property change</em> on any of the divs inside the scrollable content in the website. Following spynner <a href=\"https://github.com/makinacorpus/spynner/blob/master/src/spynner/tests/spynner.rst\" rel=\"noreferrer\">docs</a>, something like:</p>\n<pre><code class=\"python\">browser = spynner.Browser(debug_level=spynner.DEBUG, debug_stream=debug_stream)\n# load here your website as spynner allows\nbrowser.load_jquery(True)\nret = run_debug(browser.runjs,'window.scrollTo(0, document.body.scrollHeight);console.log(''scrolling...);')\n# continue parsing ret \n</code></pre>\n<p>It is not quite probable that an infinite scroll is triggered by an anchor link, but maybe can be triggered by a jquery action, not necesarry attached to a link. For this case use code like the following:</p>\n<pre><code class=\"python\">br.load('http://pypi.python.org/pypi')\n\nanchors = br.webframe.findAllElements('#menu ul.level-two a')\n# chooses an anchor with Browse word as key\nanchor = [a for a in anchors if 'Browse' in a.toPlainText()][0]\nbr.wk_click_element_link(anchor, timeout=10)\noutput = br.show()\n# save output in file: output.html or \n# plug this actions into your scrapy method and parse output var as you do \n# with response body\n</code></pre>\n<p>Then, run scrapy on the output.html file or, if you implemented it so, using the local memory variable you choosed to store the modified html after the js action.</p>\n<p>As another solution, the website you are trying to parse might have an <strong>alternate render</strong> version in case the visitor browser <strong>has not</strong> js activated.</p>\n<p>Try to render the website with a javascript disabled browser, and maybe that way, the website makes available an anchor link at the end of the content section.</p>\n<p>Also there are <strong>successful</strong> implementations of crawler js navigation using the approach with Scrapy together with Selenium detailed in <a href=\"https://stackoverflow.com/a/17697329/5476782\">this</a> so answer. </p>\n", "abstract": "Check the website code.  If the infinite scroll is automatically triggering js action, you could proceed as follows using the Alioth proposal: spynner Following the spynner docs, you can find that can trigger jquery events. Look up the library code to see which kind of events you can fire. Try to generate a scroll to bottom event or create a css property change on any of the divs inside the scrollable content in the website. Following spynner docs, something like: It is not quite probable that an infinite scroll is triggered by an anchor link, but maybe can be triggered by a jquery action, not necesarry attached to a link. For this case use code like the following: Then, run scrapy on the output.html file or, if you implemented it so, using the local memory variable you choosed to store the modified html after the js action. As another solution, the website you are trying to parse might have an alternate render version in case the visitor browser has not js activated. Try to render the website with a javascript disabled browser, and maybe that way, the website makes available an anchor link at the end of the content section. Also there are successful implementations of crawler js navigation using the approach with Scrapy together with Selenium detailed in this so answer. "}, {"id": 43368616, "score": 8, "vote": 0, "content": "<p>I use <code>Selenium</code> rather than <code>scrapy</code> but you must be able to do the equivalent and what I do is run some JavaScript on loading the file, namely:</p>\n<p><code>driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")</code></p>\n<p>And I jut keep doing that till it won't scroll any longer.  It's not pretty and could not be used in production but effective for specific jobs.</p>\n", "abstract": "I use Selenium rather than scrapy but you must be able to do the equivalent and what I do is run some JavaScript on loading the file, namely: driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") And I jut keep doing that till it won't scroll any longer.  It's not pretty and could not be used in production but effective for specific jobs."}, {"id": 37227791, "score": 2, "vote": 0, "content": "<p>i think what you are looking for is a pagination logic along side your normal logic</p>\n<p>In most cases ..<strong>infinite scrolling == paging</strong>, on such page when you scroll down to 3/4 of page or till to the end of the page , page fires AJAX call and downloads next page content and load the response into current page</p>\n<p>I would recommend using <code>network monitor</code> tool in firefox and notice any such page request when you scroll down</p>\n<p>-- <strong>clue</strong> : you will be using <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html?highlight=formrequest#scrapy.http.FormRequest\" rel=\"nofollow\">scrapy.FormRequest</a> or <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html?highlight=formrequest#scrapy.http.FormRequest.from_response\" rel=\"nofollow\">scrapy.FormRequest.from_response</a> while implementing this solution</p>\n", "abstract": "i think what you are looking for is a pagination logic along side your normal logic In most cases ..infinite scrolling == paging, on such page when you scroll down to 3/4 of page or till to the end of the page , page fires AJAX call and downloads next page content and load the response into current page I would recommend using network monitor tool in firefox and notice any such page request when you scroll down -- clue : you will be using scrapy.FormRequest or scrapy.FormRequest.from_response while implementing this solution"}, {"id": 37208423, "score": 1, "vote": 0, "content": "<p>I think you are looking for something like <code>DEPTH-LIMIT</code></p>\n<p><a href=\"http://doc.scrapy.org/en/latest/topics/settings.html#depth-limit\" rel=\"nofollow\">http://doc.scrapy.org/en/latest/topics/settings.html#depth-limit</a></p>\n<p><a href=\"http://bgrva.github.io/blog/2014/03/04/scrapy-after-tutorials-part-1/\" rel=\"nofollow\">http://bgrva.github.io/blog/2014/03/04/scrapy-after-tutorials-part-1/</a></p>\n", "abstract": "I think you are looking for something like DEPTH-LIMIT http://doc.scrapy.org/en/latest/topics/settings.html#depth-limit http://bgrva.github.io/blog/2014/03/04/scrapy-after-tutorials-part-1/"}, {"id": 43367555, "score": 1, "vote": 0, "content": "<p>Obviously, that target site upload its content <em>dynamically</em>. Hence there are two appropriate solutions there:</p>\n<ol>\n<li><p>Decrypt jQuery interaction in subtleties and try to simulate data exchange with server <a href=\"https://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax\">manually</a></p></li>\n<li><p>Use another tool for this particular job. For example <a href=\"http://0--key.github.io/emacs/spynner-installation.html\" rel=\"nofollow noreferrer\">spynner</a> seems to me a right choice to pay attention.</p></li>\n</ol>\n", "abstract": "Obviously, that target site upload its content dynamically. Hence there are two appropriate solutions there: Decrypt jQuery interaction in subtleties and try to simulate data exchange with server manually Use another tool for this particular job. For example spynner seems to me a right choice to pay attention."}, {"id": 51082152, "score": 1, "vote": 0, "content": "<p>In some cases, you can find in the source code the element called to run the \"next\" pagination, even in infinite scroll. So you just have to click on this element and it will show the rest of the posts. With scrapy/selenium :</p>\n<pre><code class=\"python\">next = self.driver.find_element_by_xpath('//a[@class=\"nextResults\"]')\nnext.click()\ntime.sleep(2) \n</code></pre>\n", "abstract": "In some cases, you can find in the source code the element called to run the \"next\" pagination, even in infinite scroll. So you just have to click on this element and it will show the rest of the posts. With scrapy/selenium :"}]}, {"link": "https://stackoverflow.com/questions/52910187/how-to-make-a-polygon-radar-spider-chart-in-python", "question": {"id": "52910187", "title": "How to make a polygon radar (spider) chart in python", "content": "<pre><code class=\"python\">import matplotlib.pyplot as plt\nimport numpy as np\n\nlabels=['Siege', 'Initiation', 'Crowd_control', 'Wave_clear', 'Objective_damage']\nmarkers = [0, 1, 2, 3, 4, 5]\nstr_markers = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n\ndef make_radar_chart(name, stats, attribute_labels=labels,\n                     plot_markers=markers, plot_str_markers=str_markers):\n\n    labels = np.array(attribute_labels)\n\n    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n    stats = np.concatenate((stats,[stats[0]]))\n    angles = np.concatenate((angles,[angles[0]]))\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, polar=True)\n    ax.plot(angles, stats, 'o-', linewidth=2)\n    ax.fill(angles, stats, alpha=0.25)\n    ax.set_thetagrids(angles * 180/np.pi, labels)\n    plt.yticks(markers)\n    ax.set_title(name)\n    ax.grid(True)\n\n    fig.savefig(\"static/images/%s.png\" % name)\n\n    return plt.show()\n\nmake_radar_chart(\"Agni\", [2,3,4,4,5]) # example\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/bIpwH.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/bIpwH.png\"/></a></p>\n<p><a href=\"https://i.stack.imgur.com/XgQDe.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/XgQDe.png\"/></a></p>\n<p>Basically I want the chart to be a pentagon instead of circle. Can anyone help with this. I am using python matplotlib to save an image which will stored and displayed later. I want my chart to have the form of the second picture</p>\n<p>EDIT:</p>\n<pre><code class=\"python\">    gridlines = ax.yaxis.get_gridlines()\n    for gl in gridlines:\n        gl.get_path()._interpolation_steps = 5\n</code></pre>\n<p>adding this section of code from answer below helped a lot. I am getting this chart. Still need to figure out how to get rid of the outer most ring: <a href=\"https://i.stack.imgur.com/7VWBh.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/7VWBh.png\"/></a></p>\n", "abstract": "  Basically I want the chart to be a pentagon instead of circle. Can anyone help with this. I am using python matplotlib to save an image which will stored and displayed later. I want my chart to have the form of the second picture EDIT: adding this section of code from answer below helped a lot. I am getting this chart. Still need to figure out how to get rid of the outer most ring: "}, "answers": [{"id": 52911181, "score": 20, "vote": 0, "content": "<p>The <a href=\"https://matplotlib.org/gallery/specialty_plots/radar_chart.html\" rel=\"noreferrer\">radar chart demo</a> shows how to make the a radar chart. The result looks like this:</p>\n<p><a href=\"https://i.stack.imgur.com/btsxj.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/btsxj.png\"/></a></p>\n<p>Here, the outer spine is polygon shaped as desired. However the inner grid lines are circular. \nSo the open question is how to make the gridlines the same shape as the spines.</p>\n<p>This can be done by overriding the <code>draw</code> method and setting the gridlines' path interpolation step variable to the number of variables of the <code>RadarAxes</code> class.</p>\n<pre><code class=\"python\">gridlines = self.yaxis.get_gridlines()\nfor gl in gridlines:\n    gl.get_path()._interpolation_steps = num_vars\n</code></pre>\n<p>Complete example:</p>\n<pre><code class=\"python\">import numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame='circle'):\n    \"\"\"Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {'circle' | 'polygon'}\n        Shape of frame surrounding axes.\n\n    \"\"\"\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n\n    class RadarAxes(PolarAxes):\n\n        name = 'radar'\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location('N')\n\n        def fill(self, *args, closed=True, **kwargs):\n            \"\"\"Override fill so that line is closed by default\"\"\"\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            \"\"\"Override plot so that line is closed by default\"\"\"\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == 'circle':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == 'polygon':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=\"k\")\n            else:\n                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n\n        def draw(self, renderer):\n            \"\"\" Draw. If frame is polygon, make gridlines polygon-shaped \"\"\"\n            if frame == 'polygon':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == 'circle':\n                return super()._gen_axes_spines()\n            elif frame == 'polygon':\n                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n                spine = Spine(axes=self,\n                              spine_type='circle',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {'polar': spine}\n            else:\n                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [['Sulfate', 'Nitrate', 'EC', 'OC1', 'OC2', 'OC3', 'OP', 'CO', 'O3'],\n        ('Basecase', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame='polygon')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection='radar'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha='center')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d,  alpha=0.25)\nax.set_varlabels(spoke_labels)\n\nplt.show()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/NbcQ1.png\" rel=\"noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/NbcQ1.png\"/></a></p>\n", "abstract": "The radar chart demo shows how to make the a radar chart. The result looks like this:  Here, the outer spine is polygon shaped as desired. However the inner grid lines are circular. \nSo the open question is how to make the gridlines the same shape as the spines. This can be done by overriding the draw method and setting the gridlines' path interpolation step variable to the number of variables of the RadarAxes class. Complete example: "}, {"id": 71095608, "score": 0, "vote": 0, "content": "<p>As shown in this <a href=\"https://stackoverflow.com/questions/65514398/how-to-make-radar-spider-chart-with-pentagon-grid-using-matplotlib-and-python?noredirect=1&amp;lq=1\">other post</a> the answer from @ImportanceOfBeingErnest doesn't work in matplotlib&gt;3.2.2 in that you get circular grids. As shown in this <a href=\"https://github.com/matplotlib/matplotlib/pull/22458/files\" rel=\"nofollow noreferrer\">PR</a> you can use the following</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, RegularPolygon\nfrom matplotlib.path import Path\nfrom matplotlib.projections.polar import PolarAxes\nfrom matplotlib.projections import register_projection\nfrom matplotlib.spines import Spine\nfrom matplotlib.transforms import Affine2D\n\n\ndef radar_factory(num_vars, frame='circle'):\n    \"\"\"Create a radar chart with `num_vars` axes.\n\n    This function creates a RadarAxes projection and registers it.\n\n    Parameters\n    ----------\n    num_vars : int\n        Number of variables for radar chart.\n    frame : {'circle' | 'polygon'}\n        Shape of frame surrounding axes.\n\n    \"\"\"\n    # calculate evenly-spaced axis angles\n    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n    \n    class RadarTransform(PolarAxes.PolarTransform):\n        def transform_path_non_affine(self, path):\n            # Paths with non-unit interpolation steps correspond to gridlines,\n            # in which case we force interpolation (to defeat PolarTransform's\n            # autoconversion to circular arcs).\n            if path._interpolation_steps &gt; 1:\n                path = path.interpolated(num_vars)\n            return Path(self.transform(path.vertices), path.codes)\n\n    class RadarAxes(PolarAxes):\n\n        name = 'radar'\n        \n        PolarTransform = RadarTransform\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # rotate plot such that the first axis is at the top\n            self.set_theta_zero_location('N')\n\n        def fill(self, *args, closed=True, **kwargs):\n            \"\"\"Override fill so that line is closed by default\"\"\"\n            return super().fill(closed=closed, *args, **kwargs)\n\n        def plot(self, *args, **kwargs):\n            \"\"\"Override plot so that line is closed by default\"\"\"\n            lines = super().plot(*args, **kwargs)\n            for line in lines:\n                self._close_line(line)\n\n        def _close_line(self, line):\n            x, y = line.get_data()\n            # FIXME: markers at x[0], y[0] get doubled-up\n            if x[0] != x[-1]:\n                x = np.concatenate((x, [x[0]]))\n                y = np.concatenate((y, [y[0]]))\n                line.set_data(x, y)\n\n        def set_varlabels(self, labels):\n            self.set_thetagrids(np.degrees(theta), labels)\n\n        def _gen_axes_patch(self):\n            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n            # in axes coordinates.\n            if frame == 'circle':\n                return Circle((0.5, 0.5), 0.5)\n            elif frame == 'polygon':\n                return RegularPolygon((0.5, 0.5), num_vars,\n                                      radius=.5, edgecolor=\"k\")\n            else:\n                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n\n        def draw(self, renderer):\n            \"\"\" Draw. If frame is polygon, make gridlines polygon-shaped \"\"\"\n            if frame == 'polygon':\n                gridlines = self.yaxis.get_gridlines()\n                for gl in gridlines:\n                    gl.get_path()._interpolation_steps = num_vars\n            super().draw(renderer)\n\n\n        def _gen_axes_spines(self):\n            if frame == 'circle':\n                return super()._gen_axes_spines()\n            elif frame == 'polygon':\n                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n                spine = Spine(axes=self,\n                              spine_type='circle',\n                              path=Path.unit_regular_polygon(num_vars))\n                # unit_regular_polygon gives a polygon of radius 1 centered at\n                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n                # 0.5) in axes coordinates.\n                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n                                    + self.transAxes)\n\n\n                return {'polar': spine}\n            else:\n                raise ValueError(\"unknown value for 'frame': %s\" % frame)\n\n    register_projection(RadarAxes)\n    return theta\n\n\ndata = [['Sulfate', 'Nitrate', 'EC', 'OC1', 'OC2', 'OC3', 'OP', 'CO', 'O3'],\n        ('Basecase', [\n            [0.88, 0.01, 0.03, 0.03, 0.00, 0.06, 0.01, 0.00, 0.00],\n            [0.07, 0.95, 0.04, 0.05, 0.00, 0.02, 0.01, 0.00, 0.00],\n            [0.01, 0.02, 0.85, 0.19, 0.05, 0.10, 0.00, 0.00, 0.00],\n            [0.02, 0.01, 0.07, 0.01, 0.21, 0.12, 0.98, 0.00, 0.00],\n            [0.01, 0.01, 0.02, 0.71, 0.74, 0.70, 0.00, 0.00, 0.00]])]\n\nN = len(data[0])\ntheta = radar_factory(N, frame='polygon')\n\nspoke_labels = data.pop(0)\ntitle, case_data = data[0]\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection='radar'))\nfig.subplots_adjust(top=0.85, bottom=0.05)\n\nax.set_rgrids([0.2, 0.4, 0.6, 0.8])\nax.set_title(title,  position=(0.5, 1.1), ha='center')\n\nfor d in case_data:\n    line = ax.plot(theta, d)\n    ax.fill(theta, d, alpha=0.25, label='_nolegend_')\nax.set_varlabels(spoke_labels)\n\nplt.show()\n</code></pre>\n<p>to get the desired:</p>\n<p><a href=\"https://i.stack.imgur.com/Cstccm.png\" rel=\"nofollow noreferrer\"><img alt=\"enter image description here\" src=\"https://i.stack.imgur.com/Cstccm.png\"/></a></p>\n", "abstract": "As shown in this other post the answer from @ImportanceOfBeingErnest doesn't work in matplotlib>3.2.2 in that you get circular grids. As shown in this PR you can use the following to get the desired: "}]}, {"link": "https://stackoverflow.com/questions/17199457/scrapy-vs-nutch", "question": {"id": "17199457", "title": "Scrapy Vs Nutch", "content": "<p>I am planning to use webcrawling in an application i am currently working on. I did some research on Nutch and run some preliminary test using it. But then i came across scrapy. But when i did some preliminary research and went through the documentation about scrapy i found that it can capture only structed data (You have to give the div name from which you want to capture data). The backend of the application i am developing is based on Python and i understand scrapy is Python based and some have suggested that scrapy is better than Nutch.</p>\n<p>My requirement is to capture the data from more than a 1000 different webpages and run search for relevant keywords in that information.Is there any way scrapy can satisfy the same requirement. </p>\n<p>1)If yes can you point out some example on how it can be done ?</p>\n<p>2)Or Nutch+Solr is best suited for my requirement</p>\n", "abstract": "I am planning to use webcrawling in an application i am currently working on. I did some research on Nutch and run some preliminary test using it. But then i came across scrapy. But when i did some preliminary research and went through the documentation about scrapy i found that it can capture only structed data (You have to give the div name from which you want to capture data). The backend of the application i am developing is based on Python and i understand scrapy is Python based and some have suggested that scrapy is better than Nutch. My requirement is to capture the data from more than a 1000 different webpages and run search for relevant keywords in that information.Is there any way scrapy can satisfy the same requirement.  1)If yes can you point out some example on how it can be done ? 2)Or Nutch+Solr is best suited for my requirement"}, "answers": [{"id": 17200157, "score": 18, "vote": 0, "content": "<p><code>Scrapy</code> would work perfectly in your case. </p>\n<p>You are not required to give divs names - you can get anything you want:</p>\n<blockquote>\n<p>Scrapy comes with its own mechanism for extracting data. They\u2019re\n  called XPath selectors (or just \u201cselectors\u201d, for short) because they\n  \u201cselect\u201d certain parts of the HTML document specified by XPath\n  expressions.</p>\n</blockquote>\n<p>Plus, you can use <code>BeautifulSoup</code> and <code>lxml</code> for extracting the data from the page content.</p>\n<p>Besides, <code>scrapy</code> is based on twisted and is completely async and fast. </p>\n<p>There are plenty of examples scrapy spiders here on SO - just look through the <a class=\"post-tag\" href=\"/questions/tagged/scrapy\" rel=\"tag\" title=\"show questions tagged 'scrapy'\">scrapy</a> tag questions. If you have a more specific question - just ask.</p>\n<p>Hope that helps.</p>\n", "abstract": "Scrapy would work perfectly in your case.  You are not required to give divs names - you can get anything you want: Scrapy comes with its own mechanism for extracting data. They\u2019re\n  called XPath selectors (or just \u201cselectors\u201d, for short) because they\n  \u201cselect\u201d certain parts of the HTML document specified by XPath\n  expressions. Plus, you can use BeautifulSoup and lxml for extracting the data from the page content. Besides, scrapy is based on twisted and is completely async and fast.  There are plenty of examples scrapy spiders here on SO - just look through the scrapy tag questions. If you have a more specific question - just ask. Hope that helps."}]}, {"link": "https://stackoverflow.com/questions/40237952/get-scrapy-crawler-output-results-in-script-file-function", "question": {"id": "40237952", "title": "Get Scrapy crawler output/results in script file function", "content": "<p>I am using a script file to run a spider within scrapy project and spider is logging the crawler output/results. But i want to use spider output/results in that script file in some function .I did not want to save output/results in any file or DB.\nHere is Script code get from <a href=\"https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script\" rel=\"noreferrer\">https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script</a></p>\n<pre><code class=\"python\">from twisted.internet import reactor\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\nfrom scrapy.utils.project import get_project_settings\n\nconfigure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\nrunner = CrawlerRunner(get_project_settings())\n\n\nd = runner.crawl('my_spider')\nd.addBoth(lambda _: reactor.stop())\nreactor.run()\n\ndef spider_output(output):\n#     do something to that output\n</code></pre>\n<p>How can i get spider output in 'spider_output' method. It is possible to get output/results.</p>\n", "abstract": "I am using a script file to run a spider within scrapy project and spider is logging the crawler output/results. But i want to use spider output/results in that script file in some function .I did not want to save output/results in any file or DB.\nHere is Script code get from https://doc.scrapy.org/en/latest/topics/practices.html#run-from-script How can i get spider output in 'spider_output' method. It is possible to get output/results."}, "answers": [{"id": 40240712, "score": 27, "vote": 0, "content": "<p>Here is the solution that get all output/results in a list</p>\n<pre><code class=\"python\">from scrapy import signals\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nfrom scrapy.signalmanager import dispatcher\n\n\ndef spider_results():\n    results = []\n\n    def crawler_results(signal, sender, item, response, spider):\n        results.append(item)\n\n    dispatcher.connect(crawler_results, signal=signals.item_scraped)\n\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(MySpider)\n    process.start()  # the script will block here until the crawling is finished\n    return results\n\n\nif __name__ == '__main__':\n    print(spider_results())\n</code></pre>\n", "abstract": "Here is the solution that get all output/results in a list"}, {"id": 62902603, "score": 7, "vote": 0, "content": "<p>This is an old question, but for future reference. If you are working with python 3.6+ I recommend using <a href=\"https://github.com/jschnurr/scrapyscript\" rel=\"noreferrer\">scrapyscript</a> that allows you to run your Spiders and get the results in a super simple way:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">from scrapyscript import Job, Processor\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\nimport json\n\n# Define a Scrapy Spider, which can accept *args or **kwargs\n# https://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments\nclass PythonSpider(Spider):\n    name = 'myspider'\n\n    def start_requests(self):\n        yield Request(self.url)\n\n    def parse(self, response):\n        title = response.xpath('//title/text()').extract()\n        return {'url': response.request.url, 'title': title}\n\n# Create jobs for each instance. *args and **kwargs supplied here will\n# be passed to the spider constructor at runtime\ngithubJob = Job(PythonSpider, url='http://www.github.com')\npythonJob = Job(PythonSpider, url='http://www.python.org')\n\n# Create a Processor, optionally passing in a Scrapy Settings object.\nprocessor = Processor(settings=None)\n\n# Start the reactor, and block until all spiders complete.\ndata = processor.run([githubJob, pythonJob])\n\n# Print the consolidated results\nprint(json.dumps(data, indent=4))\n</code></pre>\n<pre><code class=\"python\">[\n    {\n        \"title\": [\n            \"Welcome to Python.org\"\n        ],\n        \"url\": \"https://www.python.org/\"\n    },\n    {\n        \"title\": [\n            \"The world's leading software development platform \\u00b7 GitHub\",\n            \"1clr-code-hosting\"\n        ],\n        \"url\": \"https://github.com/\"\n    }\n]\n</code></pre>\n", "abstract": "This is an old question, but for future reference. If you are working with python 3.6+ I recommend using scrapyscript that allows you to run your Spiders and get the results in a super simple way:"}, {"id": 40239713, "score": 1, "vote": 0, "content": "<p>AFAIK there is no way to do this, since <a href=\"https://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerRunner.crawl\" rel=\"nofollow\">crawl()</a>:</p>\n<blockquote>\n<p>Returns a deferred that is fired when the crawling is finished.</p>\n</blockquote>\n<p>And the crawler doesn't store results anywhere other than outputting them to logger.</p>\n<p>However returning ouput would conflict with the whole asynchronious nature and structure of scrapy, so saving to file then reading it is a prefered approach here.<br/>\nYou can simply devise pipeline that saves your items to file and simply read the file in your <code>spider_output</code>. You will receive your results since <code>reactor.run()</code> is blocking your script untill the output file is complete anyways.</p>\n", "abstract": "AFAIK there is no way to do this, since crawl(): Returns a deferred that is fired when the crawling is finished. And the crawler doesn't store results anywhere other than outputting them to logger. However returning ouput would conflict with the whole asynchronious nature and structure of scrapy, so saving to file then reading it is a prefered approach here.\nYou can simply devise pipeline that saves your items to file and simply read the file in your spider_output. You will receive your results since reactor.run() is blocking your script untill the output file is complete anyways."}, {"id": 51794356, "score": 0, "vote": 0, "content": "<p>My advice is to use the Python <code>subprocess</code> module to run spider from the script rather than using the method provided in the scrapy docs to run spider from python script. The reason for that is that with the <code>subprocess</code> module, you can capture the output/logs and even statements that you <code>print</code> from inside the spider.</p>\n<p>In Python 3, execute the spider with the <code>run</code> method. Ex.</p>\n<pre><code class=\"python\">import subprocess\nprocess = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nif process.returncode == 0:\n    result = process.stdout.decode('utf-8')\nelse:\n    # code to check error using 'process.stderr'\n</code></pre>\n<p>Setting the stdout/stderr to <code>subprocess.PIPE</code> will allow capture of output so it's very important to set this flag.\nHere <code>command</code> should be a sequence or a string (It it's a string, then call the <code>run</code> method with 1 more param: <code>shell=True</code>). For example:</p>\n<pre><code class=\"python\">command = ['scrapy', 'crawl', 'website', '-a', 'customArg=blahblah']\n# or\ncommand = 'scrapy crawl website -a customArg=blahblah' # with shell=True\n#or\nimport shlex\ncommand = shlex.split('scrapy crawl website -a customArg=blahblah') # without shell=True\n</code></pre>\n<p>Also, <code>process.stdout</code> will contain the output from the script but it will be of type <code>bytes</code>. You need to convert it to <code>str</code> using <code>decode('utf-8')</code></p>\n", "abstract": "My advice is to use the Python subprocess module to run spider from the script rather than using the method provided in the scrapy docs to run spider from python script. The reason for that is that with the subprocess module, you can capture the output/logs and even statements that you print from inside the spider. In Python 3, execute the spider with the run method. Ex. Setting the stdout/stderr to subprocess.PIPE will allow capture of output so it's very important to set this flag.\nHere command should be a sequence or a string (It it's a string, then call the run method with 1 more param: shell=True). For example: Also, process.stdout will contain the output from the script but it will be of type bytes. You need to convert it to str using decode('utf-8')"}, {"id": 66521322, "score": 0, "vote": 0, "content": "<p>It's going to return all the results of a Spider within a list.</p>\n<pre><code class=\"python\">from scrapyscript import Job, Processor\nfrom scrapy.utils.project import get_project_settings\n\n\ndef get_spider_output(spider, **kwargs):\n    job = Job(spider, **kwargs)\n    processor = Processor(settings=get_project_settings())\n    return processor.run([job])\n</code></pre>\n", "abstract": "It's going to return all the results of a Spider within a list."}]}, {"link": "https://stackoverflow.com/questions/3871613/scrapy-how-to-identify-already-scraped-urls", "question": {"id": "3871613", "title": "Scrapy - how to identify already scraped urls", "content": "<p>Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on <code>SgmlLinkExtractor</code>.</p>\n", "abstract": "Im using scrapy to crawl a news website on a daily basis. How do i  restrict scrapy from scraping already scraped URLs. Also is there any clear documentation or examples on SgmlLinkExtractor."}, "answers": [{"id": 4201553, "score": 15, "vote": 0, "content": "<p>You can actually do this quite easily with the scrapy snippet located here: <a href=\"http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/\" rel=\"nofollow noreferrer\">http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/</a></p>\n<p>To use it, copy the code from the link and put it into some file in your scrapy project. \nTo reference it, add a line in your settings.py to reference it:</p>\n<pre><code class=\"python\">SPIDER_MIDDLEWARES = { 'project.middlewares.ignore.IgnoreVisitedItems': 560 }\n</code></pre>\n<p>The specifics on WHY you pick the number that you do can be read up here: <a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html\" rel=\"nofollow noreferrer\">http://doc.scrapy.org/en/latest/topics/downloader-middleware.html</a></p>\n<p>Finally, you'll need to modify your items.py so that each item class has the following fields:</p>\n<pre><code class=\"python\">visit_id = Field()\nvisit_status = Field()\n</code></pre>\n<p>And I think that's it.  The next time you run your spider it should automatically try to start avoiding the same sites.  </p>\n<p>Good luck!</p>\n", "abstract": "You can actually do this quite easily with the scrapy snippet located here: http://snipplr.com/view/67018/middleware-to-avoid-revisiting-already-visited-items/ To use it, copy the code from the link and put it into some file in your scrapy project. \nTo reference it, add a line in your settings.py to reference it: The specifics on WHY you pick the number that you do can be read up here: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html Finally, you'll need to modify your items.py so that each item class has the following fields: And I think that's it.  The next time you run your spider it should automatically try to start avoiding the same sites.   Good luck!"}, {"id": 3871902, "score": 1, "vote": 0, "content": "<p>This is straight forward. Maintain all your previously crawled urls in python dict. So when you try to try them next time, see if that url is there in the dict. else crawl.</p>\n<pre><code class=\"python\">def load_urls(prev_urls):\n    prev = dict()\n    for url in prev_urls:\n        prev[url] = True\n    return prev\n\ndef fresh_crawl(prev_urls, new_urls):\n    for url in new_urls:\n        if url not in prev_urls:\n            crawl(url)\n    return\n\ndef main():\n    purls = load_urls(prev_urls)\n    fresh_crawl(purls, nurls)\n    return\n</code></pre>\n<p>The above code was typed in SO text editor aka browser. Might have syntax errors. You might also need to make a few changes. But the logic is there...</p>\n<p><strong>NOTE:</strong> But beware that some websites constantly keep changing their content. So sometimes you might have to recrawl a particular webpage (i.e. same url) just to get the updated content.</p>\n", "abstract": "This is straight forward. Maintain all your previously crawled urls in python dict. So when you try to try them next time, see if that url is there in the dict. else crawl. The above code was typed in SO text editor aka browser. Might have syntax errors. You might also need to make a few changes. But the logic is there... NOTE: But beware that some websites constantly keep changing their content. So sometimes you might have to recrawl a particular webpage (i.e. same url) just to get the updated content."}, {"id": 8830983, "score": 1, "vote": 0, "content": "<p>I think jama22's answer is a little incomplete. </p>\n<p>In the snippet <code>if self.FILTER_VISITED in x.meta</code>:, you can see that you require FILTER_VISITED in your Request instance in order for that request to be ignored. This is to ensure that you can differentiate between links that you want to traverse and move around and item links that well, you don't want to see again.</p>\n", "abstract": "I think jama22's answer is a little incomplete.  In the snippet if self.FILTER_VISITED in x.meta:, you can see that you require FILTER_VISITED in your Request instance in order for that request to be ignored. This is to ensure that you can differentiate between links that you want to traverse and move around and item links that well, you don't want to see again."}, {"id": 13578588, "score": 1, "vote": 0, "content": "<p>Scrapy can auto-filter urls which are scraped, isn't it? Some different urls point to the same page will not be filtered, such as \"www.xxx.com/home/\" and \"www.xxx.com/home/index.html\".</p>\n", "abstract": "Scrapy can auto-filter urls which are scraped, isn't it? Some different urls point to the same page will not be filtered, such as \"www.xxx.com/home/\" and \"www.xxx.com/home/index.html\"."}, {"id": 59638805, "score": 0, "vote": 0, "content": "<p>For today (2019), this post is the best answer for this problem. </p>\n<p><a href=\"https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016\" rel=\"nofollow noreferrer\">https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016</a></p>\n<p>It's a lib to handle MIDDLEWARES automatcally.</p>\n<p>Hope to help someone. I've spent a lot of time seaching for this.</p>\n", "abstract": "For today (2019), this post is the best answer for this problem.  https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016 It's a lib to handle MIDDLEWARES automatcally. Hope to help someone. I've spent a lot of time seaching for this."}]}, {"link": "https://stackoverflow.com/questions/23152739/how-to-make-scrapy-show-user-agent-per-download-request-in-log", "question": {"id": "23152739", "title": "How to make Scrapy show user agent per download request in log?", "content": "<p>I am learning <a href=\"http://scrapy.org/\" rel=\"noreferrer\">Scrapy</a>, a web crawling framework.  </p>\n<p>I know I can set <code>USER_AGENT</code> in <code>settings.py</code> file of the Scrapy project. When I run the Scrapy, I can see the <code>USER_AGENT</code>'s value in <code>INFO</code> logs.<br/>\nThis <code>USER_AGENT</code> gets set in every download request to the server I want to crawl.  </p>\n<p>But I am using multiple <code>USER_AGENT</code> <strong>randomly</strong> with the help of <a href=\"http://tangww.com/2013/06/UsingRandomAgent/\" rel=\"noreferrer\">this solution</a>. I guess this randomly chosen <code>USER_AGENT</code> would be working. I want to confirm it. So, how I can make Scrapy <strong>shows</strong> <code>USER_AGENT</code> per download request so I can see the value of <code>USER_AGENT</code> in the logs? </p>\n", "abstract": "I am learning Scrapy, a web crawling framework.   I know I can set USER_AGENT in settings.py file of the Scrapy project. When I run the Scrapy, I can see the USER_AGENT's value in INFO logs.\nThis USER_AGENT gets set in every download request to the server I want to crawl.   But I am using multiple USER_AGENT randomly with the help of this solution. I guess this randomly chosen USER_AGENT would be working. I want to confirm it. So, how I can make Scrapy shows USER_AGENT per download request so I can see the value of USER_AGENT in the logs? "}, "answers": [{"id": 27426323, "score": 29, "vote": 0, "content": "<p>Just FYI.</p>\n<p>I've implemented a simple <a href=\"https://github.com/alecxe/scrapy-fake-useragent\" rel=\"noreferrer\"><code>RandomUserAgentMiddleware</code> middleware</a> based on <a href=\"https://pypi.python.org/pypi/fake-useragent\" rel=\"noreferrer\"><code>fake-useragent</code></a>.</p>\n<p>Thanks to <code>fake-useragent</code>, you don't need to configure the list of User-Agents - it picks them up <a href=\"http://www.w3schools.com/browsers/browsers_stats.asp\" rel=\"noreferrer\">based on browser usage statistics</a> from a <a href=\"http://useragentstring.com\" rel=\"noreferrer\">real-world database</a>.</p>\n", "abstract": "Just FYI. I've implemented a simple RandomUserAgentMiddleware middleware based on fake-useragent. Thanks to fake-useragent, you don't need to configure the list of User-Agents - it picks them up based on browser usage statistics from a real-world database."}, {"id": 32873801, "score": 18, "vote": 0, "content": "<p>You can see it by using this:</p>\n<pre><code class=\"python\">def parse(self, response):\n    print response.request.headers['User-Agent']\n</code></pre>\n<p>You can use <code>scrapy-fake-useragent</code> python library. It works perfectly and it chooses user agent based on world usage statistic. But be careful, check if it's already working perfectly using above code since you might do some mistake when applying it. Read the instruction carefully.</p>\n", "abstract": "You can see it by using this: You can use scrapy-fake-useragent python library. It works perfectly and it chooses user agent based on world usage statistic. But be careful, check if it's already working perfectly using above code since you might do some mistake when applying it. Read the instruction carefully."}, {"id": 23219821, "score": 11, "vote": 0, "content": "<p>You can add logging to <a href=\"http://tangww.com/2013/06/UsingRandomAgent/\">solution</a> you're using:</p>\n<pre><code class=\"python\">#!/usr/bin/python\n#-*-coding:utf-8-*-\nimport random\n\nfrom scrapy import log\nfrom scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n\nclass RotateUserAgentMiddleware(UserAgentMiddleware):\n    def __init__(self, user_agent=''):\n        self.user_agent = user_agent\n\n    def process_request(self, request, spider):\n        ua = random.choice(self.user_agent_list)\n        if ua:\n            request.headers.setdefault('User-Agent', ua)\n\n            # Add desired logging message here.\n            spider.log(\n                u'User-Agent: {} {}'.format(request.headers.get('User-Agent'), request),\n                level=log.DEBUG\n            )\n\n\n    #the default user_agent_list composes chrome,IE,firefox,Mozilla,opera,netscape\n    #for more user agent strings,you can find it in http://www.useragentstring.com/pages/useragentstring.php\n    user_agent_list = [\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\",\n        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\n    ]\n</code></pre>\n", "abstract": "You can add logging to solution you're using:"}, {"id": 29033474, "score": 2, "vote": 0, "content": "<p><strong>EDIT:</strong> I came here because I was looking for the functionality to change the user agent.</p>\n<p>Based to alecxe's RandomUserAgent, this is what I use to set the user agent only once per crawl and only from a predefined list (works for me with scrapy v. 0.24 &amp; 0.25):</p>\n<pre><code class=\"python\">    \"\"\"\n    Choose a user agent from the settings but do it only once per crawl.\n    \"\"\"\n    import random\n    import scrapy\n\n    SETTINGS = scrapy.utils.project.get_project_settings()\n\n\n    class RandomUserAgentMiddleware(object):\n        def __init__(self):\n            super(RandomUserAgentMiddleware, self).__init__()\n            self.fixedUserAgent = random.choice(SETTINGS.get('USER_AGENTS'))\n            scrapy.log.msg('User Agent for this crawl is: {}'.\n                           format(self.fixedUserAgent))\n\n        def process_start_requests(self, start_requests, spider):\n            for r in start_requests:\n                r.headers.setdefault('User-Agent', self.fixedUserAgent)\n                yield r\n</code></pre>\n<p><strong>The actual answer to your question is: Check for the UA by using a local webserver and see check the logs (e.g. /var/log/apache2/access.log on *NIX).</strong></p>\n", "abstract": "EDIT: I came here because I was looking for the functionality to change the user agent. Based to alecxe's RandomUserAgent, this is what I use to set the user agent only once per crawl and only from a predefined list (works for me with scrapy v. 0.24 & 0.25): The actual answer to your question is: Check for the UA by using a local webserver and see check the logs (e.g. /var/log/apache2/access.log on *NIX)."}]}, {"link": "https://stackoverflow.com/questions/8386481/web-crawler-ignore-robots-txt-file", "question": {"id": "8386481", "title": "Web Crawler - Ignore Robots.txt file?", "content": "<p>Some servers have a robots.txt file in order to stop web crawlers from crawling through their websites. Is there a way to make a web crawler ignore the robots.txt file? I am using Mechanize for python.</p>\n", "abstract": "Some servers have a robots.txt file in order to stop web crawlers from crawling through their websites. Is there a way to make a web crawler ignore the robots.txt file? I am using Mechanize for python."}, "answers": [{"id": 8386555, "score": 29, "vote": 0, "content": "<p>The <a href=\"http://wwwsearch.sourceforge.net/mechanize/\" rel=\"noreferrer\">documentation</a> for mechanize has this sample code:</p>\n<pre><code class=\"python\">br = mechanize.Browser()\n....\n# Ignore robots.txt.  Do not do this without thought and consideration.\nbr.set_handle_robots(False)\n</code></pre>\n<p>That does exactly what you want.</p>\n", "abstract": "The documentation for mechanize has this sample code: That does exactly what you want."}, {"id": 8386542, "score": 9, "vote": 0, "content": "<p><a href=\"http://dinomite.net/2007/web-browsing-with-python/\" rel=\"noreferrer\"><strong>This</strong></a> looks like what you need:</p>\n<pre><code class=\"python\">from mechanize import Browser\nbr = Browser()\n\n# Ignore robots.txt\nbr.set_handle_robots( False )\n</code></pre>\n<p>but you know what you're doing\u2026</p>\n", "abstract": "This looks like what you need: but you know what you're doing\u2026"}]}, {"link": "https://stackoverflow.com/questions/7766414/replay-a-scrapy-spider-on-stored-data", "question": {"id": "7766414", "title": "Replay a Scrapy spider on stored data", "content": "<p>I have started using <a href=\"http://scrapy.org/\" rel=\"noreferrer\">Scrapy</a> to scrape a few websites. If I later add a new field to my model or change my parsing functions, I'd like to be able to \"replay\" the downloaded raw data offline to scrape it again. It looks like Scrapy had the ability to store raw data in a replay file at one point:</p>\n<p><a href=\"http://dev.scrapy.org/browser/scrapy/trunk/scrapy/command/commands/replay.py?rev=168\" rel=\"noreferrer\">http://dev.scrapy.org/browser/scrapy/trunk/scrapy/command/commands/replay.py?rev=168</a></p>\n<p>But this functionality seems to have been removed in the current version of Scrapy. Is there another way to achieve this?</p>\n", "abstract": "I have started using Scrapy to scrape a few websites. If I later add a new field to my model or change my parsing functions, I'd like to be able to \"replay\" the downloaded raw data offline to scrape it again. It looks like Scrapy had the ability to store raw data in a replay file at one point: http://dev.scrapy.org/browser/scrapy/trunk/scrapy/command/commands/replay.py?rev=168 But this functionality seems to have been removed in the current version of Scrapy. Is there another way to achieve this?"}, "answers": [{"id": 7830024, "score": 21, "vote": 0, "content": "<p>If you run <code>crawl --record=[cache.file] [scraper]</code>, you'll be able then use <code>replay [scraper]</code>.</p>\n<p>Alternatively, you can cache all responses with the <a href=\"http://readthedocs.org/docs/scrapy/en/latest/topics/downloader-middleware.html#module-scrapy.contrib.downloadermiddleware.httpcache\" rel=\"noreferrer\"><code>HttpCacheMiddleware</code></a> by including it in <code>DOWNLOADER_MIDDLEWARES</code>:  </p>\n<pre><code class=\"python\">DOWNLOADER_MIDDLEWARES = {\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 300,\n}\n</code></pre>\n<p>If you do this, every time you run the scraper, it will check the file system first.</p>\n", "abstract": "If you run crawl --record=[cache.file] [scraper], you'll be able then use replay [scraper]. Alternatively, you can cache all responses with the HttpCacheMiddleware by including it in DOWNLOADER_MIDDLEWARES:   If you do this, every time you run the scraper, it will check the file system first."}, {"id": 12363754, "score": 5, "vote": 0, "content": "<p>You can enable HTTPCACHE_ENABLED as said <a href=\"http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html?highlight=FilesystemCacheStorage#httpcache-enabled\" rel=\"noreferrer\">http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html?highlight=FilesystemCacheStorage#httpcache-enabled</a></p>\n<p>to cache all http request and response to implement resume crawling.</p>\n<p>OR try Jobs to pause and resume crawling\n<a href=\"http://scrapy.readthedocs.org/en/latest/topics/jobs.html\" rel=\"noreferrer\">http://scrapy.readthedocs.org/en/latest/topics/jobs.html</a></p>\n", "abstract": "You can enable HTTPCACHE_ENABLED as said http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html?highlight=FilesystemCacheStorage#httpcache-enabled to cache all http request and response to implement resume crawling. OR try Jobs to pause and resume crawling\nhttp://scrapy.readthedocs.org/en/latest/topics/jobs.html"}]}, {"link": "https://stackoverflow.com/questions/9699049/how-do-i-stop-all-spiders-and-the-engine-immediately-after-a-condition-in-a-pipe", "question": {"id": "9699049", "title": "How do I stop all spiders and the engine immediately after a condition in a pipeline is met?", "content": "<p>We have a system written with scrapy to crawl a few websites. There are <strong>several spiders</strong>, and a <strong>few cascaded pipelines for all</strong> items passed by all crawlers.\nOne of the pipeline components queries the <strong>google servers for geocoding addresses</strong>.\nGoogle imposes a limit of <strong>2500 requests per day per IP address</strong>, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'.</p>\n<p>Hence I want to know about any mechanism which I can invoke from within the pipeline that will <strong>completely and immediately stop</strong> all further crawling/processing of all spiders and also the main engine.</p>\n<p>I have checked other similar questions and their answers have not worked:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/4448724/force-my-scrapy-spider-to-stop-crawling\">Force my scrapy spider to stop crawling</a></li>\n</ul>\n<blockquote>\n<pre><code class=\"python\">from scrapy.project import crawler\ncrawler._signal_shutdown(9,0) #Run this if the cnxn fails.\n</code></pre>\n</blockquote>\n<p>this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address)</p>\n<blockquote>\n<pre><code class=\"python\">import sys\nsys.exit(\"SHUT DOWN EVERYTHING!\")\n</code></pre>\n</blockquote>\n<p>this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -&gt; exceptions.SystemExit raised (to no effect)</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/9524923/how-can-i-make-scrapy-crawl-break-and-exit-when-encountering-the-first-exception\">How can I make scrapy crawl break and exit when encountering the first exception?</a></li>\n</ul>\n<blockquote>\n<pre><code class=\"python\">crawler.engine.close_spider(self, 'log message')\n</code></pre>\n</blockquote>\n<p>this one has the same problem as the first case mentioned above.</p>\n<p>I tried:</p>\n<blockquote>\n<pre><code class=\"python\">scrapy.project.crawler.engine.stop()\n</code></pre>\n</blockquote>\n<p>To no avail</p>\n<p><strong>EDIT</strong>:\nIf I do in the pipeline:</p>\n<blockquote>\n<p>from scrapy.contrib.closespider import CloseSpider</p>\n</blockquote>\n<p>what should I pass as the 'crawler' argument to the CloseSpider's <strong>init</strong>() from the scope of my pipeline?</p>\n", "abstract": "We have a system written with scrapy to crawl a few websites. There are several spiders, and a few cascaded pipelines for all items passed by all crawlers.\nOne of the pipeline components queries the google servers for geocoding addresses.\nGoogle imposes a limit of 2500 requests per day per IP address, and threatens to ban an IP address if it continues querying google even after google has responded with a warning message: 'OVER_QUERY_LIMIT'. Hence I want to know about any mechanism which I can invoke from within the pipeline that will completely and immediately stop all further crawling/processing of all spiders and also the main engine. I have checked other similar questions and their answers have not worked: this does not work as it takes time for the spider to stop execution and hence many more requests are made to google (which could potentially ban my IP address) this one doesn't work at all; items keep getting generated and passed to the pipeline, although the log vomits sys.exit() -> exceptions.SystemExit raised (to no effect) this one has the same problem as the first case mentioned above. I tried: To no avail EDIT:\nIf I do in the pipeline: from scrapy.contrib.closespider import CloseSpider what should I pass as the 'crawler' argument to the CloseSpider's init() from the scope of my pipeline?"}, "answers": [{"id": 9699317, "score": 17, "vote": 0, "content": "<p>You can <a href=\"http://doc.scrapy.org/en/0.14/topics/exceptions.html#closespider\" rel=\"noreferrer\">raise a CloseSpider exception</a> to close down a spider.\nHowever, I don't think this will work from a pipeline. </p>\n<p><strong>EDIT</strong>: avaleske notes in the comments to this answer that he was able to raise a CloseSpider exception from a pipeline. Most wise would be to use this.</p>\n<p>A similar situation has been described on the Scrapy Users group, <a href=\"http://groups.google.com/group/scrapy-users/browse_thread/thread/c4e2c6edae0fcbfb\" rel=\"noreferrer\">in this thread.</a></p>\n<p>I quote:</p>\n<blockquote>\n<p>To close an spider for any part of your code you should use \n  <code>engine.close_spider</code>  method. See this extension for an usage\n  example: \n  <a href=\"https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61\" rel=\"noreferrer\">https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61</a></p>\n</blockquote>\n<p>You could write your own extension, whilst looking at closespider.py as an example, which will shut down a spider if a certain condition has been met.</p>\n<p><strong>Another</strong> \"hack\" would be to set a flag on the spider in the pipeline. For example:</p>\n<p>pipeline:</p>\n<pre><code class=\"python\">def process_item(self, item, spider):\n    if some_flag:\n        spider.close_down = True\n</code></pre>\n<p>spider:</p>\n<pre><code class=\"python\">def parse(self, response):\n    if self.close_down:\n        raise CloseSpider(reason='API usage exceeded')\n</code></pre>\n", "abstract": "You can raise a CloseSpider exception to close down a spider.\nHowever, I don't think this will work from a pipeline.  EDIT: avaleske notes in the comments to this answer that he was able to raise a CloseSpider exception from a pipeline. Most wise would be to use this. A similar situation has been described on the Scrapy Users group, in this thread. I quote: To close an spider for any part of your code you should use \n  engine.close_spider  method. See this extension for an usage\n  example: \n  https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/closespider.py#L61 You could write your own extension, whilst looking at closespider.py as an example, which will shut down a spider if a certain condition has been met. Another \"hack\" would be to set a flag on the spider in the pipeline. For example: pipeline: spider:"}]}, {"link": "https://stackoverflow.com/questions/8335630/how-to-crawl-a-website-extract-data-into-database-with-python", "question": {"id": "8335630", "title": "How to crawl a website/extract data into database with python?", "content": "<p>I'd like to build a webapp to help other students at my university create their schedules. To do that I need to crawl the master schedules (one huge html page) as well as a link to a detailed description for each course into a database, preferably in python. Also, I need to log in to access the data.</p>\n<ul>\n<li>How would that work?</li>\n<li>What tools/libraries can/should I use?</li>\n<li>Are there good tutorials on that?</li>\n<li>How do I best deal with binary data (e.g. pretty pdf)?</li>\n<li>Are there already good solutions for that?</li>\n</ul>\n", "abstract": "I'd like to build a webapp to help other students at my university create their schedules. To do that I need to crawl the master schedules (one huge html page) as well as a link to a detailed description for each course into a database, preferably in python. Also, I need to log in to access the data."}, "answers": [{"id": 8335661, "score": 12, "vote": 0, "content": "<ul>\n<li><a href=\"http://python-requests.org\" rel=\"nofollow noreferrer\"><code>requests</code></a> for downloading the pages.\n\n<ul>\n<li>Here's an example of how to login to a website and download pages: <a href=\"https://stackoverflow.com/a/8316989/311220\">https://stackoverflow.com/a/8316989/311220</a></li>\n</ul></li>\n<li><a href=\"http://lxml.de\" rel=\"nofollow noreferrer\"><code>lxml</code></a> for scraping the data.</li>\n</ul>\n<p>If you want to use a powerful scraping framework there's <a href=\"http://doc.scrapy.org\" rel=\"nofollow noreferrer\"><code>Scrapy</code></a>. It has some good documentation too. It may be a little overkill depending on your task though.</p>\n", "abstract": "If you want to use a powerful scraping framework there's Scrapy. It has some good documentation too. It may be a little overkill depending on your task though."}, {"id": 8335681, "score": 4, "vote": 0, "content": "<p><a href=\"http://www.scrapy.org/\" rel=\"nofollow\">Scrapy</a> is probably the best Python library for crawling. It can maintain state for authenticated sessions. </p>\n<p>Dealing with binary data should be handled separately. For each file type, you'll have to handle it differently according to your own logic. For almost any kind of format, you'll probably be able to find a library. For instance take a look at <a href=\"https://pypi.python.org/pypi/pyPdf\" rel=\"nofollow\">PyPDF</a> for handling PDFs. For excel files you can try xlrd.</p>\n", "abstract": "Scrapy is probably the best Python library for crawling. It can maintain state for authenticated sessions.  Dealing with binary data should be handled separately. For each file type, you'll have to handle it differently according to your own logic. For almost any kind of format, you'll probably be able to find a library. For instance take a look at PyPDF for handling PDFs. For excel files you can try xlrd."}, {"id": 8335694, "score": 3, "vote": 0, "content": "<p>I liked using <a href=\"http://www.crummy.com/software/BeautifulSoup/documentation.html\" rel=\"nofollow\">BeatifulSoup</a> for extracting html data</p>\n<p>It's as easy as this:</p>\n<pre><code class=\"python\">from BeautifulSoup import BeautifulSoup \nimport urllib\n\nur = urllib.urlopen(\"http://pragprog.com/podcasts/feed.rss\")\nsoup = BeautifulSoup(ur.read())\nitems = soup.findAll('item')\n\nurls = [item.enclosure['url'] for item in items]\n</code></pre>\n", "abstract": "I liked using BeatifulSoup for extracting html data It's as easy as this:"}, {"id": 25957099, "score": 0, "vote": 0, "content": "<p>For this purpose there is a very useful tool called web-harvest\nLink to their website <a href=\"http://web-harvest.sourceforge.net/\" rel=\"nofollow\">http://web-harvest.sourceforge.net/</a>\nI use this to crawl webpages</p>\n", "abstract": "For this purpose there is a very useful tool called web-harvest\nLink to their website http://web-harvest.sourceforge.net/\nI use this to crawl webpages"}]}, {"link": "https://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites", "question": {"id": "2396529", "title": "Using one Scrapy spider for several websites", "content": "<p>I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI.</p>\n<p>How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow.</p>\n", "abstract": "I need to create a user configurable web spider/crawler, and I'm thinking about using Scrapy. But, I can't hard-code the domains and allowed URL regex:es -- this will instead be configurable in a GUI. How do I (as simple as possible) create a spider or a set of spiders with Scrapy where the domains and allowed URL regex:es are dynamically configurable? E.g. I write the configuration to a file, and the spider reads it somehow."}, "answers": [{"id": 2397334, "score": 10, "vote": 0, "content": "<p><strong>WARNING: This answer was for Scrapy v0.7, spider manager api changed a lot since then.</strong></p>\n<p>Override default SpiderManager class, load your custom rules from a database or somewhere else and instanciate a custom spider with your own rules/regexes and domain_name</p>\n<p>in mybot/settings.py:</p>\n<pre><code class=\"python\">SPIDER_MANAGER_CLASS = 'mybot.spidermanager.MySpiderManager'\n</code></pre>\n<p>in mybot/spidermanager.py:</p>\n<pre><code class=\"python\">from mybot.spider import MyParametrizedSpider\n\nclass MySpiderManager(object):\n    loaded = True\n\n    def fromdomain(self, name):\n        start_urls, extra_domain_names, regexes = self._get_spider_info(name)\n        return MyParametrizedSpider(name, start_urls, extra_domain_names, regexes)\n\n    def close_spider(self, spider):\n        # Put here code you want to run before spiders is closed\n        pass\n\n    def _get_spider_info(self, name):\n        # query your backend (maybe a sqldb) using `name` as primary key, \n        # and return start_urls, extra_domains and regexes\n        ...\n        return (start_urls, extra_domains, regexes)\n</code></pre>\n<p>and now your custom spider class, in mybot/spider.py:</p>\n<pre><code class=\"python\">from scrapy.spider import BaseSpider\n\nclass MyParametrizedSpider(BaseSpider):\n\n    def __init__(self, name, start_urls, extra_domain_names, regexes):\n        self.domain_name = name\n        self.start_urls = start_urls\n        self.extra_domain_names = extra_domain_names\n        self.regexes = regexes\n\n     def parse(self, response):\n         ...\n</code></pre>\n<p>Notes:</p>\n<ul>\n<li>You can extend CrawlSpider too if you want to take advantage of its Rules system</li>\n<li>To run a spider use:  <code>./scrapy-ctl.py crawl &lt;name&gt;</code>, where <code>name</code> is passed to SpiderManager.fromdomain and is the key to retreive more spider info from the backend system</li>\n<li>As solution overrides default SpiderManager, coding a classic spider (a python module per SPIDER) doesn't works, but, I think this is not an issue for you. More info on default spiders manager <a href=\"http://hg.scrapy.org/scrapy/file/96eabeeccefe/scrapy/contrib/spidermanager.py\" rel=\"nofollow noreferrer\">TwistedPluginSpiderManager</a></li>\n</ul>\n", "abstract": "WARNING: This answer was for Scrapy v0.7, spider manager api changed a lot since then. Override default SpiderManager class, load your custom rules from a database or somewhere else and instanciate a custom spider with your own rules/regexes and domain_name in mybot/settings.py: in mybot/spidermanager.py: and now your custom spider class, in mybot/spider.py: Notes:"}, {"id": 2397107, "score": 4, "vote": 0, "content": "<p>What you need is to dynamically create spider classes, subclassing your favorite generic spider class as supplied by <code>scrapy</code> (<code>CrawlSpider</code> subclasses with your <code>rules</code> added, or <code>XmlFeedSpider</code>, or whatever) and adding <code>domain_name</code>, <code>start_urls</code>, and possibly <code>extra_domain_names</code> (and/or <code>start_requests()</code>, etc), as you get or deduce them from your GUI (or config file, or whatever).</p>\n<p>Python makes it easy to perform such dynamic creation of class objects; a very simple example might be:</p>\n<pre><code class=\"python\">from scrapy import spider\n\ndef makespider(domain_name, start_urls,\n               basecls=spider.BaseSpider):\n  return type(domain_name + 'Spider',\n              (basecls,),\n              {'domain_name': domain_name,\n               'start_urls': start_urls})\n\nallspiders = []\nfor domain, urls in listofdomainurlpairs:\n  allspiders.append(makespider(domain, urls))\n</code></pre>\n<p>This gives you a list of very bare-bone spider classes -- you'll probably want to add <code>parse</code> methods to them before you instantiate them.  Season to taste...;-).</p>\n", "abstract": "What you need is to dynamically create spider classes, subclassing your favorite generic spider class as supplied by scrapy (CrawlSpider subclasses with your rules added, or XmlFeedSpider, or whatever) and adding domain_name, start_urls, and possibly extra_domain_names (and/or start_requests(), etc), as you get or deduce them from your GUI (or config file, or whatever). Python makes it easy to perform such dynamic creation of class objects; a very simple example might be: This gives you a list of very bare-bone spider classes -- you'll probably want to add parse methods to them before you instantiate them.  Season to taste...;-)."}, {"id": 2397014, "score": 3, "vote": 0, "content": "<p>Shameless self promotion on <a href=\"http://github.com/hinoglu/Domo/\" rel=\"nofollow noreferrer\">domo</a>! you'll need to instantiate the crawler as given in the examples, for your project.</p>\n<p>Also you'll need to make the crawler configurable on runtime, which is simply passing the configuration to crawler, and overriding the settings on runtime, when configuration changed. </p>\n", "abstract": "Shameless self promotion on domo! you'll need to instantiate the crawler as given in the examples, for your project. Also you'll need to make the crawler configurable on runtime, which is simply passing the configuration to crawler, and overriding the settings on runtime, when configuration changed. "}, {"id": 41233840, "score": 0, "vote": 0, "content": "<p>Now it is extremely easy to configure scrapy for these purposes:</p>\n<ol>\n<li><p>About the first urls to visit, you can pass it as an attribute on the spider call with <code>-a</code>, and use the <code>start_requests</code> function to setup how to start the spider</p></li>\n<li><p>You don't need to setup the <code>allowed_domains</code> variable for the spiders. If you don't include that class variable, the spider will be able to allow every domain.</p></li>\n</ol>\n<p>It should end up to something like:</p>\n<pre><code class=\"python\">class MySpider(Spider):\n\n    name = \"myspider\"\n\n    def start_requests(self):\n        yield Request(self.start_url, callback=self.parse)\n\n\n    def parse(self, response):\n        ...\n</code></pre>\n<p>and you should call it with:</p>\n<pre><code class=\"python\">scrapy crawl myspider -a start_url=\"http://example.com\"\n</code></pre>\n", "abstract": "Now it is extremely easy to configure scrapy for these purposes: About the first urls to visit, you can pass it as an attribute on the spider call with -a, and use the start_requests function to setup how to start the spider You don't need to setup the allowed_domains variable for the spiders. If you don't include that class variable, the spider will be able to allow every domain. It should end up to something like: and you should call it with:"}]}, {"link": "https://stackoverflow.com/questions/40823516/how-to-disable-robots-txt-when-you-launch-scrapy-shell", "question": {"id": "40823516", "title": "How to disable robots.txt when you launch scrapy shell?", "content": "<p>I use Scrapy shell without problems with several websites, but I find problems when the robots (robots.txt) does not allow access to a site.\nHow can I disable robots detection by Scrapy (ignored the existence)?\nThank you in advance.\n<strong>I'm not talking about the project created by Scrapy, but Scrapy shell command: <code>scrapy shell 'www.example.com'</code></strong></p>\n", "abstract": "I use Scrapy shell without problems with several websites, but I find problems when the robots (robots.txt) does not allow access to a site.\nHow can I disable robots detection by Scrapy (ignored the existence)?\nThank you in advance.\nI'm not talking about the project created by Scrapy, but Scrapy shell command: scrapy shell 'www.example.com'"}, "answers": [{"id": 40823612, "score": 15, "vote": 0, "content": "<p>In the settings.py file of your scrapy project, look for <strong>ROBOTSTXT_OBEY</strong> and set it to <strong>False</strong>.</p>\n", "abstract": "In the settings.py file of your scrapy project, look for ROBOTSTXT_OBEY and set it to False."}, {"id": 40824219, "score": 10, "vote": 0, "content": "<p>If you run scrapy from project directory <code>scrapy shell</code> will use the projects <code>settings.py</code>. If you run outside of the project scrapy will use default settings. However you can override and add settings via <code>--set</code> flag.<br/>\nSo to turn off <code>ROBOTSTXT_OBEY</code> setting you can simply: </p>\n<pre><code class=\"python\">scrapy shell http://stackoverflow.com --set=\"ROBOTSTXT_OBEY=False\"\n</code></pre>\n", "abstract": "If you run scrapy from project directory scrapy shell will use the projects settings.py. If you run outside of the project scrapy will use default settings. However you can override and add settings via --set flag.\nSo to turn off ROBOTSTXT_OBEY setting you can simply: "}]}, {"link": "https://stackoverflow.com/questions/33747174/how-to-specify-parameters-on-a-request-using-scrapy", "question": {"id": "33747174", "title": "How to specify parameters on a Request using scrapy", "content": "<p>How do I pass parameters to a a request on a url like this:</p>\n<pre><code class=\"python\">site.com/search/?action=search&amp;description=My Search here&amp;e_author=\n</code></pre>\n<p>How do I put the arguments on the structure of a Spider Request, something like this exemple:</p>\n<pre><code class=\"python\">req = Request(url=\"site.com/\",parameters={x=1,y=2,z=3})\n</code></pre>\n", "abstract": "How do I pass parameters to a a request on a url like this: How do I put the arguments on the structure of a Spider Request, something like this exemple:"}, "answers": [{"id": 33747209, "score": 17, "vote": 0, "content": "<p>Pass your GET parameters inside the URL itself:</p>\n<pre><code class=\"python\">return Request(url=\"https://yoursite.com/search/?action=search&amp;description=MySearchhere&amp;e_author=\")\n</code></pre>\n<p>You should probably define your parameters in a\u00a0dictionary and then <a href=\"http://docs.python.org/library/urllib.html#urllib.urlencode\" rel=\"nofollow noreferrer\">\"urlencode\"</a> it:</p>\n<pre><code class=\"python\">from urllib.parse import urlencode\n\nparams = { \n    \"action\": \"search\",\n    \"description\": \"My search here\",\n    \"e_author\": \"\"\n}\nurl = \"https://yoursite.com/search/?\" + urlencode(params)\n\nreturn Request(url=url)\n</code></pre>\n", "abstract": "Pass your GET parameters inside the URL itself: You should probably define your parameters in a\u00a0dictionary and then \"urlencode\" it:"}, {"id": 59955709, "score": 5, "vote": 0, "content": "<p>To create GET request with params, using scrapy, you can use the following example:</p>\n<pre><code class=\"python\">yield scrapy.FormRequest(\n    url=url,\n    method='GET',\n    formdata=params,\n    callback=self.parse_result\n)\n</code></pre>\n<p>where 'params' is a dict with your parameters.</p>\n", "abstract": "To create GET request with params, using scrapy, you can use the following example: where 'params' is a dict with your parameters."}, {"id": 54826621, "score": 3, "vote": 0, "content": "<p>You have to make url yourself with whatever parameters you have.  </p>\n<p><strong>Python 3 or above</strong> </p>\n<pre><code class=\"python\">import urllib\nparams = {\n    'key': self.access_key,\n    'part': 'snippet,replies',\n    'videoId': self.video_id,\n    'maxResults': 100\n}\nurl = f'https://www.googleapis.com/youtube/v3/commentThreads/?{urllib.parse.urlencode(params)}'\nrequest = scrapy.Request(url, callback=self.parse)\nyield request\n</code></pre>\n<p><strong>Python 3+ example</strong><br/>\nHere I am trying to fetch all reviews for some youtube video using official youtube api. Reviews will come in paginated format. So see how I am constructing url from params to call it.</p>\n<pre><code class=\"python\">import scrapy\nimport urllib\nimport json\nimport datetime\nfrom youtube_scrapy.items import YoutubeItem\n\nclass YoutubeSpider(scrapy.Spider):\n    name = 'youtube'\n    BASE_URL = 'https://www.googleapis.com/youtube/v3'\n\n    def __init__(self):\n        self.access_key = 'you_yuotube_api_access_key'\n        self.video_id = 'any_youtube_video_id'\n\n    def start_requests(self):\n        params = {\n            'key': self.access_key,\n            'part': 'snippet,replies',\n            'videoId': self.video_id,\n            'maxResults': 100\n        }\n        url = f'{self.BASE_URL}/commentThreads/?{urllib.parse.urlencode(params)}'\n        request = scrapy.Request(url, callback=self.parse)\n        request.meta['params'] = params\n        return [request]\n\n    def parse(self, response):\n        data = json.loads(response.body)\n\n        # lets collect comment and reply\n        items = data.get('items', [])\n        for item in items:\n            created_date = item['snippet']['topLevelComment']['snippet']['publishedAt']\n            _created_date = datetime.datetime.strptime(created_date, '%Y-%m-%dT%H:%M:%S.000Z')\n            id = item['snippet']['topLevelComment']['id']\n            record = {\n                'created_date': _created_date,\n                'body': item['snippet']['topLevelComment']['snippet']['textOriginal'],\n                'creator_name': item['snippet']['topLevelComment']['snippet'].get('authorDisplayName', {}),\n                'id': id,\n                'url': f'https://www.youtube.com/watch?v={self.video_id}&amp;lc={id}',\n            }\n\n            yield YoutubeItem(**record)\n\n\n        # lets paginate if next page is available for more comments\n        next_page_token = data.get('nextPageToken', None)\n        if next_page_token:\n            params = response.meta['params']\n            params['pageToken'] = next_page_token\n            url = f'{self.BASE_URL}/commentThreads/?{urllib.parse.urlencode(params)}'\n            request = scrapy.Request(url, callback=self.parse)\n            request.meta['params'] = params\n            yield request\n</code></pre>\n", "abstract": "You have to make url yourself with whatever parameters you have.   Python 3 or above  Python 3+ example\nHere I am trying to fetch all reviews for some youtube video using official youtube api. Reviews will come in paginated format. So see how I am constructing url from params to call it."}, {"id": 54861795, "score": 1, "vote": 0, "content": "<p>Can use add_or_replace_parameters from w3lib.</p>\n<pre><code class=\"python\">from w3lib.url import add_or_replace_parameters\n\ndef abc(self, response):\n  url = \"https://yoursite.com/search/\" # can be response.url or any\n  params = { \n      \"action\": \"search\",\n      \"description\": \"My search here\",\n      \"e_author\": \"\"\n  }\n\n  return Request(url=add_or_replace_parameters(url, prams))\n</code></pre>\n", "abstract": "Can use add_or_replace_parameters from w3lib."}, {"id": 33747215, "score": -1, "vote": 0, "content": "<p>Scrapy doesn't offer this directly. What you are trying to do is to create a url, for which you can use the <a href=\"https://docs.python.org/2/library/urlparse.html\" rel=\"nofollow\"><code>urlparse</code></a> module</p>\n", "abstract": "Scrapy doesn't offer this directly. What you are trying to do is to create a url, for which you can use the urlparse module"}]}, {"link": "https://stackoverflow.com/questions/12145067/scrapy-select-specific-link-based-on-text", "question": {"id": "12145067", "title": "Scrapy - Select specific link based on text", "content": "<p>This should be easy but I'm stuck.</p>\n<pre><code class=\"python\">&lt;div class=\"paginationControl\"&gt;\n  &lt;a href=\"/en/overview/0-All_manufactures/0-All_models.html?page=2&amp;amp;powerunit=2\"&gt;Link Text 2&lt;/a&gt; | \n  &lt;a href=\"/en/overview/0-All_manufactures/0-All_models.html?page=3&amp;amp;powerunit=2\"&gt;Link Text 3&lt;/a&gt; | \n  &lt;a href=\"/en/overview/0-All_manufactures/0-All_models.html?page=4&amp;amp;powerunit=2\"&gt;Link Text 4&lt;/a&gt; | \n  &lt;a href=\"/en/overview/0-All_manufactures/0-All_models.html?page=5&amp;amp;powerunit=2\"&gt;Link Text 5&lt;/a&gt; |   \n\n&lt;!-- Next page link --&gt; \n  &lt;a href=\"/en/overview/0-All_manufactures/0-All_models.html?page=2&amp;amp;powerunit=2\"&gt;Link Text Next &gt;&lt;/a&gt;\n&lt;/div&gt;\n</code></pre>\n<p>I'm trying to use Scrapy (Basespider) to select a link based on it's Link text using:</p>\n<pre><code class=\"python\">nextPage = HtmlXPathSelector(response).select(\"//div[@class='paginationControl']/a/@href\").re(\"(.+)*?Next\")\n</code></pre>\n<p>For example, I want to select the next page link based on the fact that it's text is \"Link Text Next\". Any ideas?</p>\n", "abstract": "This should be easy but I'm stuck. I'm trying to use Scrapy (Basespider) to select a link based on it's Link text using: For example, I want to select the next page link based on the fact that it's text is \"Link Text Next\". Any ideas?"}, "answers": [{"id": 12145164, "score": 16, "vote": 0, "content": "<p>Use <code>a[contains(text(),'Link Text Next')]</code>:</p>\n<pre><code class=\"python\">nextPage = HtmlXPathSelector(response).select(\n    \"//div[@class='paginationControl']/a[contains(text(),'Link Text Next')]/@href\")\n</code></pre>\n<p>Reference: Documentation on the XPath <a href=\"http://www.w3.org/TR/xpath/#function-contains\" rel=\"noreferrer\">contains</a> function</p>\n<hr/>\n<p>PS. Your text <code>Link Text Next</code> has a space at the end. To avoid having to include that space in the code: </p>\n<pre><code class=\"python\">text()=\"Link Text Next \"\n</code></pre>\n<p>I think using <code>contains</code> is a bit more general while still being specific enough.</p>\n", "abstract": "Use a[contains(text(),'Link Text Next')]: Reference: Documentation on the XPath contains function PS. Your text Link Text Next has a space at the end. To avoid having to include that space in the code:  I think using contains is a bit more general while still being specific enough."}, {"id": 12145193, "score": 6, "vote": 0, "content": "<p>You can use the following XPath expression:</p>\n<pre><code class=\"python\">//div[@class='paginationControl']/a[text()=\"Link Text Next\"]/@href\n</code></pre>\n<p>This selects the <code>href</code> attributes of the link with text <code>\"Link Text Next\"</code>.</p>\n<p>See <a href=\"http://www.w3.org/TR/xpath/#section-String-Functions\" rel=\"noreferrer\">XPath string functions</a> if you need more control.</p>\n", "abstract": "You can use the following XPath expression: This selects the href attributes of the link with text \"Link Text Next\". See XPath string functions if you need more control."}, {"id": 12145157, "score": 1, "vote": 0, "content": "<p>Your xpath is selecting the href not the text in the <code>a</code> tag.  It doesn't look from your example like the href has <code>next</code> in it, so you can't find it with an RE.</p>\n", "abstract": "Your xpath is selecting the href not the text in the a tag.  It doesn't look from your example like the href has next in it, so you can't find it with an RE."}]}, {"link": "https://stackoverflow.com/questions/30452395/selenium-pdf-automatic-download-not-working", "question": {"id": "30452395", "title": "Selenium pdf automatic download not working", "content": "<p>I am new to selenium and I am writing a scraper to download pdf files automatically from a given site. </p>\n<p>Below is my code:</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nfp = webdriver.FirefoxProfile()\n\nfp.set_preference(\"browser.download.folderList\",2);\nfp.set_preference(\"browser.download.manager.showWhenStarting\",False)\nfp.set_preference(\"browser.download.dir\", \"/home/jill/Downloads/Dinamalar\")\nfp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n\nbrowser = webdriver.Firefox(firefox_profile=fp)\nbrowser.get(\"http://epaper.dinamalar.com/PUBLICATIONS/DM/MADHURAI/2015/05/26/PagePrint//26_05_2015_001_b2b69fda315301809dda359a6d3d9689.pdf\");\nwebobj = browser.find_element_by_id(\"download\").click();\n</code></pre>\n<p>I followed the steps mentioned in Selenium <a href=\"https://selenium-python.readthedocs.org/en/latest/faq.html?highlight=firefoxprofile\" rel=\"noreferrer\">documentation</a> and in the this <a href=\"http://software-testing-tutorials-automation.blogspot.in/2014/05/how-to-download-different-files-using.html.\" rel=\"noreferrer\">link</a>. I am not sure why download dialog box is getting shown every time. </p>\n<p>Is there anyway to fix it else can there be a way to give \"application/all\" so that all the files can be downloaded (work-around)?</p>\n", "abstract": "I am new to selenium and I am writing a scraper to download pdf files automatically from a given site.  Below is my code: I followed the steps mentioned in Selenium documentation and in the this link. I am not sure why download dialog box is getting shown every time.  Is there anyway to fix it else can there be a way to give \"application/all\" so that all the files can be downloaded (work-around)?"}, "answers": [{"id": 30455695, "score": 19, "vote": 0, "content": "<p>Disable the built-in <code>pdfjs</code> plugin and navigate to the URL - the PDF file would be downloaded automatically, the code:</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nfp = webdriver.FirefoxProfile()\n\nfp.set_preference(\"browser.download.folderList\", 2)\nfp.set_preference(\"browser.download.manager.showWhenStarting\",False)\nfp.set_preference(\"browser.download.dir\", \"/home/jill/Downloads/Dinamalar\")\nfp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf,application/x-pdf\")\n\nfp.set_preference(\"pdfjs.disabled\", \"true\")  # &lt; KEY PART HERE\n\nbrowser = webdriver.Firefox(firefox_profile=fp)\nbrowser.get(\"http://epaper.dinamalar.com/PUBLICATIONS/DM/MADHURAI/2015/05/26/PagePrint//26_05_2015_001_b2b69fda315301809dda359a6d3d9689.pdf\");\n</code></pre>\n<hr/>\n<p>Update (the complete code that worked for me):</p>\n<pre><code class=\"python\">from selenium import webdriver\n\nmime_types = \"application/pdf,application/vnd.adobe.xfdf,application/vnd.fdf,application/vnd.adobe.xdp+xml\"\n\nfp = webdriver.FirefoxProfile()\nfp.set_preference(\"browser.download.folderList\", 2)\nfp.set_preference(\"browser.download.manager.showWhenStarting\", False)\nfp.set_preference(\"browser.download.dir\", \"/home/aafanasiev/Downloads\")\nfp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", mime_types)\nfp.set_preference(\"plugin.disable_full_page_plugin_for_types\", mime_types)\nfp.set_preference(\"pdfjs.disabled\", True)\n\nbrowser = webdriver.Firefox(firefox_profile=fp)\nbrowser.get(\"http://epaper.dinamalar.com/\")\n\nwebobj_get_link = browser.find_element_by_id(\"liSavePdf\")\nwebobj_get_object = webobj_get_link.find_element_by_tag_name(\"a\")\nwebobj_get_object.click()\n</code></pre>\n", "abstract": "Disable the built-in pdfjs plugin and navigate to the URL - the PDF file would be downloaded automatically, the code: Update (the complete code that worked for me):"}, {"id": 30459547, "score": 1, "vote": 0, "content": "<p>I tested the following code and I succesfully downloaded your pdf on Windows 7:</p>\n<pre><code class=\"python\">fp = webdriver.FirefoxProfile()\nfp.set_preference(\"browser.download.folderList\", 2)\nfp.set_preference(\"browser.download.manager.showWhenStarting\", False)\nfp.set_preference(\"browser.download.dir\", download_location)\nfp.set_preference(\"plugin.disable_full_page_plugin_for_types\", \"application/pdf\")\nfp.set_preference(\"pdfjs.disabled\", True)\nfp.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n\n\n\ndriver = webdriver.Firefox(fp)\ndriver.implicitly_wait(10)\ndriver.maximize_window()\ndriver.get(\"http://epaper.dinamalar.com/\")\nelement = driver.find_element_by_css_selector(\"li#liSavePdf&gt;a&gt;img\")\nelement.click()\n</code></pre>\n", "abstract": "I tested the following code and I succesfully downloaded your pdf on Windows 7:"}, {"id": 30453785, "score": 0, "vote": 0, "content": "<p>Since there is not HTML code available, my guess is that this line </p>\n<pre><code class=\"python\">webobj = browser.find_element_by_id(\"download\").click();\n</code></pre>\n<p>actually calls the <code>onclick</code> event, but you don't handle it properly. In other words, what you're missing is the location where this .pdf file will be stored. I have very little experience with python programming, but one solution could be to use HTTP webclient lib, that will allow you to automatically download files. Something like <a href=\"https://msdn.microsoft.com/en-us/library/ez801hhe%28v=vs.110%29.aspx\" rel=\"nofollow noreferrer\">CSharp's WebClient.DownloadFile Method (String, String)</a>. And if used properly, you can skip any Selenium commands for this action.</p>\n<p>Maybe something like <a href=\"https://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python\">this post</a> will be a good start.</p>\n", "abstract": "Since there is not HTML code available, my guess is that this line  actually calls the onclick event, but you don't handle it properly. In other words, what you're missing is the location where this .pdf file will be stored. I have very little experience with python programming, but one solution could be to use HTTP webclient lib, that will allow you to automatically download files. Something like CSharp's WebClient.DownloadFile Method (String, String). And if used properly, you can skip any Selenium commands for this action. Maybe something like this post will be a good start."}]}, {"link": "https://stackoverflow.com/questions/44527996/scrapy-understanding-crawlspider-and-linkextractor", "question": {"id": "44527996", "title": "Scrapy - Understanding CrawlSpider and LinkExtractor", "content": "<p>So I'm trying to use CrawlSpider and understand the following example in the <a href=\"https://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider-example\" rel=\"noreferrer\">Scrapy Docs</a>: </p>\n<pre><code class=\"python\">import scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nclass MySpider(CrawlSpider):\n    name = 'example.com'\n    allowed_domains = ['example.com']\n    start_urls = ['http://www.example.com']\n\nrules = (\n    # Extract links matching 'category.php' (but not matching 'subsection.php')\n    # and follow links from them (since no callback means follow=True by default).\n    Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))),\n\n    # Extract links matching 'item.php' and parse them with the spider's method parse_item\n    Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'),\n)\n\ndef parse_item(self, response):\n    self.logger.info('Hi, this is an item page! %s', response.url)\n    item = scrapy.Item()\n    item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)')\n    item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract()\n    item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract()\n    return item\n</code></pre>\n<p>The description then given is: </p>\n<blockquote>\n<p>This spider would start crawling example.com\u2019s home page, collecting category links, and item links, parsing the latter with the parse_item method. For each item response, some data will be extracted from the HTML using XPath, and an Item will be filled with it.</p>\n</blockquote>\n<p>I understand that for the second rule, it extracts links from <code>item.php</code> and then extracts the information using the <code>parse_item</code> method. However, what exactly is the purpose of the first rule? It just says that it \"collects\" the links. What does that mean and why is it useful if they are not extracting any data from it? </p>\n", "abstract": "So I'm trying to use CrawlSpider and understand the following example in the Scrapy Docs:  The description then given is:  This spider would start crawling example.com\u2019s home page, collecting category links, and item links, parsing the latter with the parse_item method. For each item response, some data will be extracted from the HTML using XPath, and an Item will be filled with it. I understand that for the second rule, it extracts links from item.php and then extracts the information using the parse_item method. However, what exactly is the purpose of the first rule? It just says that it \"collects\" the links. What does that mean and why is it useful if they are not extracting any data from it? "}, "answers": [{"id": 44528146, "score": 19, "vote": 0, "content": "<p>CrawlSpider is very useful when crawling forums searching for posts for example, or categorized online stores when searching for product pages.</p>\n<p>The idea is that \"somehow\" you have to go into each category, searching for links that correspond to product/item information you want to extract. Those product links are the ones specified on the second rule of that example (it says the ones that have <code>item.php</code> in the url).</p>\n<p>Now how should the spider keep visiting links until finding those containing <code>item.php</code>? that's the first rule for. It says to visit every Link containing <code>category.php</code> but not <code>subsection.php</code>, which means it won't exactly extract any \"item\" from those links, but it defines the path of the spider to find the real items. </p>\n<p>That's why you see it doesn't contain a <code>callback</code> method inside the rule, as it won't return that link response for you to process, because it will be directly followed.</p>\n", "abstract": "CrawlSpider is very useful when crawling forums searching for posts for example, or categorized online stores when searching for product pages. The idea is that \"somehow\" you have to go into each category, searching for links that correspond to product/item information you want to extract. Those product links are the ones specified on the second rule of that example (it says the ones that have item.php in the url). Now how should the spider keep visiting links until finding those containing item.php? that's the first rule for. It says to visit every Link containing category.php but not subsection.php, which means it won't exactly extract any \"item\" from those links, but it defines the path of the spider to find the real items.  That's why you see it doesn't contain a callback method inside the rule, as it won't return that link response for you to process, because it will be directly followed."}]}, {"link": "https://stackoverflow.com/questions/2350049/how-to-build-a-web-crawler-based-on-scrapy-to-run-forever", "question": {"id": "2350049", "title": "How to build a web crawler based on Scrapy to run forever?", "content": "<p>I want to build a web crawler based on Scrapy to grab news pictures from several news portal website. I want to this crawler to be:</p>\n<ol>\n<li><p>Run forever</p>\n<p>Means it will periodical re-visit some portal pages to get updates.</p></li>\n<li><p>Schedule priorities. </p>\n<p>Give different priorities to different type of URLs. </p></li>\n<li><p>Multi thread fetch</p></li>\n</ol>\n<p>I've read the Scrapy document but have not found something related to what I listed (maybe I am not careful enough). Is there anyone here know how to do that ? or just give some idea/example about it. Thanks!</p>\n", "abstract": "I want to build a web crawler based on Scrapy to grab news pictures from several news portal website. I want to this crawler to be: Run forever Means it will periodical re-visit some portal pages to get updates. Schedule priorities.  Give different priorities to different type of URLs.  Multi thread fetch I've read the Scrapy document but have not found something related to what I listed (maybe I am not careful enough). Is there anyone here know how to do that ? or just give some idea/example about it. Thanks!"}, "answers": [{"id": 2350109, "score": 12, "vote": 0, "content": "<p>Scrapy is a framework for the spidering of websites, as such, it is intended to support your criteria but it isn't going to dance for you out of the box; you will probably have to get relatively familiar with the module for some tasks. </p>\n<ol>\n<li>Running forever is up to your application that calls Scrapy. You tell <a href=\"http://doc.scrapy.org/topics/spiders.html\" rel=\"noreferrer\">the spiders</a> where to go and when to go there.</li>\n<li>Giving priorities is the job of <a href=\"http://doc.scrapy.org/experimental/scheduler-middleware.html\" rel=\"noreferrer\">Scheduler middleware</a> which you'd have to create and plug into Scrapy. The documentation on this appears spotty and I've not looked at the code - in principle the function is there.</li>\n<li>Scrapy is inherently, <a href=\"http://doc.scrapy.org/topics/architecture.html#event-driven-networking\" rel=\"noreferrer\">fundamentally asynchronous</a> which may well be what you are desiring: request B can be satisfied while request A is still outstanding. The underlying connection engine does not prevent you from <em>bona fide</em> multi-threading, but Scrapy doesn't provide threading services.</li>\n</ol>\n<p>Scrapy is a library, not an application. There is a non-trivial amount of work (code) that a user of the module needs to make.</p>\n", "abstract": "Scrapy is a framework for the spidering of websites, as such, it is intended to support your criteria but it isn't going to dance for you out of the box; you will probably have to get relatively familiar with the module for some tasks.  Scrapy is a library, not an application. There is a non-trivial amount of work (code) that a user of the module needs to make."}, {"id": 39009546, "score": 0, "vote": 0, "content": "<p>About the requirement on running-forever, here's some details.</p>\n<p>You need to catch the <code>signals.spider_idle</code> signal, and in your method that \nconnected to the signal, you need to raise a <code>DontCloseSpider</code> exception. The <code>spider_idle</code> signal is sent to the scrapy engine when there is no pending requests, and by default the spider will shutdown. You can intercept this process.</p>\n<p>See codes blow:</p>\n<pre><code class=\"python\">import scrapy\nfrom scrapy.exceptions import DontCloseSpider\nfrom scrapy.xlib.pydispatch import dispatcher\n\nclass FooSpider(scrapy.Spider):\n    def __init__(self, *args, **kwargs):\n        super(FooSpider, self).__init__(*args, **kwargs)\n        dispatcher.connect(self.spider_idle, signals.spider_idle)\n\n    def spider_idle(self):\n        #you can revisit your portal urls in this method\n        raise DontCloseSpider \n</code></pre>\n", "abstract": "About the requirement on running-forever, here's some details. You need to catch the signals.spider_idle signal, and in your method that \nconnected to the signal, you need to raise a DontCloseSpider exception. The spider_idle signal is sent to the scrapy engine when there is no pending requests, and by default the spider will shutdown. You can intercept this process. See codes blow:"}]}, {"link": "https://stackoverflow.com/questions/62084602/how-does-cloudflare-differentiate-selenium-and-requests-traffic", "question": {"id": "62084602", "title": "How does Cloudflare differentiate Selenium and Requests traffic?", "content": "<h1>Context</h1>\n<p>I am currently attempting to build a small-scale bot using Selenium and Requests module in Python.<br/>\nHowever, the webpage I want to interact with is running behind Cloudflare.<br/>\nMy python script is running over Tor using stem module.<br/>\nMy traffic analysis is based on Firefox's \"Developer options-&gt;Network\" using Persist Logs.</p>\n<h1>My findings so far:</h1>\n<ul>\n<li>Selenium's Firefox webdriver can often access the webpage without going through \"checking browser page\" (return code 503) and \"captcha page\" (return code 403).</li>\n<li>Requests session object with the same user agent always results in \"captcha page\" (return code 403).</li>\n</ul>\n<p>If Cloudflare was checking my Javascript functionality, shouldn't my requests module return 503 ?</p>\n<h1>Code Example</h1>\n<pre><code class=\"python\">driver = webdriver.Firefox(firefox_profile=fp, options=fOptions)\ndriver.get(\"https://www.cloudflare.com\")   # usually returns code 200 without verifying the browser\n\nsession = requests.Session()\n# ... applied socks5 proxy for both http and https ... #\nsession.headers.update({\"user-agent\": driver.execute_script(\"return navigator.userAgent;\")})\npage = session.get(\"https://www.cloudflare.com\")\nprint(page.status_code) # return code 403\nprint(page.text)        # returns \"captcha page\"\n</code></pre>\n<p>Both Selenium and Requests modules are using the same user agent and ip.<br/>\nBoth are using GET without any parameters.<br/>\nHow does Cloudflare distinguish these traffic?<br/>\nAm I missing something?</p>\n<hr/>\n<p>I tried to transfer cookies from the webdriver to the requests session to see if a bypass is possible but had no luck.<br/>\nHere is the used code:</p>\n<pre><code class=\"python\">for c in driver.get_cookies():\n    session.cookies.set(c['name'], c['value'], domain=c['domain'])\n</code></pre>\n", "abstract": "I am currently attempting to build a small-scale bot using Selenium and Requests module in Python.\nHowever, the webpage I want to interact with is running behind Cloudflare.\nMy python script is running over Tor using stem module.\nMy traffic analysis is based on Firefox's \"Developer options->Network\" using Persist Logs. If Cloudflare was checking my Javascript functionality, shouldn't my requests module return 503 ? Both Selenium and Requests modules are using the same user agent and ip.\nBoth are using GET without any parameters.\nHow does Cloudflare distinguish these traffic?\nAm I missing something? I tried to transfer cookies from the webdriver to the requests session to see if a bypass is possible but had no luck.\nHere is the used code:"}, "answers": [{"id": 66576054, "score": 1, "vote": 0, "content": "<p>There are additional JavaScript APIs exposed to the webpage when using Selenium. If you can disable them, you may be able to fix the problem.</p>\n", "abstract": "There are additional JavaScript APIs exposed to the webpage when using Selenium. If you can disable them, you may be able to fix the problem."}, {"id": 67494333, "score": 1, "vote": 0, "content": "<p>Cloudflare doesn't only check HTTP headers or javascript \u2014 it also analyses the TLS header. I'm not sure exactly how it does it, but I've found that it can be circumvented by using NSS instead of OpenSSL (though it's not well integrated into Requests).</p>\n", "abstract": "Cloudflare doesn't only check HTTP headers or javascript \u2014 it also analyses the TLS header. I'm not sure exactly how it does it, but I've found that it can be circumvented by using NSS instead of OpenSSL (though it's not well integrated into Requests)."}, {"id": 64854227, "score": 0, "vote": 0, "content": "<p>The captcha response depends on the browser fingerprint. It's not about just sending Cookies and User-agent.</p>\n<p>Copy all the headers from Network Tab in Developers console, and send all the key value pairs as headers in request library.</p>\n<p>This method should work logically.</p>\n", "abstract": "The captcha response depends on the browser fingerprint. It's not about just sending Cookies and User-agent. Copy all the headers from Network Tab in Developers console, and send all the key value pairs as headers in request library. This method should work logically."}]}, {"link": "https://stackoverflow.com/questions/31232681/scrapy-crawler-caught-exception-reading-instance-data", "question": {"id": "31232681", "title": "scrapy crawler caught exception reading instance data", "content": "<p>I am new to python and want to use scrapy to build a web crawler. I go through the tutorial in <a href=\"http://blog.siliconstraits.vn/building-web-crawler-scrapy/\">http://blog.siliconstraits.vn/building-web-crawler-scrapy/</a>. Spider code likes following:</p>\n<pre><code class=\"python\">from scrapy.spider         import BaseSpider\nfrom scrapy.selector         import HtmlXPathSelector\nfrom nettuts.items        import NettutsItem\nfrom scrapy.http        import Request\n\nclass MySpider(BaseSpider):\n     name         = \"nettuts\"\n     allowed_domains    = [\"net.tutsplus.com\"]\n     start_urls    = [\"http://net.tutsplus.com/\"]\n\ndef parse(self, response):\n    hxs     = HtmlXPathSelector(response)\n    titles     = hxs.select('//h1[@class=\"post_title\"]/a/text()').extract()\n    for title in titles:\n        item = NettutsItem()\n        item[\"title\"] = title\n        yield item\n</code></pre>\n<p>When launch the spider with command line: scrapy crawl nettus, it has following error:</p>\n<pre><code class=\"python\">[boto] DEBUG: Retrieving credentials from metadata server.\n2015-07-05 18:27:17 [boto] ERROR: Caught exception reading instance data\n\nTraceback (most recent call last):\n  File \"/anaconda/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n    r = opener.open(req, timeout=timeout)\n\n File \"/anaconda/lib/python2.7/urllib2.py\", line 431, in open\nresponse = self._open(req, data)\n\n File \"/anaconda/lib/python2.7/urllib2.py\", line 449, in _open\n'_open', req)\n\n File \"/anaconda/lib/python2.7/urllib2.py\", line 409, in _call_chain\nresult = func(*args)\n\n File \"/anaconda/lib/python2.7/urllib2.py\", line 1227, in http_open\nreturn self.do_open(httplib.HTTPConnection, req)\n\nFile \"/anaconda/lib/python2.7/urllib2.py\", line 1197, in do_open\nraise URLError(err)\n\nURLError: &lt;urlopen error [Errno 65] No route to host&gt;\n2015-07-05 18:27:17 [boto] ERROR: Unable to read instance data, giving up\n</code></pre>\n<p>really do not know what's wrong. Hope somebody could help</p>\n", "abstract": "I am new to python and want to use scrapy to build a web crawler. I go through the tutorial in http://blog.siliconstraits.vn/building-web-crawler-scrapy/. Spider code likes following: When launch the spider with command line: scrapy crawl nettus, it has following error: really do not know what's wrong. Hope somebody could help"}, "answers": [{"id": 31233576, "score": 29, "vote": 0, "content": "<p>in the settings.py file: add following code settings:</p>\n<p>DOWNLOAD_HANDLERS = {'s3': None,}</p>\n", "abstract": "in the settings.py file: add following code settings: DOWNLOAD_HANDLERS = {'s3': None,}"}, {"id": 31232727, "score": 0, "vote": 0, "content": "<p>The important information is:</p>\n<pre><code class=\"python\">URLError: &lt;urlopen error [Errno 65] No route to host&gt;\n</code></pre>\n<p>That is trying to tell you that your computer doesn't know how to communicate with the site you're trying to scrape. Are you able to access the site normally (i.e. in a web-browser) from the machine you're trying to run this python on?</p>\n", "abstract": "The important information is: That is trying to tell you that your computer doesn't know how to communicate with the site you're trying to scrape. Are you able to access the site normally (i.e. in a web-browser) from the machine you're trying to run this python on?"}]}, {"link": "https://stackoverflow.com/questions/45750739/scrapyd-client-command-not-found", "question": {"id": "45750739", "title": "scrapyd-client command not found", "content": "<p>I'd just installed the scrapyd-client(1.1.0) in a virtualenv, and run command 'scrapyd-deploy' successfully, but when I run 'scrapyd-client', the terminal said: command not found: scrapyd-client.</p>\n<p>According to the readme file(<a href=\"https://github.com/scrapy/scrapyd-client\" rel=\"noreferrer\">https://github.com/scrapy/scrapyd-client</a>), there should be a 'scrapyd-client' command.</p>\n<p>I had checked the path '/lib/python2.7/site-packages/scrapyd-client', only 'scrapyd-deploy' in the folder.</p>\n<p>Is the command 'scrapyd-client' being removed for now? </p>\n", "abstract": "I'd just installed the scrapyd-client(1.1.0) in a virtualenv, and run command 'scrapyd-deploy' successfully, but when I run 'scrapyd-client', the terminal said: command not found: scrapyd-client. According to the readme file(https://github.com/scrapy/scrapyd-client), there should be a 'scrapyd-client' command. I had checked the path '/lib/python2.7/site-packages/scrapyd-client', only 'scrapyd-deploy' in the folder. Is the command 'scrapyd-client' being removed for now? "}, "answers": [{"id": 45753444, "score": 24, "vote": 0, "content": "<p>Create a fresh environment and install scrapyd-client first using below</p>\n<pre><code class=\"python\">pip install git+https://github.com/scrapy/scrapyd-client\n</code></pre>\n<p>And it should work. I was able to get it</p>\n<pre><code class=\"python\">$ which scrapyd-client\n/Users/tarun.lalwani/.virtualenvs/sclient/bin/scrapyd-client\n</code></pre>\n<p>The package on pip may not be the latest one</p>\n", "abstract": "Create a fresh environment and install scrapyd-client first using below And it should work. I was able to get it The package on pip may not be the latest one"}]}, {"link": "https://stackoverflow.com/questions/30404364/scrapy-delay-request", "question": {"id": "30404364", "title": "Scrapy delay request", "content": "<p>every time i run my code my ip gets banned. I need help to delay each request for 10 seconds. I've tried to place DOWNLOAD_DELAY in code but it gives no results. Any help is appreciated.</p>\n<pre><code class=\"python\"># item class included here\n        class DmozItem(scrapy.Item):\n            # define the fields for your item here like:\n            link = scrapy.Field()\n            attr = scrapy.Field()\n\n\n        class DmozSpider(scrapy.Spider):\n            name = \"dmoz\"\n            allowed_domains = [\"craigslist.org\"]\n            start_urls = [\n            \"https://washingtondc.craigslist.org/search/fua\"\n            ]\n\n            BASE_URL = 'https://washingtondc.craigslist.org/'\n\n            def parse(self, response):\n                links = response.xpath('//a[@class=\"hdrlnk\"]/@href').extract()\n                for link in links:\n                    absolute_url = self.BASE_URL + link\n                    yield scrapy.Request(absolute_url, callback=self.parse_attr)\n\n            def parse_attr(self, response):\n                match = re.search(r\"(\\w+)\\.html\", response.url)\n                if match:\n                    item_id = match.group(1)\n                    url = self.BASE_URL + \"reply/nos/vgm/\" + item_id\n\n                    item = DmozItem()\n                    item[\"link\"] = response.url\n\n                    return scrapy.Request(url, meta={'item': item}, callback=self.parse_contact)\n\n            def parse_contact(self, response):\n                item = response.meta['item']\n                item[\"attr\"] = \"\".join(response.xpath(\"//div[@class='anonemail']//text()\").extract())\n                return item\n</code></pre>\n", "abstract": "every time i run my code my ip gets banned. I need help to delay each request for 10 seconds. I've tried to place DOWNLOAD_DELAY in code but it gives no results. Any help is appreciated."}, "answers": [{"id": 30410408, "score": 22, "vote": 0, "content": "<p>You need to set <a href=\"http://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY\">DOWNLOAD_DELAY in settings.py</a> of your project. Note that you may also need to limit concurrency. By default concurrency is 8 so you are hitting website with 8 simultaneous requests. </p>\n<pre><code class=\"python\"># settings.py\nDOWNLOAD_DELAY = 1\nCONCURRENT_REQUESTS_PER_DOMAIN = 2\n</code></pre>\n<p>Starting with <a href=\"https://pypi.python.org/pypi/Scrapy/1.0.0rc1\">Scrapy 1.0</a> you can also place custom settings in spider, so you could do something like this:</p>\n<pre><code class=\"python\">class DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\",\n    ]\n\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 5,\n        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 2\n    }\n</code></pre>\n<p>Delay and concurrency are set per downloader slot not per requests. To actually check what download you have you could try something like this</p>\n<pre><code class=\"python\">def parse(self, response):\n    \"\"\"\n    \"\"\"\n    delay = self.crawler.engine.downloader.slots[\"www.dmoz.org\"].delay\n    concurrency = self.crawler.engine.downloader.slots[\"www.dmoz.org\"].concurrency\n    self.log(\"Delay {}, concurrency {} for request {}\".format(delay, concurrency, response.request))\n    return\n</code></pre>\n", "abstract": "You need to set DOWNLOAD_DELAY in settings.py of your project. Note that you may also need to limit concurrency. By default concurrency is 8 so you are hitting website with 8 simultaneous requests.  Starting with Scrapy 1.0 you can also place custom settings in spider, so you could do something like this: Delay and concurrency are set per downloader slot not per requests. To actually check what download you have you could try something like this"}]}, {"link": "https://stackoverflow.com/questions/1934088/rotating-proxies-for-web-scraping", "question": {"id": "1934088", "title": "Rotating Proxies for web scraping", "content": "<p>I've got a python web crawler and I want to distribute the download requests among many different proxy servers, probably running squid (though I'm open to alternatives). For example, it could work in a round-robin fashion, where request1 goes to proxy1, request2 to proxy2, and eventually looping back around. Any idea how to set this up?</p>\n<p>To make it harder, I'd also like to be able to dynamically change the list of available proxies, bring some down, and add others.</p>\n<p>If it matters, IP addresses are assigned dynamically.</p>\n<p>Thanks :)</p>\n", "abstract": "I've got a python web crawler and I want to distribute the download requests among many different proxy servers, probably running squid (though I'm open to alternatives). For example, it could work in a round-robin fashion, where request1 goes to proxy1, request2 to proxy2, and eventually looping back around. Any idea how to set this up? To make it harder, I'd also like to be able to dynamically change the list of available proxies, bring some down, and add others. If it matters, IP addresses are assigned dynamically. Thanks :)"}, "answers": [{"id": 8612054, "score": 13, "vote": 0, "content": "<p>I've setted up rotating proxies using HAProxy + DeleGate + Multiple Tor Instances. With Tor you don't have good control of bandwidth and latency but it's useful for web scraping. I've just published an article on the subject: <a href=\"http://blog.databigbang.com/running-your-own-anonymous-rotating-proxies/\" rel=\"noreferrer\">Running Your Own Anonymous Rotating Proxies</a></p>\n", "abstract": "I've setted up rotating proxies using HAProxy + DeleGate + Multiple Tor Instances. With Tor you don't have good control of bandwidth and latency but it's useful for web scraping. I've just published an article on the subject: Running Your Own Anonymous Rotating Proxies"}, {"id": 1934198, "score": 6, "vote": 0, "content": "<p>Make your crawler have a list of proxies and with each HTTP request let it use the next proxy from the list in a round robin fashion. However, this will prevent you from using HTTP/1.1 persistent connections. Modifying the proxy list will eventually result in using a new or not using a proxy.</p>\n<p>Or have several connections open in parallel, one to each proxy, and distribute your crawling requests to each of the open connections. Dynamics may be implemented by having the connetor registering itself with the request dispatcher.</p>\n", "abstract": "Make your crawler have a list of proxies and with each HTTP request let it use the next proxy from the list in a round robin fashion. However, this will prevent you from using HTTP/1.1 persistent connections. Modifying the proxy list will eventually result in using a new or not using a proxy. Or have several connections open in parallel, one to each proxy, and distribute your crawling requests to each of the open connections. Dynamics may be implemented by having the connetor registering itself with the request dispatcher."}, {"id": 45699831, "score": 1, "vote": 0, "content": "<p>Edit: There is even Python wrapper for gimmeproxy: <a href=\"https://github.com/ericfourrier/gimmeproxy-api\" rel=\"nofollow noreferrer\">https://github.com/ericfourrier/gimmeproxy-api</a></p>\n<p>If you don't mind Node, you can use <a href=\"https://github.com/chill117/proxy-lists\" rel=\"nofollow noreferrer\">proxy-lists</a> to collect public proxies and <a href=\"https://github.com/256cats/check-proxy\" rel=\"nofollow noreferrer\">check-proxy</a> to check them. It's exactly how <a href=\"https://gimmeproxy.com\" rel=\"nofollow noreferrer\">https://gimmeproxy.com</a> works, more info <a href=\"http://256cats.com/gimmeproxy-source-part1/\" rel=\"nofollow noreferrer\">here</a></p>\n", "abstract": "Edit: There is even Python wrapper for gimmeproxy: https://github.com/ericfourrier/gimmeproxy-api If you don't mind Node, you can use proxy-lists to collect public proxies and check-proxy to check them. It's exactly how https://gimmeproxy.com works, more info here"}]}, {"link": "https://stackoverflow.com/questions/40479789/how-to-scrape-all-the-content-of-each-link-with-scrapy", "question": {"id": "40479789", "title": "How to scrape all the content of each link with scrapy?", "content": "<p>I am new with scrapy I would like to extract all the content of each advertise from this <a href=\"https://sfbay.craigslist.org/search/jjj?employment_type=2\" rel=\"nofollow noreferrer\">website</a>. So I tried the following:</p>\n<pre><code class=\"python\">from scrapy.spiders import Spider\nfrom craigslist_sample.items import CraigslistSampleItem\n\nfrom scrapy.selector import Selector\nclass MySpider(Spider):\n    name = \"craig\"\n    allowed_domains = [\"craigslist.org\"]\n    start_urls = [\"http://sfbay.craigslist.org/search/npo\"]\n\n    def parse(self, response):\n        links = response.selector.xpath(\".//*[@id='sortable-results']//ul//li//p\")\n        for link in links:\n            content = link.xpath(\".//*[@id='titletextonly']\").extract()\n            title = link.xpath(\"a/@href\").extract()\n            print(title,content)\n</code></pre>\n<p>items:</p>\n<pre><code class=\"python\"># Define here the models for your scraped items\n\nfrom scrapy.item import Item, Field\n\nclass CraigslistSampleItem(Item):\n    title = Field()\n    link = Field()\n</code></pre>\n<p>However, when I run the crawler I got nothing:</p>\n<pre><code class=\"python\">$ scrapy crawl --nolog craig\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n</code></pre>\n<p>Thus, my question is: How can I walk over each url, get inside each link and crawl the content and the title?, and which is the best way of do this?.</p>\n", "abstract": "I am new with scrapy I would like to extract all the content of each advertise from this website. So I tried the following: items: However, when I run the crawler I got nothing: Thus, my question is: How can I walk over each url, get inside each link and crawl the content and the title?, and which is the best way of do this?."}, "answers": [{"id": 40609976, "score": 14, "vote": 0, "content": "<p>To scaffold a basic scrapy project you can use the <a href=\"https://doc.scrapy.org/en/latest/intro/tutorial.html#creating-a-project\" rel=\"nofollow noreferrer\">command</a>:</p>\n<pre><code class=\"python\">scrapy startproject craig\n</code></pre>\n<p>Then add the spider and items:</p>\n<blockquote>\n<p>craig/spiders/spider.py</p>\n</blockquote>\n<pre><code class=\"python\">from scrapy import Spider\nfrom craig.items import CraigslistSampleItem\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\nfrom scrapy.selector import Selector\nfrom scrapy import Request\nimport urlparse, re\n\nclass CraigSpider(Spider):\n    name = \"craig\"\n    start_url = \"https://sfbay.craigslist.org/search/npo\"\n\n    def start_requests(self):\n\n        yield Request(self.start_url, callback=self.parse_results_page)\n\n\n    def parse_results_page(self, response):\n\n        sel = Selector(response)\n\n        # Browse paging.\n        page_urls = sel.xpath(\"\"\".//span[@class='buttons']/a[@class='button next']/@href\"\"\").getall()\n\n        for page_url in page_urls + [response.url]:\n            page_url = urlparse.urljoin(self.start_url, page_url)\n\n            # Yield a request for the next page of the list, with callback to this same function: self.parse_results_page().\n            yield Request(page_url, callback=self.parse_results_page)\n\n        # Browse items.\n        item_urls = sel.xpath(\"\"\".//*[@id='sortable-results']//li//a/@href\"\"\").getall()\n\n        for item_url in item_urls:\n            item_url = urlparse.urljoin(self.start_url, item_url)\n\n            # Yield a request for each item page, with callback self.parse_item().\n            yield Request(item_url, callback=self.parse_item)\n\n\n    def parse_item(self, response):\n\n        sel = Selector(response)\n\n        item = CraigslistSampleItem()\n\n        item['title'] = sel.xpath('//*[@id=\"titletextonly\"]').extract_first()\n        item['body'] = sel.xpath('//*[@id=\"postingbody\"]').extract_first()\n        item['link'] = response.url\n\n        yield item\n</code></pre>\n<blockquote>\n<p>craig/items.py</p>\n</blockquote>\n<pre><code class=\"python\"># -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n\nfrom scrapy.item import Item, Field\n\nclass CraigslistSampleItem(Item):\n    title = Field()\n    body = Field()\n    link = Field()\n</code></pre>\n<blockquote>\n<p>craig/settings.py</p>\n</blockquote>\n<pre><code class=\"python\"># -*- coding: utf-8 -*-\n\nBOT_NAME = 'craig'\n\nSPIDER_MODULES = ['craig.spiders']\nNEWSPIDER_MODULE = 'craig.spiders'\n\nITEM_PIPELINES = {\n   'craig.pipelines.CraigPipeline': 300,\n}\n</code></pre>\n<blockquote>\n<p>craig/pipelines.py</p>\n</blockquote>\n<pre><code class=\"python\">from scrapy import signals\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy.exporters import CsvItemExporter\n\nclass CraigPipeline(object):\n\n    def __init__(self):\n        dispatcher.connect(self.spider_opened, signals.spider_opened)\n        dispatcher.connect(self.spider_closed, signals.spider_closed)\n        self.files = {}\n\n    def spider_opened(self, spider):\n        file = open('%s_ads.csv' % spider.name, 'w+b')\n        self.files[spider] = file\n        self.exporter = CsvItemExporter(file)\n        self.exporter.start_exporting()\n\n    def spider_closed(self, spider):\n        self.exporter.finish_exporting()\n        file = self.files.pop(spider)\n        file.close()\n\n    def process_item(self, item, spider):\n        self.exporter.export_item(item)\n        return item\n</code></pre>\n<p>You can run the spider by running the <a href=\"https://doc.scrapy.org/en/latest/topics/commands.html#runspider\" rel=\"nofollow noreferrer\">command</a>:</p>\n<pre><code class=\"python\">scrapy runspider craig/spiders/spider.py\n</code></pre>\n<p>From the root of your project. </p>\n<p>It should create a <code>craig_ads.csv</code> in the root of your project.</p>\n", "abstract": "To scaffold a basic scrapy project you can use the command: Then add the spider and items: craig/spiders/spider.py craig/items.py craig/settings.py craig/pipelines.py You can run the spider by running the command: From the root of your project.  It should create a craig_ads.csv in the root of your project."}, {"id": 40539257, "score": 5, "vote": 0, "content": "<p>I am trying to answer your question.</p>\n<p>First of all, because of your <strong>incorrect XPath query</strong>, you got blank results. By XPath <code>\".//*[@id='sortable-results']//ul//li//p\"</code>, you located relevant <code>&lt;p&gt;</code> nodes correctly, though I don't like your query expression. However, I have no idea of your following XPath expression <code>\".//*[@id='titletextonly']\"</code> and <code>\"a/@href\"</code>, they couldn't locate link and title as you expected. Maybe your meaning is to locate the text of title and the hyperlink of the title. If yes, I believe you have to learn Xpath, and please start with <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction\" rel=\"nofollow noreferrer\">HTML DOM</a>.</p>\n<p>I do want to instruct you how to do XPath query, as there are lots of resources online. I would like to mention some features of Scrapy XPath selector:</p>\n<ol>\n<li><a href=\"https://doc.scrapy.org/en/0.10.3/topics/selectors.html#topics-selectors-ref\" rel=\"nofollow noreferrer\">Scrapy XPath Selector</a> is an improved wrapper of standard XPath query.</li>\n</ol>\n<p>In standard XPath query, it returns an array of DOM nodes you queried. You can open development mode of your browser(<code>F12</code>), use console command <code>$x(x_exp)</code> to test. I highly suggest that test your XPath expression through this way. It will give you instant results and save lots of time. If you have time, be familiar with the web development tools of your browser, which will have you quick understand web page structure and locate the entry you are looking for.</p>\n<p>While, Scrapy <code>response.xpath(x_exp)</code> returns an array of <code>Selector</code> objects corresponding to actual XPath query, which is actually a <code>SelectorList</code> object. This means XPath results is reprented by <code>SelectorsList</code>. And both <code>Selector</code> and <code>SelectorList</code> class provides some useful functions to operate the results:</p>\n<ul>\n<li><code>extract</code>, return a list of serialized document nodes (to unicode strings)</li>\n<li><code>extract_first</code>, return scalar, <code>first</code> of the <code>extract</code> results</li>\n<li><code>re</code>, return a list, <code>re</code> of the <code>extract</code> results</li>\n<li><code>re_first</code>, return scalar, <code>first</code> of the <code>re</code> results.</li>\n</ul>\n<p>These functions make your programming much more convenient. One example is that you can call <code>xpath</code> function directly on <code>SelectorList</code> object. If you tried <a href=\"http://lxml.de/\" rel=\"nofollow noreferrer\"><code>lxml</code></a> before, you would see that this is super useful: if you want to call <code>xpath</code> function on the results of a former <code>xpath</code> results in <code>lxml</code>, you have to iterate over the former results. Another example is that when you definitely sure that there is at most one element in that list, you can use <code>extract_first</code> to get a scalar value, instead of using list index method (e.g., <code>rlist[0]</code>) which would cause out of index exception when no element matched. Remember that there are always exceptions when you parse the web page, be careful and robust of your programming.</p>\n<ol start=\"2\">\n<li>Absolute XPath vs. <a href=\"https://doc.scrapy.org/en/0.10.3/topics/selectors.html#working-with-relative-xpaths\" rel=\"nofollow noreferrer\">relative XPath</a></li>\n</ol>\n<blockquote>\n<p>Keep in mind that if you are nesting XPathSelectors and use an XPath that starts with /, that XPath will be absolute to the document and not relative to the XPathSelector you\u2019re calling it from.</p>\n</blockquote>\n<p>When you do operation <code>node.xpath(x_expr)</code>, if <code>x_expr</code> starts with <code>/</code>, it is an absolute query, XPath will search from <code>root</code>; else if <code>x_expr</code> starts with <code>.</code>, it is a relative query. This is also noted in standards <a href=\"https://www.w3.org/TR/xpath/#NT-RelativeLocationPath\" rel=\"nofollow noreferrer\">2.5 Abbreviated Syntax</a></p>\n<blockquote>\n<p>. selects the context node</p>\n<p>.//para selects the para element descendants of the context node</p>\n<p>.. selects the parent of the context node</p>\n<p>../@lang selects the lang attribute of the parent of the context node</p>\n</blockquote>\n<ol start=\"3\">\n<li>How to follow the next page and end of following.</li>\n</ol>\n<p>For your application, you probably need to following the next page. Here, the next page node is easy to locate -- there are next buttons. However, you need also take care of the time to stop following. Look carefully for your URL query parameter to tell the URL pattern of your application. Here, to determine when to stop follow the next page, you can compare current item range with the total number of items.</p>\n<p><strong>New Edited</strong></p>\n<p>I was a little confused with the meaning of <strong>content of the link</strong>. Now I got it that @student wanted to crawl the link to extract AD content as well. The following is a solution.</p>\n<ol start=\"4\">\n<li>Send Request and attach its parser</li>\n</ol>\n<p>As you may notice that I use Scrapy <code>Request</code> class to follow the next page. Actually, the power of <a href=\"https://doc.scrapy.org/en/latest/topics/request-response.html\" rel=\"nofollow noreferrer\">Request</a> class is beyond that -- you can attach desired parse function for each request by setting parameter <code>callback</code>.</p>\n<blockquote>\n<p>callback (callable) \u2013 the function that will be called with the response of this request (once its downloaded) as its first parameter. For more information see Passing additional data to callback functions below. If a Request doesn\u2019t specify a callback, the spider\u2019s parse() method will be used. Note that if exceptions are raised during processing, errback is called instead.</p>\n</blockquote>\n<p>In step 3, I did not set <code>callback</code> when sending next page requests, as these request should be handled by default <code>parse</code> function. Now comes to the specified AD page, a different page then the former AD list page. Thus we need to define a new page parser function, let's say <code>parse_ad</code>, when we send each AD page request, attach this <code>parse_ad</code> function with the requests.</p>\n<p>Let's go to the revised sample code that works for me:</p>\n<p>items.py</p>\n<pre><code class=\"python\"># -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass ScrapydemoItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    link = scrapy.Field()\n\n\nclass AdItem(scrapy.Item):\n    title = scrapy.Field()\n    description = scrapy.Field()\n</code></pre>\n<p>The spider</p>\n<pre><code class=\"python\"># -*- coding: utf-8 -*-\nfrom scrapy.spiders import Spider\nfrom scrapy.http import Request\nfrom scrapydemo.items import ScrapydemoItem\nfrom scrapydemo.items import AdItem\ntry:\n    from urllib.parse import urljoin\nexcept ImportError:\n    from urlparse import urljoin\n\n\nclass MySpider(Spider):\n    name = \"demo\"\n    allowed_domains = [\"craigslist.org\"]\n    start_urls = [\"http://sfbay.craigslist.org/search/npo\"]\n\n    def parse(self, response):\n        # locate list of each item\n        s_links = response.xpath(\"//*[@id='sortable-results']/ul/li\")\n        # locate next page and extract it\n        next_page = response.xpath(\n            '//a[@title=\"next page\"]/@href').extract_first()\n        next_page = urljoin(response.url, next_page)\n        to = response.xpath(\n            '//span[@class=\"rangeTo\"]/text()').extract_first()\n        total = response.xpath(\n            '//span[@class=\"totalcount\"]/text()').extract_first()\n        # test end of following\n        if int(to) &lt; int(total):\n            # important, send request of next page\n            # default parsing function is 'parse'\n            yield Request(next_page)\n\n        for s_link in s_links:\n            # locate and extract\n            title = s_link.xpath(\"./p/a/text()\").extract_first().strip()\n            link = s_link.xpath(\"./p/a/@href\").extract_first()\n            link = urljoin(response.url, link)\n            if title is None or link is None:\n                print('Warning: no title or link found: %s', response.url)\n            else:\n                yield ScrapydemoItem(title=title, link=link)\n                # important, send request of ad page\n                # parsing function is 'parse_ad'\n                yield Request(link, callback=self.parse_ad)\n\n    def parse_ad(self, response):\n        ad_title = response.xpath(\n            '//span[@id=\"titletextonly\"]/text()').extract_first().strip()\n        ad_description = ''.join(response.xpath(\n            '//section[@id=\"postingbody\"]//text()').extract())\n        if ad_title is not None and ad_description is not None:\n            yield AdItem(title=ad_title, description=ad_description)\n        else:\n            print('Waring: no title or description found %s', response.url)\n</code></pre>\n<p><strong>Key Note</strong></p>\n<ul>\n<li>Two parse function, <code>parse</code> for requests of AD list page and <code>parse_ad</code> for request of specified AD page.</li>\n<li>To extract content of the AD post, you need some tricks. See <a href=\"https://stackoverflow.com/questions/23156780/how-can-i-get-all-the-plain-text-from-a-website-with-scrapy\">How can I get all the plain text from a website with Scrapy</a></li>\n</ul>\n<p>A snapshot of output:</p>\n<pre><code class=\"python\">2016-11-10 21:25:14 [scrapy] DEBUG: Scraped from &lt;200 http://sfbay.craigslist.org/eby/npo/5869108363.html&gt;\n{'description': '\\n'\n                '        \\n'\n                '            QR Code Link to This Post\\n'\n                '            \\n'\n                '        \\n'\n                'Agency History:\\n' ........\n 'title': 'Staff Accountant'}\n2016-11-10 21:25:14 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 39259,\n 'downloader/request_count': 117,\n 'downloader/request_method_count/GET': 117,\n 'downloader/response_bytes': 711320,\n 'downloader/response_count': 117,\n 'downloader/response_status_count/200': 117,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2016, 11, 11, 2, 25, 14, 878628),\n 'item_scraped_count': 314,\n 'log_count/DEBUG': 432,\n 'log_count/INFO': 8,\n 'request_depth_max': 2,\n 'response_received_count': 117,\n 'scheduler/dequeued': 116,\n 'scheduler/dequeued/memory': 116,\n 'scheduler/enqueued': 203,\n 'scheduler/enqueued/memory': 203,\n 'start_time': datetime.datetime(2016, 11, 11, 2, 24, 59, 242456)}\n2016-11-10 21:25:14 [scrapy] INFO: Spider closed (shutdown)\n</code></pre>\n<p>Thanks. Hope this would be helpful and have fun.</p>\n", "abstract": "I am trying to answer your question. First of all, because of your incorrect XPath query, you got blank results. By XPath \".//*[@id='sortable-results']//ul//li//p\", you located relevant <p> nodes correctly, though I don't like your query expression. However, I have no idea of your following XPath expression \".//*[@id='titletextonly']\" and \"a/@href\", they couldn't locate link and title as you expected. Maybe your meaning is to locate the text of title and the hyperlink of the title. If yes, I believe you have to learn Xpath, and please start with HTML DOM. I do want to instruct you how to do XPath query, as there are lots of resources online. I would like to mention some features of Scrapy XPath selector: In standard XPath query, it returns an array of DOM nodes you queried. You can open development mode of your browser(F12), use console command $x(x_exp) to test. I highly suggest that test your XPath expression through this way. It will give you instant results and save lots of time. If you have time, be familiar with the web development tools of your browser, which will have you quick understand web page structure and locate the entry you are looking for. While, Scrapy response.xpath(x_exp) returns an array of Selector objects corresponding to actual XPath query, which is actually a SelectorList object. This means XPath results is reprented by SelectorsList. And both Selector and SelectorList class provides some useful functions to operate the results: These functions make your programming much more convenient. One example is that you can call xpath function directly on SelectorList object. If you tried lxml before, you would see that this is super useful: if you want to call xpath function on the results of a former xpath results in lxml, you have to iterate over the former results. Another example is that when you definitely sure that there is at most one element in that list, you can use extract_first to get a scalar value, instead of using list index method (e.g., rlist[0]) which would cause out of index exception when no element matched. Remember that there are always exceptions when you parse the web page, be careful and robust of your programming. Keep in mind that if you are nesting XPathSelectors and use an XPath that starts with /, that XPath will be absolute to the document and not relative to the XPathSelector you\u2019re calling it from. When you do operation node.xpath(x_expr), if x_expr starts with /, it is an absolute query, XPath will search from root; else if x_expr starts with ., it is a relative query. This is also noted in standards 2.5 Abbreviated Syntax . selects the context node .//para selects the para element descendants of the context node .. selects the parent of the context node ../@lang selects the lang attribute of the parent of the context node For your application, you probably need to following the next page. Here, the next page node is easy to locate -- there are next buttons. However, you need also take care of the time to stop following. Look carefully for your URL query parameter to tell the URL pattern of your application. Here, to determine when to stop follow the next page, you can compare current item range with the total number of items. New Edited I was a little confused with the meaning of content of the link. Now I got it that @student wanted to crawl the link to extract AD content as well. The following is a solution. As you may notice that I use Scrapy Request class to follow the next page. Actually, the power of Request class is beyond that -- you can attach desired parse function for each request by setting parameter callback. callback (callable) \u2013 the function that will be called with the response of this request (once its downloaded) as its first parameter. For more information see Passing additional data to callback functions below. If a Request doesn\u2019t specify a callback, the spider\u2019s parse() method will be used. Note that if exceptions are raised during processing, errback is called instead. In step 3, I did not set callback when sending next page requests, as these request should be handled by default parse function. Now comes to the specified AD page, a different page then the former AD list page. Thus we need to define a new page parser function, let's say parse_ad, when we send each AD page request, attach this parse_ad function with the requests. Let's go to the revised sample code that works for me: items.py The spider Key Note A snapshot of output: Thanks. Hope this would be helpful and have fun."}]}, {"link": "https://stackoverflow.com/questions/61978049/reverse-search-an-image-in-yandex-images-using-python", "question": {"id": "61978049", "title": "Reverse search an image in Yandex Images using Python", "content": "<p>I'm interested in automatizing reverse image search. Yandex in particular is great for busting catfishes, even better than Google Images. So, consider this Python code:</p>\n<pre><code class=\"python\">import requests\nimport webbrowser\n\ntry:\n    filePath = \"C:\\\\path\\\\whateverThisIs.png\"\n    searchUrl = 'https://yandex.ru/images/'\n    multipart = {'encoded_image': (filePath, open(filePath, 'rb')), 'image_content': ''}\n    response = requests.post(searchUrl, files=multipart, allow_redirects=False)\n    #fetchUrl = response.headers['Location']\n    print(response)\n    print(dir(response))\n    print(response.content)\n    input()\nexcept Exception as e:\n    print(e)\n    print(e.with_traceback)\n    input()```\n</code></pre>\n<p>The script fails with KeyError, <code>'location'</code> is not found. I know the code works cause if you substitute <code>searchUrl</code> with <code>http://www.google.hr/searchbyimage/upload</code> then the script returns the correct url.\nSo, in short the expected outcome would be a url with an image search. In actuality we get a KeyError where that url was supposed to be stored.\nEvidently, Yandex doesn't work in exactly the same way, maybe the url is off (although I tried a heap ton of variations) or the reason may be completely different.</p>\n<p>Regardless of that, help in solving this problem is much appreciated!</p>\n", "abstract": "I'm interested in automatizing reverse image search. Yandex in particular is great for busting catfishes, even better than Google Images. So, consider this Python code: The script fails with KeyError, 'location' is not found. I know the code works cause if you substitute searchUrl with http://www.google.hr/searchbyimage/upload then the script returns the correct url.\nSo, in short the expected outcome would be a url with an image search. In actuality we get a KeyError where that url was supposed to be stored.\nEvidently, Yandex doesn't work in exactly the same way, maybe the url is off (although I tried a heap ton of variations) or the reason may be completely different. Regardless of that, help in solving this problem is much appreciated!"}, "answers": [{"id": 62136342, "score": 12, "vote": 0, "content": "<p>You can get url with an image search by using this code. Tested on ubuntu 18.04, with python 3.7 and requests 2.23.0</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import json\n\nimport requests\n\nfile_path = \"C:\\\\path\\\\whateverThisIs.png\"\nsearch_url = 'https://yandex.ru/images/search'\nfiles = {'upfile': ('blob', open(file_path, 'rb'), 'image/jpeg')}\nparams = {'rpt': 'imageview', 'format': 'json', 'request': '{\"blocks\":[{\"block\":\"b-page_type_search-by-image__link\"}]}'}\nresponse = requests.post(search_url, params=params, files=files)\nquery_string = json.loads(response.content)['blocks'][0]['params']['url']\nimg_search_url = search_url + '?' + query_string\nprint(img_search_url)\n</code></pre>\n", "abstract": "You can get url with an image search by using this code. Tested on ubuntu 18.04, with python 3.7 and requests 2.23.0"}, {"id": 62136211, "score": 1, "vote": 0, "content": "<p>There are no API for developers. You can try reverse inginer queries from your browser, but you will have to deal with anty robot protect. </p>\n<p>Another way to speed up process (but still manual) </p>\n<ol>\n<li>As described here\n<a href=\"https://yandex.com/support/images/loaded-image.html\" rel=\"nofollow noreferrer\">https://yandex.com/support/images/loaded-image.html</a> install\nYandex.Browser where you have hot key for image search  </li>\n<li>Host/make your site with all source images foe search queries </li>\n<li>Open your site in Yandex.Browser use \"right mouse click\"+\"serch image at\nyandex\"</li>\n<li>Copy what you need from page with results</li>\n</ol>\n", "abstract": "There are no API for developers. You can try reverse inginer queries from your browser, but you will have to deal with anty robot protect.  Another way to speed up process (but still manual) "}]}, {"link": "https://stackoverflow.com/questions/48946320/scrapy-get-all-links-from-any-website", "question": {"id": "48946320", "title": "Scrapy get all links from any website", "content": "<p>I have the following code for a web crawler in Python 3:</p>\n<pre><code class=\"python\">import requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef get_links(link):\n\n    return_links = []\n\n    r = requests.get(link)\n\n    soup = BeautifulSoup(r.content, \"lxml\")\n\n    if r.status_code != 200:\n        print(\"Error. Something is wrong here\")\n    else:\n        for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n            return_links.append(link.get('href')))\n\ndef recursive_search(links)\n    for i in links:\n        links.append(get_links(i))\n    recursive_search(links)\n\n\nrecursive_search(get_links(\"https://www.brandonskerritt.github.io\"))\n</code></pre>\n<p>The code basically gets all the links off of my GitHub pages website, and then it gets all the links off of those links, and so on until the end of time or an error occurs.</p>\n<p>I want to recreate this code in Scrapy so it can obey robots.txt and be a better web crawler overall. I've researched online and I can only find tutorials / guides / stackoverflow / quora / blog posts about how to scrape a specific domain (allowed_domains=[\"google.com\"], for example). I do not want to do this. I want to create code that will scrape all websites recursively.</p>\n<p>This isn't much of a problem but all the blog posts etc only show how to get the links from a specific website (for example, it might be that he links are in list tags). The code I have above works for all anchor tags, regardless of what website it's being run on.</p>\n<p>I do not want to use this in the wild, I need it for demonstration purposes so I'm not going to suddenly annoy everyone with excessive web crawling.</p>\n<p>Any help will be appreciated!</p>\n", "abstract": "I have the following code for a web crawler in Python 3: The code basically gets all the links off of my GitHub pages website, and then it gets all the links off of those links, and so on until the end of time or an error occurs. I want to recreate this code in Scrapy so it can obey robots.txt and be a better web crawler overall. I've researched online and I can only find tutorials / guides / stackoverflow / quora / blog posts about how to scrape a specific domain (allowed_domains=[\"google.com\"], for example). I do not want to do this. I want to create code that will scrape all websites recursively. This isn't much of a problem but all the blog posts etc only show how to get the links from a specific website (for example, it might be that he links are in list tags). The code I have above works for all anchor tags, regardless of what website it's being run on. I do not want to use this in the wild, I need it for demonstration purposes so I'm not going to suddenly annoy everyone with excessive web crawling. Any help will be appreciated!"}, "answers": [{"id": 48964940, "score": 7, "vote": 0, "content": "<p>There is an entire section of scrapy guide dedicated to <a href=\"https://docs.scrapy.org/en/latest/topics/broad-crawls.html\" rel=\"noreferrer\">broad crawls</a>. I suggest you to fine-grain your settings for doing this succesfully.</p>\n<p>For recreating the behaviour you need in scrapy, you must</p>\n<ul>\n<li> set your start url in your page.\n<li>write a parse function that follow all links and recursively call itself, adding to a spider variable the requested urls\n</li></li></ul>\n<p>An untested example (that can be, of course, refined):</p>\n<pre><code class=\"python\">class AllSpider(scrapy.Spider):\n    name = 'all'\n\n    start_urls = ['https://yourgithub.com']\n\n    def __init__(self):\n        self.links=[]\n\n    def parse(self, response):\n        self.links.append(response.url)\n        for href in response.css('a::attr(href)'):\n            yield response.follow(href, self.parse)\n</code></pre>\n", "abstract": "There is an entire section of scrapy guide dedicated to broad crawls. I suggest you to fine-grain your settings for doing this succesfully. For recreating the behaviour you need in scrapy, you must An untested example (that can be, of course, refined):"}, {"id": 48947055, "score": 5, "vote": 0, "content": "<p>If you want to allow crawling of all domains, simply don't specify <code>allowed_domains</code>, and use a <code>LinkExtractor</code> which extracts all links.</p>\n<p>A simple spider that follows all links:</p>\n<pre><code class=\"python\">class FollowAllSpider(CrawlSpider):\n    name = 'follow_all'\n\n    start_urls = ['https://example.com']\n    rules = [Rule(LinkExtractor(), callback='parse_item', follow=True)]\n\n    def parse_item(self, response):\n        pass\n</code></pre>\n", "abstract": "If you want to allow crawling of all domains, simply don't specify allowed_domains, and use a LinkExtractor which extracts all links. A simple spider that follows all links:"}]}, {"link": "https://stackoverflow.com/questions/4988297/trying-to-get-scrapy-into-a-project-to-run-crawl-command", "question": {"id": "4988297", "title": "Trying to get Scrapy into a project to run Crawl command", "content": "<p>I'm new to Python and Scrapy and I'm walking through the Scrapy tutorial. I've been able to create my project by using DOS interface and typing:</p>\n<pre><code class=\"python\">scrapy startproject dmoz\n</code></pre>\n<p>The tutorial later refers to the Crawl command:</p>\n<pre><code class=\"python\">scrapy crawl dmoz.org\n</code></pre>\n<p>But each time I try to run that I get a message that this is not a legit command.  In looking around further it looks like I need to be inside a project and that's what I can't figure out.  I've tried changing directories into the \"dmoz\" folder I created in startproject but that does not recognize Scrapy at all. </p>\n<p>I'm sure I'm missing something obvious and I'm hoping someone can point it out.</p>\n", "abstract": "I'm new to Python and Scrapy and I'm walking through the Scrapy tutorial. I've been able to create my project by using DOS interface and typing: The tutorial later refers to the Crawl command: But each time I try to run that I get a message that this is not a legit command.  In looking around further it looks like I need to be inside a project and that's what I can't figure out.  I've tried changing directories into the \"dmoz\" folder I created in startproject but that does not recognize Scrapy at all.  I'm sure I'm missing something obvious and I'm hoping someone can point it out."}, "answers": [{"id": 5019817, "score": 9, "vote": 0, "content": "<p>You have to execute it in your 'startproject' folder. You will have another commands if it finds your scrapy.cfg file. You can see the diference here: </p>\n<pre><code class=\"python\">$ scrapy startproject bar\n$ cd bar/\n$ ls\nbar  scrapy.cfg\n$ scrapy\nScrapy 0.12.0.2536 - project: bar\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  crawl         Start crawling from a spider or URL\n  deploy        Deploy project in Scrapyd target\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  list          List available spiders\n  parse         Parse URL (using its spider) and print the results\n  queue         Deprecated command. See Scrapyd documentation.\n  runserver     Deprecated command. Use 'server' command instead\n  runspider     Run a self-contained spider (without creating a project)\n  server        Start Scrapyd server for this project\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\n\n$ cd ..\n$ scrapy\nScrapy 0.12.0.2536 - no active project\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  fetch         Fetch a URL using the Scrapy downloader\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n</code></pre>\n", "abstract": "You have to execute it in your 'startproject' folder. You will have another commands if it finds your scrapy.cfg file. You can see the diference here: "}, {"id": 6671146, "score": 2, "vote": 0, "content": "<p>The PATH environmental variables aren't set.</p>\n<p>You can set the PATH environmental variables for both Python and Scrapy by finding System Properties (My Computer &gt; Properties &gt; Advanced System Settings) navigating to the Advanced tab and clicking the Environment Variables button. In the new window, scroll to Variable Path in the System Variables window and add the following lines separated by semi-colons</p>\n<pre>\nC:\\{path to python folder}\nC:\\{path to python folder}\\Scripts\n</pre>\n<p>example</p>\n<p><code>C:\\Python27;C:\\Python27\\Scripts</code></p>\n", "abstract": "The PATH environmental variables aren't set. You can set the PATH environmental variables for both Python and Scrapy by finding System Properties (My Computer > Properties > Advanced System Settings) navigating to the Advanced tab and clicking the Environment Variables button. In the new window, scroll to Variable Path in the System Variables window and add the following lines separated by semi-colons example C:\\Python27;C:\\Python27\\Scripts"}]}, {"link": "https://stackoverflow.com/questions/1811132/scrapy-sgmllinkextractor-is-ignoring-allowed-links", "question": {"id": "1811132", "title": "Scrapy SgmlLinkExtractor is ignoring allowed links", "content": "<p>Please take a look at <a href=\"http://doc.scrapy.org/topics/spiders.html#crawlspider-example\" rel=\"noreferrer\">this spider example</a> in Scrapy documentation. The explanation is:</p>\n<blockquote>\n<p>This spider would start crawling example.com\u2019s home page, collecting category links, and item links, parsing the latter with the parse_item method. For each item response, some data will be extracted from the HTML using XPath, and a Item will be filled with it. </p>\n</blockquote>\n<p>I copied the same spider exactly, and replaced \"example.com\" with another initial url.</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.item import Item\nfrom stb.items import StbItem\n\nclass StbSpider(CrawlSpider):\n    domain_name = \"stb\"\n    start_urls = ['http://www.stblaw.com/bios/MAlpuche.htm']\n\n    rules = (Rule(SgmlLinkExtractor(allow=(r'/bios/.\\w+\\.htm', )), callback='parse', follow=True), )\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n\n        item = StbItem()\n        item['JD'] = hxs.select('//td[@class=\"bodycopysmall\"]').re('\\d\\d\\d\\d\\sJ.D.')\n        return item\n\nSPIDER = StbSpider()\n</code></pre>\n<p>But my spider \"stb\" does not collect links from \"/bios/\" as it is supposed to do. It runs the initial url, scrapes the <code>item['JD']</code> and writes it on a file and then quits.</p>\n<p>Why is it that <code>SgmlLinkExtractor</code> is ignored? The <code>Rule</code> is read because it catches syntax errors inside the <code>Rule</code> line.</p>\n<p>Is this a bug? is there something wrong in my code? There are no errors except a bunch unhandled errors that I see with every run.</p>\n<p>It would be nice to know what I am doing wrong here. Thanks for any clues. Am I misunderstanding what <code>SgmlLinkExtractor</code> is supposed to do?</p>\n", "abstract": "Please take a look at this spider example in Scrapy documentation. The explanation is: This spider would start crawling example.com\u2019s home page, collecting category links, and item links, parsing the latter with the parse_item method. For each item response, some data will be extracted from the HTML using XPath, and a Item will be filled with it.  I copied the same spider exactly, and replaced \"example.com\" with another initial url. But my spider \"stb\" does not collect links from \"/bios/\" as it is supposed to do. It runs the initial url, scrapes the item['JD'] and writes it on a file and then quits. Why is it that SgmlLinkExtractor is ignored? The Rule is read because it catches syntax errors inside the Rule line. Is this a bug? is there something wrong in my code? There are no errors except a bunch unhandled errors that I see with every run. It would be nice to know what I am doing wrong here. Thanks for any clues. Am I misunderstanding what SgmlLinkExtractor is supposed to do?"}, "answers": [{"id": 2074821, "score": 11, "vote": 0, "content": "<p>The <code>parse</code> function is actually implemented and used in the CrawlSpider class, and you're unintentionally overriding it. If you change the name to something else, like <code>parse_item</code>, then the Rule should work.</p>\n", "abstract": "The parse function is actually implemented and used in the CrawlSpider class, and you're unintentionally overriding it. If you change the name to something else, like parse_item, then the Rule should work."}]}, {"link": "https://stackoverflow.com/questions/22702277/crawl-site-that-has-infinite-scrolling-using-python", "question": {"id": "22702277", "title": "crawl site that has infinite scrolling using python", "content": "<p>I have been doing research and so far I found out the python package that I will plan on using its <a href=\"http://scrapy.org/\" rel=\"noreferrer\">scrapy</a>, now I am trying to find out what is a good way to build a scraper using scrapy to crawl site with infinite scrolling. After digging around I found out that there is a package call selenium and it has python module. I have a feeling someone has already done that using Scrapy and <a href=\"https://pypi.python.org/pypi/selenium\" rel=\"noreferrer\">Selenium</a> to scrape site with infinite scrolling. It would be great if someone can point towards to an example. </p>\n", "abstract": "I have been doing research and so far I found out the python package that I will plan on using its scrapy, now I am trying to find out what is a good way to build a scraper using scrapy to crawl site with infinite scrolling. After digging around I found out that there is a package call selenium and it has python module. I have a feeling someone has already done that using Scrapy and Selenium to scrape site with infinite scrolling. It would be great if someone can point towards to an example. "}, "answers": [{"id": 26814374, "score": 9, "vote": 0, "content": "<p>You can use selenium to scrap the infinite scrolling website like twitter or facebook. </p>\n<p>Step 1 : Install Selenium using pip </p>\n<pre><code class=\"python\">pip install selenium \n</code></pre>\n<p>Step 2 : use the code below to automate infinite scroll and extract the source code</p>\n<pre><code class=\"python\">from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.common.exceptions import NoAlertPresentException\nimport sys\n\nimport unittest, time, re\n\nclass Sel(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.implicitly_wait(30)\n        self.base_url = \"https://twitter.com\"\n        self.verificationErrors = []\n        self.accept_next_alert = True\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + \"/search?q=stackoverflow&amp;src=typd\")\n        driver.find_element_by_link_text(\"All\").click()\n        for i in range(1,100):\n            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n            time.sleep(4)\n        html_source = driver.page_source\n        data = html_source.encode('utf-8')\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre>\n<p>The for loop allows you to parse through the infinite scrolls and post which you can extract the loaded data.</p>\n<p>Step 3 : Print the data if required.</p>\n", "abstract": "You can use selenium to scrap the infinite scrolling website like twitter or facebook.  Step 1 : Install Selenium using pip  Step 2 : use the code below to automate infinite scroll and extract the source code The for loop allows you to parse through the infinite scrolls and post which you can extract the loaded data. Step 3 : Print the data if required."}, {"id": 60336607, "score": 7, "vote": 0, "content": "<p>This is short &amp; simple code which is working for me:</p>\n<pre><code class=\"python\">SCROLL_PAUSE_TIME = 20\n\n# Get scroll height\nlast_height = driver.execute_script(\"return document.body.scrollHeight\")\n\nwhile True:\n    # Scroll down to bottom\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n    # Wait to load page\n    time.sleep(SCROLL_PAUSE_TIME)\n\n    # Calculate new scroll height and compare with last scroll height\n    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n    if new_height == last_height:\n        break\n    last_height = new_height\n\nposts = driver.find_elements_by_class_name(\"post-text\")\n\nfor block in posts:\n    print(block.text)\n</code></pre>\n", "abstract": "This is short & simple code which is working for me:"}, {"id": 23069377, "score": 5, "vote": 0, "content": "<pre><code class=\"python\">from selenium.webdriver.common.keys import Keys\nimport selenium.webdriver\ndriver = selenium.webdriver.Firefox()\ndriver.get(\"http://www.something.com\")\nlastElement = driver.find_elements_by_id(\"someId\")[-1]\nlastElement.send_keys(Keys.NULL)\n</code></pre>\n<p>This will open a page, find the bottom-most element with the given <code>id</code> and the scroll that element into view. You'll have to keep querying the driver to get the last element as the page loads more, and I've found this to be pretty slow as pages get large. The time is dominated by the call to <code>driver.find_element_*</code> because I don't know of a way to explicitly query the last element in the page. </p>\n<p>Through experimentation you might find there is an upper limit to the amount of elements the page loads dynamically, and it would be best if you wrote something that loaded that number and only then made a call to <code>driver.find_element_*</code>.</p>\n", "abstract": "This will open a page, find the bottom-most element with the given id and the scroll that element into view. You'll have to keep querying the driver to get the last element as the page loads more, and I've found this to be pretty slow as pages get large. The time is dominated by the call to driver.find_element_* because I don't know of a way to explicitly query the last element in the page.  Through experimentation you might find there is an upper limit to the amount of elements the page loads dynamically, and it would be best if you wrote something that loaded that number and only then made a call to driver.find_element_*."}, {"id": 58514707, "score": 2, "vote": 0, "content": "<p>For infinite scrolling data are requested to Ajax calls. Open web browser --&gt; network_tab --&gt; clear previous requests history by clicking icon like stop--&gt; scroll the webpage--&gt; now you can find the new request for scroll event--&gt; open the request header --&gt; you can find the URL of request ---&gt; copy and paste URL in an seperare tab--&gt; you can find the result of Ajax call --&gt; just form the requested URL to get the data page until end of the page</p>\n", "abstract": "For infinite scrolling data are requested to Ajax calls. Open web browser --> network_tab --> clear previous requests history by clicking icon like stop--> scroll the webpage--> now you can find the new request for scroll event--> open the request header --> you can find the URL of request ---> copy and paste URL in an seperare tab--> you can find the result of Ajax call --> just form the requested URL to get the data page until end of the page"}, {"id": 72792296, "score": 0, "vote": 0, "content": "<h1>Great Question! \ud83e\udd13</h1>\n<h2>The Challange</h2>\n<p>When working with an infinite scroll page (or dynamically loading site), there's no way to really know how long new items will take to load, as such it is hard to know how long to wait before new items load and we can hit <code>page-down</code>.</p>\n<p>Additionally, even if we can solve the first problem, we want to make sure that we're scrolling enough to actually reach the bottom of the page, so we want hit page-down enough times to actually reach the bottom of the page.</p>\n<p><strong>TLDR;If the site isn't so fast or for whatever reason data takes a while to load, we don't want to exit too early.</strong></p>\n<h2>My Solution</h2>\n<ul>\n<li>First, define a <code>scroll_down</code> function which takes a driver and a positive integer <code>n</code> as input.</li>\n<li>The function contains a <code>for-loop</code> which hits page down <code>n</code> times waiting .01 seconds (this can be changed) between page-downs</li>\n<li>Store the current window height in a variable named <code>prev_height</code></li>\n<li>Within a <code>for-loop</code> utilize a predefined function to scroll down.</li>\n<li>Within each iteration, take a significant pause allowing more items to load (I waited 10 seconds)</li>\n<li>After the pause, compare <code>prev_height</code> with the current height. If they are the same, then exit, otherwise continue.</li>\n</ul>\n<h2>Code</h2>\n<p><strong>Scroll function:</strong></p>\n<pre><code class=\"python\">def scroll_down(elem, num):\n    for _ in range(num):\n        time.sleep(.01)\n        elem.send_keys(Keys.PAGE_DOWN)\n</code></pre>\n<p><strong>Main code:</strong></p>\n<pre><code class=\"python\">    driver = &lt;load driver etc.&gt; \n    SCROLL_PAUSE_TIME = 10\n    elem = driver.find_element_by_tag_name(\"body\")\n    prev_height = elem.get_attribute(\"scrollHeight\")\n    \n    \n    for i in range(0, 500):\n        # note that the pause between page downs is only .01 seconds\n        # in this case that would be a sum of 1 second waiting time\n        scroll_down(elem,100)\n        # Wait to allow new items to load\n        time.sleep(SCROLL_PAUSE_TIME)\n\n        #check to see if scrollable space got larger\n        #also we're waiting until the second iteration to give time for the initial loading\n        if elem.get_attribute(\"scrollHeight\") == prev_height and i &gt; 0:\n            break\n        prev_height = elem.get_attribute(\"scrollHeight\") \n</code></pre>\n<p>Note: The actual numbers I used within my program may not work for you. But I do believe the solution itself is a reliable approach. Additionally, while the solution has been quite reliable for me, it is also one which takes time.</p>\n", "abstract": "When working with an infinite scroll page (or dynamically loading site), there's no way to really know how long new items will take to load, as such it is hard to know how long to wait before new items load and we can hit page-down. Additionally, even if we can solve the first problem, we want to make sure that we're scrolling enough to actually reach the bottom of the page, so we want hit page-down enough times to actually reach the bottom of the page. TLDR;If the site isn't so fast or for whatever reason data takes a while to load, we don't want to exit too early. Scroll function: Main code: Note: The actual numbers I used within my program may not work for you. But I do believe the solution itself is a reliable approach. Additionally, while the solution has been quite reliable for me, it is also one which takes time."}]}, {"link": "https://stackoverflow.com/questions/2360291/concurrent-downloads-python", "question": {"id": "2360291", "title": "Concurrent downloads - Python", "content": "<p>the plan is this:</p>\n<p>I download a webpage, collect a list of images parsed in the DOM and then download these. After this I would iterate through the images in order to evaluate which image is best suited to represent the webpage.</p>\n<p>Problem is that images are downloaded 1 by 1 and this can take quite some time.</p>\n<hr/>\n<p>It would be great if someone could point me in some direction regarding the topic.</p>\n<p>Help would be very much appreciated.</p>\n", "abstract": "the plan is this: I download a webpage, collect a list of images parsed in the DOM and then download these. After this I would iterate through the images in order to evaluate which image is best suited to represent the webpage. Problem is that images are downloaded 1 by 1 and this can take quite some time. It would be great if someone could point me in some direction regarding the topic. Help would be very much appreciated."}, "answers": [{"id": 2361129, "score": 13, "vote": 0, "content": "<p>Speeding up crawling is basically <a href=\"http://eventlet.net/\" rel=\"noreferrer\">Eventlet</a>'s main use case.  It's deeply fast -- we have an application that has to hit 2,000,000 urls in a few minutes.  It makes use of the fastest event interface on your system (epoll, generally), and uses greenthreads (which are built on top of coroutines and are very inexpensive) to make it easy to write.</p>\n<p>Here's <a href=\"http://eventlet.net/doc/examples.html#web-crawler\" rel=\"noreferrer\">an example from the docs</a>:</p>\n<pre><code class=\"python\">urls = [\"http://www.google.com/intl/en_ALL/images/logo.gif\",\n     \"https://wiki.secondlife.com/w/images/secondlife.jpg\",\n     \"http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif\"]\n\nimport eventlet\nfrom eventlet.green import urllib2  \n\ndef fetch(url):\n  body = urllib2.urlopen(url).read()\n  return url, body\n\npool = eventlet.GreenPool()\nfor url, body in pool.imap(fetch, urls):\n  print \"got body from\", url, \"of length\", len(body)\n</code></pre>\n<p>This is a pretty good starting point for developing a more fully-featured crawler.  Feel free to pop in to #eventlet on Freenode to ask for help.</p>\n<p>[update: I added a more-complex <a href=\"http://eventlet.net/doc/examples.html#producer-consumer-recursive-web-crawler\" rel=\"noreferrer\">recursive web crawler example</a> to the docs.  I swear it was in the works before this question was asked, but the question did finally inspire me to finish it.  :)]</p>\n", "abstract": "Speeding up crawling is basically Eventlet's main use case.  It's deeply fast -- we have an application that has to hit 2,000,000 urls in a few minutes.  It makes use of the fastest event interface on your system (epoll, generally), and uses greenthreads (which are built on top of coroutines and are very inexpensive) to make it easy to write. Here's an example from the docs: This is a pretty good starting point for developing a more fully-featured crawler.  Feel free to pop in to #eventlet on Freenode to ask for help. [update: I added a more-complex recursive web crawler example to the docs.  I swear it was in the works before this question was asked, but the question did finally inspire me to finish it.  :)]"}, {"id": 2360431, "score": 6, "vote": 0, "content": "<p>While threading is certainly a possibility, I would instead suggest <a href=\"http://docs.python.org/library/asyncore.html?highlight=asyncore#module-asyncore\" rel=\"nofollow noreferrer\"><code>asyncore</code></a> -- there's an excellent example <a href=\"http://broadcast.oreilly.com/2009/03/pymotw-asyncore.html\" rel=\"nofollow noreferrer\">here</a> which shows exactly the simultaneous fetching of two URLs (easy to generalize to any list of URLs!).</p>\n", "abstract": "While threading is certainly a possibility, I would instead suggest asyncore -- there's an excellent example here which shows exactly the simultaneous fetching of two URLs (easy to generalize to any list of URLs!)."}, {"id": 2360388, "score": 4, "vote": 0, "content": "<p><a href=\"http://www.ibm.com/developerworks/aix/library/au-threadingpython/\" rel=\"nofollow noreferrer\">Here</a> is an article on threading which uses url fetching as an example.</p>\n", "abstract": "Here is an article on threading which uses url fetching as an example."}, {"id": 9017007, "score": 0, "vote": 0, "content": "<p>Nowadays there are excellent Python libs you might want to use - <a href=\"http://urllib3.readthedocs.org/\" rel=\"nofollow\">urllib3</a> and <a href=\"http://docs.python-requests.org/\" rel=\"nofollow\">requests</a></p>\n", "abstract": "Nowadays there are excellent Python libs you might want to use - urllib3 and requests"}]}, {"link": "https://stackoverflow.com/questions/43447221/removing-all-spaces-in-text-file-with-python-3-x", "question": {"id": "43447221", "title": "Removing all spaces in text file with Python 3.x", "content": "<p>So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this:</p>\n<pre><code class=\"python\">https://example.com/asdf.html                                (note the spaces)\nhttps://example.com/johndoe.php                              (again)\n</code></pre>\n<p>I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files.</p>\n", "abstract": "So I have this crazy long text file made by my crawler and it for some reason added some spaces inbetween the links, like this: I want to get rid of that, but keep the new line. Keep in mind that the text file is 4.000+ lines long. I tried to do it myself but figured that I have no idea how to loop through new lines in files."}, "answers": [{"id": 43447546, "score": 16, "vote": 0, "content": "<p>Seems like you can't directly edit a python file, so here is my suggestion:</p>\n<pre><code class=\"python\"># first get all lines from file\nwith open('file.txt', 'r') as f:\n    lines = f.readlines()\n\n# remove spaces\nlines = [line.replace(' ', '') for line in lines]\n\n# finally, write lines in the file\nwith open('file.txt', 'w') as f:\n    f.writelines(lines)\n</code></pre>\n", "abstract": "Seems like you can't directly edit a python file, so here is my suggestion:"}, {"id": 43447313, "score": 3, "vote": 0, "content": "<p>You can open file and read line by line and remove white space - </p>\n<p><strong>Python 3.x:</strong></p>\n<pre><code class=\"python\">with open('filename') as f:\n    for line in f:\n        print(line.strip())\n</code></pre>\n<p><strong>Python 2.x:</strong></p>\n<pre><code class=\"python\">with open('filename') as f:\n    for line in f:\n        print line.strip()\n</code></pre>\n<p>It will remove space from each line and print it.</p>\n<p>Hope it helps!</p>\n", "abstract": "You can open file and read line by line and remove white space -  Python 3.x: Python 2.x: It will remove space from each line and print it. Hope it helps!"}, {"id": 59959638, "score": 3, "vote": 0, "content": "<h3>Read text from file, remove spaces, write text to file:</h3>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">with open('file.txt', 'r') as f:\n    txt = f.read().replace(' ', '')\n\nwith open('file.txt', 'w') as f:\n    f.write(txt)\n</code></pre>\n<p>In @Leonardo Chiriv\u00ec's solution it's unnecessary to create a list to store file contents when a string is sufficient and more memory efficient.  The <code>.replace(' ', '')</code> operation is only called once on the string, which is more efficient than iterating through a list performing replace for each line individually.</p>\n<h3>To avoid opening the file twice:</h3>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">with open('file.txt', 'r+') as f:\n    txt = f.read().replace(' ', '')\n    f.seek(0)\n    f.write(txt)\n    f.truncate()\n</code></pre>\n<p>It would be more efficient to only open the file once.  This requires moving the file pointer back to the start of the file after reading, as well as truncating any possibly remaining content left over after you write back to the file.  A drawback to this solution however is that is not as easily readable.</p>\n", "abstract": "In @Leonardo Chiriv\u00ec's solution it's unnecessary to create a list to store file contents when a string is sufficient and more memory efficient.  The .replace(' ', '') operation is only called once on the string, which is more efficient than iterating through a list performing replace for each line individually. It would be more efficient to only open the file once.  This requires moving the file pointer back to the start of the file after reading, as well as truncating any possibly remaining content left over after you write back to the file.  A drawback to this solution however is that is not as easily readable."}, {"id": 71190882, "score": 0, "vote": 0, "content": "<h1>I had something similar that I'd been dealing with.</h1>\n<p>This is what worked for me (Note: This converts from 2+ spaces into a comma, but if you read below the code block, I explain how you can get rid of ALL whitespaces):</p>\n<pre><code class=\"python\">import re\n\n# read the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt') as f:\n    read_file = f.read()\n    print(type(read_file)) # to confirm that it's a string\n\nread_file = re.sub(r'\\s{2,}', ',', read_file) # find/convert 2+ whitespace into ','\n\n# write the file\nwith open('C:\\\\path\\\\to\\\\test_file.txt', 'w') as f:\n    f.writelines('read_file')\n</code></pre>\n<p>This helped me then send the updated data to a CSV, which suited my need, but it can help for you as well, so instead of converting it to a comma (','), you can convert it to an empty string (''), and then [or] use a read_file.replace(' ', '') method if you don't need any whitespaces at all.</p>\n", "abstract": "This is what worked for me (Note: This converts from 2+ spaces into a comma, but if you read below the code block, I explain how you can get rid of ALL whitespaces): This helped me then send the updated data to a CSV, which suited my need, but it can help for you as well, so instead of converting it to a comma (','), you can convert it to an empty string (''), and then [or] use a read_file.replace(' ', '') method if you don't need any whitespaces at all."}, {"id": 71642376, "score": 0, "vote": 0, "content": "<p>Lets not forget about adding back the \\n to go to the next row.</p>\n<p>The complete function would be :</p>\n<pre><code class=\"python\">with open(str_path, 'r') as file :\n    str_lines = file.readlines()\n\n# remove spaces    \nif bl_right is True:    \n    str_lines = [line.rstrip() + '\\n' for line in str_lines]\nelif bl_left is True:   \n    str_lines = [line.lstrip() + '\\n' for line in str_lines]\nelse:                   \n    str_lines = [line.strip() + '\\n' for line in str_lines]\n\n# Write the file out again\nwith open(str_path, 'w') as file:\n    file.writelines(str_lines)\n</code></pre>\n", "abstract": "Lets not forget about adding back the \\n to go to the next row. The complete function would be :"}]}, {"link": "https://stackoverflow.com/questions/27805952/scrapy-set-depth-limit-per-allowed-domains", "question": {"id": "27805952", "title": "Scrapy set depth limit per allowed_domains", "content": "<p>I am crawling 6 different allowed_domains and would like to limit the depth of 1 domain.  How would I go about limiting the depth of that 1 domain in scrapy?\nOr would it be possible to crawl only 1 depth of an offsite domains?</p>\n", "abstract": "I am crawling 6 different allowed_domains and would like to limit the depth of 1 domain.  How would I go about limiting the depth of that 1 domain in scrapy?\nOr would it be possible to crawl only 1 depth of an offsite domains?"}, "answers": [{"id": 27810497, "score": 21, "vote": 0, "content": "<p>Scrapy doesn't provide anything like this. You can <a href=\"https://github.com/scrapy/scrapy/issues/935\" rel=\"nofollow noreferrer\">set the <code>DEPTH_LIMIT</code> per-spider</a>, but not per-domain.</p>\n<p>What can we do? <a href=\"http://blog.codinghorror.com/learn-to-read-the-source-luke/\" rel=\"nofollow noreferrer\">Read the code</a>, drink coffee and solve it (order is important).</p>\n<p>The idea is to disable Scrapy's built-in <a href=\"https://github.com/scrapy/scrapy/blob/e748ca50ca3e83ac703e02538a27236fedd53a7d/scrapy/contrib/spidermiddleware/depth.py\" rel=\"nofollow noreferrer\"><code>DepthMiddleware</code></a> and <a href=\"http://doc.scrapy.org/en/latest/topics/spider-middleware.html\" rel=\"nofollow noreferrer\">provide our custom one</a> instead.</p>\n<p>First, let's define settings:</p>\n<ul>\n<li><code>DOMAIN_DEPTHS</code> would be a dictionary with depth limits per domain</li>\n<li><code>DEPTH_LIMIT</code> setting we'll leave as a default one in case a domain is not configured</li>\n</ul>\n<p>Example settings:</p>\n<pre><code class=\"python\">DOMAIN_DEPTHS = {'amazon.com': 1, 'homedepot.com': 4}\nDEPTH_LIMIT = 3\n</code></pre>\n<p>Okay, now the custom middleware (based on <code>DepthMiddleware</code>):</p>\n<pre><code class=\"python\">from scrapy import log\nfrom scrapy.http import Request\nimport tldextract\n\n\nclass DomainDepthMiddleware(object):\n    def __init__(self, domain_depths, default_depth):\n        self.domain_depths = domain_depths\n        self.default_depth = default_depth\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        domain_depths = settings.getdict('DOMAIN_DEPTHS', default={})\n        default_depth = settings.getint('DEPTH_LIMIT', 1)\n\n        return cls(domain_depths, default_depth)\n\n    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request):\n                # get max depth per domain\n                domain = tldextract.extract(request.url).registered_domain\n                maxdepth = self.domain_depths.get(domain, self.default_depth)\n\n                depth = response.meta.get('depth', 0) + 1\n                request.meta['depth'] = depth\n\n                if maxdepth and depth &gt; maxdepth:\n                    log.msg(format=\"Ignoring link (depth &gt; %(maxdepth)d): %(requrl)s \",\n                            level=log.DEBUG, spider=spider,\n                            maxdepth=maxdepth, requrl=request.url)\n                    return False\n            return True\n\n        return (r for r in result or () if _filter(r))\n</code></pre>\n<p>Note that it requires <a href=\"https://github.com/john-kurkowski/tldextract\" rel=\"nofollow noreferrer\"><code>tldextract</code></a> module to be installed (used for extracting a domain name from url):</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import tldextract\n&gt;&gt;&gt; url = 'http://stackoverflow.com/questions/27805952/scrapy-set-depth-limit-per-allowed-domains'\n&gt;&gt;&gt; tldextract.extract(url).registered_domain\n'stackoverflow.com'\n</code></pre>\n<p>Now we need to turn off the default middleware and use the one we implemented:</p>\n<pre><code class=\"python\">SPIDER_MIDDLEWARES = {\n    'myproject.middlewares.DomainDepthMiddleware': 900,\n    'scrapy.spidermiddlewares.depth.DepthMiddleware': None\n}\n</code></pre>\n", "abstract": "Scrapy doesn't provide anything like this. You can set the DEPTH_LIMIT per-spider, but not per-domain. What can we do? Read the code, drink coffee and solve it (order is important). The idea is to disable Scrapy's built-in DepthMiddleware and provide our custom one instead. First, let's define settings: Example settings: Okay, now the custom middleware (based on DepthMiddleware): Note that it requires tldextract module to be installed (used for extracting a domain name from url): Now we need to turn off the default middleware and use the one we implemented:"}]}, {"link": "https://stackoverflow.com/questions/20118753/python-scrapy-populate-start-urls-from-mysql", "question": {"id": "20118753", "title": "Python Scrapy - populate start_urls from mysql", "content": "<p>I am trying to populate start_url with a SELECT from a MYSQL table using <strong>spider.py</strong>. When i run \"scrapy runspider spider.py\" i get no output, just that it finished with no error.</p>\n<p>I have tested the SELECT query in a python script and start_url get populated with the entrys from the MYSQL table.</p>\n<p><strong>spider.py</strong></p>\n<pre><code class=\"python\">from scrapy.spider import BaseSpider\nfrom scrapy.selector import Selector\nimport MySQLdb\n\n\nclass ProductsSpider(BaseSpider):\n    name = \"Products\"\n    allowed_domains = [\"test.com\"]\n    start_urls = []\n\n    def parse(self, response):\n        print self.start_urls\n\n    def populate_start_urls(self, url):\n        conn = MySQLdb.connect(\n                user='user',\n                passwd='password',\n                db='scrapy',\n                host='localhost',\n                charset=\"utf8\",\n                use_unicode=True\n                )\n        cursor = conn.cursor()\n        cursor.execute(\n            'SELECT url FROM links;'\n            )\n    rows = cursor.fetchall()\n\n    for row in rows:\n        start_urls.append(row[0])\n    conn.close()\n</code></pre>\n", "abstract": "I am trying to populate start_url with a SELECT from a MYSQL table using spider.py. When i run \"scrapy runspider spider.py\" i get no output, just that it finished with no error. I have tested the SELECT query in a python script and start_url get populated with the entrys from the MYSQL table. spider.py"}, "answers": [{"id": 20137164, "score": 13, "vote": 0, "content": "<p>A better approach is to override the <a href=\"http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.BaseSpider.start_requests\" rel=\"noreferrer\">start_requests</a> method.</p>\n<p>This can query your database, much like <code>populate_start_urls</code>, and return a sequence of <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\" rel=\"noreferrer\">Request</a> objects. </p>\n<p>You would just need to rename your <code>populate_start_urls</code> method to <code>start_requests</code> and modify the following lines:</p>\n<pre><code class=\"python\">for row in rows:\n    yield self.make_requests_from_url(row[0])\n</code></pre>\n", "abstract": "A better approach is to override the start_requests method. This can query your database, much like populate_start_urls, and return a sequence of Request objects.  You would just need to rename your populate_start_urls method to start_requests and modify the following lines:"}, {"id": 20124807, "score": 5, "vote": 0, "content": "<p>Write the populating in the <code>__init__</code>:</p>\n<pre><code class=\"python\">def __init__(self):\n    super(ProductsSpider,self).__init__()\n    self.start_urls = get_start_urls()\n</code></pre>\n<p>Assuming <code>get_start_urls()</code> returns the urls.</p>\n", "abstract": "Write the populating in the __init__: Assuming get_start_urls() returns the urls."}]}, {"link": "https://stackoverflow.com/questions/1809817/scrapy-sgmllinkextractor-question", "question": {"id": "1809817", "title": "Scrapy SgmlLinkExtractor question", "content": "<p>I am trying to make the SgmlLinkExtractor to work.</p>\n<p>This is the signature:</p>\n<pre><code class=\"python\">SgmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths(), tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None)\n</code></pre>\n<p>I am just using <code>allow=()</code></p>\n<p>So, I enter</p>\n<pre><code class=\"python\">rules = (Rule(SgmlLinkExtractor(allow=(\"/aadler/\", )), callback='parse'),)\n</code></pre>\n<p>So, the initial url is <code>'http://www.whitecase.com/jacevedo/'</code> and I am entering <code>allow=('/aadler',)</code> and expect that \n<code>'/aadler/'</code> will get scanned as well. But instead, the spider scans the initial url and then closes:</p>\n<pre><code class=\"python\">[wcase] INFO: Domain opened\n[wcase] DEBUG: Crawled &lt;/jacevedo/&gt; (referer: &lt;None&gt;)\n[wcase] INFO: Passed NuItem(school=[u'JD, ', u'Columbia Law School, Harlan Fiske Stone Scholar, Parker School Recognition of Achievement in International and Foreign Law, ', u'2005'])\n[wcase] INFO: Closing domain (finished)\n</code></pre>\n<p>What am I doing wrong here?</p>\n<p>Is there anyone here who used Scrapy successfully who can help me to finish this spider?</p>\n<p>Thank you for the help.</p>\n<p>I include the code for the spider below:</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom scrapy.item import Item\nfrom Nu.items import NuItem\nfrom urls import u\n\nclass NuSpider(CrawlSpider):\n    domain_name = \"wcase\"\n    start_urls = ['xxxxxx/jacevedo/']\n\n    rules = (Rule(SgmlLinkExtractor(allow=(\"/aadler/\", )), callback='parse'),)\n\n    def parse(self, response):\n        hxs = HtmlXPathSelector(response)\n\n        item = NuItem()\n        item['school'] = hxs.select('//td[@class=\"mainColumnTDa\"]').re('(?&lt;=(JD,\\s))(.*?)(\\d+)')\n        return item\n\nSPIDER = NuSpider()\n</code></pre>\n<p>Note: SO will not let me post more than 1 url so substitute the initial url as necessary. Sorry about that.</p>\n", "abstract": "I am trying to make the SgmlLinkExtractor to work. This is the signature: I am just using allow=() So, I enter So, the initial url is 'http://www.whitecase.com/jacevedo/' and I am entering allow=('/aadler',) and expect that \n'/aadler/' will get scanned as well. But instead, the spider scans the initial url and then closes: What am I doing wrong here? Is there anyone here who used Scrapy successfully who can help me to finish this spider? Thank you for the help. I include the code for the spider below: Note: SO will not let me post more than 1 url so substitute the initial url as necessary. Sorry about that."}, "answers": [{"id": 1818537, "score": 10, "vote": 0, "content": "<p>You are overriding the \"parse\" method it appears. \"parse\", is a private method in CrawlSpider used to follow links.</p>\n", "abstract": "You are overriding the \"parse\" method it appears. \"parse\", is a private method in CrawlSpider used to follow links."}, {"id": 7887642, "score": 3, "vote": 0, "content": "<p>if you check documentation a \"<strong>Warning</strong>\" is clearly written  </p>\n<p><em>\"When writing crawl spider rules, avoid using parse as callback, since the Crawl Spider uses the parse method itself to implement its logic. So if you override the parse method, the crawl spider will no longer work.\"</em></p>\n<p><a href=\"http://readthedocs.org/docs/scrapy/en/latest/topics/spiders.html?highlight=rule\" rel=\"nofollow\">url for verification</a></p>\n", "abstract": "if you check documentation a \"Warning\" is clearly written   \"When writing crawl spider rules, avoid using parse as callback, since the Crawl Spider uses the parse method itself to implement its logic. So if you override the parse method, the crawl spider will no longer work.\" url for verification"}, {"id": 1810205, "score": 1, "vote": 0, "content": "<p>allow=(r'/aadler/', ...</p>\n", "abstract": "allow=(r'/aadler/', ..."}, {"id": 4516819, "score": 1, "vote": 0, "content": "<p>You are missing comma after first element for \"rules\" to be a tuple..</p>\n<pre><code class=\"python\">rules = (Rule(SgmlLinkExtractor(allow=('/careers/n.\\w+', )), callback='parse', follow=True),)\n</code></pre>\n", "abstract": "You are missing comma after first element for \"rules\" to be a tuple.."}]}, {"link": "https://stackoverflow.com/questions/37997702/how-to-convert-a-string-into-a-beautifulsoup-object", "question": {"id": "37997702", "title": "How to convert a String into a BeautifulSoup object?", "content": "<p>I'm trying to crawl a news website and I need to change one parameter. I changed it with replace with the next code:</p>\n<pre><code class=\"python\">while i &lt; len(links):\n    conn = urllib.urlopen(links[i])\n    html = conn.read()\n    soup = BeautifulSoup(html)\n    t = html.replace('class=\"row bigbox container mi-df-local locked-single\"', 'class=\"row bigbox container mi-df-local single-local\"')\n    n = str(t.find(\"div\", attrs={'class':'entry cuerpo-noticias'}))\n    print(p)\n</code></pre>\n<p>The problem is that \"t\" type is string and find with attributes is only applicable to types <code>&lt;class 'BeautifulSoup.BeautifulSoup'&gt;</code>. Do you know how can I convert \"t\" to that type?</p>\n", "abstract": "I'm trying to crawl a news website and I need to change one parameter. I changed it with replace with the next code: The problem is that \"t\" type is string and find with attributes is only applicable to types <class 'BeautifulSoup.BeautifulSoup'>. Do you know how can I convert \"t\" to that type?"}, "answers": [{"id": 37997717, "score": 13, "vote": 0, "content": "<p>Just do the replacement <em>before parsing</em>:</p>\n<pre><code class=\"python\">html = html.replace('class=\"row bigbox container mi-df-local locked-single\"', 'class=\"row bigbox container mi-df-local single-local\"')\nsoup = BeautifulSoup(html, \"html.parser\")\n</code></pre>\n<hr/>\n<p>Note that it would also be possible (I would even say <strong>preferred</strong>) to parse the HTML, locate the element(s) and <em>modify the attributes</em> of a <code>Tag</code> instance, e.g.:</p>\n<pre><code class=\"python\">soup = BeautifulSoup(html, \"html.parser\")\nfor elm in soup.select(\".row.bigbox.container.mi-df-local.locked-single\"):\n    elm[\"class\"] = [\"row\", \"bigbox\", \"container\", \"mi-df-local\", \"single-local\"]\n</code></pre>\n<p>Note that <code>class</code> is a special <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-by-css-class\" rel=\"noreferrer\">multi-valued attribute</a> - that's why we are setting the value to a list of individual classes.</p>\n<p>Demo:</p>\n<pre><code class=\"python\">from bs4 import BeautifulSoup\n\nhtml = \"\"\"\n&lt;div class=\"row bigbox container mi-df-local locked-single\"&gt;test&lt;/div&gt;\n\"\"\"\n\nsoup = BeautifulSoup(html, \"html.parser\")\nfor elm in soup.select(\".row.bigbox.container.mi-df-local.locked-single\"):\n    elm[\"class\"] = [\"row\", \"bigbox\", \"container\", \"mi-df-local\", \"single-local\"]\n\nprint(soup.prettify())\n</code></pre>\n<p>Now see how the <code>div</code> element classes were updated:</p>\n<pre><code class=\"python\">&lt;div class=\"row bigbox container mi-df-local single-local\"&gt;\n test\n&lt;/div&gt;\n</code></pre>\n", "abstract": "Just do the replacement before parsing: Note that it would also be possible (I would even say preferred) to parse the HTML, locate the element(s) and modify the attributes of a Tag instance, e.g.: Note that class is a special multi-valued attribute - that's why we are setting the value to a list of individual classes. Demo: Now see how the div element classes were updated:"}]}, {"link": "https://stackoverflow.com/questions/6591255/following-links-scrapy-web-crawler-framework", "question": {"id": "6591255", "title": "Following links, Scrapy web crawler framework", "content": "<p>After several readings to Scrapy docs I'm still not catching the diferrence between using CrawlSpider rules and implementing my own link extraction mechanism on the callback method.</p>\n<p>I'm about to write a new web crawler using the latter approach, but just becuase I had a bad experience in a past project using rules. I'd really like to know exactly what I'm doing and why.</p>\n<p>Anyone familiar with this tool?</p>\n<p>Thanks for your help!</p>\n", "abstract": "After several readings to Scrapy docs I'm still not catching the diferrence between using CrawlSpider rules and implementing my own link extraction mechanism on the callback method. I'm about to write a new web crawler using the latter approach, but just becuase I had a bad experience in a past project using rules. I'd really like to know exactly what I'm doing and why. Anyone familiar with this tool? Thanks for your help!"}, "answers": [{"id": 6593158, "score": 11, "vote": 0, "content": "<p>CrawlSpider inherits BaseSpider. It just added rules to extract and follow links.\nIf these rules are not enough flexible for you - use BaseSpider:</p>\n<pre><code class=\"python\">class USpider(BaseSpider):\n    \"\"\"my spider. \"\"\"\n\n    start_urls = ['http://www.amazon.com/s/?url=search-alias%3Dapparel&amp;sort=relevance-fs-browse-rank']\n    allowed_domains = ['amazon.com']\n\n    def parse(self, response):\n        '''Parse main category search page and extract subcategory search link.'''\n        self.log('Downloaded category search page.', log.DEBUG)\n        if response.meta['depth'] &gt; 5:\n            self.log('Categories depth limit reached (recursive links?). Stopping further following.', log.WARNING)\n\n        hxs = HtmlXPathSelector(response)\n        subcategories = hxs.select(\"//div[@id='refinements']/*[starts-with(.,'Department')]/following-sibling::ul[1]/li/a[span[@class='refinementLink']]/@href\").extract()\n        for subcategory in subcategories:\n            subcategorySearchLink = urlparse.urljoin(response.url, subcategorySearchLink)\n            yield Request(subcategorySearchLink, callback = self.parseSubcategory)\n\n    def parseSubcategory(self, response):\n        '''Parse subcategory search page and extract item links.'''\n        hxs = HtmlXPathSelector(response)\n\n        for itemLink in hxs.select('//a[@class=\"title\"]/@href').extract():\n            itemLink = urlparse.urljoin(response.url, itemLink)\n            self.log('Requesting item page: ' + itemLink, log.DEBUG)\n            yield Request(itemLink, callback = self.parseItem)\n\n        try:\n            nextPageLink = hxs.select(\"//a[@id='pagnNextLink']/@href\").extract()[0]\n            nextPageLink = urlparse.urljoin(response.url, nextPageLink)\n            self.log('\\nGoing to next search page: ' + nextPageLink + '\\n', log.DEBUG)\n            yield Request(nextPageLink, callback = self.parseSubcategory)\n        except:\n            self.log('Whole category parsed: ' + categoryPath, log.DEBUG)\n\n    def parseItem(self, response):\n        '''Parse item page and extract product info.'''\n\n        hxs = HtmlXPathSelector(response)\n        item = UItem()\n\n        item['brand'] = self.extractText(\"//div[@class='buying']/span[1]/a[1]\", hxs)\n        item['title'] = self.extractText(\"//span[@id='btAsinTitle']\", hxs)\n        ...\n</code></pre>\n<p>Even if BaseSpider's start_urls are not enough flexible for you, override <a href=\"http://doc.scrapy.org/topics/spiders.html#scrapy.spider.BaseSpider.start_requests\" rel=\"noreferrer\">start_requests</a> method.</p>\n", "abstract": "CrawlSpider inherits BaseSpider. It just added rules to extract and follow links.\nIf these rules are not enough flexible for you - use BaseSpider: Even if BaseSpider's start_urls are not enough flexible for you, override start_requests method."}, {"id": 6591511, "score": 1, "vote": 0, "content": "<p>If you want selective crawling, like fetching \"Next\" links for pagination etc., it's better to write your own crawler. But for general crawling, you should use crawlspider and filter out the links that you don't need to follow using Rules &amp; process_links function.</p>\n<p>Take a look at the crawlspider code in <code>\\scrapy\\contrib\\spiders\\crawl.py</code> , it isn't too complicated. </p>\n", "abstract": "If you want selective crawling, like fetching \"Next\" links for pagination etc., it's better to write your own crawler. But for general crawling, you should use crawlspider and filter out the links that you don't need to follow using Rules & process_links function. Take a look at the crawlspider code in \\scrapy\\contrib\\spiders\\crawl.py , it isn't too complicated. "}]}, {"link": "https://stackoverflow.com/questions/14870694/how-to-collect-data-from-multiple-pages-into-single-data-structure-with-scrapy", "question": {"id": "14870694", "title": "How to collect data from multiple pages into single data structure with scrapy", "content": "<p>I am trying to scrape data from a site.The data is structured as multiple objects each with a set of data.\nFor example, people with names, ages, and occupations.</p>\n<p>My problem is that this data is split across two levels in the website.<br/> The first page is, say, a list of names and ages with a link to each persons profile page.<br/> Their profile page lists their occupation.</p>\n<p>I already have a spider written with scrapy in python which can collect the data from the top layer and crawl through multiple paginations.<br/> But, how can I collect the data from the inner pages while keeping it linked to the appropriate object?</p>\n<p>Currently, I have the output structured with json as </p>\n<pre><code class=\"python\">   {[name='name',age='age',occupation='occupation'],\n   [name='name',age='age',occupation='occupation']} etc\n</code></pre>\n<p>Can the parse function reach across pages like that?</p>\n", "abstract": "I am trying to scrape data from a site.The data is structured as multiple objects each with a set of data.\nFor example, people with names, ages, and occupations. My problem is that this data is split across two levels in the website. The first page is, say, a list of names and ages with a link to each persons profile page. Their profile page lists their occupation. I already have a spider written with scrapy in python which can collect the data from the top layer and crawl through multiple paginations. But, how can I collect the data from the inner pages while keeping it linked to the appropriate object? Currently, I have the output structured with json as  Can the parse function reach across pages like that?"}, "answers": [{"id": 14871241, "score": 9, "vote": 0, "content": "<p>here is a way you need to deal. you need to yield/return item once when item has all attributes </p>\n<pre><code class=\"python\">yield Request(page1,\n              callback=self.page1_data)\n\ndef page1_data(self, response):\n    hxs = HtmlXPathSelector(response)\n    i = TestItem()\n    i['name']='name'\n    i['age']='age'\n    url_profile_page = 'url to the profile page'\n\n    yield Request(url_profile_page,\n                  meta={'item':i},\n    callback=self.profile_page)\n\n\ndef profile_page(self,response):\n    hxs = HtmlXPathSelector(response)\n    old_item=response.request.meta['item']\n    # parse other fileds\n    # assign them to old_item\n\n    yield old_item\n</code></pre>\n", "abstract": "here is a way you need to deal. you need to yield/return item once when item has all attributes "}]}, {"link": "https://stackoverflow.com/questions/45886068/scrapy-crawlspider-splash-how-to-follow-links-through-linkextractor", "question": {"id": "45886068", "title": "Scrapy CrawlSpider + Splash: how to follow links through linkextractor?", "content": "<p>I have the following code that is partially working, </p>\n<pre><code class=\"python\">class ThreadSpider(CrawlSpider):\n    name = 'thread'\n    allowed_domains = ['bbs.example.com']\n    start_urls = ['http://bbs.example.com/diy']\n\n    rules = (\n        Rule(LinkExtractor(\n            allow=(),\n            restrict_xpaths=(\"//a[contains(text(), 'Next Page')]\")\n        ),\n            callback='parse_item',\n            process_request='start_requests',\n            follow=True),\n    )\n\ndef start_requests(self):\n    for url in self.start_urls:\n        yield SplashRequest(url, self.parse_item, args={'wait': 0.5})\n\ndef parse_item(self, response):\n    # item parser\n</code></pre>\n<p>the code will run only for <code>start_urls</code> but will not follow the links specified in <code>restricted_xpaths</code>, if i comment out <code>start_requests()</code> method and the line <code>process_request='start_requests',</code> in the rules, it will run and follow links at intended, of course without js rendering. </p>\n<p>I have read the two related questions, <a href=\"https://stackoverflow.com/questions/37978365/crawlspider-with-splash-getting-stuck-after-first-url\">CrawlSpider with Splash getting stuck after first URL</a> and <a href=\"https://stackoverflow.com/questions/35341268/crawlspider-with-splash\">CrawlSpider with Splash</a> and specifically changed <code>scrapy.Request()</code> to <code>SplashRequest()</code> in the <code>start_requests()</code> method, but that does not seem to work. What is wrong with my code?\nThanks,</p>\n", "abstract": "I have the following code that is partially working,  the code will run only for start_urls but will not follow the links specified in restricted_xpaths, if i comment out start_requests() method and the line process_request='start_requests', in the rules, it will run and follow links at intended, of course without js rendering.  I have read the two related questions, CrawlSpider with Splash getting stuck after first URL and CrawlSpider with Splash and specifically changed scrapy.Request() to SplashRequest() in the start_requests() method, but that does not seem to work. What is wrong with my code?\nThanks,"}, "answers": [{"id": 47757005, "score": 3, "vote": 0, "content": "<p>I've had a similar issue that seemed specific to integrating Splash with a Scrapy CrawlSpider. It would visit only the start url and then close. The only way I managed to get it to work was to not use the scrapy-splash plugin and instead use the 'process_links' method to preppend the Splash http api url to all of the links scrapy collects. Then I made other adjustments to compensate for the new issues that arise from this method. Here's what I did:</p>\n<p>You'need these two tools to put together the splash url and then take it apart if you intend to store it somewhere.</p>\n<pre><code class=\"python\">from urllib.parse import urlencode, parse_qs\n</code></pre>\n<p>With the splash url being preppended to every link, scrapy will filter them all out as 'off site domain requests', so we make make 'localhost' the allowed domain.</p>\n<pre><code class=\"python\">allowed_domains = ['localhost']\nstart_urls = ['https://www.example.com/']\n</code></pre>\n<p>However, this poses a problem because then we may end up endlessly crawling the web when we only want to crawl one site. Let's fix this with the LinkExtractor rules. By only scraping links from our desired domain, we get around the offsite request problem.</p>\n<pre><code class=\"python\">LinkExtractor(allow=r'(http(s)?://)?(.*\\.)?{}.*'.format(r'example.com')),\nprocess_links='process_links',\n</code></pre>\n<p>Here's the process_links method. The dictionary in the urlencode method is where you'll put all of your splash arguments.</p>\n<pre><code class=\"python\">def process_links(self, links):\n    for link in links:\n        if \"http://localhost:8050/render.html?&amp;\" not in link.url:\n            link.url = \"http://localhost:8050/render.html?&amp;\" + urlencode({'url':link.url,\n                                                                          'wait':2.0})\n    return links\n</code></pre>\n<p>Finally, to take the url back out of the splash url, use the parse_qs method.</p>\n<pre><code class=\"python\">parse_qs(response.url)['url'][0] \n</code></pre>\n<p>One final note about this approach. You'll notice that I have an '&amp;' in the splash url right at the beginning. (...render.html?<strong>&amp;</strong>). This makes parsing the splash url to take out the actual url consistent no matter what order you have the arguments when you're using the urlencode method.</p>\n", "abstract": "I've had a similar issue that seemed specific to integrating Splash with a Scrapy CrawlSpider. It would visit only the start url and then close. The only way I managed to get it to work was to not use the scrapy-splash plugin and instead use the 'process_links' method to preppend the Splash http api url to all of the links scrapy collects. Then I made other adjustments to compensate for the new issues that arise from this method. Here's what I did: You'need these two tools to put together the splash url and then take it apart if you intend to store it somewhere. With the splash url being preppended to every link, scrapy will filter them all out as 'off site domain requests', so we make make 'localhost' the allowed domain. However, this poses a problem because then we may end up endlessly crawling the web when we only want to crawl one site. Let's fix this with the LinkExtractor rules. By only scraping links from our desired domain, we get around the offsite request problem. Here's the process_links method. The dictionary in the urlencode method is where you'll put all of your splash arguments. Finally, to take the url back out of the splash url, use the parse_qs method. One final note about this approach. You'll notice that I have an '&' in the splash url right at the beginning. (...render.html?&). This makes parsing the splash url to take out the actual url consistent no matter what order you have the arguments when you're using the urlencode method."}, {"id": 49466031, "score": 2, "vote": 0, "content": "<p>Seems to be related to <a href=\"https://github.com/scrapy-plugins/scrapy-splash/issues/92\" rel=\"nofollow noreferrer\">https://github.com/scrapy-plugins/scrapy-splash/issues/92</a> </p>\n<p>Personnaly I use dont_process_response=True so response is HtmlResponse (which is required by the code in _request_to_follows).</p>\n<p>And I also redefine the _build_request method in my spyder, like so:</p>\n<pre><code class=\"python\">def _build_request(self, rule, link):\n    r = SplashRequest(url=link.url, callback=self._response_downloaded, args={'wait': 0.5}, dont_process_response=True)\n    r.meta.update(rule=rule, link_text=link.text)\n    return r \n</code></pre>\n<p>In the github issues, some users just redefine the _request_to_follow method in their class.</p>\n", "abstract": "Seems to be related to https://github.com/scrapy-plugins/scrapy-splash/issues/92  Personnaly I use dont_process_response=True so response is HtmlResponse (which is required by the code in _request_to_follows). And I also redefine the _build_request method in my spyder, like so: In the github issues, some users just redefine the _request_to_follow method in their class."}, {"id": 45897698, "score": 0, "vote": 0, "content": "<p>Use below code - Just copy and paste</p>\n<pre><code class=\"python\">restrict_xpaths=('//a[contains(text(), \"Next Page\")]')\n</code></pre>\n<p>Instead of</p>\n<pre><code class=\"python\">restrict_xpaths=(\"//a[contains(text(), 'Next Page')]\")\n</code></pre>\n", "abstract": "Use below code - Just copy and paste Instead of"}]}, {"link": "https://stackoverflow.com/questions/36270867/crawling-google-scholar", "question": {"id": "36270867", "title": "Crawling Google Scholar", "content": "<p>I am trying to get information on a large number of scholarly articles as part of my research study. The number of articles is on the order of thousands. Since Google Scholar does not have an API, I am trying to scrape/crawl scholar. Now I now, that this is technically against the EULA, but I am trying to be very polite and reasonable about this. I understand that Google doesn't allow bots in order to keep traffic within reasonable limits. I started with a test batch of ~500 hundred requests with 1s in between each request. I got blocked after about the first 100 requests. I tried multiple other strategies including:</p>\n<ol>\n<li>Extending the pauses to ~20s and adding some random noise to them</li>\n<li>Making the pauses log-normally distributed (so that most pauses are on the order of seconds but every now and then there are longer pauses of several minutes and more)</li>\n<li>Doing long pauses (several hours) between blocks of requests (~100). </li>\n</ol>\n<p>I doubt that at this point my script is adding any considerable traffic over what any human would. But one way or the other I always get blocked after ~100-200 requests. Does anyone know of a good strategy to overcome this (I don't care if it takes weeks, as long as it is automated). Also, does anyone have experience dealign with Google directly and asking for permission to do something similar (for research etc.)? Is it worth trying to write them and explain what I'm trying to do and how, and see whether I can get permission for my project? And how would I go about contacting them? Thanks! </p>\n", "abstract": "I am trying to get information on a large number of scholarly articles as part of my research study. The number of articles is on the order of thousands. Since Google Scholar does not have an API, I am trying to scrape/crawl scholar. Now I now, that this is technically against the EULA, but I am trying to be very polite and reasonable about this. I understand that Google doesn't allow bots in order to keep traffic within reasonable limits. I started with a test batch of ~500 hundred requests with 1s in between each request. I got blocked after about the first 100 requests. I tried multiple other strategies including: I doubt that at this point my script is adding any considerable traffic over what any human would. But one way or the other I always get blocked after ~100-200 requests. Does anyone know of a good strategy to overcome this (I don't care if it takes weeks, as long as it is automated). Also, does anyone have experience dealign with Google directly and asking for permission to do something similar (for research etc.)? Is it worth trying to write them and explain what I'm trying to do and how, and see whether I can get permission for my project? And how would I go about contacting them? Thanks! "}, "answers": [{"id": 37187565, "score": 2, "vote": 0, "content": "<p>Without testing, I'm still pretty sure one of the following does the trick :</p>\n<ol>\n<li><p><strong>Easy, but small chance of success :</strong></p>\n<p>Delete all cookies from site in question after every rand(0,100) request,<br/>\nthen change your user-agent, accepted language, etc. and repeat.</p></li>\n<li><p><strong>A bit more work, but a much sturdier spider as result :</strong> </p>\n<p>Send your requests through Tor, other proxies, mobile networks, etc. to mask your IP (<strong>also do suggestion 1 at every turn</strong>)</p></li>\n</ol>\n<p><strong>Update regarding Selenium</strong>\nI missed the fact that you're using Selenium, took for granted it was some kind of modern programming language only (I know that Selenium can be driven by most widely used languages, but <em>also</em> as some sort of browser plug-in, demanding very little programming skills). </p>\n<p>As I then presume your coding skills aren't (or weren't?) mind-boggling, and for others with the same limitations when using Selenium, my answer is to either learn a simple, <em>scripting</em> language (PowerShell?!) or JavaScript (since it's the web you're on ;-)) and take it from there.</p>\n<p>If automating scraping smoothly was as simple as a browser plug-in, the web would have to be a much more messy, obfuscated and credential demanding place.</p>\n", "abstract": "Without testing, I'm still pretty sure one of the following does the trick : Easy, but small chance of success : Delete all cookies from site in question after every rand(0,100) request,\nthen change your user-agent, accepted language, etc. and repeat. A bit more work, but a much sturdier spider as result :  Send your requests through Tor, other proxies, mobile networks, etc. to mask your IP (also do suggestion 1 at every turn) Update regarding Selenium\nI missed the fact that you're using Selenium, took for granted it was some kind of modern programming language only (I know that Selenium can be driven by most widely used languages, but also as some sort of browser plug-in, demanding very little programming skills).  As I then presume your coding skills aren't (or weren't?) mind-boggling, and for others with the same limitations when using Selenium, my answer is to either learn a simple, scripting language (PowerShell?!) or JavaScript (since it's the web you're on ;-)) and take it from there. If automating scraping smoothly was as simple as a browser plug-in, the web would have to be a much more messy, obfuscated and credential demanding place."}, {"id": 71175070, "score": 0, "vote": 0, "content": "<p>The ideal solution is when you have reliable proxies and a CAPTCHA solving service because on Google Scholar your IP can be blocked or set an IP rate limit or it will throw a CAPTCHA, so having both of these services will lead to what you want.</p>\n<p>Alternatively, if you don't want to deal with spending time on finding those reliable services, you can do it using <a href=\"https://serpapi.com/google-scholar-api\" rel=\"nofollow noreferrer\">Google Scholar API</a> from SerpApi.</p>\n<p>It's a paid API with a free plan that does all of the possible problems you might have on their backend so users don't have to think about it or maintain it and build it from scratch.</p>\n<p>Example code to integrate to scrape Google Scholar organic results:</p>\n<pre class=\"lang-py prettyprint-override\"><code class=\"python\">import os\nfrom serpapi import GoogleSearch\nfrom urllib.parse import urlsplit, parse_qsl\n\n\ndef organic_results():\n    print(\"extracting organic results..\")\n\n    params = {\n        \"api_key\": os.getenv(\"API_KEY\"),\n        \"engine\": \"google_scholar\",\n        \"q\": \"minecraft redstone system structure characteristics strength\",  # search query\n        \"hl\": \"en\",        # language\n        \"as_ylo\": \"2017\",  # from 2017\n        \"as_yhi\": \"2021\",  # to 2021\n        \"start\": \"0\"       # first page\n    }\n\n    search = GoogleSearch(params)\n\n    organic_results_data = []\n\n    organic_results_is_present = True\n    while organic_results_is_present:\n        results = search.get_dict()\n\n        print(f\"Currently extracting page #{results['serpapi_pagination']['current']}..\")\n\n        for result in results[\"organic_results\"]:\n            position = result[\"position\"]\n            title = result[\"title\"]\n            publication_info_summary = result[\"publication_info\"][\"summary\"]\n            result_id = result[\"result_id\"]\n            link = result.get(\"link\")\n            result_type = result.get(\"type\")\n            snippet = result.get(\"snippet\")\n  \n            try:\n              file_title = result[\"resources\"][0][\"title\"]\n            except: file_title = None\n  \n            try:\n              file_link = result[\"resources\"][0][\"link\"]\n            except: file_link = None\n  \n            try:\n              file_format = result[\"resources\"][0][\"file_format\"]\n            except: file_format = None\n  \n            try:\n              cited_by_count = int(result[\"inline_links\"][\"cited_by\"][\"total\"])\n            except: cited_by_count = None\n  \n            cited_by_id = result.get(\"inline_links\", {}).get(\"cited_by\", {}).get(\"cites_id\", {})\n            cited_by_link = result.get(\"inline_links\", {}).get(\"cited_by\", {}).get(\"link\", {})\n  \n            try:\n              total_versions = int(result[\"inline_links\"][\"versions\"][\"total\"])\n            except: total_versions = None\n  \n            all_versions_link = result.get(\"inline_links\", {}).get(\"versions\", {}).get(\"link\", {})\n            all_versions_id = result.get(\"inline_links\", {}).get(\"versions\", {}).get(\"cluster_id\", {})\n  \n            organic_results_data.append({\n              \"page_number\": results[\"serpapi_pagination\"][\"current\"],\n              \"position\": position + 1,\n              \"result_type\": result_type,\n              \"title\": title,\n              \"link\": link,\n              \"result_id\": result_id,\n              \"publication_info_summary\": publication_info_summary,\n              \"snippet\": snippet,\n              \"cited_by_count\": cited_by_count,\n              \"cited_by_link\": cited_by_link,\n              \"cited_by_id\": cited_by_id,\n              \"total_versions\": total_versions,\n              \"all_versions_link\": all_versions_link,\n              \"all_versions_id\": all_versions_id,\n              \"file_format\": file_format,\n              \"file_title\": file_title,\n              \"file_link\": file_link,\n            })\n            \n            # if next page is present -&gt; split URL in parts and pass it to GoogleSearch() as a dictionary\n            # if no next page -&gt; exit the while loop\n            if \"next\" in results[\"serpapi_pagination\"]:\n                search.params_dict.update(dict(parse_qsl(urlsplit(results[\"serpapi_pagination\"][\"next\"]).query)))\n            else:\n                organic_results_data = False\n\n    return organic_results_data\n\n\n# part of the output:\n'''\n]\n  {\n    \"page_number\": 3,\n    \"position\": 7,\n    \"result_type\": \"Pdf\",\n    \"title\": \"A methodology proposal for MMORPG content expansion analysis\",\n    \"link\": \"https://www.sbgames.org/sbgames2017/papers/ArtesDesignFull/175051.pdf\",\n    \"result_id\": \"wKa3TpX-gn8J\",\n    \"publication_info_summary\": \"AMM Santos, A Franco, JGR Maia, F Gomes\u2026 - \u2026 Simp\u00f3sio Brasileiro de \u2026, 2017 - sbgames.org\",\n    \"snippet\": \"\u2026 Given the problem statement, this work aims to analyze MMORPGs to identify strengths and \u2026 Minecraft players built computers 5, calculators and even printers 6 on top of the \u201credstone\u201d \u2026\",\n    \"cited_by_count\": 4,\n    \"cited_by_link\": \"https://scholar.google.com/scholar?cites=9188186107013473984&amp;as_sdt=8000005&amp;sciodt=0,19&amp;hl=en\",\n    \"cited_by_id\": \"9188186107013473984\",\n    \"total_versions\": 2,\n    \"all_versions_link\": \"https://scholar.google.com/scholar?cluster=9188186107013473984&amp;hl=en&amp;as_sdt=0,19&amp;as_ylo=2017&amp;as_yhi=2021\",\n    \"all_versions_id\": \"9188186107013473984\",\n    \"file_format\": \"PDF\",\n    \"file_title\": \"sbgames.org\",\n    \"file_link\": \"https://www.sbgames.org/sbgames2017/papers/ArtesDesignFull/175051.pdf\"\n  }, ... other results\n]\n'''\n</code></pre>\n<p>Additionally, if you want to scrape profile and author results, there's a dedicated <a href=\"https://serpapi.com/blog/p/7e63c737-562e-4f95-a63f-122ea0a48d79/\" rel=\"nofollow noreferrer\">scrape all Google Scholar Profile, Author Results to CSV with Python and SerpApi</a> blog post of mine.</p>\n<blockquote>\n<p>Disclaimer, I work for SerpApi.</p>\n</blockquote>\n", "abstract": "The ideal solution is when you have reliable proxies and a CAPTCHA solving service because on Google Scholar your IP can be blocked or set an IP rate limit or it will throw a CAPTCHA, so having both of these services will lead to what you want. Alternatively, if you don't want to deal with spending time on finding those reliable services, you can do it using Google Scholar API from SerpApi. It's a paid API with a free plan that does all of the possible problems you might have on their backend so users don't have to think about it or maintain it and build it from scratch. Example code to integrate to scrape Google Scholar organic results: Additionally, if you want to scrape profile and author results, there's a dedicated scrape all Google Scholar Profile, Author Results to CSV with Python and SerpApi blog post of mine. Disclaimer, I work for SerpApi."}]}, {"link": "https://stackoverflow.com/questions/24884011/how-to-prevent-scrapy-from-url-encoding-request-urls", "question": {"id": "24884011", "title": "How to prevent Scrapy from URL encoding request URLs", "content": "<p>I would like Scrapy to not URL encode my Requests. I see that scrapy.http.Request is importing scrapy.utils.url which imports w3lib.url which contains the variable _ALWAYS_SAFE_BYTES. I just need to add a set of characters to _ALWAYS_SAFE_BYTES but I am not sure how to do that from within my spider class.</p>\n<p>scrapy.http.Request relevant line:</p>\n<pre><code class=\"python\">fp.update(canonicalize_url(request.url))\n</code></pre>\n<p>canonicalize_url is from scrapy.utils.url, relevant line in scrapy.utils.url:</p>\n<pre><code class=\"python\">path = safe_url_string(_unquotepath(path)) or '/'\n</code></pre>\n<p>safe_url_string() is from w3lib.url, relevant lines in w3lib.url:</p>\n<pre><code class=\"python\">_ALWAYS_SAFE_BYTES = (b'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_.-')\n</code></pre>\n<p>within w3lib.url.safe_url_string():</p>\n<pre><code class=\"python\">_safe_chars = _ALWAYS_SAFE_BYTES + b'%' + _reserved + _unreserved_marks\nreturn moves.urllib.parse.quote(s, _safe_chars)\n</code></pre>\n", "abstract": "I would like Scrapy to not URL encode my Requests. I see that scrapy.http.Request is importing scrapy.utils.url which imports w3lib.url which contains the variable _ALWAYS_SAFE_BYTES. I just need to add a set of characters to _ALWAYS_SAFE_BYTES but I am not sure how to do that from within my spider class. scrapy.http.Request relevant line: canonicalize_url is from scrapy.utils.url, relevant line in scrapy.utils.url: safe_url_string() is from w3lib.url, relevant lines in w3lib.url: within w3lib.url.safe_url_string():"}, "answers": [{"id": 42494437, "score": 2, "vote": 0, "content": "<p>I wanted to not to encode <code>[</code> and <code>]</code> and I did this.</p>\n<p>When creating a <code>Request</code> object scrapy applies some url encoding methods. To revert these you can utilize a custom middleware and change the url to your needs.</p>\n<p>You could use a <code>Downloader Middleware</code> like this:</p>\n<pre><code class=\"python\">class MyCustomDownloaderMiddleware(object):\n\n    def process_request(self, request, spider):\n        request._url = request.url.replace(\"%5B\", \"[\", 2)\n        request._url = request.url.replace(\"%5D\", \"]\", 2)\n</code></pre>\n<p>Don't forget to \"activate\" the middleware in <code>settings.py</code> like so:</p>\n<pre><code class=\"python\">DOWNLOADER_MIDDLEWARES = {\n    'so.middlewares.MyCustomDownloaderMiddleware': 900,\n}\n</code></pre>\n<p>My project is named <code>so</code> and in the folder there is a file <code>middlewares.py</code>. You need to adjust those to your environment.</p>\n<p>Credit goes to: <a href=\"https://stackoverflow.com/a/42492045/4094231\">Frank Martin</a></p>\n", "abstract": "I wanted to not to encode [ and ] and I did this. When creating a Request object scrapy applies some url encoding methods. To revert these you can utilize a custom middleware and change the url to your needs. You could use a Downloader Middleware like this: Don't forget to \"activate\" the middleware in settings.py like so: My project is named so and in the folder there is a file middlewares.py. You need to adjust those to your environment. Credit goes to: Frank Martin"}]}, {"link": "https://stackoverflow.com/questions/46067258/scrapy-save-response-body-as-html-file", "question": {"id": "46067258", "title": "Scrapy: Save response.body as html file?", "content": "<p>My spider works, but I can't download the body of the website I crawl in a .html file. If I write self.html_fil.write('test') then it works fine. I don't know how to convert the tulpe to string.</p>\n<p>I use Python 3.6</p>\n<p><strong>Spider:</strong></p>\n<pre><code class=\"python\">class ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = ['google.com']\n    start_urls = ['http://google.com/']\n\n    def __init__(self):\n        self.path_to_html = html_path + 'index.html'\n        self.path_to_header = header_path + 'index.html'\n        self.html_file = open(self.path_to_html, 'w')\n\n    def parse(self, response):\n        url = response.url\n        self.html_file.write(response.body)\n        self.html_file.close()\n        yield {\n            'url': url\n        }\n</code></pre>\n<p><strong>Tracktrace:</strong></p>\n<pre><code class=\"python\">Traceback (most recent call last):\n  File \"c:\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line\n 653, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"c:\\Users\\kv\\AtomProjects\\example_project\\example_bot\\example_bot\\spiders\n\\example.py\", line 35, in parse\n    self.html_file.write(response.body)\nTypeError: write() argument must be str, not bytes\n</code></pre>\n", "abstract": "My spider works, but I can't download the body of the website I crawl in a .html file. If I write self.html_fil.write('test') then it works fine. I don't know how to convert the tulpe to string. I use Python 3.6 Spider: Tracktrace:"}, "answers": [{"id": 46067426, "score": 15, "vote": 0, "content": "<p>Actual problem is you are getting byte code. You need to convert it to string format. there are many ways for converting byte to string format.\n You can use </p>\n<pre><code class=\"python\"> self.html_file.write(response.body.decode(\"utf-8\"))\n</code></pre>\n<p>instead of </p>\n<pre><code class=\"python\">  self.html_file.write(response.body)\n</code></pre>\n<p>also you can use </p>\n<pre><code class=\"python\">  self.html_file.write(response.text)\n</code></pre>\n", "abstract": "Actual problem is you are getting byte code. You need to convert it to string format. there are many ways for converting byte to string format.\n You can use  instead of  also you can use "}, {"id": 46921140, "score": 6, "vote": 0, "content": "<p>The correct way is to use <code>response.text</code>, and not <code>response.body.decode(\"utf-8\")</code>. To quote <a href=\"https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse.text\" rel=\"noreferrer\">documentation</a>:</p>\n<blockquote>\n<p>Keep in mind that <code>Response.body</code> is always a bytes object. If you want the unicode version use <code>TextResponse.text</code> (only available in <code>TextResponse</code> and subclasses).</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>text: Response body, as unicode.</p>\n<p>The same as <code>response.body.decode(response.encoding)</code>, but the result is cached after the first call, so you can access <code>response.text</code> multiple times without extra overhead.</p>\n<p>Note: <code>unicode(response.body)</code> is not a correct way to convert response body to unicode: you would be using the system default encoding (typically ascii) instead of the response encoding.</p>\n</blockquote>\n", "abstract": "The correct way is to use response.text, and not response.body.decode(\"utf-8\"). To quote documentation: Keep in mind that Response.body is always a bytes object. If you want the unicode version use TextResponse.text (only available in TextResponse and subclasses). and text: Response body, as unicode. The same as response.body.decode(response.encoding), but the result is cached after the first call, so you can access response.text multiple times without extra overhead. Note: unicode(response.body) is not a correct way to convert response body to unicode: you would be using the system default encoding (typically ascii) instead of the response encoding."}, {"id": 51181232, "score": 5, "vote": 0, "content": "<p>Taking in consideration responses above, and making it as much <em>pythonic</em> as possible adding the use of the <code>with</code> statement, the example should be rewritten like:</p>\n<pre><code class=\"python\">class ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = ['google.com']\n    start_urls = ['http://google.com/']\n\n    def __init__(self):\n        self.path_to_html = html_path + 'index.html'\n        self.path_to_header = header_path + 'index.html'\n\n    def parse(self, response):\n        with open(self.path_to_html, 'w') as html_file:\n            html_file.write(response.text)\n        yield {\n            'url': response.url\n        }\n</code></pre>\n<p>But the <code>html_file</code> will only accessible from the <code>parse</code> method.</p>\n", "abstract": "Taking in consideration responses above, and making it as much pythonic as possible adding the use of the with statement, the example should be rewritten like: But the html_file will only accessible from the parse method."}]}, {"link": "https://stackoverflow.com/questions/45651879/using-selenium-how-to-keep-logged-in-after-closing-driver-in-python", "question": {"id": "45651879", "title": "Using selenium: How to keep logged in after closing Driver in Python", "content": "<p>I want to get my Whatsapp web (web.whatsapp.com) logged in, at the second time opening the Whatsapp web on chrome driver. Following is my code based on Python need your help.</p>\n<pre><code class=\"python\">from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\n\nchrome_path = r\"chromedriver.exe\"\noptions = Options();\noptions.add_argument(\"user-data-\ndir=C:/Users/Username/AppData/Local/Google/Chrome/User Data\");\n#options.add_argument(\"--start-maximized\");\ndriver = webdriver.Chrome(chrome_path,chrome_options=options);\n\n#driver = webdriver.Chrome();\ndriver.get('https://web.whatsapp.com/')\n</code></pre>\n", "abstract": "I want to get my Whatsapp web (web.whatsapp.com) logged in, at the second time opening the Whatsapp web on chrome driver. Following is my code based on Python need your help."}, "answers": [{"id": 45654226, "score": 18, "vote": 0, "content": "<p>I tried on my Mac, below code and it worked perfectly fine, I don't need to login again</p>\n<pre><code class=\"python\">from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\n\noptions = Options()\noptions.add_argument(\"user-data-dir=/tmp/tarun\")\ndriver = webdriver.Chrome(chrome_options=options)\n\ndriver.get('https://web.whatsapp.com/')\ndriver.quit()\n</code></pre>\n<p>For window you can try changing the path as below</p>\n<pre><code class=\"python\">options.add_argument(\"user-data-dir=C:\\\\Users\\\\Username\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n</code></pre>\n", "abstract": "I tried on my Mac, below code and it worked perfectly fine, I don't need to login again For window you can try changing the path as below"}, {"id": 50400200, "score": 1, "vote": 0, "content": "<pre><code class=\"python\">from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\n\noptions = Options()\noptions.add_argument(\"user-data-dir=C:\\\\Users\\\\Username\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\ndriver = webdriver.Chrome(chrome_options=options)\n\ndriver.get('https://web.whatsapp.com/')\ndriver.quit()\n</code></pre>\n<p>Here it is for Windows. Works perfect on Python 3.6</p>\n", "abstract": "Here it is for Windows. Works perfect on Python 3.6"}]}, {"link": "https://stackoverflow.com/questions/7653276/fast-internet-crawler", "question": {"id": "7653276", "title": "Fast internet crawler", "content": "<p>I'd like to do perform data mining on a large scale. For this, I need a fast crawler. All I need is something to download a web page, extract links and follow them recursively, but without visiting the same url twice. Basically, I want to avoid looping.</p>\n<p>I already wrote a crawler in python, but it's too slow. I'm not able to saturate a 100Mbit line with it. Top speed is ~40 urls/sec. and for some reason it's hard to get better results. It seems like a problem with python's multithreading/sockets. I also ran into problems with python's gargabe collector, but that was solvable. CPU isn't the bottleneck btw.</p>\n<p>So, what should I use to write a crawler that is as fast as possible, and what's the best solution to avoid looping while crawling?</p>\n<p>EDIT:\nThe solution was to combine <code>multiprocessing</code> and <code>threading</code> modules. Spawn multiple processes with multiple threads per process for best effect. Spawning multiple threads in a single process is not effective and multiple processes with just one thread consume too much memory.</p>\n", "abstract": "I'd like to do perform data mining on a large scale. For this, I need a fast crawler. All I need is something to download a web page, extract links and follow them recursively, but without visiting the same url twice. Basically, I want to avoid looping. I already wrote a crawler in python, but it's too slow. I'm not able to saturate a 100Mbit line with it. Top speed is ~40 urls/sec. and for some reason it's hard to get better results. It seems like a problem with python's multithreading/sockets. I also ran into problems with python's gargabe collector, but that was solvable. CPU isn't the bottleneck btw. So, what should I use to write a crawler that is as fast as possible, and what's the best solution to avoid looping while crawling? EDIT:\nThe solution was to combine multiprocessing and threading modules. Spawn multiple processes with multiple threads per process for best effect. Spawning multiple threads in a single process is not effective and multiple processes with just one thread consume too much memory."}, "answers": [{"id": 7653399, "score": 8, "vote": 0, "content": "<p>Why not use something already tested for crawling, like <a href=\"http://scrapy.org/\" rel=\"nofollow noreferrer\">Scrapy</a>? I managed to reach almost 100 pages per second on a low-end VPS that has limited RAM memory (about 400Mb), while network speed was around 6-7 Mb/s (i.e. below 100Mbps).</p>\n<p>Another improvement you can do is use <code>urllib3</code> (especially when crawling many pages from a single domain). Here's a brief comparison I did some time ago:</p>\n<p><a href=\"http://attilaolah.eu/2010/08/08/urllib-benchmark\" rel=\"nofollow noreferrer\"><img alt=\"urllib benchmark\" src=\"https://i.stack.imgur.com/k9qa4.png\"/></a></p>\n<h3>UPDATE:</h3>\n<p>Scrapy now <a href=\"http://doc.scrapy.org/en/latest/topics/request-response.html\" rel=\"nofollow noreferrer\">uses the Requests library</a>, which in turn <a href=\"http://kennethreitz.org/major-progress-for-requests/\" rel=\"nofollow noreferrer\">uses urllib3</a>. That makes Scrapy the absolute go-to tool when it comes to scraping. Recent versions also support deploying projects, so scraping from a VPS is easier than ever.</p>\n", "abstract": "Why not use something already tested for crawling, like Scrapy? I managed to reach almost 100 pages per second on a low-end VPS that has limited RAM memory (about 400Mb), while network speed was around 6-7 Mb/s (i.e. below 100Mbps). Another improvement you can do is use urllib3 (especially when crawling many pages from a single domain). Here's a brief comparison I did some time ago:  Scrapy now uses the Requests library, which in turn uses urllib3. That makes Scrapy the absolute go-to tool when it comes to scraping. Recent versions also support deploying projects, so scraping from a VPS is easier than ever."}, {"id": 11539776, "score": 3, "vote": 0, "content": "<p>Around 2 years ago i have developed a crawler. And it can download almost 250urls per second. You could flow my steps.</p>\n<ol>\n<li>Optimize your file pointer use. Try to use minimal file pointer.</li>\n<li>Don't write your data every time. Try to dump your data after\nstoring around 5000 url or 10000 url.</li>\n<li>For your robustness you don't need to use different configuration.\nTry to Use a log file and when you want to resume then just try to\nread the log file and resume your crawler.</li>\n<li><p>Distributed all your webcrawler task. And process it in a interval\nwise.</p>\n<p>a. downloader</p>\n<p>b. link extractor</p>\n<p>c. URLSeen</p>\n<p>d. ContentSeen</p></li>\n</ol>\n", "abstract": "Around 2 years ago i have developed a crawler. And it can download almost 250urls per second. You could flow my steps. Distributed all your webcrawler task. And process it in a interval\nwise. a. downloader b. link extractor c. URLSeen d. ContentSeen"}, {"id": 13074456, "score": 2, "vote": 0, "content": "<p>I have written a simple multithreading crawler. It is available on GitHub as <a href=\"https://github.com/srw/discovering-web-resources\" rel=\"nofollow\">Discovering Web Resources</a> and I've written a related article: <a href=\"http://blog.databigbang.com/automated-discovery-of-blog-feeds-and-twitter-facebook-linkedin-accounts-connected-to-business-website/\" rel=\"nofollow\">Automated Discovery of Blog Feeds and Twitter, Facebook, LinkedIn Accounts Connected to Business Website</a>. You can change the number of threads being used in the NWORKERS class variable. Don't hesitate to ask any further question if you need extra help.</p>\n", "abstract": "I have written a simple multithreading crawler. It is available on GitHub as Discovering Web Resources and I've written a related article: Automated Discovery of Blog Feeds and Twitter, Facebook, LinkedIn Accounts Connected to Business Website. You can change the number of threads being used in the NWORKERS class variable. Don't hesitate to ask any further question if you need extra help."}, {"id": 7653340, "score": 1, "vote": 0, "content": "<p>It sounds like you have a design problem more than a language problem. Try looking into the <a href=\"http://docs.python.org/library/multiprocessing.html\" rel=\"nofollow\">multiprocessing</a> module for accessing more sites at the same time rather than threads. Also, consider getting some table to store your previously visited sites (a database maybe?).</p>\n", "abstract": "It sounds like you have a design problem more than a language problem. Try looking into the multiprocessing module for accessing more sites at the same time rather than threads. Also, consider getting some table to store your previously visited sites (a database maybe?)."}, {"id": 7653367, "score": 1, "vote": 0, "content": "<p>Impossible to tell what your limitations are. Your problem is similiar to the <a href=\"http://www.kegel.com/c10k.html\" rel=\"nofollow\">C10K problem</a> -- read first, don't optimize straight away. Go for the low-hanging fruit: Most probably you get significant performance improvements by analyzing your application design. Don't start out massively-mulithreaded or massively-multiprocessed.</p>\n<p>I'd use <a href=\"http://twistedmatrix.com\" rel=\"nofollow\">Twisted</a> to write the the networking part, this can be very fast. In general, I/O on the machine has to be better than average. Either you have to write your data \nto disk or to another machine, not every notebook supports 10MByte/s sustained database writes. Lastly, if you have an asynchronous internet connection, It might simply be that your upstream is saturated. <a href=\"http://www.benzedrine.cx/ackpri.html\" rel=\"nofollow\">ACK priorization</a> helps here (OpenBSD example). </p>\n", "abstract": "Impossible to tell what your limitations are. Your problem is similiar to the C10K problem -- read first, don't optimize straight away. Go for the low-hanging fruit: Most probably you get significant performance improvements by analyzing your application design. Don't start out massively-mulithreaded or massively-multiprocessed. I'd use Twisted to write the the networking part, this can be very fast. In general, I/O on the machine has to be better than average. Either you have to write your data \nto disk or to another machine, not every notebook supports 10MByte/s sustained database writes. Lastly, if you have an asynchronous internet connection, It might simply be that your upstream is saturated. ACK priorization helps here (OpenBSD example). "}]}, {"link": "https://stackoverflow.com/questions/43164411/why-do-we-still-need-parser-like-beautifulsoup-if-we-can-use-selenium", "question": {"id": "43164411", "title": "Why do we still need parser like BeautifulSoup if we can use Selenium?", "content": "<p>I am currently using Selenium to crawl data from some websites. Unlike urllib, it seems that I do not really need a parser like BeautifulSoup to parse the HTML. I can simply find an element with Selenium and use Webelement.text to get the data that I need. As I saw there are some people using Selenium and BeautifulSoup together in web crawling. Is it really necessary? Any special features that bs4 can offer to improve the crawling process? Thank you.</p>\n", "abstract": "I am currently using Selenium to crawl data from some websites. Unlike urllib, it seems that I do not really need a parser like BeautifulSoup to parse the HTML. I can simply find an element with Selenium and use Webelement.text to get the data that I need. As I saw there are some people using Selenium and BeautifulSoup together in web crawling. Is it really necessary? Any special features that bs4 can offer to improve the crawling process? Thank you."}, "answers": [{"id": 43164454, "score": 12, "vote": 0, "content": "<p>Selenium itself is quite powerful in terms of locating elements and, it basically has everything you need for extracting data from HTML. The problem is, <strong>it is slow</strong>. Every single selenium command goes through the <a href=\"https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol\" rel=\"noreferrer\">JSON wire HTTP protocol</a> and there is a substantial overhead. </p>\n<p>In order to improve the performance of the HTML parsing part, it is usually much faster to let <code>BeautifulSoup</code> or <code>lxml</code> parse the page source retrieved from <a href=\"http://selenium-python.readthedocs.io/api.html#selenium.webdriver.remote.webdriver.WebDriver.page_source\" rel=\"noreferrer\"><code>.page_source</code></a>. </p>\n<hr/>\n<p>In other words, a common workflow for a dynamic web page is something like:</p>\n<ul>\n<li>open the page in a browser controlled by selenium</li>\n<li>make the necessary browser actions</li>\n<li>once the desired data is on the page, get the <code>driver.page_source</code> and close the browser</li>\n<li>pass the page source to an HTML parser for further parsing</li>\n</ul>\n", "abstract": "Selenium itself is quite powerful in terms of locating elements and, it basically has everything you need for extracting data from HTML. The problem is, it is slow. Every single selenium command goes through the JSON wire HTTP protocol and there is a substantial overhead.  In order to improve the performance of the HTML parsing part, it is usually much faster to let BeautifulSoup or lxml parse the page source retrieved from .page_source.  In other words, a common workflow for a dynamic web page is something like:"}]}, {"link": "https://stackoverflow.com/questions/10943745/running-multiple-spiders-in-scrapy", "question": {"id": "10943745", "title": "Running Multiple spiders in scrapy", "content": "<ol>\n<li><p>In scrapy for example if i had two URL's that contains different HTML. Now i want to write two individual spiders each for one and want to run both the spiders at once. In scrapy is it possible to run multiple spiders at once.</p></li>\n<li><p>In scrapy after writing multiple spiders, how can we schedule them to run for every 6 hours(May be like cron jobs)</p></li>\n</ol>\n<p>I had no idea of above , can u suggest me how to perform the above things with an example.</p>\n<p>Thanks in advance.     </p>\n", "abstract": "In scrapy for example if i had two URL's that contains different HTML. Now i want to write two individual spiders each for one and want to run both the spiders at once. In scrapy is it possible to run multiple spiders at once. In scrapy after writing multiple spiders, how can we schedule them to run for every 6 hours(May be like cron jobs) I had no idea of above , can u suggest me how to perform the above things with an example. Thanks in advance.     "}, "answers": [{"id": 10944103, "score": 4, "vote": 0, "content": "<p>It would probably be easiest to just run two scrapy scripts at once from the OS level. They should both be able to save to the same database. Create a shell script to call both scrapy scripts to do them at the same time:</p>\n<pre><code class=\"python\">scrapy runspider foo &amp;\nscrapy runspider bar\n</code></pre>\n<p>Be sure to make this script executable with <code>chmod +x script_name</code></p>\n<p>To schedule a cronjob every 6 hours, type <code>crontab -e</code> into your terminal, and edit the file as follows:</p>\n<pre><code class=\"python\">* */6 * * * path/to/shell/script_name &gt;&gt; path/to/file.log\n</code></pre>\n<p>The first * is minutes, then hours, etc., and an asterik is a wildcard. So this says run the script at any time where the hours is divisible by 6, or every six hours.</p>\n", "abstract": "It would probably be easiest to just run two scrapy scripts at once from the OS level. They should both be able to save to the same database. Create a shell script to call both scrapy scripts to do them at the same time: Be sure to make this script executable with chmod +x script_name To schedule a cronjob every 6 hours, type crontab -e into your terminal, and edit the file as follows: The first * is minutes, then hours, etc., and an asterik is a wildcard. So this says run the script at any time where the hours is divisible by 6, or every six hours."}, {"id": 42942075, "score": 4, "vote": 0, "content": "<p>You can try using <code>CrawlerProcess</code></p>\n<pre><code class=\"python\">from scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nfrom myproject.spiders import spider1, spider2\n\n1Spider = spider1.1Spider()\n2Spider = spider2.2Spider()\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl(1Spider)\nprocess.crawl(2Spider)\nprocess.start()\n</code></pre>\n<p>If you want to see the full log of the crawl, set <code>LOG_FILE</code> in your <code>settings.py</code>.</p>\n<pre><code class=\"python\">LOG_FILE = \"logs/mylog.log\"\n</code></pre>\n", "abstract": "You can try using CrawlerProcess If you want to see the full log of the crawl, set LOG_FILE in your settings.py."}, {"id": 13333204, "score": 2, "vote": 0, "content": "<p>You should use scrapyd to handle multiple crawler\n<a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\" rel=\"nofollow\">http://doc.scrapy.org/en/latest/topics/scrapyd.html</a></p>\n", "abstract": "You should use scrapyd to handle multiple crawler\nhttp://doc.scrapy.org/en/latest/topics/scrapyd.html"}, {"id": 43927136, "score": 2, "vote": 0, "content": "<p>Here the code that allow you to run multiple spiders in scrapy. Save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3 and it works) :</p>\n<pre><code class=\"python\">from scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (\"Running spider %s\" % (spider_name))\n    process.crawl(spider_name,query=\"dvh\") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n</code></pre>\n<p>and then you can schedule this python program to run with cronjob.</p>\n", "abstract": "Here the code that allow you to run multiple spiders in scrapy. Save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3 and it works) : and then you can schedule this python program to run with cronjob."}]}, {"link": "https://stackoverflow.com/questions/31070660/scrapy-get-all-children-ignore-br", "question": {"id": "31070660", "title": "Scrapy get all children / ignore &lt;br&gt;?", "content": "<p>I have this piece of html and I want to extract all text in one element with scrapy.</p>\n<pre><code class=\"python\">&lt;td class=\"infoOffre\"&gt;Rue de Trazegnies 41&lt;br&gt;6031&amp;nbsp;\n      Monceau sur Sambre&lt;br&gt;BELGIQUE&lt;/td&gt;\n</code></pre>\n<p>When I use xpath('.//td//text()').extract() I get 3 elements instead</p>\n", "abstract": "I have this piece of html and I want to extract all text in one element with scrapy. When I use xpath('.//td//text()').extract() I get 3 elements instead"}, "answers": [{"id": 31071124, "score": 5, "vote": 0, "content": "<p>You probably want XPath <code>string()</code> function:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import scrapy\n&gt;&gt;&gt; selector = scrapy.Selector(text=\"\"\"&lt;td class=\"infoOffre\"&gt;Rue de Trazegnies 41&lt;br&gt;6031&amp;nbsp;\n...       Monceau sur Sambre&lt;br&gt;BELGIQUE&lt;/td&gt;\"\"\")\n&gt;&gt;&gt; selector.xpath('.//td//text()').extract()\n[u'Rue de Trazegnies 41', u'6031\\xa0\\n      Monceau sur Sambre', u'BELGIQUE']\n&gt;&gt;&gt; selector.xpath('string(.//td)').extract()\n[u'Rue de Trazegnies 416031\\xa0\\n      Monceau sur SambreBELGIQUE']\n&gt;&gt;&gt; \n</code></pre>\n<p>or you can <code>join()</code> each text node:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; \" \".join(selector.xpath('.//td//text()').extract())\nu'Rue de Trazegnies 41 6031\\xa0\\n      Monceau sur Sambre BELGIQUE'\n&gt;&gt;&gt; \n</code></pre>\n<hr/>\n<p>Edit after OP's comment:</p>\n<p><code>string()</code> indeed only works on the first node in a node set, so either you have a more selective XPath to have the correct node (using  <code>[3]</code> position predicates), or you look on elements, and apply <code>string(.)</code> on each.</p>\n<p>For your specific example, you can use something like this:</p>\n<pre><code class=\"python\">(scrapy10)paul@paul$ scrapy shell \"https://www.leforem.be/HotJob/servlet/JobOffs.View?id=20729863&amp;nextUrl=\"\n&gt;&gt;&gt; for t in response.xpath('.//table[@class=\"visualOffre\"]'):\n...     print t.xpath('string(.)').extract()\n... \n[u\"Nombre de postes demand\\xe9s\\xa0: 1Cat\\xe9gorie de m\\xe9tier\\xa0:Soudeur\\xa0\\n              (4413201)\\n              [PERSONNEL DE LA CONSTRUCTION MECANIQUE ET DU TRAVAIL DES METAUX]Secteur d'activit\\xe9\\xa0:Fabrication d'autres machines d'usage sp\\xe9cifiqueLieu(x) de travail\\xa0:CHARLEROI [ARRONDISSEMENT](CHARLEROI [ARRONDISSEMENT])\\n          Votre fonction\\xa0:Une soci\\xe9t\\xe9 du Grand Charleroi recherche un monteur soudeur. \\uf02d\\tLire et interpr\\xe9ter des plans, effectuer un tra\\xe7age,\\uf02d\\tD\\xe9couper des pi\\xe8ces m\\xe9talliques\\uf02d\\tUsiner les diff\\xe9rentes pi\\xe8ces entrantes dans les ouvrages.\\uf02d\\tMettre en forme des pi\\xe8ces m\\xe9talliques. Assembler les pi\\xe8ces et raccorder ( semi et arc). .\\uf02d\\tMettre en couleur\\uf02d\\tContr\\xf4ler la conformit\\xe9 par rapport aux sp\\xe9cifications techniques.\\uf02d\\tMonter les structures fabriqu\\xe9es (escaliers, portails, garde-fous, + constructions b\\xe2timents industriels\\uf02d\\t R\\xe9aliser des travaux d'ajustage, de montage et d'assemblage (forage, taraudage, filetage, soudage).\"]\n[u\"Exp\\xe9rience(s) professionnelle(s)\\xa0:M\\xe9tier\\xa0:Dur\\xe9e\\xa0:Secteur\\xa0:Description\\xa0:Soudeur\\n\\t     \\xa0\\n      \\n          \\t\\tSans importance\\n          \\t\\xa0\\n      Fabrication d'autres machines d'usage sp\\xe9cifique\\xa0\\n      \\uf02d\\tVous poss\\xe9dez de l'exp\\xe9rience comme soudeur (semi-) et comme monteurPermis de conduire\\xa0:Permis de conduireDescription[B] V\\xe9hicules &lt; 3,5 tonnes et 8 places maximumd\\xe9placements possiblesQualification(s)\\xa0:QualificationDescription\\xa0Brevet  VCA de base--Commentaire (qualifications)\\xa0:\\uf02dDescription libre\\xa0:Vous aimez et vous savez travailler en \\xe9quipe\\uf02d\\tVous travaillez en atelier\\uf02d\\tVous supportez la chaleur et les intemp\\xe9ries (pas de ch\\xf4mage intemp\\xe9ries)\\uf02d\\uf02d\\tEtre minutieux, pr\\xe9cis et soucieux du travail bien fait\"]\n[u'R\\xe9gime de travail\\xa0:Temps plein de jourHoraire\\xa0:du  lundi au jeudi de 7h \\xe0 15h45 et le vendredi de 7h \\xe0 14hCommentaire additionnel\\xa0:ou PFI en fonction du profilType\\xa0:A dur\\xe9e d\\xe9termin\\xe9eSalaire\\xa0:Selon la CP 111 - 11,890EUR brut/h']\n[u\"\\n      Modalit\\xe9s de candidature :Si vous correspondez aux crit\\xe8res, faites nous parvenir votre cv et votre lettre de motivation \\xe0 pascal.noel@mirec.net (ou par courrier ou par fax au 071/30.08.23)Nom de l'entreprise\\xa0:MIRECNom de la personne\\xa0:M.\\xa0Gauthier Mulatin (Responsable de projets)Adresse\\xa0:Rue de Trazegnies 416031\\xa0\\n      Monceau sur SambreBELGIQUEE-mail\\xa0:pascal.noel@mirec.netURL\\xa0:www.mirec.net\"]\n&gt;&gt;&gt; \n</code></pre>\n", "abstract": "You probably want XPath string() function: or you can join() each text node: Edit after OP's comment: string() indeed only works on the first node in a node set, so either you have a more selective XPath to have the correct node (using  [3] position predicates), or you look on elements, and apply string(.) on each. For your specific example, you can use something like this:"}, {"id": 44157878, "score": 5, "vote": 0, "content": "<p>You may want to remove <code>/text()</code> from your XPath, and then get rid of <code>&lt;td&gt;</code> and <code>&lt;br&gt;</code> tags later.</p>\n<pre><code class=\"python\">selector = scrapy.Selector(text='&lt;td class=\"infoOffre\"&gt;Rue de Trazegnies 41&lt;br&gt;6031&amp;nbsp;Monceau sur Sambre&lt;br&gt;BELGIQUE&lt;/td&gt;')\n&gt;&gt;&gt; selector.xpath('.//td//text()').extract()\n['Rue de Trazegnies 41', '6031\\xa0Monceau sur Sambre', 'BELGIQUE']\n&gt;&gt;&gt; selector.xpath('.//td').extract()\n['&lt;td class=\"infoOffre\"&gt;Rue de Trazegnies 41&lt;br&gt;6031\\xa0Monceau sur Sambre&lt;br&gt;BELGIQUE&lt;/td&gt;']\n</code></pre>\n<p>Second option is to replace <code>&lt;br&gt;</code> by <code>/n</code>. Below I used Python 3, therefore unicode is converted into bytes.</p>\n<pre><code class=\"python\">response = response.replace(body=response.body.replace(b'&lt;br&gt;', b'\\n'))\n</code></pre>\n", "abstract": "You may want to remove /text() from your XPath, and then get rid of <td> and <br> tags later. Second option is to replace <br> by /n. Below I used Python 3, therefore unicode is converted into bytes."}]}, {"link": "https://stackoverflow.com/questions/17363458/running-code-when-scrapy-spider-has-finished-crawling", "question": {"id": "17363458", "title": "Running code when Scrapy spider has finished crawling", "content": "<p>Is there a way to get Scrapy to execute code once the crawl has completely finished to deal with moving / cleaning the data? Am sure it is trivial but my Google-fu seems to have left me for this issue.</p>\n", "abstract": "Is there a way to get Scrapy to execute code once the crawl has completely finished to deal with moving / cleaning the data? Am sure it is trivial but my Google-fu seems to have left me for this issue."}, "answers": [{"id": 52693379, "score": 6, "vote": 0, "content": "<p>It all depends on how you're launching Scrapy.</p>\n<p>If running from a command line with <code>crawl</code> or <code>runspider</code>, just wait for the process to finish. Beware that 0 exit code <a href=\"https://github.com/scrapy/scrapy/issues/1231\" rel=\"noreferrer\">won't mean</a> you've crawled everything successfully.</p>\n<p>If <a href=\"https://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script\" rel=\"noreferrer\">using as a library</a>, you can append the code after <a href=\"https://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess.start\" rel=\"noreferrer\"><code>CrawlerProcess.start()</code></a> call.</p>\n<p>If you need to reliably track the status, first you have to do is to track <a href=\"https://doc.scrapy.org/en/latest/topics/signals.html#spider-closed\" rel=\"noreferrer\"><code>spider_closed</code></a> signal and check its <code>reason</code> parameter. There's an example at the start of <a href=\"https://doc.scrapy.org/en/latest/topics/signals.html\" rel=\"noreferrer\">the page</a>, it expects you to modify the code of the spider.</p>\n<p>To track all spiders you have added, when using as a library:</p>\n<pre><code class=\"python\">process = CrawlerProcess({})\nprocess.crawl(MySpider)\n\ndef spider_ended(spider, reason):\n    print('Spider ended:', spider.name, reason)\n\nfor crawler in process.crawlers:\n    crawler.signals.connect(spider_ended, signal=scrapy.signals.spider_closed)\n\nprocess.start()\n</code></pre>\n<p>Check the <code>reason</code>, if it is not <code>'finished'</code>, something has interrupted the crawler.<br/>\nThe function will be called for each spider, so it may require some complex error handling if you have many. Also take in mind that after receiving two keyboard interrupts, Scrapy begins unclean shutdown and the function won't be called, but the code that is placed after <code>process.start()</code> will run anyway.</p>\n<p>Alternatively you can use the <a href=\"https://doc.scrapy.org/en/latest/topics/extensions.html\" rel=\"noreferrer\">extensions</a> mechanism to connect to these signals without messing with the rest of the code base. The <a href=\"https://doc.scrapy.org/en/latest/topics/extensions.html#sample-extension\" rel=\"noreferrer\">sample extension</a> shows how to track this signal.</p>\n<p>But all of this was just to detect a failure because of interruption. You also need to subscribe to <a href=\"https://doc.scrapy.org/en/latest/topics/signals.html#spider-error\" rel=\"noreferrer\"><code>spider_error</code></a> signal that'll be called in case of a Python exception in a spider. And there is also network error handling that has to be done, see <a href=\"https://stackoverflow.com/questions/31146046/how-do-i-catch-errors-with-scrapy-so-i-can-do-something-when-i-get-user-timeout\">this question</a>.</p>\n<p>In the end I've ditched the idea of tracking failures and have just tracked success with a global variable that is checked after <code>process.start()</code> returns. In my case the moment of success was not finding the \"next page\" link. But I had a linear scraper, so it was easy, your case may be different.</p>\n", "abstract": "It all depends on how you're launching Scrapy. If running from a command line with crawl or runspider, just wait for the process to finish. Beware that 0 exit code won't mean you've crawled everything successfully. If using as a library, you can append the code after CrawlerProcess.start() call. If you need to reliably track the status, first you have to do is to track spider_closed signal and check its reason parameter. There's an example at the start of the page, it expects you to modify the code of the spider. To track all spiders you have added, when using as a library: Check the reason, if it is not 'finished', something has interrupted the crawler.\nThe function will be called for each spider, so it may require some complex error handling if you have many. Also take in mind that after receiving two keyboard interrupts, Scrapy begins unclean shutdown and the function won't be called, but the code that is placed after process.start() will run anyway. Alternatively you can use the extensions mechanism to connect to these signals without messing with the rest of the code base. The sample extension shows how to track this signal. But all of this was just to detect a failure because of interruption. You also need to subscribe to spider_error signal that'll be called in case of a Python exception in a spider. And there is also network error handling that has to be done, see this question. In the end I've ditched the idea of tracking failures and have just tracked success with a global variable that is checked after process.start() returns. In my case the moment of success was not finding the \"next page\" link. But I had a linear scraper, so it was easy, your case may be different."}, {"id": 17363573, "score": 4, "vote": 0, "content": "<p>You can write an <a href=\"http://doc.scrapy.org/en/latest/topics/extensions.html\" rel=\"nofollow\">extension</a> catching the <a href=\"http://doc.scrapy.org/en/latest/topics/signals.html#spider-closed\" rel=\"nofollow\">spider_closed</a> signal, which will execute your custom code.</p>\n", "abstract": "You can write an extension catching the spider_closed signal, which will execute your custom code."}]}, {"link": "https://stackoverflow.com/questions/4090795/cant-get-scrapy-pipeline-to-work", "question": {"id": "4090795", "title": "Can&#39;t get Scrapy pipeline to work", "content": "<p>I have spider that I have written using the Scrapy framework. I am having some trouble getting any pipelines to work. I have the following code in my pipelines.py:</p>\n<pre><code class=\"python\">class FilePipeline(object):\n\n    def __init__(self):\n        self.file = open('items.txt', 'wb')\n\n    def process_item(self, item, spider):\n        line = item['title'] + '\\n'\n        self.file.write(line)\n        return item\n</code></pre>\n<p>and my CrawlSpider subclass has this line to activate the pipeline for this class.</p>\n<pre><code class=\"python\">ITEM_PIPELINES = [\n        'event.pipelines.FilePipeline'\n    ]\n</code></pre>\n<p>However when I run it using</p>\n<pre><code class=\"python\">scrapy crawl my_spider\n</code></pre>\n<p>I get a line that says</p>\n<pre><code class=\"python\">2010-11-03 20:24:06+0000 [scrapy] DEBUG: Enabled item pipelines:\n</code></pre>\n<p>with no pipelines (I presume this is where the logging should output them).</p>\n<p>I have tried looking through the documentation but there doesn't seem to be any full examples of a whole project to see if I have missed anything.</p>\n<p>Any suggestions on what to try next? or where to look for further documentation?</p>\n", "abstract": "I have spider that I have written using the Scrapy framework. I am having some trouble getting any pipelines to work. I have the following code in my pipelines.py: and my CrawlSpider subclass has this line to activate the pipeline for this class. However when I run it using I get a line that says with no pipelines (I presume this is where the logging should output them). I have tried looking through the documentation but there doesn't seem to be any full examples of a whole project to see if I have missed anything. Any suggestions on what to try next? or where to look for further documentation?"}, "answers": [{"id": 4100942, "score": 8, "vote": 0, "content": "<p>Got it! The line needs to go in the settings module for the project. Now it works!</p>\n", "abstract": "Got it! The line needs to go in the settings module for the project. Now it works!"}, {"id": 4090855, "score": 0, "vote": 0, "content": "<p>I'm willing to bet that it's a capitalisation difference in the word pipeline somewhere:</p>\n<p>Pipeline vs. PipeLine</p>\n<p>I notice <code>'event.pipelines.FilePipeline'</code> uses the former, whereas your code uses the latter: which do your filenames use?</p>\n<p>(I have fallen victim to this spelling mistake many times!)</p>\n", "abstract": "I'm willing to bet that it's a capitalisation difference in the word pipeline somewhere: Pipeline vs. PipeLine I notice 'event.pipelines.FilePipeline' uses the former, whereas your code uses the latter: which do your filenames use? (I have fallen victim to this spelling mistake many times!)"}]}, {"link": "https://stackoverflow.com/questions/32053746/selenium-find-all-elements-by-xpath", "question": {"id": "32053746", "title": "Selenium find all elements by xpath", "content": "<p>I used selenium to scrap a scrolling website and conducted the code below   </p>\n<pre><code class=\"python\">import requests\nfrom bs4 import BeautifulSoup\nimport csv\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nimport unittest\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\nimport unittest\nimport re\n\noutput_file = open(\"Kijubi.csv\", \"w\", newline='')  \n\nclass Crawling(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.set_window_size(1024, 768)\n        self.base_url = \"http://www.viatorcom.de/\"\n        self.accept_next_alert = True\n\n    def test_sel(self):\n        driver = self.driver\n        delay = 3\n        driver.get(self.base_url + \"de/7132/Seoul/d973-allthingstodo\")\n        for i in range(1,1):\n            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n            time.sleep(2)\n    html_source = driver.page_source\n    data = html_source.encode(\"utf-8\")\n</code></pre>\n<p>My next step was to crawl specific information from the website like the price.</p>\n<p>Hence, I added the following code:</p>\n<pre><code class=\"python\"> all_spans = driver.find_elements_by_xpath(\"/html/body/div[5]/div/div[3]/div[2]/div[2]/div[1]/div[1]/div\")\n    print(all_spans)\n    for price in all_spans:\n        Header = driver.find_elements_by_xpath(\"/html/body/div[5]/div/div[3]/div[2]/div[2]/div[1]/div[1]/div/div[2]/div[2]/span[2]\")\n        for span in Header:\n            print(span.text)\n</code></pre>\n<p>But I get just one price instead all of them. Could you provide me feedback on what I could improve my code? Thanks:)</p>\n<p><strong>EDIT</strong></p>\n<p>Thanks to your guys I managed to get it running. Here is the additional code:</p>\n<pre><code class=\"python\">    elements = driver.find_elements_by_xpath(\"//div[@id='productList']/div/div\")\n\n    innerElements = 15\n\n    outerElements = len(elements)/innerElements\n\n    print(innerElements,  \"\\t\", outerElements, \"\\t\", len(elements))\n\n    for j in range(1, int(outerElements)):\n\n        for i in range(1, int(innerElements)):\n\n\n            headline = driver.find_element_by_xpath(\"//div[@id='productList']/div[\"+str(j)+\"]/div[\"+str(i)+\"]/div/div[2]/h2/a\").text\n\n            price = driver.find_element_by_xpath(\"//div[@id='productList']/div[\"+str(j)+\"]/div[\"+str(i)+\"]/div/div[2]/div[2]/span[2]\").text\n            deeplink = driver.find_element_by_xpath(\"//div[@id='productList']/div[\"+str(j)+\"]/div[\"+str(i)+\"]/div/div[2]/h2/a\").get_attribute(\"href\")\n\n            print(\"Header: \" + headline + \" | \" + \"Price: \" + price + \" | \" + \"Deeplink: \" + deeplink)\n</code></pre>\n<p>Now my last issue is that I still do not get the last 20  prices back, which have a English description. I only get back the prices which have German description. For English ones, they do not get fetched although they share the same html structure.</p>\n<p>E.g. html structure for the English items </p>\n<pre><code class=\"python\">     headline =   driver.find_element_by_xpath(\"//div[@id='productList']/div[6]/div[1]/div/div[2]/h2/a\")\n</code></pre>\n<p>Do you guys know what I have to modify? Any feedback is appreciated:) </p>\n", "abstract": "I used selenium to scrap a scrolling website and conducted the code below    My next step was to crawl specific information from the website like the price. Hence, I added the following code: But I get just one price instead all of them. Could you provide me feedback on what I could improve my code? Thanks:) EDIT Thanks to your guys I managed to get it running. Here is the additional code: Now my last issue is that I still do not get the last 20  prices back, which have a English description. I only get back the prices which have German description. For English ones, they do not get fetched although they share the same html structure. E.g. html structure for the English items  Do you guys know what I have to modify? Any feedback is appreciated:) "}, "answers": [{"id": 32077953, "score": 7, "vote": 0, "content": "<p>To grab all prices on that page you should use such XPATH:</p>\n<pre><code class=\"python\">Header = driver.find_elements_by_xpath(\"//span[contains(concat(' ', normalize-space(@class), ' '), 'price-amount')]\")\n</code></pre>\n<p>which means: find all span elements with class=price-amount, why so complex - <a href=\"https://stackoverflow.com/questions/1604471/how-can-i-find-an-element-by-css-class-with-xpath\">see here</a></p>\n<p>But more simply to find the same elements is by CSS locator:</p>\n<pre><code class=\"python\">.price-amount\n</code></pre>\n", "abstract": "To grab all prices on that page you should use such XPATH: which means: find all span elements with class=price-amount, why so complex - see here But more simply to find the same elements is by CSS locator:"}]}, {"link": "https://stackoverflow.com/questions/23921986/web-scraping-without-knowledge-of-page-structure", "question": {"id": "23921986", "title": "Web scraping without knowledge of page structure", "content": "<p>I'm trying to teach myself a concept by writing a script.  Basically, I'm trying to write a Python script that, given a few <strong>keywords</strong>, will crawl web pages until it finds the data I need.  For example, say I want to find a list of venemous snakes that live in the US.  I might run my script with the keywords <code>list,venemous,snakes,US</code>, and I want to be able to trust with at least 80% certainty that it will return a list of snakes in the US.</p>\n<p>I already know how to implement the web spider part, I just want to learn how I can determine a web page's relevancy without knowing a single thing about the page's structure.  I have researched web scraping techniques but they all seem to assume knowledge of the page's html tag structure.  Is there a certain algorithm out there that would allow me to pull data from the page and determine its relevancy?</p>\n<p>Any pointers would be greatly appreciated.  I am using <code>Python</code> with <code>urllib</code> and <code>BeautifulSoup</code>.</p>\n", "abstract": "I'm trying to teach myself a concept by writing a script.  Basically, I'm trying to write a Python script that, given a few keywords, will crawl web pages until it finds the data I need.  For example, say I want to find a list of venemous snakes that live in the US.  I might run my script with the keywords list,venemous,snakes,US, and I want to be able to trust with at least 80% certainty that it will return a list of snakes in the US. I already know how to implement the web spider part, I just want to learn how I can determine a web page's relevancy without knowing a single thing about the page's structure.  I have researched web scraping techniques but they all seem to assume knowledge of the page's html tag structure.  Is there a certain algorithm out there that would allow me to pull data from the page and determine its relevancy? Any pointers would be greatly appreciated.  I am using Python with urllib and BeautifulSoup."}, "answers": [{"id": 23922277, "score": 5, "vote": 0, "content": "<p>using a crawler like scrapy (just for handling concurrent downloads), you can write a simple spider like this and probably start with Wikipedia as a good start point. This script is a complete example using <code>scrapy</code>, <code>nltk</code> and <code>whoosh</code>. it will never stop and will index the links for later search using <code>whoosh</code>\nIt's a small Google:</p>\n<pre><code class=\"python\">_Author = Farsheed Ashouri\nimport os\nimport sys\nimport re\n## Spider libraries\nfrom scrapy.spider import BaseSpider\nfrom scrapy.selector import Selector\nfrom main.items import MainItem\nfrom scrapy.http import Request\nfrom urlparse import urljoin\n## indexer libraries\nfrom whoosh.index import create_in, open_dir\nfrom whoosh.fields import *\n## html to text conversion module\nimport nltk\n\ndef open_writer():\n    if not os.path.isdir(\"indexdir\"):\n        os.mkdir(\"indexdir\")\n        schema = Schema(title=TEXT(stored=True), content=TEXT(stored=True))\n        ix = create_in(\"indexdir\", schema)\n    else:\n        ix = open_dir(\"indexdir\")\n    return ix.writer()\n\nclass Main(BaseSpider):\n    name        = \"main\"\n    allowed_domains = [\"en.wikipedia.org\"]\n    start_urls  = [\"http://en.wikipedia.org/wiki/Snakes\"]\n    \n    def parse(self, response):\n        writer = open_writer()  ## for indexing\n        sel = Selector(response)\n        email_validation = re.compile(r'^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$')\n        #general_link_validation = re.compile(r'')\n        #We stored already crawled links in this list\n        crawledLinks    = set()\n        titles = sel.xpath('//div[@id=\"content\"]//h1[@id=\"firstHeading\"]//span/text()').extract()\n        contents = sel.xpath('//body/div[@id=\"content\"]').extract()\n        if contents:\n            content = contents[0]\n        if titles: \n            title = titles[0]\n        else:\n            return\n        links   = sel.xpath('//a/@href').extract()\n\n        \n        for link in links:\n            # If it is a proper link and is not checked yet, yield it to the Spider\n            url = urljoin(response.url, link)\n            #print url\n            ## our url must not have any \":\" character in it. link /wiki/talk:company\n            if not url in crawledLinks and re.match(r'http://en.wikipedia.org/wiki/[^:]+$', url):\n                crawledLinks.add(url)\n                  #print url, depth\n                yield Request(url, self.parse)\n        item = MainItem()\n        item[\"title\"] = title\n        print '*'*80\n        print 'crawled: %s | it has %s links.' % (title, len(links))\n        #print content\n        print '*'*80\n        item[\"links\"] = list(crawledLinks)\n        writer.add_document(title=title, content=nltk.clean_html(content))  ## I save only text from content.\n        #print crawledLinks\n        writer.commit()\n        yield item\n</code></pre>\n", "abstract": "using a crawler like scrapy (just for handling concurrent downloads), you can write a simple spider like this and probably start with Wikipedia as a good start point. This script is a complete example using scrapy, nltk and whoosh. it will never stop and will index the links for later search using whoosh\nIt's a small Google:"}, {"id": 23922228, "score": 2, "vote": 0, "content": "<p>You're basically asking \"how do I write a search engine.\"  This is... not trivial.</p>\n<p>The right way to do this is to just use Google's (or Bing's, or Yahoo!'s, or...) search API and show the top n results.  But if you're just working on a personal project to teach yourself some concepts (not sure which ones those would be exactly though), then here are a few suggestions:</p>\n<ul>\n<li>search the text content of the appropriate tags (<code>&lt;p&gt;</code>, <code>&lt;div&gt;</code>, and so forth) for relevant keywords (duh)</li>\n<li>use the relevant keywords to check for the presence of tags that might contain what you're looking for.  For example, if you're looking for a list of things, then a page containing <code>&lt;ul&gt;</code> or <code>&lt;ol&gt;</code> or even <code>&lt;table&gt;</code> might be a good candidate</li>\n<li>build a synonym dictionary and search each page for synonyms of your keywords too.  Limiting yourself to \"US\" might mean an artificially low ranking for a page containing just \"America\"</li>\n<li>keep a list of words which are <em>not</em> in your set of keywords and give a higher ranking to pages which contain the most of them.  These pages are (arguably) more likely to contain the answer you're looking for</li>\n</ul>\n<p>good luck (you'll need it)!</p>\n", "abstract": "You're basically asking \"how do I write a search engine.\"  This is... not trivial. The right way to do this is to just use Google's (or Bing's, or Yahoo!'s, or...) search API and show the top n results.  But if you're just working on a personal project to teach yourself some concepts (not sure which ones those would be exactly though), then here are a few suggestions: good luck (you'll need it)!"}]}, {"link": "https://stackoverflow.com/questions/14164350/identifying-large-bodies-of-text-via-beautifulsoup-or-other-python-based-extract", "question": {"id": "14164350", "title": "Identifying large bodies of text via BeautifulSoup or other python based extractors", "content": "<p>Given <a href=\"http://www.cnn.com/2013/01/04/justice/ohio-rape-online-video/index.html?hpt=hp_c2\" rel=\"nofollow noreferrer\">some random news article</a>, I want to write a web crawler to find the largest body of text present, and extract it. The intention is to extract the physical news article on the page.</p>\n<p>The original plan was to use a <strike><code>BeautifulSoup findAll(True)</code></strike> and to sort each tag by its <code>.getText()</code> value. <strong>EDIT: don't use this for html work, use the lxml library, it's python based and much faster than BeautifulSoup.</strong> command (which means extract all html tags)</p>\n<p>But this won't work for most pages, like the one I listed as an example, because the large body of text is split into many smaller tags, like paragraph dividers for example.</p>\n<p>Does anyone have any experience with this? Any help with something like this would be amazing.</p>\n<p>At the moment I'm using BeautifulSoup along with python, but willing to explore other possibilities.</p>\n<hr/>\n<b>EDIT: Came back to this question after a few months later (wow i sounded like an idiot ^), and solved this with a combination of libraries &amp; own code.</b>\n<p>Here are some deadly helpful python libraries for the task in sorted order of how much it helped me:</p>\n<p>#1 <a href=\"https://github.com/grangier/python-goose\" rel=\"nofollow noreferrer\">goose library</a> Fast, powerful, consistent\n#2 <a href=\"https://github.com/gfxmonk/python-readability\" rel=\"nofollow noreferrer\">readability library</a> Content is passable, slower on average than goose but faster than boilerpipe\n#3 <a href=\"https://github.com/misja/python-boilerpipe\" rel=\"nofollow noreferrer\">python-boilerpipe</a> Slower &amp; hard to install, no fault to the boilerpipe library (originally in java), but to the fact that this library is build on top of another library in java, which attributes to IO time &amp; errors, etc.</p>\n<p>I'll release benchmarks perhaps if there is interest.</p>\n<hr/>\n<p><b>Indirectly related libraries, you should probably install them and read their docs:</b></p>\n<ul>\n<li><a href=\"http://nltk.org/\" rel=\"nofollow noreferrer\">NLTK text processing library</a><b> This\nis too good not to install. They provide text analysis tools along\nwith html tools (like cleanup, etc).</b></li>\n<li><a href=\"http://lxml.de/\" rel=\"nofollow noreferrer\">lxml html/xml parser</a><b> Mentioned\nabove. This beats BeautifulSoup in every aspect but usability. It's a\nbit harder to learn but the results are worth it. HTML parsing takes\nmuch less time, it's very noticeable. </b></li>\n<li><a href=\"https://code.google.com/p/webscraping/source/browse/\" rel=\"nofollow noreferrer\">python\nwebscraper library</a> <b>I think the value of this code isn't the\nlib itself, but using the lib as a reference manual to build your own\ncrawlers/extractors. It's very nicely coded / documented!</b></li>\n</ul>\n<p>A lot of the value and power in using python, a rather slow language, comes from it's open source libraries. They are especially awesome when combined and used together, and everyone should take advantage of them to solve whatever problems they may have!</p>\n<p>Goose library gets lots of solid maintenance, they just added Arabic support, it's great!</p>\n", "abstract": "Given some random news article, I want to write a web crawler to find the largest body of text present, and extract it. The intention is to extract the physical news article on the page. The original plan was to use a BeautifulSoup findAll(True) and to sort each tag by its .getText() value. EDIT: don't use this for html work, use the lxml library, it's python based and much faster than BeautifulSoup. command (which means extract all html tags) But this won't work for most pages, like the one I listed as an example, because the large body of text is split into many smaller tags, like paragraph dividers for example. Does anyone have any experience with this? Any help with something like this would be amazing. At the moment I'm using BeautifulSoup along with python, but willing to explore other possibilities. Here are some deadly helpful python libraries for the task in sorted order of how much it helped me: #1 goose library Fast, powerful, consistent\n#2 readability library Content is passable, slower on average than goose but faster than boilerpipe\n#3 python-boilerpipe Slower & hard to install, no fault to the boilerpipe library (originally in java), but to the fact that this library is build on top of another library in java, which attributes to IO time & errors, etc. I'll release benchmarks perhaps if there is interest. Indirectly related libraries, you should probably install them and read their docs: A lot of the value and power in using python, a rather slow language, comes from it's open source libraries. They are especially awesome when combined and used together, and everyone should take advantage of them to solve whatever problems they may have! Goose library gets lots of solid maintenance, they just added Arabic support, it's great!"}, "answers": [{"id": 14165771, "score": 5, "vote": 0, "content": "<p>You might look at the <a href=\"https://github.com/buriy/python-readability\" rel=\"noreferrer\">python-readability</a> package which does exactly this for you.</p>\n", "abstract": "You might look at the python-readability package which does exactly this for you."}, {"id": 14164704, "score": 2, "vote": 0, "content": "<p>You're really not going about it the right way, I would say, as all the comments above would attest to.</p>\n<p>That said, this does what you're looking for.</p>\n<pre><code class=\"python\">from bs4 import BeautifulSoup as BS\nimport requests\nhtml = requests.get('http://www.cnn.com/2013/01/04/justice/ohio-rape-online-video/index.html?hpt=hp_c2').text\nsoup = BS(html)\nprint '\\n\\n'.join([k.text for k in soup.find(class_='cnn_strycntntlft').find_all('p')])\n</code></pre>\n<p>It pulls out only the text, first by finding the main container of all the <code>&lt;p&gt;</code> tags, then by selecting only the <code>&lt;p&gt;</code> tags themselves to get the text; ignoring the <code>&lt;script&gt;</code> and other irrelevant ones.</p>\n<p>As was mentioned in the comments, this will only work for CNN--and possibly, only this page.  You might need a different strategy for every new webpage.</p>\n", "abstract": "You're really not going about it the right way, I would say, as all the comments above would attest to. That said, this does what you're looking for. It pulls out only the text, first by finding the main container of all the <p> tags, then by selecting only the <p> tags themselves to get the text; ignoring the <script> and other irrelevant ones. As was mentioned in the comments, this will only work for CNN--and possibly, only this page.  You might need a different strategy for every new webpage."}]}, {"link": "https://stackoverflow.com/questions/54011831/captcha-using-requests-even-after-changing-headers-and-ip-how-am-i-being-tracke", "question": {"id": "54011831", "title": "Captcha using requests even after changing headers and IP. How am I being tracked?", "content": "<p>I am trying to scrape some articles from xyz However, after a certain number of scrapes, a captcha appears.</p>\n<p>However, I am running into major issues.</p>\n<ol>\n<li><p>I am using <code>from fake_useragent import UserAgent</code> to randomize my header.</p>\n</li>\n<li><p>I am using random sleep times between requests</p>\n</li>\n<li><p>I am changing IP address using a VPN once a captcha appears. However, somehow a captcha still appears once my IP address appears.</p>\n</li>\n</ol>\n<p>It is also strange because while a captcha appears in the request response, a captcha does not appear in the browser.</p>\n<p>So, I assume that by header is just wrong.</p>\n<p>I turned off js and cookies when obtaining this request because with cookie and js, there is clearly info that the website is tracking me with.</p>\n<pre><code class=\"python\">headers = {\n    \"authority\": \"seekingalpha.com\",\n    \"method\": \"GET\",\n    \"path\": \"/article/4230872-dillards-still-room-downside\",\n    \"scheme\": \"https\",\n    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n    \"accept-encoding\": \"gzip, deflate, br\",\n    \"accept-language\": 'en-US,en;q=0.9',\n    \"upgrade-insecure-requests\": \"1\",\n    \"user-agent\": RANDOM\n}\n</code></pre>\n<p>This is close to what the website uses: They add</p>\n<pre><code class=\"python\">\"cache-control\": \"max-age=0\",\n\"if-none-match\": 'W/\"6f11a6f9219176fda72f3cf44b0a2059\"',\n</code></pre>\n<p>This to my research is etags which is used for carching and can be use to track people. The <code>'W/...'</code> changes each request.</p>\n<p>Also, when I use <a href=\"https://github.com/JazzCore/python-pdfkit\" rel=\"nofollow noreferrer\">wkhtmltopdf</a> to print the screen as pdf, I a captcha never appears. I have also tried using selenium which is even worse. In addition, I have tried using proxies as seen <a href=\"https://stackoverflow.com/questions/48756326/web-scraping-results-in-403-forbidden-error\">here</a>.</p>\n<p>So there definitely is a way of doing this. However, I am not doing it correctly. Does anyone have an idea what I am doing wrong?</p>\n<p>Edit:</p>\n<ol>\n<li><p>Sessions does not seems to be working</p>\n</li>\n<li><p>Random headers does not seem to be working</p>\n</li>\n<li><p>Random sleeps does not seem to be working</p>\n</li>\n<li><p>I am able to access the webpage using my VPN. Even once a capcha appears using requests, there is no captcha on the website in the browser.</p>\n</li>\n<li><p>Selenium does not work.</p>\n</li>\n<li><p>I really do not want to pay for a service to solve capchas.</p>\n</li>\n</ol>\n<p>I believe the issue is that I am not mimicking the browser well enough.</p>\n", "abstract": "I am trying to scrape some articles from xyz However, after a certain number of scrapes, a captcha appears. However, I am running into major issues. I am using from fake_useragent import UserAgent to randomize my header. I am using random sleep times between requests I am changing IP address using a VPN once a captcha appears. However, somehow a captcha still appears once my IP address appears. It is also strange because while a captcha appears in the request response, a captcha does not appear in the browser. So, I assume that by header is just wrong. I turned off js and cookies when obtaining this request because with cookie and js, there is clearly info that the website is tracking me with. This is close to what the website uses: They add This to my research is etags which is used for carching and can be use to track people. The 'W/...' changes each request. Also, when I use wkhtmltopdf to print the screen as pdf, I a captcha never appears. I have also tried using selenium which is even worse. In addition, I have tried using proxies as seen here. So there definitely is a way of doing this. However, I am not doing it correctly. Does anyone have an idea what I am doing wrong? Edit: Sessions does not seems to be working Random headers does not seem to be working Random sleeps does not seem to be working I am able to access the webpage using my VPN. Even once a capcha appears using requests, there is no captcha on the website in the browser. Selenium does not work. I really do not want to pay for a service to solve capchas. I believe the issue is that I am not mimicking the browser well enough."}, "answers": [{"id": 54012095, "score": 2, "vote": 0, "content": "<p>It is not easy to pinpoint the exact reason for being blocked and facing a Captcha. Here are few thoughts:</p>\n<h2>VPN and Proxies</h2>\n<p>Sometimes, the <strong>Captcha service</strong> (in this case, Google) may <strong>blacklist common VPN IP addresses</strong> and treat them as potential threats, since many people are using them and they generate a lot of traffic.</p>\n<p>Sometimes, <strong>proxy servers</strong> (especially free ones) are <strong>not anonymous</strong> and can send your actual IP address in the request headers (specifically, the <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For\" rel=\"nofollow noreferrer\">X-Forwarded-For</a> header)</p>\n<h2>Request Headers</h2>\n<p>There are <strong>certain headers that are important to have</strong> in your request. The easiest way to make your requests look legitimate is to use the \"Network\" tab in your browser's \"Developer Tools\", and copy all the headers your browser sends. </p>\n<p>An <strong>important header</strong> to have is <strong><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Referer\" rel=\"nofollow noreferrer\"><code>referer</code></a></strong>. While it may or may not be checked by the website, it is safer to just have it there with the URL of one of the website's pages (or homepage):</p>\n<pre><code class=\"python\">referer: https://seekingalpha.com/\n</code></pre>\n<h2>Timeouts and Sessions</h2>\n<p>Try to <strong>increase the timeouts</strong> between your requests. Few seconds should be reasonable.</p>\n<p>Finally, <strong>try using the <a href=\"http://docs.python-requests.org/en/master/user/advanced/\" rel=\"nofollow noreferrer\"><code>session</code></a> objects</strong> in <code>requests</code>. They automatically maintain the cookies and update the <code>referer</code> across multiple requests, to emulate a real user browsing the website. I found them to be the most helpful when it comes to overcoming scraping protections.</p>\n<h2>Captcha</h2>\n<p>The last-resort is to use a <strong>service to break the captcha</strong>. There are many services (mostly paid) online that do that. A popular one is DeathByCaptcha. Keep in mind that you may be breaking the website's terms of use, which I do not recommend :)</p>\n", "abstract": "It is not easy to pinpoint the exact reason for being blocked and facing a Captcha. Here are few thoughts: Sometimes, the Captcha service (in this case, Google) may blacklist common VPN IP addresses and treat them as potential threats, since many people are using them and they generate a lot of traffic. Sometimes, proxy servers (especially free ones) are not anonymous and can send your actual IP address in the request headers (specifically, the X-Forwarded-For header) There are certain headers that are important to have in your request. The easiest way to make your requests look legitimate is to use the \"Network\" tab in your browser's \"Developer Tools\", and copy all the headers your browser sends.  An important header to have is referer. While it may or may not be checked by the website, it is safer to just have it there with the URL of one of the website's pages (or homepage): Try to increase the timeouts between your requests. Few seconds should be reasonable. Finally, try using the session objects in requests. They automatically maintain the cookies and update the referer across multiple requests, to emulate a real user browsing the website. I found them to be the most helpful when it comes to overcoming scraping protections. The last-resort is to use a service to break the captcha. There are many services (mostly paid) online that do that. A popular one is DeathByCaptcha. Keep in mind that you may be breaking the website's terms of use, which I do not recommend :)"}]}, {"link": "https://stackoverflow.com/questions/25459719/understanding-scrapys-crawlspider-rules", "question": {"id": "25459719", "title": "Understanding Scrapy&#39;s CrawlSpider rules", "content": "<p>I'm having trouble understanding how to use the rules field within my own Spider that inherits from CrawlSpider. My spider is trying to crawl through yellowpage listings for pizza in san francisco.</p>\n<p>I've tried to keep my rules simple just to see if the spider would crawl through any of the links in the response, but I don't see it happening. My only results are that it yields the request for the next page and then yields a request for the subsequent page. </p>\n<p>I have two questions:\n<strong>1.</strong> Does the spider process the rules first before calling the callback when the response is received? Or vice versa?\n<strong>2.</strong> When are the rules applied?</p>\n<p><strong>EDIT:</strong>\nI figured it out. I overrode the parse method from CrawlSpider. After looking at the parse method within that class, I realized that's where it checks the rules and crawls through those websites.</p>\n<p><strong>NOTE: Know what you're overriding</strong></p>\n<p>Here's my code:</p>\n<pre><code class=\"python\">from scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy import Selector\nfrom yellowPages.items import YellowpagesItem\nfrom scrapy.http import Request\n\nclass YellowPageSpider(CrawlSpider):\n    name = \"yellowpages\"\n    allowed_domains = ['www.yellowpages.com']\n    businesses = []\n\n    # start with one page\n    start_urls = ['http://www.yellowpages.com/san-francisco-ca/pizza?g=san%20francisco%2C%20ca&amp;q=pizza']\n\n    rules = (Rule (SgmlLinkExtractor()\n    , callback=\"parse_items\", follow= True),\n    )\n\n    base_url = 'http://www.yellowpages.com'\n\n    def parse(self, response):\n        yield Request(response.url, callback=self.parse_business_listings_page)\n\n    def parse_items(self, response):\n        print \"PARSE ITEMS. Visiting %s\" % response.url\n        return []\n\n    def parse_business_listings_page(self, response):\n        print \"Visiting %s\" % response.url\n\n        self.businesses.append(self.extract_businesses_from_response(response))\n        hxs = Selector(response)\n        li_tags = hxs.xpath('//*[@id=\"main-content\"]/div[4]/div[5]/ul/li')\n        next_exist = False\n\n        # Check to see if there's a \"Next\". If there is, store the links.\n        # If not, return. \n        # This requires a linear search through the list of li_tags. Is there a faster way?\n        for li in li_tags:\n            li_text = li.xpath('.//a/text()').extract()\n            li_data_page = li.xpath('.//a/@data-page').extract()\n            # Note: sometimes li_text is an empty list so check to see if it is nonempty first\n            if (li_text and li_text[0] == 'Next'):\n                next_exist = True\n                next_page_num = li_data_page[0]\n                url = 'http://www.yellowpages.com/san-francisco-ca/pizza?g=san%20francisco%2C%20ca&amp;q=pizza&amp;page='+next_page_num\n                yield Request(url, callback=self.parse_business_listings_page)\n</code></pre>\n", "abstract": "I'm having trouble understanding how to use the rules field within my own Spider that inherits from CrawlSpider. My spider is trying to crawl through yellowpage listings for pizza in san francisco. I've tried to keep my rules simple just to see if the spider would crawl through any of the links in the response, but I don't see it happening. My only results are that it yields the request for the next page and then yields a request for the subsequent page.  I have two questions:\n1. Does the spider process the rules first before calling the callback when the response is received? Or vice versa?\n2. When are the rules applied? EDIT:\nI figured it out. I overrode the parse method from CrawlSpider. After looking at the parse method within that class, I realized that's where it checks the rules and crawls through those websites. NOTE: Know what you're overriding Here's my code:"}, "answers": [{"id": 43019411, "score": 2, "vote": 0, "content": "<p><br/>\nSo to the point to your two questions..\n<br/></p>\n<ol>\n<li><p>before the request is made, the crawlers rules are processed before making the request... and of course if the response does not comply to the allowed domain, response is received in theory but just gets drop'ed.\n<br/></p></li>\n<li><p>Again Crawler rules are used before a request is made. </p></li>\n</ol>\n<h1>NOTE!</h1>\n<p>In your example, when you call the parse() method... though in your case your using it right??!  Would have to run it to confirm but those of you reading unless your explicitly overriding parse() method in a CRAWL spider... when using a crawl spider... the equivalant of pare in a spider to a crawler is parse_item()...  parse() in a crawler is a its own logic function... USING AS A CALLBACK IN RULESET should not be done</p>\n<p><a href=\"https://doc.scrapy.org/en/latest/topics/spiders.html\" rel=\"nofollow noreferrer\">https://doc.scrapy.org/en/latest/topics/spiders.html</a></p>\n", "abstract": "\nSo to the point to your two questions..\n before the request is made, the crawlers rules are processed before making the request... and of course if the response does not comply to the allowed domain, response is received in theory but just gets drop'ed.\n Again Crawler rules are used before a request is made.  In your example, when you call the parse() method... though in your case your using it right??!  Would have to run it to confirm but those of you reading unless your explicitly overriding parse() method in a CRAWL spider... when using a crawl spider... the equivalant of pare in a spider to a crawler is parse_item()...  parse() in a crawler is a its own logic function... USING AS A CALLBACK IN RULESET should not be done https://doc.scrapy.org/en/latest/topics/spiders.html"}]}, {"link": "https://stackoverflow.com/questions/13505194/scrapy-crawling-speed-is-slow-60-pages-min", "question": {"id": "13505194", "title": "Scrapy Crawling Speed is Slow (60 pages / min)", "content": "<p>I am experiencing slow crawl speeds with scrapy (around 1 page / sec).\nI'm crawling a major website from aws servers so I don't think its a network issue. Cpu utilization is nowhere near 100 and if I start multiple scrapy processes crawl speed is much faster.</p>\n<p>Scrapy seems to crawl a bunch of pages, then hangs for several seconds, and then repeats.</p>\n<p>I've tried playing with:\nCONCURRENT_REQUESTS = CONCURRENT_REQUESTS_PER_DOMAIN = 500</p>\n<p>but this doesn't really seem to move the needle past about 20.</p>\n", "abstract": "I am experiencing slow crawl speeds with scrapy (around 1 page / sec).\nI'm crawling a major website from aws servers so I don't think its a network issue. Cpu utilization is nowhere near 100 and if I start multiple scrapy processes crawl speed is much faster. Scrapy seems to crawl a bunch of pages, then hangs for several seconds, and then repeats. I've tried playing with:\nCONCURRENT_REQUESTS = CONCURRENT_REQUESTS_PER_DOMAIN = 500 but this doesn't really seem to move the needle past about 20."}, "answers": [{"id": 13585472, "score": 2, "vote": 0, "content": "<p>Are you sure you are allowed to crawl the destination site at high speed? Many sites implement download threshold and \"after a while\" start responding slowly.</p>\n", "abstract": "Are you sure you are allowed to crawl the destination site at high speed? Many sites implement download threshold and \"after a while\" start responding slowly."}]}, {"link": "https://stackoverflow.com/questions/2615830/storing-urls-while-spidering", "question": {"id": "2615830", "title": "Storing URLs while Spidering", "content": "<p>I created a little web spider in Python which I'm using to collect URLs. I'm not interested in the content. Right now I'm keeping all the visited URLs in a set in memory, because I don't want my spider to visit URLs twice. Of course that's a very limited way of accomplishing this.</p>\n<p>So what's the best way to keep track of my visited URLs? </p>\n<p>Should I use a database?</p>\n<ul>\n<li>which one? MySQL, SQLite, PostgreSQL? </li>\n<li>how should I save the URLs? As a primary key trying to insert every URL before visiting it?</li>\n</ul>\n<p>Or should I write them to a file?</p>\n<ul>\n<li>one file?</li>\n<li>multiple files? how should I design the file-structure?</li>\n</ul>\n<p>I'm sure there are books and a lot of papers on this or similar topics. Can you give me some advice what I should read?</p>\n", "abstract": "I created a little web spider in Python which I'm using to collect URLs. I'm not interested in the content. Right now I'm keeping all the visited URLs in a set in memory, because I don't want my spider to visit URLs twice. Of course that's a very limited way of accomplishing this. So what's the best way to keep track of my visited URLs?  Should I use a database? Or should I write them to a file? I'm sure there are books and a lot of papers on this or similar topics. Can you give me some advice what I should read?"}, "answers": [{"id": 2615904, "score": 9, "vote": 0, "content": "<p>I've written a lot of spiders. To me, a bigger problem than running out of memory is the potential of losing all the URLs you've spidered already if the code or machine crashes or you decide you need to tweak the code. If you run out of RAM most machines and OSes these days will page so you'll slow down but still function. Having to rebuild a set of URLs gathered over hours and hours of run-time because its no longer available can be a real blow to productivity.</p>\n<p>Keeping information in RAM that you do NOT want to lose is bad. Obviously a database is the way to go at that point because you need fast random access to see if you've already found a URL. Of course in-memory lookups are faster but the trade-off of figuring out WHICH urls to keep in memory adds overhead. Rather than try writing code to determine which URLs I need/don't-need, I keep it in the database and concentrate on making my code clean and maintainable and my SQL queries and schemas sensible. Make your URL field a unique index and the DBM will be able to find them in no time while automatically avoiding redundant links.</p>\n<p>Your connection to the internet and sites you're accessing will probably be a lot slower than your connection to a database on a machine on your internal network. A SQLite database on the same machine might be the fastest, though the DBM itself isn't as sophisticated as Postgres, which is my favorite. I found that putting the database on another machine on the same switch as my spidering machine to be extremely fast; Making one machine handle the spidering, parsing, and then the database reads/writes is pretty intensive so if you have an old box throw Linux on it, install Postgres, and go to town. Throw some extra RAM in the box if you need more speed. Having that separate box for database use can be very nice.</p>\n", "abstract": "I've written a lot of spiders. To me, a bigger problem than running out of memory is the potential of losing all the URLs you've spidered already if the code or machine crashes or you decide you need to tweak the code. If you run out of RAM most machines and OSes these days will page so you'll slow down but still function. Having to rebuild a set of URLs gathered over hours and hours of run-time because its no longer available can be a real blow to productivity. Keeping information in RAM that you do NOT want to lose is bad. Obviously a database is the way to go at that point because you need fast random access to see if you've already found a URL. Of course in-memory lookups are faster but the trade-off of figuring out WHICH urls to keep in memory adds overhead. Rather than try writing code to determine which URLs I need/don't-need, I keep it in the database and concentrate on making my code clean and maintainable and my SQL queries and schemas sensible. Make your URL field a unique index and the DBM will be able to find them in no time while automatically avoiding redundant links. Your connection to the internet and sites you're accessing will probably be a lot slower than your connection to a database on a machine on your internal network. A SQLite database on the same machine might be the fastest, though the DBM itself isn't as sophisticated as Postgres, which is my favorite. I found that putting the database on another machine on the same switch as my spidering machine to be extremely fast; Making one machine handle the spidering, parsing, and then the database reads/writes is pretty intensive so if you have an old box throw Linux on it, install Postgres, and go to town. Throw some extra RAM in the box if you need more speed. Having that separate box for database use can be very nice."}, {"id": 2615836, "score": 7, "vote": 0, "content": "<p>These seem to be the important aspects to me:</p>\n<ol>\n<li>You can't keep the URLs in memory as RAM will get too high</li>\n<li>You need fast existence lookups at least O(logn)</li>\n<li>You need fast insertions </li>\n</ol>\n<p>There are many ways to do this and it depends on how big your database will get.  I think an SQL database can provide a good model for your problem.  </p>\n<p>Probably all you need is an SQLite database.  Typically string lookups for existence check is a slow operation.  To speed this up you can create a CRC hash of the URL and store both the CRC and URL in your database.  You would have an index on that CRC field. </p>\n<ul>\n<li>When you insert: You insert the URL and the hash</li>\n<li>When you want to do an existance lookup: You take the CRC of the potentially new URL and check if it is in your database already.   </li>\n</ul>\n<p>There is of course a chance of collision on the URL hashes, but if 100% spanning is not important to you then you can take the hit of not having a URL in your DB when there is a collision.  </p>\n<p>You could also decrease collisions in many ways.  For example you can increase the size of your CRC (CRC8 instead of CRC4) and use a hashing algorithm with a bigger size.  Or use CRC as well as URL length.  </p>\n", "abstract": "These seem to be the important aspects to me: There are many ways to do this and it depends on how big your database will get.  I think an SQL database can provide a good model for your problem.   Probably all you need is an SQLite database.  Typically string lookups for existence check is a slow operation.  To speed this up you can create a CRC hash of the URL and store both the CRC and URL in your database.  You would have an index on that CRC field.  There is of course a chance of collision on the URL hashes, but if 100% spanning is not important to you then you can take the hit of not having a URL in your DB when there is a collision.   You could also decrease collisions in many ways.  For example you can increase the size of your CRC (CRC8 instead of CRC4) and use a hashing algorithm with a bigger size.  Or use CRC as well as URL length.  "}, {"id": 2615862, "score": 4, "vote": 0, "content": "<p>It depends on the scale of the spidering you're going to be doing, and the kind of machine you're doing it on.  Suppose a typical URL is a string of 60 bytes or so, an in-memory set will take a bit more than 100 bytes per URL (sets and dicts in Python are never allowed to grow beyond 60% full, for speed reasons).  If you have a 64-bit machine (and Python distro) with about 16 GB of RAM available, you could surely devote over 10 GB to the crucial set in question, letting you easily spider about 100 million URLs or so; but at the other extreme, if you have a 32-bit machine with 3GB of RAM, you clearly can't devote much more than a GB to the crucial set, limiting you to about 10 million URLs.  Sqlite would help around the same range of sized where a 32-bit machine couldn't make it but a generously-endowed 64-bit one could -- say 100 or 200 million URLs.</p>\n<p>Beyond those, I'd recommend PostgreSQL, which also has the advantage of being able to run on a different machine (on a fast LAN) with basically no problems, letting you devote your main machine to spidering.  I guess MySQL &amp;c would be OK for that too, but I love PostgreSQL standard compliance and robustness;-).  This would allow, say, a few billion URLs with no problems (just a fast disk, or better a RAID arrangement, and as much RAM as you can afford to speed things up, of course).</p>\n<p>Trying to save memory/storage by using a fixed-length hash in lieu of URLs that might be quite long is fine <strong>if</strong> you're OK with an occasional false positive that will stop you from crawling what's actually a new URL.  Such \"collisions\" need not be at all likely: even if you only use 8 bytes for the hash, you should only have a substantial risk of some collision when you're looking at billions of URLs (the \"square root heuristic\" for this well-known problem).</p>\n<p>With 8-bytes strings to represent the URLs, the in-memory set architecture should easily support a billion URLs or more on a well-endowed machine as above outlined.</p>\n<p>So, roughly how many URLs do you want to spider, and how much RAM can you spare?-)</p>\n", "abstract": "It depends on the scale of the spidering you're going to be doing, and the kind of machine you're doing it on.  Suppose a typical URL is a string of 60 bytes or so, an in-memory set will take a bit more than 100 bytes per URL (sets and dicts in Python are never allowed to grow beyond 60% full, for speed reasons).  If you have a 64-bit machine (and Python distro) with about 16 GB of RAM available, you could surely devote over 10 GB to the crucial set in question, letting you easily spider about 100 million URLs or so; but at the other extreme, if you have a 32-bit machine with 3GB of RAM, you clearly can't devote much more than a GB to the crucial set, limiting you to about 10 million URLs.  Sqlite would help around the same range of sized where a 32-bit machine couldn't make it but a generously-endowed 64-bit one could -- say 100 or 200 million URLs. Beyond those, I'd recommend PostgreSQL, which also has the advantage of being able to run on a different machine (on a fast LAN) with basically no problems, letting you devote your main machine to spidering.  I guess MySQL &c would be OK for that too, but I love PostgreSQL standard compliance and robustness;-).  This would allow, say, a few billion URLs with no problems (just a fast disk, or better a RAID arrangement, and as much RAM as you can afford to speed things up, of course). Trying to save memory/storage by using a fixed-length hash in lieu of URLs that might be quite long is fine if you're OK with an occasional false positive that will stop you from crawling what's actually a new URL.  Such \"collisions\" need not be at all likely: even if you only use 8 bytes for the hash, you should only have a substantial risk of some collision when you're looking at billions of URLs (the \"square root heuristic\" for this well-known problem). With 8-bytes strings to represent the URLs, the in-memory set architecture should easily support a billion URLs or more on a well-endowed machine as above outlined. So, roughly how many URLs do you want to spider, and how much RAM can you spare?-)"}, {"id": 2615855, "score": 2, "vote": 0, "content": "<p>Are you just storing URL's?  You should take a look at mongoDB.  It's a NoSQL database that's pretty easy to implement.</p>\n<p><a href=\"http://try.mongodb.org/\" rel=\"nofollow noreferrer\">http://try.mongodb.org/</a></p>\n<p>It's got python bindings, too:</p>\n<p><a href=\"http://api.mongodb.org/python/1.5.2%2B/index.html\" rel=\"nofollow noreferrer\">http://api.mongodb.org/python/1.5.2%2B/index.html</a></p>\n", "abstract": "Are you just storing URL's?  You should take a look at mongoDB.  It's a NoSQL database that's pretty easy to implement. http://try.mongodb.org/ It's got python bindings, too: http://api.mongodb.org/python/1.5.2%2B/index.html"}, {"id": 2615857, "score": 1, "vote": 0, "content": "<p>Since it's likely that you'll see similar URLs at similar times (e.g., while spidering a website, you'll see lots of links to the main page of the website) I would advise that you keep the URLs in a dictionary until your memory becomes limited (just hardcode a reasonable number like 10M URLs or similar) and then flush the dictionary to a <a href=\"http://pypi.python.org/pypi/python-cdb/0.32\" rel=\"nofollow noreferrer\">CDB database file</a> when it becomes too large.</p>\n<p>That way, the majority of your URL checks will be in memory (which is fast) while the ones that aren't in memory will still require only 1-2 reads from disk to verify that you've visited them.</p>\n", "abstract": "Since it's likely that you'll see similar URLs at similar times (e.g., while spidering a website, you'll see lots of links to the main page of the website) I would advise that you keep the URLs in a dictionary until your memory becomes limited (just hardcode a reasonable number like 10M URLs or similar) and then flush the dictionary to a CDB database file when it becomes too large. That way, the majority of your URL checks will be in memory (which is fast) while the ones that aren't in memory will still require only 1-2 reads from disk to verify that you've visited them."}, {"id": 2618578, "score": 0, "vote": 0, "content": "<p>Consider <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">Pickling</a> for now: Simple structured storage. </p>\n<p>Mileage will vary of course because, as the other responders have said, you'll quickly exhaust your RAM.</p>\n", "abstract": "Consider Pickling for now: Simple structured storage.  Mileage will vary of course because, as the other responders have said, you'll quickly exhaust your RAM."}]}, {"link": "https://stackoverflow.com/questions/39365131/running-multiple-spiders-in-scrapy-for-1-website-in-parallel", "question": {"id": "39365131", "title": "Running Multiple spiders in scrapy for 1 website in parallel?", "content": "<p>I want to crawl a website with 2 parts and my script is not as fast as I need.</p>\n<p>Is it possible to launch 2 spiders, one for scraping the first part and the second one for the second part? </p>\n<p>I tried to have 2 different classes, and run them</p>\n<pre><code class=\"python\">scrapy crawl firstSpider\nscrapy crawl secondSpider\n</code></pre>\n<p>but i think that it is not smart.</p>\n<p>I read the <a href=\"http://scrapyd.readthedocs.io/en/latest/api.html\" rel=\"nofollow noreferrer\">documentation of scrapyd</a> but I don't know if it's good for my case.</p>\n", "abstract": "I want to crawl a website with 2 parts and my script is not as fast as I need. Is it possible to launch 2 spiders, one for scraping the first part and the second one for the second part?  I tried to have 2 different classes, and run them but i think that it is not smart. I read the documentation of scrapyd but I don't know if it's good for my case."}, "answers": [{"id": 39366885, "score": 12, "vote": 0, "content": "<p>I think what you are looking for is something like this:</p>\n<pre><code class=\"python\">import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    ...\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    ...\n\nprocess = CrawlerProcess()\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start() # the script will block here until all crawling jobs are finished\n</code></pre>\n<p>You can read more at: <a href=\"https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\" rel=\"nofollow noreferrer\">running-multiple-spiders-in-the-same-process</a>.</p>\n", "abstract": "I think what you are looking for is something like this: You can read more at: running-multiple-spiders-in-the-same-process."}, {"id": 43927085, "score": 7, "vote": 0, "content": "<p>Or you can run with like this, you need to save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3) :</p>\n<pre><code class=\"python\">from scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsetting = get_project_settings()\nprocess = CrawlerProcess(setting)\n\nfor spider_name in process.spiders.list():\n    print (\"Running spider %s\" % (spider_name))\n    process.crawl(spider_name,query=\"dvh\") #query dvh is custom argument used in your scrapy\n\nprocess.start()\n</code></pre>\n", "abstract": "Or you can run with like this, you need to save this code at the same directory with scrapy.cfg (My scrapy version is 1.3.3) :"}, {"id": 46871307, "score": 3, "vote": 0, "content": "<p>Better solution is (if you have multiple spiders) it dynamically get spiders and run them.</p>\n<pre><code class=\"python\">from scrapy import spiderloader\nfrom scrapy.utils import project\nfrom twisted.internet.defer import inlineCallbacks\n\n\n@inlineCallbacks\ndef crawl():\n    settings = project.get_project_settings()\n    spider_loader = spiderloader.SpiderLoader.from_settings(settings)\n    spiders = spider_loader.list()\n    classes = [spider_loader.load(name) for name in spiders]\n    for my_spider in classes:\n        yield runner.crawl(my_spider)\n    reactor.stop()\n\ncrawl()\nreactor.run()\n</code></pre>\n<p><strong>(Second Solution):</strong>\nBecause <code>spiders.list()</code> is deprecated in Scrapy 1.4 Yuda solution should be converted to something like</p>\n<pre><code class=\"python\">from scrapy import spiderloader    \nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerProcess\n\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings)\nspider_loader = spiderloader.SpiderLoader.from_settings(settings)\n\nfor spider_name in spider_loader.list():\n    print(\"Running spider %s\" % (spider_name))\n    process.crawl(spider_name)\nprocess.start()\n</code></pre>\n", "abstract": "Better solution is (if you have multiple spiders) it dynamically get spiders and run them. (Second Solution):\nBecause spiders.list() is deprecated in Scrapy 1.4 Yuda solution should be converted to something like"}]}, {"link": "https://stackoverflow.com/questions/23662069/scrapy-parse-javascript", "question": {"id": "23662069", "title": "Scrapy parse javascript", "content": "<p>I have a javascript on the page like below: </p>\n<pre><code class=\"python\">new Shopify.OptionSelectors(\"product-select\", { product: {\"id\":185310341,\"title\":\"10. Design | Siyah \\u0026 beyaz kalpli\",\n</code></pre>\n<p>i want to get \"185310341\". I am searching on google about a few hours but couldn't find anything, I hope u can help me. How can i scrape that javascript and get that id? </p>\n<p>I tried that code :</p>\n<pre><code class=\"python\">id = sel.search('\"id\":(.*?),',text).group(1)\nprint id\n</code></pre>\n<p>but i got:</p>\n<pre><code class=\"python\">exceptions.AttributeError: 'Selector' object has no attribute 'search'\n</code></pre>\n", "abstract": "I have a javascript on the page like below:  i want to get \"185310341\". I am searching on google about a few hours but couldn't find anything, I hope u can help me. How can i scrape that javascript and get that id?  I tried that code : but i got:"}, "answers": [{"id": 23662143, "score": 7, "vote": 0, "content": "<p>Scrapy selectors have <a href=\"http://doc.scrapy.org/en/latest/topics/selectors.html#using-selectors-with-regular-expressions\" rel=\"noreferrer\">built-in support</a> for regular expressions:</p>\n<pre><code class=\"python\">sel.xpath('&lt;xpath_to_find_the_element_text&gt;').re(r'\"id\":(\\d+)')\n</code></pre>\n<p>Demo showing the work of this particular regular expression:</p>\n<pre><code class=\"python\">&gt;&gt;&gt; import re\n&gt;&gt;&gt; s = 'new Shopify.OptionSelectors(\"product-select\", { product: {\"id\":185310341,\"title\":\"10. Design | Siyah \\u0026 beyaz kalpli\",'\n&gt;&gt;&gt; re.search('\"id\":(\\d+)', s).group(1)\n'185310341' \n</code></pre>\n", "abstract": "Scrapy selectors have built-in support for regular expressions: Demo showing the work of this particular regular expression:"}, {"id": 23740289, "score": 6, "vote": 0, "content": "<p>An alternative to the regex approach is to use a Javascript parser, convert the output of that parser to an XML document, and parse it with XPath.</p>\n<p>That's what implemented in <a href=\"https://github.com/redapple/js2xml\" rel=\"noreferrer\">js2xml</a>, which uses <a href=\"https://github.com/rspivak/slimit\" rel=\"noreferrer\"><code>slimit</code></a> and <code>lxml</code>\n(disclaimer: I wrote js2xml; warning: not stable)</p>\n<p>In your case, check this sample scrapy shell session, using <code>js2xml.jsonlike.getall()</code>:</p>\n<pre><code class=\"python\">paul:~$ scrapy shell http://2loom.com/products/2loom-design-siyah-beyaz-kalpli\n2014-05-19 16:12:00+0200 [scrapy] INFO: Scrapy 0.23.0 started (bot: scrapybot)\n2014-05-19 16:12:00+0200 [scrapy] INFO: Optional features available: ssl, http11\n2014-05-19 16:12:00+0200 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\n2014-05-19 16:12:00+0200 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-05-19 16:12:00+0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-05-19 16:12:00+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-05-19 16:12:00+0200 [scrapy] INFO: Enabled item pipelines: \n2014-05-19 16:12:00+0200 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2014-05-19 16:12:00+0200 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2014-05-19 16:12:00+0200 [default] INFO: Spider opened\n2014-05-19 16:12:01+0200 [default] DEBUG: Crawled (200) &lt;GET http://2loom.com/products/2loom-design-siyah-beyaz-kalpli&gt; (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7f8552946610&gt;\n[s]   item       {}\n[s]   request    &lt;GET http://2loom.com/products/2loom-design-siyah-beyaz-kalpli&gt;\n[s]   response   &lt;200 http://2loom.com/products/2loom-design-siyah-beyaz-kalpli&gt;\n[s]   settings   &lt;CrawlerSettings module=None&gt;\n[s]   spider     &lt;Spider 'default' at 0x7f8552384b90&gt;\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n/usr/local/lib/python2.7/dist-packages/IPython/frontend.py:30: UserWarning: The top-level `frontend` package has been deprecated. All its subpackages have been moved to the top `IPython` level.\n  warn(\"The top-level `frontend` package has been deprecated. \"\n\nIn [1]: scripts = response.selector.xpath('//script/text()').extract()\n\nIn [2]: import js2xml, js2xml.jsonlike\n\nIn [3]: js = js2xml.parse(scripts[-1])\n\nIn [4]: js2xml.jsonlike.getall(js)\nOut[4]: \n[{'onVariantSelected': 'selectCallback',\n  'product': {'available': True,\n   'compare_at_price': None,\n   'compare_at_price_max': 0,\n   'compare_at_price_min': 0,\n   'compare_at_price_varies': False,\n   'content': u'&lt;blockquote&gt;Siyah-beyaz kalpli tulumlarimiz 100% polyester olup kap\\u015fonun i\\xe7i ve ribanas\\u0131 lacivertir. Fermuar\\u0131 iki tarafl\\u0131 a\\xe7\\u0131l\\u0131r kapan\\u0131r olup kap\\u015fonun tamam\\u0131n\\u0131 kapsar ve beyaz renklidir. Tulumlar\\u0131n her iki taraf\\u0131ndaki cepler\\xa0 beyaz fermuarl\\u0131 ve elcikler siyaht\\u0131r. Ayr\\u0131ca kar\\u0131n bolgesinde cepler vard\\u0131r Tulumlardaki logolar beyazd\\u0131r. Kad\\u0131nlar ve erkekler i\\xe7in tasarlanm\\u0131\\u015ft\\u0131r.&lt;/blockquote&gt;',\n   'created_at': '2013-11-29T13:37:11+02:00',\n   'description': u'&lt;blockquote&gt;Siyah-beyaz kalpli tulumlarimiz 100% polyester olup kap\\u015fonun i\\xe7i ve ribanas\\u0131 lacivertir. Fermuar\\u0131 iki tarafl\\u0131 a\\xe7\\u0131l\\u0131r kapan\\u0131r olup kap\\u015fonun tamam\\u0131n\\u0131 kapsar ve beyaz renklidir. Tulumlar\\u0131n her iki taraf\\u0131ndaki cepler\\xa0 beyaz fermuarl\\u0131 ve elcikler siyaht\\u0131r. Ayr\\u0131ca kar\\u0131n bolgesinde cepler vard\\u0131r Tulumlardaki logolar beyazd\\u0131r. Kad\\u0131nlar ve erkekler i\\xe7in tasarlanm\\u0131\\u015ft\\u0131r.&lt;/blockquote&gt;',\n   'featured_image': '//cdn.shopify.com/s/files/1/0305/9953/products/11._Zwarte_hartjes_vk_girls.jpg?v=1389259261',\n   'handle': '2loom-design-siyah-beyaz-kalpli',\n   'id': 185310341,\n   'images': ['//cdn.shopify.com/s/files/1/0305/9953/products/11._Zwarte_hartjes_vk_girls.jpg?v=1389259261',\n    '//cdn.shopify.com/s/files/1/0305/9953/products/6._Zwarte_hartjes_ak_girls.jpg?v=1389259259',\n    '//cdn.shopify.com/s/files/1/0305/9953/products/11._Zwarte_hartjes_vk_boys.jpg?v=1389259264',\n    '//cdn.shopify.com/s/files/1/0305/9953/products/6._Zwartje_hartjes_ak_boys.jpg?v=1389259264'],\n   'options': ['Size'],\n   'price': 15900,\n   'price_max': 15900,\n   'price_min': 15900,\n   'price_varies': False,\n   'published_at': '2013-11-29T13:34:20+02:00',\n   'tags': [u'2\\xb7Loom',\n    'Beyaz',\n    'Design',\n    'Ekrek',\n    u'Kad\\u0131n',\n    'Kalpli',\n    'Lacivert'],\n   'title': '10. Design | Siyah &amp; beyaz kalpli',\n   'type': '2 Loom Limiteds',\n   'variants': [{'available': True,\n     'barcode': None,\n     'compare_at_price': None,\n     'id': 424584985,\n     'inventory_management': 'shopify',\n     'inventory_policy': 'deny',\n     'inventory_quantity': 3,\n     'option1': 'XS (34-36: 1.60m-1.70m)',\n     'option2': None,\n     'option3': None,\n     'options': ['XS (34-36: 1.60m-1.70m)'],\n     'price': 15900,\n     'requires_shipping': True,\n     'sku': 'T01-BLWH-1-XS',\n     'taxable': True,\n     'title': 'XS (34-36: 1.60m-1.70m)',\n     'weight': 0},\n    {'available': True,\n     'barcode': None,\n     'compare_at_price': None,\n     'id': 424584989,\n     'inventory_management': 'shopify',\n     'inventory_policy': 'deny',\n     'inventory_quantity': 3,\n     'option1': 'S (36-38: 1.65m-1.75m)',\n     'option2': None,\n     'option3': None,\n     'options': ['S (36-38: 1.65m-1.75m)'],\n     'price': 15900,\n     'requires_shipping': True,\n     'sku': 'T01-BLWH-1-S',\n     'taxable': True,\n     'title': 'S (36-38: 1.65m-1.75m)',\n     'weight': 0},\n    {'available': True,\n     'barcode': None,\n     'compare_at_price': None,\n     'id': 424584997,\n     'inventory_management': 'shopify',\n     'inventory_policy': 'deny',\n     'inventory_quantity': 7,\n     'option1': 'M (38-40: 1.70m-1.80m)',\n     'option2': None,\n     'option3': None,\n     'options': ['M (38-40: 1.70m-1.80m)'],\n     'price': 15900,\n     'requires_shipping': True,\n     'sku': 'T01-BLWH-1-M',\n     'taxable': True,\n     'title': 'M (38-40: 1.70m-1.80m)',\n     'weight': 0},\n    {'available': True,\n     'barcode': None,\n     'compare_at_price': None,\n     'id': 424585001,\n     'inventory_management': 'shopify',\n     'inventory_policy': 'deny',\n     'inventory_quantity': 7,\n     'option1': 'L (40-42: 1.75m-1.85m)',\n     'option2': None,\n     'option3': None,\n     'options': ['L (40-42: 1.75m-1.85m)'],\n     'price': 15900,\n     'requires_shipping': True,\n     'sku': 'T01-BLWH-1-L',\n     'taxable': True,\n     'title': 'L (40-42: 1.75m-1.85m)',\n     'weight': 0}],\n   'vendor': u'2\\xb7Loom'}}]\n\nIn [5]: \n</code></pre>\n", "abstract": "An alternative to the regex approach is to use a Javascript parser, convert the output of that parser to an XML document, and parse it with XPath. That's what implemented in js2xml, which uses slimit and lxml\n(disclaimer: I wrote js2xml; warning: not stable) In your case, check this sample scrapy shell session, using js2xml.jsonlike.getall():"}]}, {"link": "https://stackoverflow.com/questions/46933679/scraping-text-in-h3-and-div-tags-using-beautifulsoup-python", "question": {"id": "46933679", "title": "Scraping text in h3 and div tags using beautifulSoup, Python", "content": "<p>I have no experience with python, BeautifulSoup, Selenium etc. but I'm eager to scrape data from a website and store as a csv file.\nA single sample of data I need is coded as follows (a single row of data).</p>\n<pre><code class=\"python\">&lt;div class=\"box effect\"&gt;\n&lt;div class=\"row\"&gt;\n&lt;div class=\"col-lg-10\"&gt;\n    &lt;h3&gt;HEADING&lt;/h3&gt;\n        &lt;div&gt;&lt;i class=\"fa user\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;NAME&lt;/div&gt;\n        &lt;div&gt;&lt;i class=\"fa phone\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;MOBILE&lt;/div&gt;\n        &lt;div&gt;&lt;i class=\"fa mobile-phone fa-2\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;NUMBER&lt;/div&gt;\n        &lt;div&gt;&lt;i class=\"fa address\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;XYZ_ADDRESS&lt;/div&gt;\n    &lt;div class=\"space\"&gt;&amp;nbsp;&lt;/div&gt;\n\n&lt;div style=\"padding:10px;padding-left:0px;\"&gt;&lt;a class=\"btn btn-primary btn-sm\" href=\"www.link_to_another_page.com\"&gt;&lt;i class=\"fa search-plus\"&gt;&lt;/i&gt; &amp;nbsp;more info&lt;/a&gt;&lt;/div&gt;\n\n&lt;/div&gt;\n&lt;div class=\"col-lg-2\"&gt;\n\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n</code></pre>\n<p>The output I need is\n <code>Heading,NAME,MOBILE,NUMBER,XYZ_ADDRESS</code></p>\n<p>I found those data don't have a id or class yet being in website as general text.\nI'm trying BeautifulSoup and Python Selenium separately for that, where I got stuck to extract in both the methods as no tutorials I saw, guided me to extract text from these  and  tags</p>\n<p>My code using BeautifulSoup</p>\n<pre><code class=\"python\">import urllib2\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\n\nMAX = 2\n\n'''with open(\"lg.csv\", \"a\") as f:\n  w=csv.writer(f)'''\n##for i in range(1,MAX+1)\nurl=\"http://www.example_site.com\"\n\npage=requests.get(url)\nsoup = BeautifulSoup(page.content,\"html.parser\")\n\nfor h in soup.find_all('h3'):\n    print(h.get('h3'))\n</code></pre>\n<p>My selenium code</p>\n<pre><code class=\"python\">import csv\nfrom selenium import webdriver\nMAX_PAGE_NUM = 2\ndriver = webdriver.Firefox()\nfor i in range(1, MAX_PAGE_NUM+1):\n  url = \"http://www.example_site.com\"\n  driver.get(url)\n  name = driver.find_elements_by_xpath('//div[@class = \"col-lg-10\"]/h3')\n  #contact = driver.find_elements_by_xpath('//span[@class=\"item-price\"]')\n#  phone = \n#  mobile = \n#  address =\n#  print(len(buyers))\n#  num_page_items = len(buyers)\n#  with open('res.csv','a') as f:\n#    for i in range(num_page_items):\n#      f.write(buyers[i].text + \",\" + prices[i].text + \"\\n\")\n  print (name)          \ndriver.close()\n</code></pre>\n", "abstract": "I have no experience with python, BeautifulSoup, Selenium etc. but I'm eager to scrape data from a website and store as a csv file.\nA single sample of data I need is coded as follows (a single row of data). The output I need is\n Heading,NAME,MOBILE,NUMBER,XYZ_ADDRESS I found those data don't have a id or class yet being in website as general text.\nI'm trying BeautifulSoup and Python Selenium separately for that, where I got stuck to extract in both the methods as no tutorials I saw, guided me to extract text from these  and  tags My code using BeautifulSoup My selenium code"}, "answers": [{"id": 46933925, "score": 10, "vote": 0, "content": "<p>You can use CSS selectors to find the data you need.\nIn your case <code>div &gt; h3 ~ div</code> will find all <code>div</code> elements that are directly inside a <code>div</code> element and are proceeded by a <code>h3</code> element.</p>\n<pre><code class=\"python\">import bs4\n\npage= \"\"\"\n&lt;div class=\"box effect\"&gt;\n&lt;div class=\"row\"&gt;\n&lt;div class=\"col-lg-10\"&gt;\n    &lt;h3&gt;HEADING&lt;/h3&gt;\n    &lt;div&gt;&lt;i class=\"fa user\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;NAME&lt;/div&gt;\n    &lt;div&gt;&lt;i class=\"fa phone\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;MOBILE&lt;/div&gt;\n    &lt;div&gt;&lt;i class=\"fa mobile-phone fa-2\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;NUMBER&lt;/div&gt;\n    &lt;div&gt;&lt;i class=\"fa address\"&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;XYZ_ADDRESS&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n\nsoup = bs4.BeautifulSoup(page, 'lxml')\n\n# find all div elements that are inside a div element\n# and are proceeded by an h3 element\nselector = 'div &gt; h3 ~ div'\n\n# find elements that contain the data we want\nfound = soup.select(selector)\n\n# Extract data from the found elements\ndata = [x.text.split(';')[-1].strip() for x in found]\n\nfor x in data:\n    print(x)\n</code></pre>\n<p>Edit: To scrape the text in heading..</p>\n<pre><code class=\"python\">heading = soup.find('h3') \nheading_data = heading.text\nprint(heading_data)\n</code></pre>\n<p>Edit: Or you can get the heading and other div elements at once by using a selector like this: <code>div.col-lg-10 &gt; *</code>. This finds all elements inside a <code>div</code> element that belongs to <code>col-lg-10</code> class.</p>\n<pre><code class=\"python\">soup = bs4.BeautifulSoup(page, 'lxml')\n\n# find all elements inside a div element of class col-lg-10\nselector = 'div.col-lg-10 &gt; *'\n\n# find elements that contain the data we want\nfound = soup.select(selector)\n\n# Extract data from the found elements\ndata = [x.text.split(';')[-1].strip() for x in found]\n\nfor x in data:\n    print(x)\n</code></pre>\n", "abstract": "You can use CSS selectors to find the data you need.\nIn your case div > h3 ~ div will find all div elements that are directly inside a div element and are proceeded by a h3 element. Edit: To scrape the text in heading.. Edit: Or you can get the heading and other div elements at once by using a selector like this: div.col-lg-10 > *. This finds all elements inside a div element that belongs to col-lg-10 class."}, {"id": 65153523, "score": 2, "vote": 0, "content": "<p>So it seemed quite nice:</p>\n<pre><code class=\"python\">    #  -*- coding: utf-8 -*-\n    # by Faguiro #\n    # run using Python 3.8.6  on Linux#\n    import requests\n    from bs4 import BeautifulSoup\n\n    # insert your site here\n    url= input(\"Enter the url--&gt;\")\n\n    #use requests\n    r = requests.get(url)\n    content = r.content\n\n    #soup!\n    soup = BeautifulSoup(content, \"html.parser\")\n\n    #find all tag in the soup.\n    heading = soup.find_all(\"h3\")\n\n    #print(heading) &lt;--- result...\n\n    #...ptonic organization!\n    n=len(heading)\n    for x in range(n): \n        print(str.strip(heading[x].text))\n</code></pre>\n<p>Dependencies:\nOn terminal (linux):</p>\n<p>sudo apt-get install python3-bs4</p>\n", "abstract": "So it seemed quite nice: Dependencies:\nOn terminal (linux): sudo apt-get install python3-bs4"}, {"id": 46933936, "score": 0, "vote": 0, "content": "<p>Try this:</p>\n<pre><code class=\"python\">import urllib2\nfrom bs4 import BeautifulSoup\nimport requests\nimport csv\n\nMAX = 2\n\n'''with open(\"lg.csv\", \"a\") as f:\n  w=csv.writer(f)'''\n##for i in range(1,MAX+1)\nurl=\"http://www.example_site.com\"\n\npage=requests.get(url)\nsoup = BeautifulSoup(page,\"html.parser\")\n\nprint(soup.text)\n</code></pre>\n", "abstract": "Try this:"}]}]